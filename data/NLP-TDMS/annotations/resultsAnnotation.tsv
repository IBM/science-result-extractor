1705.05952.pdf	part-of-speech_tagging#UD#Avg Accuracy#95.55
D18-1205.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#41.54$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#18.18$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#36.47$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
C18-1121.pdf	summarization#Gigaword#ROUGE-1#35.33$summarization#Gigaword#ROUGE-2#17.27$summarization#Gigaword#ROUGE-L#33.19$summarization#DUC 2004 Task 1#ROUGE-1#29.33$summarization#DUC 2004 Task 1#ROUGE-2#10.24$summarization#DUC 2004 Task 1#ROUGE-L#25.24
P18-1063.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#39.66$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#15.85$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#37.34$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
5635-grammar-as-a-foreign-language.pdf	constituency_parsing#Penn Treebank#F1#92.1
wang13a.pdf	sentiment_analysis#SUBJ#Accuracy#93.60
1804.00079.pdf	natural_language_inference#MultiNLI#Matched Accuracy#71.4$natural_language_inference#MultiNLI#Mismatched Accuracy#71.3$semantic_textual_similarity#SICK-R#Pearson Correlation#0.888$semantic_textual_similarity#SICK-E#Accuracy#87.8$semantic_textual_similarity#Quora Question Pairs#Accuracy#87.01
1805.05286.pdf	amr_parsing#LDC2015E86#Smatch#73.7$amr_parsing#LDC2016E25#Smatch#74.4
P15-1089.pdf	lexical_normalization#LexNorm#Accuracy#86.08
P17-1111.pdf	word_segmentation#Chinese Treebank 7#F1#96.2
.pdf	word_segmentation#VLSP 2013 word segmentation shared task#F1#96.90$relation_prediction#FB15K-237#H@10#40.67$relation_prediction#FB15K-237#H@1#16.35$relation_prediction#FB15K-237#MRR#24.44
1506.03340.pdf	question_answering#CNN / Daily Mail#Accuracy on CNN#63.8$question_answering#CNN / Daily Mail#Accuracy on Daily Mail#68.0
1806.00187.pdf	machine_translation#WMT 2014 EN-DE#BLEU#29.3$machine_translation#WMT 2014 EN-FR#BLEU#43.2
P16-2034.pdf	relationship_extraction#SemEval-2010 Task 8#F1#84.0
1804.06512.pdf	dialogue_state_tracking#Second dialogue state tracking challenge#Request Accuracy#-$dialogue_state_tracking#Second dialogue state tracking challenge#Joint goal Accuracy#72
1612.08083.pdf	language_modeling#WikiText-103#Validation perplexity#-$language_modeling#WikiText-103#Test perplexity#37.2$language_modeling#WikiText-103#Number of params#$language_modeling#1B Words / Google Billion Word benchmark#Test perplexity#31.9$language_modeling#1B Words / Google Billion Word benchmark#Number of params#?
1707.07045.pdf	coreference_resolution#CoNLL 2012#Avg F1#67.2
1803.10049.pdf	language_modeling#WikiText-103#Validation perplexity#29.0$language_modeling#WikiText-103#Test perplexity#29.2$language_modeling#WikiText-103#Number of params#
D15-1062.pdf	relationship_extraction#SemEval-2010 Task 8#F1#85.6
1806.09029.pdf	sql_parsing#ATIS#Question Split#51$sql_parsing#ATIS#Query Split#32$sql_parsing#Advising#Question Split#80$sql_parsing#Advising#Query Split#0$sql_parsing#GeoQuery#Question Split#71$sql_parsing#GeoQuery#Query Split#20$sql_parsing#Scholar#Question Split#59$sql_parsing#Scholar#Query Split#5$sql_parsing#Smaller Datasets#Question Split#100$sql_parsing#Smaller Datasets#Query Split#4
D13-1007.pdf	lexical_normalization#LexNorm#Accuracy#82.06
Q17-1029.pdf	constituency_parsing#Penn Treebank#F1#94.2
P18-2045.pdf	question_answering#Story Cloze Test#Accuracy#78.7
1804.09530.pdf	sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on DVD#78.14$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Books#74.86$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Electronics#81.45$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Kitchen#82.14$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Average#79.15
1807.03955.pdf	dependency_parsing#Penn Treebank#POS#97.97$dependency_parsing#Penn Treebank#UAS#94.51$dependency_parsing#Penn Treebank#LAS#92.87
1504.05070.pdf	sentiment_analysis#SUBJ#Accuracy#95.50
N18-1064.pdf	summarization#Gigaword#ROUGE-1#37.04$summarization#Gigaword#ROUGE-2#16.66$summarization#Gigaword#ROUGE-L#34.93
1709.04348.pdf	semantic_textual_similarity#Quora Question Pairs#Accuracy#89.06
N16-1030.pdf	named_entity_recognition#VLSP 2016 NER shared task#F1#87.71$part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy#95.31
1802.00923.pdf	multimodal_sentiment_analysis#MOSI#Accuracy#77.1%
pdf_id_HkAClQgA-.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#39.87$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#15.82$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#36.90$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
P15-2141.pdf	amr_parsing#LDC2014T12#F1 on Newswire#70$amr_parsing#LDC2014T12#F1 on Full#66
1702.08400.pdf	sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on DVD#76.17$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Books#72.97$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Electronics#80.47$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Kitchen#83.97$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Average#78.39
1611.01603.pdf	question_answering#SQuAD#EM#73.744$question_answering#SQuAD#F1#81.525$question_answering#CNN / Daily Mail#Accuracy on CNN#76.9$question_answering#CNN / Daily Mail#Accuracy on Daily Mail#79.6$question_answering#NarrativeQA#BLEU-1#33.45$question_answering#NarrativeQA#BLEU-4#15.69$question_answering#NarrativeQA#METEOR#15.68$question_answering#NarrativeQA#ROUGE-L#36.74$question_answering#Quasar#EM (Quasar-T)#25.9$question_answering#Quasar#F1 (Quasar-T)#28.5
1705.09980.pdf	amr_parsing#LDC2016E25#Smatch#71.0
1601.03651.pdf	relationship_extraction#SemEval-2010 Task 8#F1#86.1
1804.07461.pdf	natural_language_inference#MultiNLI#Matched Accuracy#72.2$natural_language_inference#MultiNLI#Mismatched Accuracy#72.1
1801.00102.pdf	natural_language_inference#SciTail#Accuracy#83.3
S18-1146.pdf	taxonomy_learning#SemEval 2018#MAP#2.68$taxonomy_learning#SemEval 2018#MRR#6.01$taxonomy_learning#SemEval 2018#P@5#2.69
50.pdf	temporal_information_extraction#TimeBank#F1#0.507
N15-1067.pdf	dependency_parsing#Penn Treebank#UAS#66.2
S16-1186.pdf	amr_parsing#LDC2015E86#Smatch#67.0
1805.08237.pdf	part-of-speech_tagging#Penn Treebank#Accuracy#97.96
1611.04230.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#﻿39.6$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#16.2$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#35.3$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
forum_id_BJEX-H1Pf.pdf	language_modeling#WikiText-103#Validation perplexity#-$language_modeling#WikiText-103#Test perplexity#45.2$language_modeling#WikiText-103#Number of params#
S18-1150.pdf	taxonomy_learning#SemEval 2018#MAP#13.77$taxonomy_learning#SemEval 2018#MRR#40.76$taxonomy_learning#SemEval 2018#P@5#12.76
S17-2126.pdf	sentiment_analysis#SemEval-2017 Task 4 subtask A Tweet Sentiment Classification dataset#F1-score#0.677
N18-1150.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#41.69$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#19.47$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.92$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
N18-1193.pdf	multimodal_emotion_recognition#IEMOCAP#Weighted Accuracy (WAA)#77.62%
W17-4422.pdf	named_entity_recognition#Long-tail emerging entities#F1#40.78$named_entity_recognition#Long-tail emerging entities#F1 (surface form)#39.33
1801.08290.pdf	question_answering#NewsQA#F1#63.7$question_answering#NewsQA#EM#48.4$question_answering#SearchQA#Unigram Acc#46.8$question_answering#SearchQA#N-gram F1#56.6$question_answering#SearchQA#EM#-$question_answering#SearchQA#F1#-
1711.03953.pdf	language_modeling#Penn Treebank#Validation perplexity#48.33$language_modeling#Penn Treebank#Test perplexity#47.69$language_modeling#Penn Treebank#Number of params#22M$language_modeling#WikiText-2#Validation perplexity#42.41$language_modeling#WikiText-2#Test perplexity#40.68$language_modeling#WikiText-2#Number of params#35M
1806.06228.pdf	multimodal_emotion_recognition#IEMOCAP#Accuracy#76.5%
E17-2047.pdf	summarization#Gigaword#ROUGE-1#36.30$summarization#Gigaword#ROUGE-2#17.31$summarization#Gigaword#ROUGE-L#33.88$summarization#DUC 2004 Task 1#ROUGE-1#32.28$summarization#DUC 2004 Task 1#ROUGE-2#10.54$summarization#DUC 2004 Task 1#ROUGE-L#27.8
1807.01270.pdf	grammatical_error_correction#CoNLL-2014 Shared Task Unrestricted#F0.5#61.34$grammatical_error_correction#CoNLL-2014 10 Annotations Unrestricted#F0.5#76.88$grammatical_error_correction#JFLEG Unrestricted#GLEU#62.37
P16-1072.pdf	relationship_extraction#SemEval-2010 Task 8#F1#86.3
1809.10853.pdf	language_modeling#WikiText-103#Validation perplexity#19.8$language_modeling#WikiText-103#Test perplexity#20.5$language_modeling#WikiText-103#Number of params#247M$language_modeling#1B Words / Google Billion Word benchmark#Test perplexity#23.7$language_modeling#1B Words / Google Billion Word benchmark#Number of params#0.8B
P16-1101.pdf	named_entity_recognition#VLSP 2016 NER shared task#F1#88.28$part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy#95.40
1511.00830.pdf	sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on DVD#76.57$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Books#73.40$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Electronics#80.53$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Kitchen#82.93$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Average#78.36
P11-1055.pdf	relationship_extraction#New York Times Corpus#P@10%#60.9+$relationship_extraction#New York Times Corpus#P@30%#-
COLI_a_00164.pdf	word_sense_disambiguation#Senseval 2#F1#64.2$word_sense_disambiguation#Senseval 3#F1#54.8$word_sense_disambiguation#SemEval 2007#F1#40$word_sense_disambiguation#SemEval 2013#F1#64.5$word_sense_disambiguation#SemEval 2015#F1#64.5
D15-1279.pdf	text_classification#TREC#Error#4
1705.00108.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#91.93
yu_gormley_dredze.nipsw.2014.pdf	relationship_extraction#SemEval-2010 Task 8#F1#83.0
1808.10792.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#41.22$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#18.68$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#38.34$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
1604.05529.pdf	part-of-speech_tagging#Penn Treebank#Accuracy#97.22$part-of-speech_tagging#UD#Avg Accuracy#96.40
1711.04903.pdf	part-of-speech_tagging#Penn Treebank#Accuracy#97.59$part-of-speech_tagging#UD#Avg Accuracy#96.73
1508.01991.pdf	named_entity_recognition#VLSP 2016 NER shared task#F1#86.48$part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy#95.06
16101.pdf	entity_linking#AIDA CoNLL-YAGO Dataset#Micro-Precision#84.4
1805.06280.pdf	dialogue_act_classification#Switchboard corpus#Accuracy#77.34
C16-1311.pdf	sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)#75.63$sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)#68.13
1711.05568.pdf	dialogue_act_classification#Switchboard corpus#Accuracy#81.3$dialogue_act_classification#ICSI Meeting Recorder Dialog Act (MRDA) corpus#Accuracy#91.7
1709.07432.pdf	language_modeling#Penn Treebank#Validation perplexity#51.6$language_modeling#Penn Treebank#Test perplexity#51.1$language_modeling#Penn Treebank#Number of params#24M$language_modeling#WikiText-2#Validation perplexity#46.4$language_modeling#WikiText-2#Test perplexity#44.3$language_modeling#WikiText-2#Number of params#33M$language_modeling#Hutter Prize#Bit per Character (BPC)#1.08$language_modeling#Hutter Prize#Number of params#46M$language_modeling#Text8#Bit per Character (BPC)#1.19$language_modeling#Text8#Number of params#45M
8546.pdf	relation_prediction#FB15K-237#H@10#41.32$relation_prediction#FB15K-237#H@1#5.79$relation_prediction#FB15K-237#MRR#17.66
P16-1001.pdf	amr_parsing#LDC2014T12#F1 on Newswire#70$amr_parsing#LDC2014T12#F1 on Full#--
1710.10723.pdf	question_answering#SQuAD#EM#72.139$question_answering#SQuAD#F1#81.048
P15-2047.pdf	relationship_extraction#SemEval-2010 Task 8#F1#83.6
D13-1204.pdf	dependency_parsing#Penn Treebank#UAS#64.4
D16-1036.pdf	retrieval-based_chatbot#Ubuntu Corpus#R_2@1#90.8$retrieval-based_chatbot#Ubuntu Corpus#R_10@1#66.2
N18-2015.pdf	question_answering#Story Cloze Test#Accuracy#76.5
D18-1264.pdf	amr_parsing#LDC2014T12#F1 on Newswire#73.3$amr_parsing#LDC2014T12#F1 on Full#68.4
1606.01549.pdf	question_answering#Quasar#EM (Quasar-T)#26.4$question_answering#Quasar#F1 (Quasar-T)#26.4
1712.03556.pdf	question_answering#SQuAD#EM#79.608$question_answering#SQuAD#F1#86.496
1708.02182.pdf	language_modeling#Penn Treebank#Validation perplexity#53.9$language_modeling#Penn Treebank#Test perplexity#52.8$language_modeling#Penn Treebank#Number of params#24M$language_modeling#WikiText-2#Validation perplexity#53.8$language_modeling#WikiText-2#Test perplexity#52.0$language_modeling#WikiText-2#Number of params#33M
P18-2038.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#91.38
RIVF.2016.7800279.pdf	word_segmentation#VLSP 2013 word segmentation shared task#F1#97.87
P18-1064.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#39.81$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#17.64$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#36.54$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#18.54$summarization#Gigaword#ROUGE-1#35.98$summarization#Gigaword#ROUGE-2#17.76$summarization#Gigaword#ROUGE-L#33.63
1808.08644.pdf	relation_prediction#WN18RR#H@10#59.02$relation_prediction#WN18RR#H@1#45.37$relation_prediction#WN18RR#MRR#49.83
1704.00051.pdf	question_answering#SQuAD#EM#70.733$question_answering#SQuAD#F1#79.353
1802.05365.pdf	semantic_role_labeling#OntoNotes#F1#84.6$named_entity_recognition#CoNLL 2003 (English)#F1#92.22$coreference_resolution#CoNLL 2012#Avg F1#70.4$sentiment_analysis#SST-5#Accuracy#54.7
1612.04211.pdf	question_answering#SQuAD#EM#73.765$question_answering#SQuAD#F1#81.257
Q14-1019.pdf	word_sense_disambiguation#Senseval 2#F1#67$word_sense_disambiguation#Senseval 3#F1#63.5$word_sense_disambiguation#SemEval 2007#F1#51.6$word_sense_disambiguation#SemEval 2013#F1#66.4$word_sense_disambiguation#SemEval 2015#F1#70.3
1808.05326.pdf	common_sense#SWAG#Accuracy on Dev#59.1$common_sense#SWAG#Accuracy on Test#59.2
1705.03122.pdf	machine_translation#WMT 2014 EN-DE#BLEU#25.16$machine_translation#WMT 2014 EN-FR#BLEU#40.46
1711.04436.pdf	sql_parsing#WikiSQL#Acc ex#68.0
D17-1129.pdf	amr_parsing#LDC2014T12#F1 on Newswire#--$amr_parsing#LDC2014T12#F1 on Full#68.1
1612.01627.pdf	retrieval-based_chatbot#Ubuntu Corpus#R_2@1#92.3$retrieval-based_chatbot#Ubuntu Corpus#R_10@1#72.3
P18-2028.pdf	summarization#Google Dataset#F1#0.851$summarization#Google Dataset#CR#0.39
D17-1120.pdf	word_sense_disambiguation#Senseval 2#F1#72.0$word_sense_disambiguation#Senseval 3#F1#69.4$word_sense_disambiguation#SemEval 2007#F1#63.7*$word_sense_disambiguation#SemEval 2013#F1#66.4$word_sense_disambiguation#SemEval 2015#F1#72.4
trouillon16.pdf	relation_prediction#WN18RR#H@10#51.00$relation_prediction#WN18RR#H@1#41.00$relation_prediction#WN18RR#MRR#44.00$relation_prediction#FB15K-237#H@10#42.80$relation_prediction#FB15K-237#H@1#15.80$relation_prediction#FB15K-237#MRR#24.70
1609.07959.pdf	language_modeling#Hutter Prize#Bit per Character (BPC)#1.24$language_modeling#Hutter Prize#Number of params#46M$language_modeling#Text8#Bit per Character (BPC)#1.27$language_modeling#Text8#Number of params#45M
K18-2008.pdf	dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS#71.72$dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS#79.26$part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy#95.61
context-dependent-sentiment-analysis-in-user-generated-videos.pdf	multimodal_emotion_recognition#IEMOCAP#Accuracy#74.10%$multimodal_sentiment_analysis#MOSI#Accuracy#80.3%
P17-1043.pdf	amr_parsing#LDC2015E86#Smatch#70.7
f2205c56378e715d8d12c521d045c0756a76.pdf	named_entity_recognition#Ontonotes v5 (English)#F1#84.04
S18-1151.pdf	taxonomy_learning#SemEval 2018#MAP#8.13$taxonomy_learning#SemEval 2018#MRR#20.56$taxonomy_learning#SemEval 2018#P@5#8.32
D17-1168.pdf	question_answering#Story Cloze Test#Accuracy#77.6
P16-1231.pdf	dependency_parsing#Penn Treebank#POS#97.44$dependency_parsing#Penn Treebank#UAS#94.61$dependency_parsing#Penn Treebank#LAS#92.79
1707.01476.pdf	relation_prediction#WN18RR#H@10#52.00$relation_prediction#WN18RR#H@1#40.00$relation_prediction#WN18RR#MRR#43.00$relation_prediction#FB15K-237#H@10#50.10$relation_prediction#FB15K-237#H@1#23.70$relation_prediction#FB15K-237#MRR#32.50
D16-1058.pdf	sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)#77.20$sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)#68.70
english.pdf	relationship_extraction#SemEval-2010 Task 8#F1#85.2
7181-attention-is-all-you-need.pdf	machine_translation#The IWSLT 2015 Evaluation Campaign#BLEU#28.9
P08-1076.pdf	chunking#Penn Treebank#F1#95.15
D18-1208.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#41.5$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#18.7$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.6$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
1801.06146.pdf	text_classification#AG News#Error#5.01$text_classification#DBpedia#Error#0.80$text_classification#TREC#Error#3.6$sentiment_analysis#IMDb#Accuracy#95.4$sentiment_analysis#Yelp#Error#29.98
citation.cfm_id_1620923.pdf	constituency_parsing#Penn Treebank#F1#92.4
C16-1329.pdf	text_classification#TREC#Error#3.9$sentiment_analysis#SST-2#Accuracy#89.5
1611.01578.pdf	language_modeling#Penn Treebank#Bit per Character (BPC)#1.214$language_modeling#Penn Treebank#Number of params#16.3M
blocksparsepaper.pdf	sentiment_analysis#IMDb#Accuracy#94.99$sentiment_analysis#SST-2#Accuracy#93.2
language_understanding_paper.pdf	question_answering#RACE#Accuracy on RACE-m#62.9$question_answering#RACE#Accuracy on RACE-h#57.4$question_answering#RACE#Accuracy on RACE#59.0$question_answering#Story Cloze Test#Accuracy#86.5$natural_language_inference#MultiNLI#Matched Accuracy#82.1$natural_language_inference#MultiNLI#Mismatched Accuracy#81.4$natural_language_inference#SciTail#Accuracy#88.3
10838.pdf	lexical_normalization#LexNorm#Accuracy#87.58*
E17-1038.pdf	sentiment_analysis#SST-2#Accuracy#89.7
1705.08639.pdf	language_modeling#Hutter Prize#Bit per Character (BPC)#1.245$language_modeling#Hutter Prize#Number of params#47M$language_modeling#Penn Treebank#Bit per Character (BPC)#1.190$language_modeling#Penn Treebank#Number of params#27M
1705.02364.pdf	semantic_textual_similarity#SICK-R#Pearson Correlation#0.884$semantic_textual_similarity#SICK-E#Accuracy#86.3
1711.00066.pdf	language_modeling#Penn Treebank#Validation perplexity#58.9$language_modeling#Penn Treebank#Test perplexity#56.8$language_modeling#Penn Treebank#Number of params#24M$language_modeling#WikiText-2#Validation perplexity#66.8$language_modeling#WikiText-2#Test perplexity#64.1$language_modeling#WikiText-2#Number of params#34M
P17-1127.pdf	summarization#Google Dataset#F1#0.8$summarization#Google Dataset#CR#0.43
1805.06939.pdf	common_sense#Event2Mind#Average Cross-Entropy on Dev#4.25$common_sense#Event2Mind#Average Cross-Entropy on Test#4.22
P17-1044.pdf	semantic_role_labeling#OntoNotes#F1#81.7
D17-1079.pdf	word_segmentation#Chinese Treebank 6#F1#96.2
1609.09106.pdf	language_modeling#Penn Treebank#Bit per Character (BPC)#1.219$language_modeling#Penn Treebank#Number of params#14.4M
P18-1015.pdf	summarization#Gigaword#ROUGE-1#37.04$summarization#Gigaword#ROUGE-2#19.03$summarization#Gigaword#ROUGE-L#34.46
N18-1158.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#40.0$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#18.2$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#36.6$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
16137.pdf	grammatical_error_correction#CoNLL-2014 Shared Task Restricted#F0.5#54.79$grammatical_error_correction#CoNLL-2014 10 Annotations Restricted#F0.5#70.14 (measured by Ge et al., 2018)
1901.02860.pdf	language_modeling#Penn Treebank#Validation perplexity#56.72$language_modeling#Penn Treebank#Test perplexity#54.52$language_modeling#Penn Treebank#Number of params#24M$language_modeling#WikiText-103#Validation perplexity#17.7$language_modeling#WikiText-103#Test perplexity#18.3$language_modeling#WikiText-103#Number of params#257M$language_modeling#1B Words / Google Billion Word benchmark#Test perplexity#21.8$language_modeling#1B Words / Google Billion Word benchmark#Number of params#0.8B$language_modeling#Hutter Prize#Bit per Character (BPC)#0.99$language_modeling#Hutter Prize#Number of params#277M$language_modeling#Text8#Bit per Character (BPC)#1.08$language_modeling#Text8#Number of params#277M
1805.09655.pdf	dialogue_state_tracking#Second dialogue state tracking challenge#Request Accuracy#97.5$dialogue_state_tracking#Second dialogue state tracking challenge#Joint goal Accuracy#74.5$dialogue_state_tracking#Wizard-of-Oz#Request Accuracy#97.1$dialogue_state_tracking#Wizard-of-Oz#Joint goal Accuracy#88.1
1810.04805.pdf	question_answering#SQuAD#EM#87.433$question_answering#SQuAD#F1#93.160$common_sense#SWAG#Accuracy on Dev#86.6$common_sense#SWAG#Accuracy on Test#86.3$named_entity_recognition#CoNLL 2003 (English)#F1#92.8
1703.04617.pdf	question_answering#SQuAD#EM#73.010$question_answering#SQuAD#F1#81.517
1808.08762.pdf	natural_language_inference#SciTail#Accuracy#86.0
D16-1065.pdf	amr_parsing#LDC2014T12#F1 on Newswire#71$amr_parsing#LDC2014T12#F1 on Full#66
1803.11175.pdf	text_classification#TREC#Error#1.93$sentiment_analysis#SUBJ#Accuracy#93.90
htyo14.pdf	dialogue_state_tracking#Second dialogue state tracking challenge#Request Accuracy#95.7$dialogue_state_tracking#Second dialogue state tracking challenge#Joint goal Accuracy#69$dialogue_state_tracking#Wizard-of-Oz#Request Accuracy#87.1$dialogue_state_tracking#Wizard-of-Oz#Joint goal Accuracy#70.8
N18-5012.pdf	dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS#70.23$dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS#76.93$named_entity_recognition#VLSP 2016 NER shared task#F1#88.55
P17-1101.pdf	summarization#Gigaword#ROUGE-1#36.15$summarization#Gigaword#ROUGE-2#17.54$summarization#Gigaword#ROUGE-L#33.63$summarization#DUC 2004 Task 1#ROUGE-1#29.21$summarization#DUC 2004 Task 1#ROUGE-2#9.56$summarization#DUC 2004 Task 1#ROUGE-L#25.51
P18-2027.pdf	summarization#Gigaword#ROUGE-1#36.3$summarization#Gigaword#ROUGE-2#18.0$summarization#Gigaword#ROUGE-L#33.8
C16-1146.pdf	sentiment_analysis#Sentihood#Aspect (F1)#69.3$sentiment_analysis#Sentihood#Sentiment (acc)#81.9
TsaiRo16b.pdf	entity_linking#AIDA CoNLL-YAGO Dataset#Micro-Precision#83.6
1706.03762.pdf	machine_translation#WMT 2014 EN-DE#BLEU#28.4$machine_translation#WMT 2014 EN-FR#BLEU#41.0$constituency_parsing#Penn Treebank#F1#92.7
P16-1039.pdf	word_segmentation#PKU#F1#95.7$word_segmentation#MSR#F1#96.4
1711.00106.pdf	question_answering#SQuAD#EM#78.852$question_answering#SQuAD#F1#85.996
ke18a.pdf	question_answering#SearchQA#Unigram Acc#46.8$question_answering#SearchQA#N-gram F1#53.4$question_answering#SearchQA#EM#-$question_answering#SearchQA#F1#-
Y06-1028.pdf	word_segmentation#VLSP 2013 word segmentation shared task#F1#97.06
C16-1007.pdf	temporal_information_extraction#TimeBank#F1#0.511
1702.02098.pdf	named_entity_recognition#Ontonotes v5 (English)#F1#86.99
1708.00107.pdf	text_classification#TREC#Error#4.2$sentiment_analysis#IMDb#Accuracy#91.8$sentiment_analysis#SST-5#Accuracy#53.7$sentiment_analysis#SST-2#Accuracy#90.3
P04-1061.pdf	dependency_parsing#Penn Treebank#UAS#35.9
1705.02798.pdf	question_answering#SQuAD#EM#82.283$question_answering#SQuAD#F1#88.533
1704.04565.pdf	semantic_textual_similarity#Quora Question Pairs#Accuracy#88.40
1701.06538.pdf	machine_translation#WMT 2014 EN-DE#BLEU#26.03$machine_translation#WMT 2014 EN-FR#BLEU#40.56
D15-1141.pdf	word_segmentation#Chinese Treebank 6#F1#96.0
S18-1012.pdf	timex_normalisation#PNT#F1#0.70
1804.09849.pdf	machine_translation#WMT 2014 EN-DE#BLEU#28.5*$machine_translation#WMT 2014 EN-FR#BLEU#41.0*
D18-1274.pdf	grammatical_error_correction#CoNLL-2014 Shared Task Restricted#F0.5#56.52
D17-1047.pdf	sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)#80.23$sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)#74.49
D18-1088.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#41.05$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#18.77$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.54$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
P16-1123.pdf	relationship_extraction#SemEval-2010 Task 8#F1#88.0
E17-1007.pdf	taxonomy_learning#SemEval 2018#MAP#1.36$taxonomy_learning#SemEval 2018#MRR#3.18$taxonomy_learning#SemEval 2018#P@5#1.30
D15-1044.pdf	summarization#Gigaword#ROUGE-1#29.76$summarization#Gigaword#ROUGE-2#11.88$summarization#Gigaword#ROUGE-L#26.96$summarization#DUC 2004 Task 1#ROUGE-1#28.18$summarization#DUC 2004 Task 1#ROUGE-2#8.49$summarization#DUC 2004 Task 1#ROUGE-L#23.81
U17-1013.pdf	part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy#95.88
1806.03489.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#91.73$named_entity_recognition#Ontonotes v5 (English)#F1#87.95
D15-1176.pdf	part-of-speech_tagging#Penn Treebank#Accuracy#97.78
1412.6575.pdf	relation_prediction#WN18RR#H@10#49.00$relation_prediction#WN18RR#H@1#40.00$relation_prediction#WN18RR#MRR#43.00$relation_prediction#FB15K-237#H@10#41.90$relation_prediction#FB15K-237#H@1#15.50$relation_prediction#FB15K-237#MRR#24.10
1710.10504.pdf	question_answering#SQuAD#EM#78.433$question_answering#SQuAD#F1#85.517
E17-1051.pdf	amr_parsing#LDC2015E86#Smatch#64.0
1803.08240.pdf	language_modeling#Hutter Prize#Bit per Character (BPC)#1.232$language_modeling#Hutter Prize#Number of params#47M$language_modeling#Penn Treebank#Bit per Character (BPC)#1.175$language_modeling#Penn Treebank#Number of params#13.8M
N18-2102.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#40.43$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#18.00$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.10$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#20.02
64f0dab74295e5eb139c160ed79ff262558a.pdf	constituency_parsing#Penn Treebank#F1#92.1
1704.07415.pdf	question_answering#SQuAD#EM#70.639$question_answering#SQuAD#F1#79.456
S17-2083.pdf	stance_detection#RumourEval#Accuracy#0.784
1609.01704.pdf	language_modeling#Text8#Bit per Character (BPC)#1.29$language_modeling#Text8#Number of params#35M
P16-1085.pdf	word_sense_disambiguation#Senseval 2#F1#72.7$word_sense_disambiguation#Senseval 3#F1#70.6$word_sense_disambiguation#SemEval 2007#F1#63.1$word_sense_disambiguation#SemEval 2013#F1#66.8$word_sense_disambiguation#SemEval 2015#F1#71.8
view.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#93.09
C16-1119.pdf	relationship_extraction#SemEval-2010 Task 8#F1#84.3
1703.04816.pdf	question_answering#SQuAD#EM#70.849$question_answering#SQuAD#F1#78.741$question_answering#NewsQA#F1#56.1$question_answering#NewsQA#EM#43.7
S18-1152.pdf	taxonomy_learning#SemEval 2018#MAP#8.95$taxonomy_learning#SemEval 2018#MRR#19.44$taxonomy_learning#SemEval 2018#P@5#8.63
1808.10143.pdf	language_modeling#Penn Treebank#Validation perplexity#48.63$language_modeling#Penn Treebank#Test perplexity#47.17$language_modeling#Penn Treebank#Number of params#185M$language_modeling#WikiText-2#Validation perplexity#54.19$language_modeling#WikiText-2#Test perplexity#53.09$language_modeling#WikiText-2#Number of params#185M
P18-1061.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#41.59$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#19.01$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.98$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
1605.07725.pdf	sentiment_analysis#IMDb#Accuracy#94.1
16118.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#41.25$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#18.87$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#37.75$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
409.pdf	word_segmentation#Chinese Treebank 6#F1#95.5$word_segmentation#PKU#F1#95.7$word_segmentation#MSR#F1#97.6
1809.03449.pdf	question_answering#SQuAD#EM#76.125$question_answering#SQuAD#F1#83.538
press.html.pdf	machine_translation#WMT 2014 EN-DE#BLEU#33.3$machine_translation#WMT 2014 EN-FR#BLEU#45.9
P18-1014.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#41.6$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#18.3$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#37.7$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
D18-1207.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#40.02$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#15.53$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#37.44$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#40.19$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#17.38$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.52$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
D17-1108.pdf	temporal_information_extraction#TempEval-3#Temporal awareness#67.2
S13-2002.pdf	temporal_information_extraction#TempEval-3#Temporal awareness#30.98
S18-1116.pdf	taxonomy_learning#SemEval 2018#MAP#19.78$taxonomy_learning#SemEval 2018#MRR#36.10$taxonomy_learning#SemEval 2018#P@5#19.03
P17-1055.pdf	question_answering#CNN / Daily Mail#Accuracy on CNN#74.4$question_answering#CNN / Daily Mail#Accuracy on Daily Mail#-
Q18-1025.pdf	timex_normalisation#PNT#F1#0.764
1709.04109.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#91.24$part-of-speech_tagging#Penn Treebank#Accuracy#97.53
1603.01354.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#91.21$part-of-speech_tagging#Penn Treebank#Accuracy#97.55
P16-1200.pdf	relationship_extraction#New York Times Corpus#P@10%#69.4$relationship_extraction#New York Times Corpus#P@30%#51.8
C18-1146.pdf	summarization#Gigaword#ROUGE-1#35.47$summarization#Gigaword#ROUGE-2#17.66$summarization#Gigaword#ROUGE-L#33.52
1810.13097.pdf	named_entity_recognition#VLSP 2016 NER shared task#F1#89.58
N16-1012.pdf	summarization#Gigaword#ROUGE-1#33.78$summarization#Gigaword#ROUGE-2#15.97$summarization#Gigaword#ROUGE-L#31.15$summarization#DUC 2004 Task 1#ROUGE-1#28.97$summarization#DUC 2004 Task 1#ROUGE-2#8.26$summarization#DUC 2004 Task 1#ROUGE-L#24.06
D18-1238.pdf	question_answering#NarrativeQA#BLEU-1#36.55$question_answering#NarrativeQA#BLEU-4#19.79$question_answering#NarrativeQA#METEOR#17.87$question_answering#NarrativeQA#ROUGE-L#41.44$question_answering#SearchQA#Unigram Acc#49.4$question_answering#SearchQA#N-gram F1#59.5$question_answering#SearchQA#EM#-$question_answering#SearchQA#F1#-
16165.pdf	question_answering#Quasar#EM (Quasar-T)#35.3$question_answering#Quasar#F1 (Quasar-T)#41.7$question_answering#SearchQA#Unigram Acc#-$question_answering#SearchQA#N-gram F1#-$question_answering#SearchQA#EM#49.0$question_answering#SearchQA#F1#55.3
0568.pdf	sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)#78.60$sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)#72.10
1712.01586.pdf	semantic_role_labeling#OntoNotes#F1#82.7
N18-2053.pdf	relation_prediction#WN18RR#H@10#52.50$relation_prediction#WN18RR#H@1#-$relation_prediction#WN18RR#MRR#24.80$relation_prediction#FB15K-237#H@10#51.70$relation_prediction#FB15K-237#H@1#-$relation_prediction#FB15K-237#MRR#39.60
stamp.jsp_arnumber_8344797.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#42.3$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#17.8$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#37.6$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
P17-2096.pdf	word_segmentation#AS#F1#95.3$word_segmentation#CityU#F1#95.6$word_segmentation#PKU#F1#95.8$word_segmentation#MSR#F1#97.1
1707.03058.pdf	constituency_parsing#Penn Treebank#F1#94.66
5071-translating-embeddings-for-modeling-multi-relational-data.pdf	relation_prediction#WN18RR#H@10#55.55$relation_prediction#WN18RR#H@1#42.26$relation_prediction#WN18RR#MRR#46.59$relation_prediction#FB15K-237#H@10#47.09$relation_prediction#FB15K-237#H@1#19.87$relation_prediction#FB15K-237#MRR#29.04
W15-4313.pdf	lexical_normalization#LexNorm2015#F1#84.21$lexical_normalization#LexNorm2015#Precision#90.61$lexical_normalization#LexNorm2015#Recall#78.65
N16-1024.pdf	constituency_parsing#Penn Treebank#F1#﻿93.3
303553351_Improving_Question_Classification_by_Feature_Extraction_and_Selection.pdf	text_classification#TREC#Error#8.4
1812.11459.pdf	dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS#72.56$dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS#79.75$part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy#95.93$word_segmentation#VLSP 2013 word segmentation shared task#F1#97.78
1809.06858.pdf	language_modeling#Penn Treebank#Validation perplexity#47.38$language_modeling#Penn Treebank#Test perplexity#46.54$language_modeling#Penn Treebank#Number of params#22M$language_modeling#WikiText-2#Validation perplexity#40.85$language_modeling#WikiText-2#Test perplexity#39.14$language_modeling#WikiText-2#Number of params#35M
1607.03474.pdf	language_modeling#Hutter Prize#Bit per Character (BPC)#1.27$language_modeling#Hutter Prize#Number of params#46M$language_modeling#Text8#Bit per Character (BPC)#1.27$language_modeling#Text8#Number of params#46M
P10-4014.pdf	word_sense_disambiguation#Senseval 2#F1#71.3$word_sense_disambiguation#Senseval 3#F1#68.8$word_sense_disambiguation#SemEval 2007#F1#60.2$word_sense_disambiguation#SemEval 2013#F1#65.8$word_sense_disambiguation#SemEval 2015#F1#70.0
P09-1113.pdf	relationship_extraction#New York Times Corpus#P@10%#39.9+$relationship_extraction#New York Times Corpus#P@30%#-
1603.01547.pdf	question_answering#SearchQA#Unigram Acc#41.3$question_answering#SearchQA#N-gram F1#22.8$question_answering#SearchQA#EM#-$question_answering#SearchQA#F1#-
D18-1529.pdf	word_segmentation#Chinese Treebank 6#F1#96.7$word_segmentation#Chinese Treebank 7#F1#96.6$word_segmentation#AS#F1#96.2$word_segmentation#CityU#F1#97.2$word_segmentation#PKU#F1#96.1$word_segmentation#MSR#F1#98.1
P15-1067.pdf	relation_prediction#FB15K-237#H@10#46.05$relation_prediction#FB15K-237#H@1#14.83$relation_prediction#FB15K-237#MRR#25.27
P15-1032.pdf	dependency_parsing#Penn Treebank#POS#97.44$dependency_parsing#Penn Treebank#UAS#93.99$dependency_parsing#Penn Treebank#LAS#92.05
1603.03793.pdf	dependency_parsing#Penn Treebank#POS#97.3$dependency_parsing#Penn Treebank#UAS#93.56$dependency_parsing#Penn Treebank#LAS#91.42
D17-1222.pdf	summarization#Gigaword#ROUGE-1#36.27$summarization#Gigaword#ROUGE-2#17.57$summarization#Gigaword#ROUGE-L#33.62$summarization#DUC 2004 Task 1#ROUGE-1#31.79$summarization#DUC 2004 Task 1#ROUGE-2#10.75$summarization#DUC 2004 Task 1#ROUGE-L#27.48
1511.08308.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#91.62$named_entity_recognition#Ontonotes v5 (English)#F1#86.28
N18-1127.pdf	named_entity_recognition#Long-tail emerging entities#F1#45.55$named_entity_recognition#Long-tail emerging entities#F1 (surface form)#
978-3-540-88282-4_23.pdf	word_segmentation#VLSP 2013 word segmentation shared task#F1#97.33
1808.09381.pdf	machine_translation#WMT 2014 EN-DE#BLEU#35.0$machine_translation#WMT 2014 EN-FR#BLEU#45.6
15392.pdf	common_sense#Winograd Schema Challenge#Accuracy#52.8
1806.02847.pdf	common_sense#Winograd Schema Challenge#Accuracy#62.6
1809.08370.pdf	machine_translation#The IWSLT 2015 Evaluation Campaign#BLEU#29.6$dependency_parsing#Penn Treebank#POS#---$dependency_parsing#Penn Treebank#UAS#96.61$dependency_parsing#Penn Treebank#LAS#95.02$named_entity_recognition#CoNLL 2003 (English)#F1#92.61$named_entity_recognition#Ontonotes v5 (English)#F1#88.81$ccg_supertagging#CCGBank#Accuracy#96.1
P18-2058.pdf	semantic_role_labeling#OntoNotes#F1#85.5
1602.02410.pdf	language_modeling#1B Words / Google Billion Word benchmark#Test perplexity#23.7$language_modeling#1B Words / Google Billion Word benchmark#Number of params#43B?
N16-1027.pdf	ccg_supertagging#CCGBank#Accuracy#94.24$part-of-speech_tagging#Penn Treebank#Accuracy#97.4
N18-2009.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#38.95$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#17.12$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#35.68$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
D15-1063.pdf	timex_normalisation#TimeBank#F1#0.876$timex_normalisation#PNT#F1#0.74
P16-1223.pdf	question_answering#CNN / Daily Mail#Accuracy on CNN#72.4$question_answering#CNN / Daily Mail#Accuracy on Daily Mail#75.8
P18-1103.pdf	retrieval-based_chatbot#Ubuntu Corpus#R_2@1#93.8$retrieval-based_chatbot#Ubuntu Corpus#R_10@1#76.7
N09-1009.pdf	dependency_parsing#Penn Treebank#UAS#41.4
reside_emnlp18.pdf	relationship_extraction#New York Times Corpus#P@10%#73.6$relationship_extraction#New York Times Corpus#P@30%#59.5
1510.03753.pdf	retrieval-based_chatbot#Ubuntu Corpus#R_2@1#89.5$retrieval-based_chatbot#Ubuntu Corpus#R_10@1#63.0
1801.01900.pdf	word_sense_disambiguation#Senseval 2#F1#69$word_sense_disambiguation#Senseval 3#F1#66.9$word_sense_disambiguation#SemEval 2007#F1#55.6$word_sense_disambiguation#SemEval 2013#F1#65.3$word_sense_disambiguation#SemEval 2015#F1#69.6
1709.04250.pdf	dialogue_act_classification#Switchboard corpus#Accuracy#79.2$dialogue_act_classification#ICSI Meeting Recorder Dialog Act (MRDA) corpus#Accuracy#90.9
sentic-lstm.pdf	sentiment_analysis#Sentihood#Aspect (F1)#78.2$sentiment_analysis#Sentihood#Sentiment (acc)#89.3
16492.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#39.92$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#17.65$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#36.71$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
S16-1181.pdf	amr_parsing#LDC2015E86#Smatch#66.5
1805.01052.pdf	constituency_parsing#Penn Treebank#F1#95.13
D18-1441.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#40.30$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#18.02$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.36$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
1707.09098.pdf	question_answering#SQuAD#EM#78.234$question_answering#SQuAD#F1#85.344
N18-2045.pdf	sentiment_analysis#Sentihood#Aspect (F1)#78.5$sentiment_analysis#Sentihood#Sentiment (acc)#91.0
S17-2094.pdf	sentiment_analysis#SemEval-2017 Task 4 subtask A Tweet Sentiment Classification dataset#F1-score#0.685
1609.05284.pdf	question_answering#SQuAD#EM#75.034$question_answering#SQuAD#F1#82.552
1611.01734.pdf	dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS#73.53$dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS#80.84$dependency_parsing#Penn Treebank#POS#97.3$dependency_parsing#Penn Treebank#UAS#95.44$dependency_parsing#Penn Treebank#LAS#93.76
nmt.pdf	machine_translation#The IWSLT 2015 Evaluation Campaign#BLEU#26.1
D16-1041.pdf	taxonomy_learning#SemEval 2018#MAP#10.60$taxonomy_learning#SemEval 2018#MRR#23.83$taxonomy_learning#SemEval 2018#P@5#9.91
1606.01781.pdf	text_classification#AG News#Error#8.67$text_classification#DBpedia#Error#1.29
C18-1139.pdf	chunking#Penn Treebank#F1#96.72$named_entity_recognition#Long-tail emerging entities#F1#50.20$named_entity_recognition#Long-tail emerging entities#F1 (surface form)#$named_entity_recognition#Ontonotes v5 (English)#F1#89.71$part-of-speech_tagging#Penn Treebank#Accuracy#97.85
1709.00103.pdf	sql_parsing#WikiSQL#Acc ex#59.4
1710.02772.pdf	question_answering#SQuAD#EM#71.415$question_answering#SQuAD#F1#80.160
luong-manning-iwslt15.pdf	machine_translation#The IWSLT 2015 Evaluation Campaign#BLEU#23.3
clin27.paper.pdf	lexical_normalization#LexNorm#Accuracy#87.63$lexical_normalization#LexNorm2015#F1#86.39$lexical_normalization#LexNorm2015#Precision#93.53$lexical_normalization#LexNorm2015#Recall#80.26
P18-1161.pdf	question_answering#Quasar#EM (Quasar-T)#42.2$question_answering#Quasar#F1 (Quasar-T)#49.3$question_answering#SearchQA#Unigram Acc#-$question_answering#SearchQA#N-gram F1#-$question_answering#SearchQA#EM#58.8$question_answering#SearchQA#F1#64.5
1711.04434.pdf	summarization#Gigaword#ROUGE-1#37.27$summarization#Gigaword#ROUGE-2#17.65$summarization#Gigaword#ROUGE-L#34.24
1805.08092.pdf	question_answering#NewsQA#F1#63.2$question_answering#NewsQA#EM#50.1
P18-1129.pdf	dependency_parsing#Penn Treebank#POS#97.3$dependency_parsing#Penn Treebank#UAS#94.05$dependency_parsing#Penn Treebank#LAS#92.14
5782-character-level-convolutional-networks-for-text-classification.pdf	text_classification#AG News#Error#9.51$text_classification#DBpedia#Error#1.55$sentiment_analysis#Yelp#Error#37.95
43852.pdf	summarization#Google Dataset#F1#0.82$summarization#Google Dataset#CR#0.38
1806.05516.pdf	text_classification#TREC#Error#4$sentiment_analysis#SUBJ#Accuracy#94.80
1611.01604.pdf	question_answering#SQuAD#EM#71.625$question_answering#SQuAD#F1#80.383
1609.07561.pdf	dependency_parsing#Penn Treebank#POS#97.3$dependency_parsing#Penn Treebank#UAS#94.26$dependency_parsing#Penn Treebank#LAS#92.06
D17-1206.pdf	chunking#Penn Treebank#F1#95.77
S18-1147.pdf	taxonomy_learning#SemEval 2018#MAP#5.77$taxonomy_learning#SemEval 2018#MRR#10.56$taxonomy_learning#SemEval 2018#P@5#5.96
P17-1099.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#40.34$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#17.70$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#36.57$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#22.21
L12-1015.pdf	timex_normalisation#TimeBank#F1#0.89
1712.03609.pdf	question_answering#SQuAD#EM#77.583$question_answering#SQuAD#F1#84.163
1704.01444.pdf	sentiment_analysis#SST-2#Accuracy#91.8$sentiment_analysis#SUBJ#Accuracy#94.60
Q16-1023.pdf	dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS#73.17$dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS#79.39$dependency_parsing#Penn Treebank#POS#97.3$dependency_parsing#Penn Treebank#UAS#93.9$dependency_parsing#Penn Treebank#LAS#91.9
P05-1012.pdf	dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS#70.29$dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS#76.47
P17-1110.pdf	word_segmentation#Chinese Treebank 6#F1#96.2$word_segmentation#AS#F1#94.8$word_segmentation#CityU#F1#95.6$word_segmentation#PKU#F1#94.3$word_segmentation#MSR#F1#96.0
P18-1087.pdf	sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)#80.79$sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)#76.01
1703.10722.pdf	language_modeling#1B Words / Google Billion Word benchmark#Test perplexity#35.1$language_modeling#1B Words / Google Billion Word benchmark#Number of params#0.151B
P18-1013.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#40.68$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#17.97$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.13$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#-
1804.09769.pdf	sql_parsing#WikiSQL#Acc ex#82.6
pdf_id_HyeVtoRqtQ.pdf	language_modeling#Penn Treebank#Validation perplexity#-$language_modeling#Penn Treebank#Test perplexity#54.19$language_modeling#Penn Treebank#Number of params#34M$language_modeling#WikiText-103#Validation perplexity#-$language_modeling#WikiText-103#Test perplexity#30.35$language_modeling#WikiText-103#Number of params#180M$language_modeling#Penn Treebank#Bit per Character (BPC)#1.159
D17-1130.pdf	amr_parsing#LDC2014T12#F1 on Newswire#69$amr_parsing#LDC2014T12#F1 on Full#64
1703.06345.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#91.26$part-of-speech_tagging#Penn Treebank#Accuracy#97.55
1612.04426.pdf	language_modeling#WikiText-103#Validation perplexity#-$language_modeling#WikiText-103#Test perplexity#40.8$language_modeling#WikiText-103#Number of params#
1505.07818.pdf	sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on DVD#75.40$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Books#71.43$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Electronics#77.67$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Kitchen#80.53$sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Average#76.26
K16-1028.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#35.46$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#13.30$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#32.65$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-$summarization#Gigaword#ROUGE-1#36.4$summarization#Gigaword#ROUGE-2#17.7$summarization#Gigaword#ROUGE-L#33.71$summarization#DUC 2004 Task 1#ROUGE-1#28.61$summarization#DUC 2004 Task 1#ROUGE-2#9.42$summarization#DUC 2004 Task 1#ROUGE-L#25.24
D15-1136.pdf	amr_parsing#LDC2014T12#F1 on Newswire#--$amr_parsing#LDC2014T12#F1 on Full#66
C14-1220.pdf	relationship_extraction#SemEval-2010 Task 8#F1#82.7
1606.03777.pdf	dialogue_state_tracking#Second dialogue state tracking challenge#Request Accuracy#96.5$dialogue_state_tracking#Second dialogue state tracking challenge#Joint goal Accuracy#73.4$dialogue_state_tracking#Wizard-of-Oz#Request Accuracy#96.5$dialogue_state_tracking#Wizard-of-Oz#Joint goal Accuracy#84.4
C16-1238.pdf	relationship_extraction#SemEval-2010 Task 8#F1#84.385.9<a href="#footnote">*
1508.03720.pdf	relationship_extraction#SemEval-2010 Task 8#F1#83.7
P17-1168.pdf	question_answering#CNN / Daily Mail#Accuracy on CNN#77.9$question_answering#CNN / Daily Mail#Accuracy on Daily Mail#80.9
P15-2041.pdf	ccg_supertagging#CCGBank#Accuracy#93.00
1611.05774.pdf	dependency_parsing#Penn Treebank#UAS#95.8$dependency_parsing#Penn Treebank#LAS#94.6$constituency_parsing#Penn Treebank#F1#93.6
1711.07341.pdf	question_answering#SQuAD#EM#78.978$question_answering#SQuAD#F1#86.016
S18-1148.pdf	taxonomy_learning#SemEval 2018#MAP#9.37$taxonomy_learning#SemEval 2018#MRR#17.29$taxonomy_learning#SemEval 2018#P@5#9.19
1811.04210.pdf	question_answering#NewsQA#F1#66.3$question_answering#NewsQA#EM#53.1$question_answering#NarrativeQA#BLEU-1#44.35$question_answering#NarrativeQA#BLEU-4#27.61$question_answering#NarrativeQA#METEOR#21.80$question_answering#NarrativeQA#ROUGE-L#44.69$question_answering#Quasar#EM (Quasar-T)#38.6$question_answering#Quasar#F1 (Quasar-T)#46.9$question_answering#SearchQA#Unigram Acc#62.2$question_answering#SearchQA#N-gram F1#70.8$question_answering#SearchQA#EM#56.8$question_answering#SearchQA#F1#63.6
P17-1052.pdf	text_classification#AG News#Error#6.87$text_classification#DBpedia#Error#0.88$sentiment_analysis#Yelp#Error#30.58
1702.03814.pdf	semantic_textual_similarity#Quora Question Pairs#Accuracy#88.17
S17-2080.pdf	stance_detection#RumourEval#Accuracy#0.780
N16-1026.pdf	ccg_supertagging#CCGBank#Accuracy#94.7
55.pdf	word_segmentation#VLSP 2013 word segmentation shared task#F1#97.90
1603.09025.pdf	language_modeling#Text8#Bit per Character (BPC)#1.36$language_modeling#Text8#Number of params#16M
P16-2038.pdf	chunking#Penn Treebank#F1#95.57$ccg_supertagging#CCGBank#Accuracy#93.26
Y15-1009.pdf	relationship_extraction#SemEval-2010 Task 8#F1#82.784.3<a href="#footnote">*
N18-1140.pdf	question_answering#CliCR#F1#33.9
P17-1078.pdf	word_segmentation#Chinese Treebank 6#F1#96.2$word_segmentation#AS#F1#95.7
human.pdf	question_answering#SQuAD#EM#82.304$question_answering#SQuAD#F1#91.221
1703.00572.pdf	question_answering#SQuAD#EM#74.090$question_answering#SQuAD#F1#81.761
D10-1117.pdf	dependency_parsing#Penn Treebank#UAS#55.7
1603.01360.pdf	named_entity_recognition#CoNLL 2003 (English)#F1#90.94
1808.04444.pdf	language_modeling#Hutter Prize#Bit per Character (BPC)#1.06$language_modeling#Hutter Prize#Number of params#235M$language_modeling#Text8#Bit per Character (BPC)#1.13$language_modeling#Text8#Number of params#235M
D13-1090.pdf	semantic_textual_similarity#SICK-R#Pearson Correlation#-$semantic_textual_similarity#SICK-E#Accuracy#-
7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf	sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)#80.95$sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)#72.21
D12-1042.pdf	relationship_extraction#New York Times Corpus#P@10%#60.7+$relationship_extraction#New York Times Corpus#P@30%#-
E14-2005.pdf	part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy#95.11
D18-1440.pdf	summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1#40.66$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2#17.87$summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L#37.06$summarization#CNN / Daily Mail (Non-anonymized version)#METEOR#20.51
owoputi_etal.naacl13.pdf	part-of-speech_tagging#Social media#Accuracy#90.0 ± 0.5
N18-2046.pdf	grammatical_error_correction#CoNLL-2014 Shared Task Restricted#F0.5#56.25$grammatical_error_correction#CoNLL-2014 10 Annotations Restricted#F0.5#72.04
P18-1230.pdf	word_sense_disambiguation#Senseval 2#F1#72.0$word_sense_disambiguation#Senseval 3#F1#70.0$word_sense_disambiguation#SemEval 2007#F1#--*$word_sense_disambiguation#SemEval 2013#F1#66.7$word_sense_disambiguation#SemEval 2015#F1#71.6
P15-1061.pdf	relationship_extraction#SemEval-2010 Task 8#F1#84.1
N18-1055.pdf	grammatical_error_correction#CoNLL-2014 Shared Task Restricted#F0.5#55.8
C16-1116.pdf	text_classification#TREC#Error#2.8
nle07.pdf	dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS#69.10$dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS#74.91
N18-1202.pdf	machine_translation#The IWSLT 2015 Evaluation Campaign#BLEU#29.3$word_sense_disambiguation#Senseval 2#F1#71.6$word_sense_disambiguation#Senseval 3#F1#69.6$word_sense_disambiguation#SemEval 2007#F1#62.2$word_sense_disambiguation#SemEval 2013#F1#66.2$word_sense_disambiguation#SemEval 2015#F1#71.3
D16-1257.pdf	dependency_parsing#Penn Treebank#UAS#95.9$dependency_parsing#Penn Treebank#LAS#94.1$constituency_parsing#Penn Treebank#F1#93.8
27496a2ee337db705e7c611dea1fd8e6f41437c2.pdf	named_entity_recognition#Ontonotes v5 (English)#F1#83.45
S17-2157.pdf	amr_parsing#LDC2016E25#Smatch#61.9
r13-1026.pdf	part-of-speech_tagging#Social media#Accuracy#88.69
D12-1110.pdf	relationship_extraction#SemEval-2010 Task 8#F1#82.4
P17-1108.pdf	summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1#38.1$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2#13.9$summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L#34.0$summarization#CNN / Daily Mail (Anonymized version)#METEOR#-
1804.06536.pdf	sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)#81.20$sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)#74.50
P18-4013.pdf	chunking#Penn Treebank#F1#95.06$named_entity_recognition#CoNLL 2003 (English)#F1#91.35$part-of-speech_tagging#Penn Treebank#Accuracy#97.49
K16-1006.pdf	word_sense_disambiguation#Senseval 2#F1#71.8$word_sense_disambiguation#Senseval 3#F1#69.1$word_sense_disambiguation#SemEval 2007#F1#61.3$word_sense_disambiguation#SemEval 2013#F1#65.6$word_sense_disambiguation#SemEval 2015#F1#71.9
E17-1010.pdf	word_sense_disambiguation#Senseval 2#F1#65.6$word_sense_disambiguation#Senseval 3#F1#66.0$word_sense_disambiguation#SemEval 2007#F1#54.5$word_sense_disambiguation#SemEval 2013#F1#63.8$word_sense_disambiguation#SemEval 2015#F1#67.1
1704.08381.pdf	amr_parsing#LDC2015E86#Smatch#62.1
1803.09074.pdf	question_answering#RACE#Accuracy on RACE-m#60.2$question_answering#RACE#Accuracy on RACE-h#50.3$question_answering#RACE#Accuracy on RACE#53.3
1602.02373.pdf	text_classification#AG News#Error#6.57$text_classification#DBpedia#Error#0.84$sentiment_analysis#IMDb#Accuracy#94.1$sentiment_analysis#Yelp#Error#32.39
N18-2108.pdf	coreference_resolution#CoNLL 2012#Avg F1#73.0
P17-1089.pdf	sql_parsing#ATIS#Question Split#45$sql_parsing#ATIS#Query Split#17$sql_parsing#Advising#Question Split#41$sql_parsing#Advising#Query Split#1$sql_parsing#GeoQuery#Question Split#66$sql_parsing#GeoQuery#Query Split#40$sql_parsing#Scholar#Question Split#44$sql_parsing#Scholar#Query Split#3$sql_parsing#Smaller Datasets#Question Split#100$sql_parsing#Smaller Datasets#Query Split#8
