section: title
Multi-range Reasoning for Machine Comprehension
section: abstract
We propose MRU (Multi-Range Reasoning Units), anew fast compositional encoder for machine comprehension (MC). Our proposed MRU encoders are characterized by multi-ranged gating, executing a series of parameterized contract-and-expand layers for learning gating vectors that benefit from long and short-term dependencies. The aims of our approach are as follows: (1) learning representations that are concurrently aware of long and short-term context, (2) modeling relationships between intra-document blocks and (3) fast and efficient sequence encoding. We show that our proposed encoder demonstrates promising results both as a standalone encoder and as well as a complementary building block. We conduct extensive experiments on three challenging MC datasets, namely RACE, SearchQA and NarrativeQA, achieving highly competitive performance on all. On the RACE benchmark, our model outperforms DFN (Dynamic Fusion Networks) by 1.5% − 6% without using any recurrent or convolution layers. Similarly, we achieve competitive performance relative to AMANDA [17] on the SearchQA benchmark and BiDAF [23] on the NarrativeQA benchmark without using any LSTM/GRU layers. Finally, incorporating MRU encoders with standard BiLSTM architectures further improves performance, achieving state-of-the-art results.
section: Introduction
Teaching machines to read, comprehend and reason lives at the heart of machine comprehension (MC) tasks. In these tasks, the goal is to answer questions based on a given passage, effectively testing the learner's capability to understand natural language. This has been an extremely productive area of research in the recent years, giving rise to many highly advanced neural network architectures. A common denominator in many of these models is the compositional encoder, i.e., usually a bidirectional recurrent-based (LSTM or GRU) encoder that sequentially parses the text sequence word-by-word. This helps to model compositionality of words, capturing rich and complex linguistic and syntactic structure in language.
While the usage of recurrent encoder is often regarded as indispensable in highly complex MC tasks, there are still several challenges and problems pertaining to it's usage in modern MC tasks. Firstly, documents can be extremely long to the point where running a BiRNN model across along document is computationally prohibitive. This is aggravated since MC tasks can be easily extended to reasoning over multiple long documents. Secondly, recurrent encoders have limited access to long term context since each word is sequentially parsed. This restricts any form of multi-sentence and intra-document reasoning from happening within compositional encoder layer.
To this end, we propose anew compositional encoder that can either be used in-place of standard RNN encoders or serve as anew module that is complementary to existing neural architectures. Our proposed MRU encoders learns gating vectors via multiple contract-and-expand layers at multiple dilated resolutions. Specifically, we compress the input document an arbitrary k times at multi-ranges (e.g., 1, 2, 4, 10, 25) into a neural bag-of-words (summed) representation. The compact sequence is then passed through affine transformation layers and then re-expanded to the original sequence length. The k document representations (at multiple ranges and n-gram blocks) are then combined and modeled with fully connected layers to form the final compositional gate which are applied onto the original input document. This can be interpreted as compositional gating by exploiting information at multiple-ranges, modeling relationships across different granularities and hierarchies. Intuitively, this is because 1-gram blocks are compared with 2-gram blocks and 10-gram blocks and soon.
This has several advantages. Firstly, we enable a major speedup by avoiding either costly step-by-step gate construction while still maintaining interactions between neighboring words. As such, our model belongs to a class of architectures which is inspired by QRNNs and SRUs. The key difference is that our gates are not constructed by convolution layers but explicit block-based matching across multiple ranges. Secondly, modeling at along range (e.g., 25 or 50) enables our model to look further ahead as opposed to only one step forward. As such, the learned gastes possess not only information about nearby words but also a larger overview of the context. This is in similar spirit to self-attention, albeit executing within the encoder. Thirdly, the final gates are formed by modeling relationships between multi-range projections (n-gram blocks), allowing for fine-grained intra-document relationships to be captured. The overall contributions of our work is as follows:
• We propose MRU (Multi-range Reasoning Units), anew compositional encoder which construct gates from a novel contract-and-expand operation. We propose an overall architecture that utilizes MRU within a bi-attentive framework for both multiple choice and span prediction MC tasks. MRU can be used as a standalone (without RNNs) for fast reading and/or together with RNN models (i.e., MRU-LSTM) for more expressive reading.
• We conduct extensive experiments on three large-scale and challenging machine comprehension datasets -RACE, SearchQA and NarrativeQA. Our model is lightweight, fast and efficient, achieving state-of-the-art or highly competitive performance on all benchmarked datasets. Since MC datasets often require a considerable amount of reasoning and natural language understanding, we believe that they serve as good testbeds for benchmarking encoders.
• On RACE, our model outperforms Dynamic Fusion Networks (DFN), a highly complex model. While DFN takes approximately a week to train, spending at least several hours per epoch, our model converges in less than 12 hours with only 4 − 5 minutes per epoch. Moreover, our model outperforms DFN by 2% − 6% on the RACE benchmark and other strong baselines such as the Gated Attention Reader by 10%. On RACE, we outperform DFN without any recurrent and convolution layers. Ablation studies show an improvement of up to 6% when using MRU over a LSTM/GRU encoder.
• On the recent SearchQA benchmark, we achieve competitive performance relative to AMANDA, a state-of-the-art model without using any recurrent or convolution layers. Our model runs at 2 minutes per epoch, approximately five times faster than AMANDA. Incorporating our MRU block with standard BiLSTM architectures (MRU-LSTM) outperforms AMANDA by a reasonable margin.
• On the NarrativeQA benchmark (summaries setting), our MRU encoders achieves highly competitive performance relative to BiDAF a strong MC baseline without using any LSTM/GRU layers. On the other hand, MRU-LSTM significantly outperforms BiDAF, achieving state-of-the-art performance on this dataset.
section: Our Proposed MRU Encoder
In this section, we describe our proposed MRU encoder. The inputs to the MRU encoder is an input document {w 1 , w 2 · · · w }, and list of ranges {r 1 , r 2 · · · r k } where k is the number of times the contract and expand operation is executed. The final output of the encoder is a sequence of vectors which retain the same dimensionality as its inputs. (left most block) provides an illustration of the overall encoder architecture.
section: Contract-and-Expand Operation
This section describes the operation for each r j . For the sake of brevity, we drop the superscripts j.
For each r j and the input document, the contract operation performs takes the summation of every r j words. This reduces the overall document length to /r j where each item in the sequence is the sum of every r j words. Given the new sequence of /r i tokens, we then pass each token into a single layered feed-forward neural network:
where W a ∈ R d×d and b a ∈ Rd are the parameters of the contract layer. σ r is the ReLU activation function. wt is the t-th token in the sequence. Given the transformed tokens ¯ w 1 , ¯ w 2 · · · ¯ w /rj , we then expand them into the original sequence length. Note that for each r j , the parameters W a , b a are not shared.
section: Reasoning over Multi-ranged Blocks
From k different calls of the Contract-and-Expand operation at different ranges, we pass the concatenated vector of all transformed tokens into a two layered feed-forward neural network.
where F 1 (.), F 2 (.) are feed-forward networks with ReLU activations, i.e., σ r (W x + b).
[; ] is the concatenation operator. gt is interpreted as a gating vector learned from multiple ranges and Equation is learning the relationships between a token's representation at multiple hierarchies depending on the values of r j . Notably, it is easy to see that every n pairs of words will have the same gating vector where n is the lowest value of r j . As such, the value of the 1gram, i.e., r j = 1 (projection of every single token) is critical as it prevents identical gating vectors across the sequence.
section: MRU Encoding Operation
To learn the MRU encoded representation of each word, we consider two variations of MRU encoders.
section: Simple MRU
In this variation, we use gt as a gating vector to control the fine-grained balanced between the projection of each word wt in the original input document and the original representation.
where {y 1 , y 2 , · · · y } is the output document representation. σ is the sigmoid function. Note that this formulation is in similar spirit to highway networks. However, since our gating function is learned via multi-range reasoning, it captures more compositionality and long range context. Note that an optional and additional projection maybe applied tow t but we found that it did not yield much empirical benefit.
section: Recurrent MRU
In the second variation, we consider a recurrent (sequential) variant. This is in similar spirit to QRNNs and SRUs which reduces computation cost by pre-learning the gating vectors. The following operations describe the operations of the recurrent MRU cell for each timestep t.
where ct , ht are the cell and hidden states at time step t. gt are the gates learned from out multi-range reasoning step. o t is an additional output gate learned via applying an affine transform on the input vector wt , i.e., o t = W o (w t ) + b o . Similar to RNNs, the Recurrent MRU parses the input sequence word-by-word. However, the cost is significantly reduced because we do not have expensive matrix operations that are executed in a non-parallel fashion. Finally, the outputs of the MRU encoder area series of hidden vectors {h 1 , h 2 · · · h } for each word in the sequence.
section: Overall Model Architectures
This section describes the overall model architecture that utilizes MRU encoders. In our experiments, we focus on both multiple-choice based (RACE) and span prediction MC tasks (SearchQA, NarrativeQA). Since the core focus of this paper is our encoder, we briefly provide the high-level details of our vanilla Bi-Attentive model. The Bi-Attentive models that are used in our experiments act as baselines, often being less complex than current competitive models such as BiDAF, AMANDA or DFN.
section: Multiple Choice Models
In MCQ models, there are three types of input sequences, namely Passage (P ), Question (Q) and Answers (A j ). The output of the model (for each answer), is a score s(P, Q, A j ) ∈ [0, 1] denoting the strength of A j . The problem is formulated as a listwise approach, in which multiple answers are modeled concurrently with respect to P, Q.
• Input Encoding -Each input sequence is passed into first a projection layer. To enhance the input word representations, we also include the standard EM (exact match) binary feature to each word. In this case, we use a three-way EM adaptation, i.e., EM (P, Q), EM (Q, A) and EM (P, A). The projected embeddings are then passed into a single-layered highway network.
• Compositional Encoder -In our experiments, we vary the encoder in this layer. Typical choices of encoders in this layer are LSTMs or GRUs. We vary this in our experiments in order to benchmark the effectiveness of our proposed MRU encoder. The output of this layer is same dimensions as its inputs (typically the hidden states of a RNN model).
• Bi-Attention Layer -This layer models the interactions between P, Q and A. Let B(.) be a standard bidirectional attention that utilizes mean-pooling aggregation. The scoring function is the bilinear product of the nonlinearly transformed input i.e., F (x) i MF (y) i . We first apply B(P, Q) to form bi-attentive P q , Q p representations. Subsequently, we apply B(P q , A j ) to learn a vector representation for each answer. A temporal sum pooling is applied on the outputs of P qa , A p j and concatenated to form a f j ∈ R 2d .
• Answer Selection Let {a 1 , a 2 · · · a Na } be the inputs to this layer and Na is the number of answer candidates. Motivated by work in retrieval-based QA, we include word overlap features to each answer candidate. This word overlap feature is in similar spirit to the EM feature. Each overlap operation between two sequence returns four features. We convert each answer vector a j into a scalar via a f j = Sof tmax(
The MCQ-based model minimizes the multi-class cross entropy where the number of classes corresponds to the number of choices.
section: Span Prediction Model
Span prediction models models the relationship between P and Q. The goal is to extract (or predict a span s, e) where P [s : e] is the answer to the query. For most part, the model architecture remains similar especially for the input encoding layers compositional encoder layer. The key difference is that we reduce the number of input sequence from three to two.
• Input Encoding -This follows the same design as the MCQ model, albeit for two sequence. Similarly, the two-way EM feature is added before passing into the highway layer.
• Compositional Encoder -This remains identical as the MCQ-based model.
• Bi-Attention Layer -We adopt a different bi-attention function for span prediction. More specifically, we use the 'SubMultNN' or the ''Mult' adaptation from (this is tuned) and compare aligned sequences between P and Q to form P q , the query-dependent passage representation.
• Answer Pointer Layer -In this layer, we pass P q through a two layered compositional encoder (which is varied). The start pointer and end pointer is determined by F (H 1 ), F (H 2 ) where H 1 , H 2 are the hidden outputs from the first and second encoder respectively. F (.) is a linear transform, projecting each hidden state to a scalar. We pass both of them into softmax functions to obtain probability distributions.
Following, we minimize the joint cross entropy loss of the start and end probability distributions. During inference, finding the best answer span follows.
section: Empirical Evaluation
In this section, we report our experimental results and comparisons against other published work.
section: Datasets
For our experiments, we use one challenging multiple choice MC dataset and two span-prediction MC datasets.
• RACE (Reading Comprehension from Examinations) is a recently proposed dataset that is constructed from real world examinations. Given a passage, there are several questions with four options each. The authors argue that RACE is more challenging compared to popular benchmarks (e.g., SQuAD) as more multi-sentence and compositional reasoning is required. There are two subsets of RACE, namely RACE-M (Middle school) and RACE-H (High school).
• SearchQA is a recent dataset that emulates areal world QA system. It involves extracting passages from search engine results and require models to answer questions by reasoning and reading these search snippets.
• NarrativeQA is a recent benchmark proposed for story-based reading comprehension.
Different from many MC datasets, the answers are handwritten by human annotators.
MCQ datasets are evaluated using the standard accuracy metric. For RACE, we train models on the entire dataset, i.e., both RACE-M and RACE-H and evaluate separately. For RACE, the model selection is based on each subset's respective development set. For SearchQA, we follow which evaluates unigram exact match (EM) and n-gram F1 scores. For NarrativeQA, since the Model RACE-M RACE-H RACE Time Sliding Window 37.3 30.4 32.2 N/A Stanford AR 44.2 43.0 43.3 N/A GA 43.7 44.2 44.1 N/A ElimiNet N/A N/A 44.5 N/A Dynamic Fusion Network 51.5 45.7 47.4 ≈8 hours (1 week * ) BiAttention 50.6 44.0 44.9 3 min (9 hours) BiAttention 48.5 42.1 44.0 16 min (2 days) BiAttention (250d LSTM) 50.3 40.9 43.6 18 min (2 days) BiAttention (250d Sim. MRU) 57.7 47.4 50.4 4 min (12 hours) BiAttention (250d MRU) 56.1 47.5 50.0 12 min (20 hours) GA + ElimiNet N/A N/A 47.2 N/A DFN Ensemble (x9) 55.6 49.4
section: N/A BiAttention (MRU) Ensemble (x9)
60.2 50.3 53.3 N/A: Comparison against other published models on RACE dataset. Competitor result are reported from. Best result for each category (single and ensemble) is in boldface. Last column reports estimated training time per epoch and total time for convergence. * estimated values that we obtain from asking the authors.
answers are human written and not constrained to spans in the passage, the evaluation metrics are Bleu-1, Bleu-4, Meteor and Rouge-L following.
section: Competitor Methods
We describe the key competitors on each dataset.
• RACE -the key competitors are the Stanford Attention Reader (Stanford AR), Gated Attention Reader (GA), and Dynamic Fusion Networks (DFN). GA incorporates a multi-hop attention mechanism that helps to refine the answer representations. DFN is an extremely complex model. It uses BiMPM's matching functions for extensive matching between Q, P and A, multi-hop reasoning powered by ReasoNet and employs reinforcement learning techniques for dynamic strategy selection.
• SearchQA -the main competitor baseline is the AMANDA model proposed by.
AMANDA uses a multi-factor self-attention module, along with a question focused span prediction. AMANDA also uses BiLSTM layers for input encoding and at the span prediction layers. We also compare against the reported ASR baselines which was reported in.
• NarrativeQA -On the NarrativeQA benchmark, we compare with the reported baselines in. We compete on the summaries setting, in which the baselines area context-less sequence to sequence (seq2seq) model, ASR and BiDAF.
section: Our Methods
Across our experiments, we benchmark several variants of our proposed MRU. The first is denoted as Sim. MRU which corresponds to the Simple MRU model described earlier. The model denoted by MRU (without any prefix) corresponds to the recurrent MRU model. Finally, the final variant is the MRU-LSTM which places a MRU encoder layer on top of a BiLSTM layer. We report the dimensions of the encoder as well as training time (per epoch) for each variant. The encompassing framework for MRU is the Bi-Attentive models described for MCQ-based problems and Span prediction problems. Unless stated otherwise, the encoder in the pointer layer for span prediction models also uses MRU. However, for the Hybrid MRU-LSTM models, answer pointer layers use BiLSTMs. For the RACEdataset, we additionally report scores of an ensemble of nine Sim. MRU models. This is to facilitate comparison against ensemble models of.
section: Implementation Details
We implement all models in TensorFlow. Word embeddings are initialized with 300d GloVe vectors and are not fine-tuned during training. Dropout rate is tuned amongst {0.1, 0.2, 0.3} on all layers including the embedding layer. For our MRU model, we use a range values of {1, 2, 4, 10, 25}.
section: Dev
section: Test
section: Model
Acc F1 Acc F1 Time TF-IDF max 13.0 N/A 12.7 N/A N/A ASR 43.9 24.2 41.3 22.8 N/A AMANDA 48.6 57.7 46.8 56.6 ≈8 * min Bi-Attention † 12.4 20.2 18.9 12.: Experimental Results on SearchQA dataset.. Unigram Accuracy and N-gram F1 are reported following. All models with † use the same encoder in the answer pointer layer. * are estimates running a replicated model with same batch size (b = 256) as our models.
MRU encoders are only applied on the passage and not the query. We adopt the Adam optimizer with a learning rate of 0.0003/0.001/0.001 for RACE/SearchQA/NarrativeQA respectively. The batch size is set to 64/256/32 accordingly. The maximum sequence lengths are 500/200/1100 respectively. For NarrativeQA, we use the Rouge-L score to find the best approximate answer relative to the human written answer for training the span model. All models are trained and all runtime benchmarks are based on a TitanXP GPU. reports our results on the RACE benchmark dataset. Our proposed MRU model achieves the best result for both single models and ensemble models. We outperform highly complex models such as DFN. We also pull ahead of other recent baselines such as ElimiNet and GA by at least 5%. The best single model score from RACE-H and RACE-M alternates between Sim. MRU and MRU. Overall, there is a 6% improvement on the RACE-H dataset and 1.8% improvement on the RACE-M dataset. Our Sim. MRU model also runs at 4 min per iteration, which is dramatically faster and simpler than DFN or other recurrent models. We believe that this finding highlights the importance of designing strong and fast baselines for the task at hand.
section: Experimental Results on RACE
In general, we also found that the usage of a recurrent cell is not really crucial on this dataset since (1) Sim. MRU and MRU can achieve comparable performance to each other, (2) GRU and LSTM models do not have a competitive edge and (3) Using no encoder already achieves comparable 1 performance to DFN. Finally, an ensemble of Sim. MRU models achieve state-of-the-art performance on the RACE dataset, achieving and overall score of 53.3%. reports our results on the SearchQA dataset. We draw the reader's attention to the performance of the 300d MRU encoder. We achieve the same accuracy as AMANDA without using any LSTM or GRU encoder. This model runs at 2 min per epoch, making it 4 times more efficient than AMANDA (estimated, with identical batch size). While, AMANDA also uses multi-factor self-attention, along with character enhanced representations, our simple MRU encoder used within a mere baseline bi-attentive framework comes close in performance. Finally, the hybrid combination, MRU-LSTM significantly outperforms AMANDA by 3%.
section: Experimental Results on SearchQA
Contrary to MCQ-based datasets, we found that Sim. MRU model could not achieve comparable results to the recurrent MRU. We hypothesize that this is due to the need to predict spans. Nevertheless, the 300d MRU outperforms an LSTM encoder and remain competitive to a BiLSTM of similar dimensionality. We also observe that LSTM and MRU are complementary. This is made evident by how stacking MRUs over LSTMs can give a performance boost relative to using each encoder separately.. reports our results on the NarrativeQA benchmark. First, we observe that 300d MRU can achieve comparable performance with BiDAF. When compared with a BiLSTM of equal output dimensions (150d), we find that our MRU model performs competitively, with less than 1% deprovement across all metrics. However, the time cost required is significantly reduced. The performance of our model is significantly better than 300d LSTM model while also being significantly faster. Here, we note that Sim. MRU does not produce reasonable results at all, which seems to be in similar vein to results on SearchQA, i.e., a recursive cell that processes word-by-word is mandatory for span prediction. However, our results show that it is not necessary to construct gates in a word-by-word fashion. Finally, the MRU-LSTM significantly outperforms all models, including BiDAF on this dataset. Performance improvement over the vanilla BiLSTM model ranges from 1% − 3% across all metrics, suggesting that MRU encoders are also effective as a complementary neural building block.
section: Experimental Results on NarrativeQA
section: Related Work
A diverse collection of MC datasets such as SQuAD and CNN/DailyMail are readily available for benchmarking new deep learning models. New datasets have been recently released, claiming to involve a greater need for going beyond simple surface-level matching. As such, these datasets often emphasize the extent of compositional and multi-sentence reasoning required to tackle its questions. In the recent years, a wide range of innovation solutions have also been proposed, mainly involving bi-attention and answer pointers. Recent work also investigates the notion of multi-hop reasoning, reinforcement learning and self-matching / self-attention. While many of these works use BiLSTMs are standard building blocks, recent work attempts a RNN-less model architecture by utilizing components inspired by the Transformer architecture. Our work is mainly concerned with designing an efficient encoder that is able to capture not only compositional information but also long-range and short-range information. More specifically, our recurrent MRU encoder takes on a similar architecture to Quasi-Recurrent Neural Networks and Simple Recurrent Units. A recent work, Cross Temporal Recurrent Networks extends QRNNs by fusing temporal gates across question-answer pairs. In these models, gates are pre-learned and then applied. However, different from existing models such as QRNNs that convolution layers as gates, we use a block-based contract-and-expand layers for learning gates. Finally, our model also draws inspiration from dilation, in particular dilated RNNs and dilated convolutions, that intuitively help to model long-range dependencies.
section: Conclusion and Future Work
We proposed a novel neural architecture, the MRU encoder and an overall bi-attentive model for both MCQ-based and span prediction MC tasks. We apply it to three MC datasets and achieve competitive performance on all without the use of recurrent layers. Our proposed method outperforms DFN, an extremely complex model, without using any LSTM or GRU layer. We also remain competitive to AMANDA and BiDAF without any LSTM/GRU. While our proposed encoder demonstrates promise on reasoning and understanding natural language, we believe that our encoder is generalizable to other domains beyond machine comprehension. However, we defer this prospect to future work.
