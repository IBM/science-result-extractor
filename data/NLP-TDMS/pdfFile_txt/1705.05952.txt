section: title
A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing
section: abstract
We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outper-forms the state-of-the-art neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in anew state of the art. Our code is open-source and available together with pre-trained models at: https://github.com/ datquocnguyen/jPTDP.
section: Introduction
Dependency parsing has become a key research topic in NLP in the last decade, boosted by the success of the shared tasks on multilingual dependency parsing). McDonald and Nivre (2011) identify two types of data-driven methodologies for dependency parsing: graph-based approaches and transition-based approaches. Most traditional graph-or transition-based parsing approaches manually define a set of core and combined features associated with one-hot representations (. Recent work shows that using deep learning in dependency parsing has obtained state-of-the-art performances. Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network-based classifiers. In addition, others propose novel neural architectures for parsing to handle feature-engineering (;.
Part-of-speech (POS) tags are essential features used inmost dependency parsers. In real-world parsing, those dependency parsers rely heavily on the use of automatically predicted POS tags, thus encountering error propagation problems., and show that parsing accuracies drop by 5+% when utilizing automatic POS tags instead of gold ones. Some attempts have been made to avoid using POS tags during dependency parsing , however, these approaches still additionally use the automatic POS tags to achieve the best accuracy. Alternatively, joint learning both POS tagging and dependency parsing has gained more attention because: i) more accurate POS tags could lead to improved parsing performance and ii) the the syntactic context of a parse tree could help resolve POS: Illustration of our jPTDP for joint POS tagging and graph-based dependency parsing.
ambiguities. In this paper, we propose a novel neural architecture for joint POS tagging and graph-based dependency parsing. Our model learns latent feature representations shared for both POS tagging and dependency parsing tasks by using BiLSTMthe bidirectional LSTM (). Not using any external resources such as pre-trained word embeddings, experimental results on 19 languages from the Universal Dependencies project show that: our joint model performs better than strong baselines and especially outperforms the neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, achieving anew state of the art.
section: Our joint model
In this section, we describe our new model for joint POS tagging and dependency parsing, which we call jPTDP. illustrates the architecture of our new model. We learn shared latent feature vectors representing word tokens in an input sentence by using BiLSTMs. Then these shared feature vectors are further used to make the prediction of POS tags as well as fed into a multi-layer perceptron with one hidden layer (MLP) to decode dependency arcs and another MLP to predict relation types for labeling the predicted arcs.
BiLSTM-based latent feature representations: Given an input sentence s consisting of n word tokens w 1 , w 2 , ..., w n , we represent each word w i in s by an embedding e (•) w i . and  show that character-based representations of words help improve POS tagging and dependency parsing performances. So, we also use a sequence BiLSTM (BiLSTM seq ) to compute a character-based vector representation for each word w i in s. For a word type w consisting of k characters w = c 1 c 2 ...c k , the input to the sequence BiLSTM consists of k character embeddings c 1:k in which each embedding vector c j represents the j th character c j in w; and the output is the character-based embedding e ( * ) w of the word type w, computed as:
For the i th word w i in the input sentence s, we create an input vector e i which is a concatenation (•) of the corresponding word embedding and character-based embedding vectors:
Then, we feed the sequence of input vectors e 1:n with an additional index i corresponding to a context position into another BiLSTM (BiLSTM ctx ), resulting in shared feature vectors vi representing the i th words w i in the sentence s:
POS tagging: Using shared BiLSTM-based latent feature vector representations, then we follow a common approach to compute the cross-entropy objective loss L POS ( ˆ t, t), in whichˆtwhichˆwhichˆt and tare the sequence of predicted POS tags and sequence of gold POS tags of words in the input sentence s, respectively).
Arc-factored graph-based parsing: Dependency trees can be formalized as directed graphs. An arc-factored parsing approach learns the scores of the arcs in a graph. Then, using an efficient decoding algorithm (in particular, we use the Eisner (1996)'s algorithm), we can find a maximum spanning tree-the highest scoring parse tree-of the graph from those arc scores:
where Y(s) is the set of all possible dependency trees for the input sentence s while score arc (h, m) measures the score of the arc between the head h th word and the modifier m th word in s. Following, we score an arc by using a MLP with one-node output layer (MLP arc ) on top of the BiLSTM ctx :
where v hand v mare the shared BiLSTM-based feature vectors representing the h th and m th words in s, respectively. We then compute a marginbased hinge loss L arc with loss-augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree.
Dependency relation types are predicted in a similar manner. We use another MLP on top of the BiLSTM ctx for predicting relation type of an head-modifier arc. Here, the number of the nodes in the output layer of this MLP (MLP rel ) is the number of relation types. Given an arc (h, m), we compute a corresponding output vector as:
Then, based on MLP output vectors v (h,m) , we also compute another margin-based hinge loss L rel for relation type prediction, using only the gold labeled parse tree.
Joint model training: The final training objective function of our joint model is the sum of the POS tagging loss L POS , the structure loss L arc and the relation labeling loss L rel . The model parameters, including word embeddings, character embeddings, two BiLSTMs and two MLPs, are learned to minimize the sum of the losses.
Discussion: Prior neural network-based joint models for POS tagging and dependency parsing are feed-forward network-and transition-based approaches (), while our model is a BiLSTM-and graphbased method. Our model can be considered as a two-component mixture of a tagging component and a parsing component. Here, the tagging component can be viewed as a simplified version without the additional auxiliary loss for rare words of the BiLSTM-based POS tagging model proposed by. The parsing component can be viewed as an extension of the graph-based dependency model proposed by, where we replace the input POS tag embeddings by the character-based representations of words.
section: Experiments
section: Experimental setup
Following Zhang and Weiss (2016) and, we conduct multilingual experiments on 19 languages from the Universal Dependencies (UD) treebanks 1 v1.2 (, using the universal POS tagset () instead of the language specific POS tagset. 2 For dependency parsing, the evaluation metric is the labeled attachment score (LAS). LAS is the percentage of words which are correctly assigned both dependency arc and relation type.
Method ar bg da de • en es eu • fa fi • fr hi id it iw nl no pl • pt sl • AVG 10.3 12.3 15.6 11.9 9.1 7.3 17.8 8.2 24.4 5.7 4.6 13.8 5.7 10.9 18.8 11.
section: Implementation details
Our jPTDP is implemented using DYNET v2.0 (Neubig et al., 2017). We optimize the objective function using Adam () with default DYNET parameter settings and no mini-batches. We use a fixed random seed, and we do not utilize pre-trained embeddings in any experiment. Following Kiperwasser and Goldberg (2016b) and, we apply a word dropout rate of 0.25 and Gaussian noise with σ = 0.2. For training, we run for 30 epochs, and evaluate the mixed accuracy of correctly assigning POS tag together with dependency arc and relation type on the development set after each training epoch. We perform a minimal grid search of hyper-parameters on English. We find that the highest mixed accuracy on the English develop-3 https://github.com/clab/dynet ment set is when using 64-dimensional character embeddings, 128-dimensional word embeddings, 128-dimensional BiLSTM states, 2 BiLSTM layers and 100 hidden nodes in MLPs with one hidden layer. We then apply those hyper-parameters to all 18 remaining languages. compares the POS tagging and dependency parsing results of our model jPTDP with results reported in prior work, using the same experimental setup.
section: Main results
Regarding POS tagging, our joint model jPTDP generally obtains similar POS tagging accuracies to the BiLSTM-aux model. Our model also achieves higher averaged POS tagging accuracy than the joint model Stackpropagation (. There are slightly higher tagging results obtained by BiLSTM-aux when utilizing pre-trained word embeddings for initialization, as presented in. However, fora fair comparison to both Stack-propagation and our jPTDP, we only compare to the results reported without using the pre-trained word embeddings.
In terms of dependency parsing, inmost cases, our model jPTDP outperforms Stack-propagation. It is somewhat unexpected that our model produces about 7% absolute lower LAS score than Stack-propagation on Dutch (nl). A possible reason is that the hyper-parameters we selected on English are not optimal for Dutch. Another reason is due to a large number of non-projective trees in Dutch test set (106/386 ≈ 27.5%), while we use the Eisner's decoding algorithm, producing only projective trees. Without taking "nl" into account, our averaged LAS score overall remaining languages is 1.1% absolute higher than Stack-propagation's.
One reason for our better LAS is probably because jPTDP uses character-based representations of words, while Stack-propagation uses feature representations for suffixes and prefixes which might not be as useful as character-based representations for capturing unknown words. The last row in shows an absolute LAS improvement of 4.4% on average when comparing our jPTDP with its simplified version of not using characterbased representations: specifically, morphologically rich languages get an averaged improvement of 9.3 %, vice versa 2.6% for others. So, our jPDTP is particularly good for morphologically rich languages, with 1.7% higher averaged LAS than Stack-propagation over these languages.
section: MQuni at the CoNLL 2017 shared task
Our team MQuni participated with jPTDP in the CoNLL 2017 shared task on multilingual parsing from raw text to universal dependencies (. Training data are 60+ universal dependency treebanks for 40+ languages from UD v2.0 (). We do not use any external resource, and we use a fixed random seed and a fixed set of hyper-parameters as presented in Section 3.2 for all treebanks. For each treebank, we train a joint model for universal POS tagging and dependency parsing. We evaluate the mixed accuracy on the development set after each training epoch, and select the model with the highest mixed accuracy. Note that for each "surprise" language where there are only few sample sentences with gold-standard annotation or a "small" treebank whose development set is not available, we simply split its sample or training set into two parts with a ratio 4:1, and then use the larger part for training and the smaller part for development.
For parsing from raw text to universal dependencies, we utilize CoNLL-U test files preprocessed by the baseline UDPipe 1.1 (. These pre-processed CoNLL-U test files are available to all participants who do not want to train their own models for any steps preceding the dependency analysis, including: tokenization, word segmentation, sentence segmentation, POS tagging and morphological analysis. Note that we only employ the tokenization, word and sentence segmentation, and we do not care about the POS tagging and morphological analysis pre-processed by UDPipe 1.1. Recall that we perform universal POS tagging and dependency parsing jointly. In addition, when we encounter an additional parallel test set in a language where multiple training treebanks exist, i.e. a parallel test set marked with language code suffix " pud" such as "ar pud", "cs pud" and "de pud", we simply use the model trained for its corresponding language code prefix, e.g., "ar", "cs" and "de". presents our official parsing results from the CoNLL 2017 shared task on UD parsing (. We obtain 1% absolute higher averaged scores than the baseline UDPipe 1.1 () in both categories: big treebank test sets (denoted as Big in) and parallel test sets (denoted as PUD in). Specifically, we obtain a highest rank at 8 th place for the PUD category, showing that our parsing model jPTDP is particularly good when it is applied to areal practical application in outof-domain data. Unlike the baseline UDPipe 1.1 and others, for each surprise language, we simply  train a joint model just on the sample data of few sentences with gold-standard annotation provided before the test phase, i.e., we utilize neither external resources nor a cross-lingual technique nor a delexicalized parser. So, it is not surprising that we obtain a very low averaged score over the 4 surprise language test sets. When the 4 surprise language test sets are not taken into account, we obtain a rank in top-10 participating systems. In fact, it is hard to make a clear comparison between our jPTDP and the parsing models used in other top participating systems. This is because other systems use various external resources and/or better pre-processing modules and/or construct ensemble models for dependency parsing. For example, UDPipe 1.2 only extends the word and sentence segmenters of the baseline UDPipe 1.1. Consequently, UDPipe 1.2 obtains 0.1% absolute higher in the macro-averaged word segmentation score 8 and 0.2% higher in the macro- Combining multiple treebanks available fora language or similar languages to obtain larger training data is also considered as a manner of exploiting external data.
8 Word segmentation results are available at: averaged sentence segmentation score 9 than the baseline UDPipe 1.1, resulting in 1+% better in the macro-averaged LAS F1 score though they use exactly the same parsing model. See for an overview of the methods, algorithms, resources and software used for all other participating systems. It is worth noting that for universal POS tagging, we obtain a highest rank at 4 th place for the Big category (i.e., 4 th on average over 55 big treebank test sets). In this Big category, we also obtain better rank than both UDPipe 1.2 and 1.1.
section: Conclusion
In this paper, we describe our novel model for joint POS tagging and graph-based dependency parsing, using bidirectional LSTM-based feature representations. Experiments on 19 languages from the Universal Dependencies (UD) v1.2 show that our model obtains state-of-the-art results in both POS tagging and dependency parsing.
With our joint model, we participated in the CoNLL 2017 shared task on UD parsing (Zeman et al., 2017). Given that we followed a strict closed setting while other top participating systems did not, we still obtained very competitive results. So, we believe our joint model can serve as anew strong baseline for further models in both POS tagging and dependency parsing tasks.
For future comparison, we provide in Table 3 the POS tagging, UAS and LAS accuracies with respect to gold-standard segmentation on the UD v2.0-CoNLL 2017 shared task test sets (
