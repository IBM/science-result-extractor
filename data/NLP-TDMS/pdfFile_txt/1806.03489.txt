section: title
Robust Lexical Features for Improved Neural Network Named-Entity Recognition
section: abstract
Neural network approaches to Named-Entity Recognition reduce the need for carefully hand-crafted features. While some features do remain in state-of-the-art systems, lexical features have been mostly discarded, with the exception of gazetteers. In this work, we show that this is unfair: lexical features are actually quite useful. We propose to embed words and entity types into a low-dimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia. From this, we compute-offline-a feature vector representing each word. When used with a vanilla recurrent neural network model, this representation yields substantial improvements. We establish anew state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance with a F1 score of 91.73 on the over-studied CONLL-2003 dataset.
section: Introduction
Named-Entity Recognition (NER) is the task of identifying textual mentions and classifying them into a predefined set of types. Various approaches have been proposed to tackle the task, from hand-crafted feature-based machine learning models like conditional random fields () and perceptron (, to deep neural models.
Word representations (, also known as word embeddings, area key element for multiple NLP tasks including NER). Due to the small amount of named-entity annotated data, embeddings are used to extend, rather than replace, hand-crafted features in order to obtain state-of-the-art performance (. Recent studies ( have explored methods for supplying deep sequential taggers with complementary features to standard embeddings. and tested special embeddings extracted from a neural language model (LM) trained on a large corpus. LM embeddings capture context-dependent aspects of word meaning using future (forward LM) and previous (backward LM) context words. When this information is added to standard features, it leads to significant improvements in NER. Also, showed that external knowledge resources (namely gazetteers) are crucial to NER performance. Gazetteer features encode the presence of word n-grams in predefined lists of NEs.
In this work, we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system. Ina nutshell, we embed words and entity types into a joint vector space by leveraging WiFiNE (, a ressource which automatically annotates mentions in Wikipedia with 120 entity types. From this vector space, we compute for each word a 120-dimensional vector, where each dimension encodes the similarity of the word with an entity type. We call this vector an LS representation, for Lexical Similarity. When included in a vanilla LSTM-CRF NER model, LS representations lead to significant gains. We establish anew state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance on the over-studied In the rest of this paper, we motivate our work in Section 2. We describe how we compute LS vectors in Section 3. We present our system in Section 4 and report results in Section 5. In Section 6, we discuss related works before concluding in Section 7.
section: Motivation
Gazetteers are lists of entities that are associated with specific NE categories. They are widely used as a feature source in NER, and have been successfully included in feature-based and neural (Chiu and Nichols, 2016) models. Typically, lists of entities are compiled from structured data sources such as) or Freebase (. The surface form of the title of a Wikipedia article, as well as aliases and redirects are mapped to an entity type using the object type attribute of the related DBpedia (or Freebase) page. use this methodology to compile 30 lists of fine-grained entity types extracted from Wikipedia, while Chiu and Nichols (2016) create 4 gazetteers that map to CoNLL categories (PER, LOC, ORG and MISC). Despite their importance, gazetteer-based features suffer from a number of limitations.
• Binary representation. Gazetteer features encode only the presence of an n-gram in each list and omit its relative frequency. For example, the word "France" can be used as a person, an organization, or a location, while it likely refers to the country most of the time. Binary features cannot capture this preference.
• Generation. At test time, we need to match every n-gram (up to the length of the longest lexicon entry) in a sentence against entries in the lexicons, which is time consuming. In their work, Chiu and Nichols (2016) use 4 lists that count over 2.3M entries.
• Non-entity words. Gazetteer features do not capture signal from non-entity words, while earlier feature-based models strived to encode that some words (or n-grams) trigger specific entity types. For instance, words such as "eat", "directed" or "born" are words that typically appear after a mention of type PER.
To overcome those limitations, we propose an alternative approach where we embed annotations mined from Wikipedia into a vector space from which we compute a feature vector that represent words. This vector compactly and efficiently encodes both gazetteer and lexical information. Note that attest time, we only have to feed our model with this feature vector, which is efficient.
section: Our Method
section: Embedding Words and Entity Types
Turning Wikipedia into a corpus of named-entities annotated with types is a task that received continuous attention over the years (. It consists mainly in exploiting the hyperlink structure of Wikipedia in order to detect entity mentions. Then, structured data from a knowledge base (for instance Freebase) are used to map hyperlinks to entity types. Because the number of anchored strings in Wikipedia is no more than 3% of the text tokens, proposed to augment Wikipedia articles with mentions unmarked in Wikipedia, thanks to a mix of heuristics that benefit the Wikipedia structure (, as well as a coreference resolution system adapted specifically to Wikipedia.
The authors applied their approach on English Wikipedia and produce coarse (4 classes) and finegrained (120 labels) named-entity annotations, leading to WiNER ( and. In this work, we adopt WiFiNE which is publicly available at http://rali.iro.umontreal.ca/rali/en/wifiner-wikipedia-for-et as our source of annotations. Each entity mention is mapped (via its Freebase object type attribute) to a pre-defined set of 120 entity types. Types are stored in a 2-level hierarchical structure (e.g. /person and /person/musician). The corpus consist of 3.2M Wikipedia articles, comprising 1.3G tokens that we annotated with 157.4M named-entity mentions and their types. We used this very large quantity of automatically annotated data for jointly embedding words and entity types into the same low-dimensional space. The key idea consists in learning an embedding for each entity type using its surrounding words. For instance, the embedding for /product/software will be trained using context words that surround all entities that were (automatically) labelled as /product/software in Wikipedia. In practice, we found that simply concatenating a sentence (v1) with its annotated version (v2), as illustrated in, offers a simple but efficient way of combining words and entity types so that embeddings can make good use of them.  We use the FastText toolkit () to learn the uncased embeddings for both words and entity types. We train a skipgram model to learn 100-dimensional vectors with a minimum word frequency cutoff of 5, and a window size of 5. This configuration (recommended by the authors) performs the best in the experiments described in Section 5. Since FastText learns representations of character ngrams, it has the ability to produce vectors for unknown words.   ) two-dimensional projection of the embedding of 6 entity types and a sample of 1500 words. Entity type embeddings are marked by big Xs, while circles indicate words. For visualization proposes, we only plot single-word mentions that were annotated in WiFiNE with one of those 6 types. Words were randomly and proportionally sampled according to the frequency of each entity type. In addition, words have the color associated with the most frequent type they were annotated within WiFiNE.
We observe that mentions often annotated by a given type in our resource tend to cluster around this entity type. For instance, "firefox" is close to the type /product/software, while "enzyme" is close to the /biology entity type. We also notice that words that are labelled with different types tend to appear between types they were annotated with. For instance, "gpx2", which is used both as a software and as a gene, has its embedding in between /product/software and /biology. We inspected some of the words plotted in, and found that "jrun" and "xp" are incorrectly labelled as /product/weapon in WiFiNE. But since these words are seen in a software context, their embeddings are closer to the /product/software embedding than the /product/weapon one. We feel this tolerance to noise is a desirable feature, one that hopefully allows a more efficient use of distant supervision. Last, we also observe the tendency of rare words to cluster around their entity type. For instance, "iota" and "x.org" are embedded near their respective types, despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE.
section: LS Representation
This joint vector space only serves the purpose of associating to each word a LS representation, that is, a 120-dimensional vector where the ith coefficient is a value in the [−1, +1] interval, equal to the cosine similarity 1 between the word embedding and the embedding of the ith entity type (we have 120 types).
section: Word
Entity. shows the topmost similar entity types for proper names (left column) and common words (right column). We observe that ambiguous mentions (those annotated with several types) are adequately handled. For instance, the LS representation of the word "hilton" encodes that it more often refers to a hotel or a restaurant than to an actress. Also, we observe that entity words that are either not or rarely annotated in WiFiNE are still adequately associated with their right type. For instance, "dammstadt", which appears only 5 times in WiFiNE, and which refers to the Damm city in Germany, is most similar to /location/city and /location/railway. Interestingly, this mention does not have its page in English Wikipedia. Furthermore, we observe that non-entity context words have a strong similarity to types they precede or succeed. For instance the verb "directed" is very close to /person/director, an entity type that usually precedes it, and to /art/film, that usually follows it. Likewise, the preposition "in" is near /date and /location/city, which frequently follow "in".
section: Strength of the LS Representation
To summarize, we propose a compact lexical representation which is computed offline, therefore incurring no computation burden attest time This representation encodes the preference of an entity-mention word fora given type, an information out of reach of binary gazetteer features. It also lends itself nicely to the inclusion of lexical features that have been successfully used in earlier feature-based systems. Also, because entity types are well represented in WiFiNE, their embeddings are robust: Our representation does accommodate unfrequent words and seems tolerant to the inherent noise of distant supervision.
section: Our NER System
In order to test the efficiency of our lexical feature representation, we implemented a state-of-the-art NER system we now describe.
section: Bi-LSTM-CRF Model
We adopt the popular Bi-LSTM-CRF architecture, a de facto baseline in many sequential tagging tasks (. Character representation of the word "Roma" given to the word-level bi-LSTM.
section: Features
In addition to the LS vector, we incorporate publicly available pre-trained embeddings, as well as character-level, and capitalization features. Those features have been shown to be crucial for stateof-the-art performance.
section: Word Embeddings
We experimented with several publicly available word embeddings, such as Senna (Collobert et al., 2011), Word2Vec (Mikolov et al., 2013), GloVe (), and SSKIP (. We find that the latter performs the best in our experiments. SSKIP embeddings are 100-dimensional case sensitive vectors that where trained using a n-skip-gram model () on 42B tokens. These embeddings were previously used by, who report good performance on CONLL, and state-of-the-art results on ONTONOTES respectively. Note that these pre-trained embeddings are adjusted during training.
section: Character Embeddings
Following (, we use a forward and a backward LSTM to derive a representation of each word from its characters (right part of. A character lookup table is randomly initialized, then trained at the same time as the Bi-LSTM model sketched in Section 4.1.
section: Capitalization Features
Similarly to previous works, we use capitalization features for characterizing certain categories of capitalization patterns: allUpper, allLower, upperFirst, upperNotFirst , numeric or noAlphaNum. We define a random lookup table for these features, and learn its parameters during training.
section: LS Vectors
Contrarily to previous features, lexical vectors are computed offline and are not adjusted during training. We found useful in practice to apply a MinMax scaler in the range [−1, +1] to each LS vector we computed; thus, [.., 0.095, .., 0.20, .., 0.76, ..] becomes [.., −1, .., −0.67, .., 1, ..].
section: Experiments
section: Data and Evaluation
We consider two well-established NER benchmarks: CONLL-2003 and ONTONOTES 5.0. provides an overview of the two datasets. As we can see, ONTONOTES is much larger. For both datasets, we convert the IOB encoding to BILOU, since found the latter to perform better. In keeping with others, we report mention-level F1 score using the conlleval script 2 .
The. This dataset is annotated with 18 entity types, and is much larger than CONLL. Following previous researches (, we use the official train/dev/test split of the CoNLL-2012 shared task (). Also, we exclude (both during training and testing) the New Testaments portion as it does not contain gold NE annotations.
section: Training and Implementation
Training is carried out by mini-batch stochastic gradient descent (SGD) with a momentum of 0.9 and a gradient clipping of 5.0. The mini-batch is 10 for both datasets, and learning rates are 0.009 and 0.013 for CONLL and ONTONOTES respectively. More sophisticated optimization algorithms such as AdaDelta or Adam () converge faster, but none outperformed SGD with exponential learning rate decay in our experiments.
Our system uses a single Bi-LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively. For both models, the character embedding size was set to 25, and the hidden dimension of the forward and backward character LSTMs are set to 50. To mitigate overfitting, we apply a dropout mask () with a probability of 0.5 on the input and output vectors of the Bi-LSTM layer. For both datasets, we set the dimension of capitalization embeddings to 25 and trained the models up to 50 epochs.
We tuned the hyper-parameters by grid search, and used early stopping based on the performance on the development set. We varied dropout), hidden units (), capitalization () and char () embedding dimensions, learning rate ([0.001, 0.015] by step 0.002), and optimization algorithms and fixed the other hyper-parameters. We implemented our system using the) library, and ran our models on a GeForce GTX TITAN Xp GPU. Training requires about 2.5 hours for CONLL and 8 hours for ONTONOTES. shows the development set performance of our final models on each dataset compared to the work of. The authors use an architecture similar to ours, but use a binary gazetteer feature set, while we use our LS representation. Since our systems involve random initialization, we report the mean as well as the standard deviation over five runs. The improvements yielded by our model on the CONLL dataset are significant although modest, while those observed on ONTONOTES are more substantial. We also observe a lower variance of our system over the 5 runs.
section: Results on the Development Set
section: CONLL
ONTONOTES (Chiu and Nichols, 2016) 94.03 (± 0.23) 84.57 Our model 94.80 (± 0.10) 86.44 (± 0.14): Development set F1 scores of our best hyper-parameter setting compared to the results reported in (.  First, we observe that our model significantly outperforms models that use extensive sets of handcrafted features as well as the system of () that uses NE and Entity Linking annotations to jointly optimize the performance on both tasks. Second, our model outperforms as well other NN models that only use standard word embeddings, which indicates that our lexical feature vector is complementary to standard word embeddings. Third, our system matches state-of-the-art performances of models that use either more complex architectures or more elaborate features. use three layers of stacked residual RNN (Bi-LSTM) with bias decoding. Our model is much simpler and faster. They report a performance of 90.43 when using an architecture similar to ours. The two systems that have slightly higher F1 scores on the CONLL dataset both use embeddings obtained from a forward and a backward Language Model trained on the One Billion Word Benchmark (. They report gains between 0.8 and 1.2 points by using such LM embeddings, which suggests that LS vectors are indeed efficient. Unfortunately, due to time and resource constraints, we were notable to measure whether both features complement each other. This is left for future investigations. reports the F1 score of our system compared to the performance reported by others on the ONTONOTES test set. To the best of our knowledge, we surpass previously reported F1 scores on this dataset. In particular, our system significantly outperforms the Bi-LSTM-CNN-CRF models of (Chiu and Nichols, 2016) and ( by an absolute gain of 1.68 and 0.96 points respectively. Less surprisingly, it surpasses systems with hand-crafted features, including that use gazetteers, and the system of which uses coreference annotation in ONTONOTES to jointly model NER, entity linking, and coreference resolution tasks.
section: Results on CONLL
section: Results on ONTONOTES
section: Model
LEX GAZ CAP EMB CHE LME LS F1 ( for an explanation of the column of features.
The ONTONOTES benchmark is annotated with 18 types (e.g. LAW, PRODUCT) and contains many rare words, especially in the Web data collection. note that the 4-class gazetteer they used yielded marginal improvements on ONTONOTES, contrarily to CONLL. In particular, they observe that mentions that match LOC entries in their gazetteer often match GPE, NORP and FAC lists.). BC = broadcast conversation, BN = broadcast news, MZ = magazine, NW = newswire, TC = telephone conversation, WB = blogs and newsgroups.
section: Model
They suggest that a finer-grained gazetteer could improve the performance of their system on ONTONOTES. Our results confirm this, since we use 120 types. We further detail the gains we observed for each sub-collection of texts in the test set. reveals that major improvements over the model of) are on noisier collections such as telephone conversations (+3 points) and blogs or newsgroups (+2 points). Those type of texts are characterized by a large set of infrequent words, for which classical embeddings are typically poorly trained. Our approach does not seem to suffer from this problem as severely, as discussed in Section 2.
section: Ablation Results
In this experiment, we directly compare the LS representation with the SSKIP word-embedding feature set. In order to maintain a high level of performance, both character and capitalization features are used in all configurations. We want to point out that LS vectors are not adapted during training, contrarily to the SSKIP embeddings. Similarly to Section 5.3, we report in, for each feature configuration, the average F1 score as well as the standard deviation over five runs.
Model CONLL ONTONOTES SSKIP 90.52 (± 0.18) 86.57 (± 0.10) LS 89.94 (± 0.16) 85.92 (± 0.12) all 91.73 (± 0.10) 87.95 (± 0.13) We observe that on both CONLL and ONTONOTES, the SSKIP model outperforms our feature vector approach by 0.65 F1 points on average. The difference is not has high as we first expected, especially since the SSKIP model is adjusted during training, while our representation is not. Still, LS vectors seem to encode a large portion of the information needed to model the NER task. Also, it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP.
We also observe that models that use both feature sets significantly outperform other configurations. To confirm that the gains came from our feature vector and not from increasing the number of hidden units, we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors. We observed a degradation of performance on both datasets, mostly due to overfitting on the training set. From those results, we conclude that our lexical representation and the SSKIP one are complementary.
section: Related Works
Traditional approaches to NER, like CRF-based () and Perceptron-based systems) have dominated the field for over a decade. They rely heavily on hand-engineered features () and external resources such as gazetteers. One major drawback of such an approach is its weak generalization power (. Current state-of-the art systems () use a combination of Convolutional Neural Networks (CNNs), Bi-LSTMs, along with a CRF decoder. CNNs are used to encode character-level features, while LSTM is used to encode word-level features. Finally, a CRF is placed on top of those models in order to decode the best tag sequence. Pre-trained embeddings obtained by unsupervised learning are core features of those models. In this work, we show that deep NN architectures can also benefit from lexical features, at least when encoded in the compact form we propose. and propose an alternative approach different from ours. They incorporate LM embeddings that were pre-trained on a large unlabelled corpus as features for NER. These embeddings allow to generate a representation fora word depending on its context. For instance, the LM embeddings of the word France in "France is a developed country" is different than that in "Anatole France began his literary career". Such embeddings are trained on very large amount of texts.
Our feature set is crafted from distant supervision applied to Wikipedia, a much less time-consuming process which we showed to be nevertheless adapted to rare words. used gazetteer features in order to establish state-of-the-art performance on both CONLL and ONTONOTES. They mined DBPedia in order to compile 4 lists of named-entities that contain over 2.3M entries. We show that LS representations outperform their gazetteer features.
section: Conclusion and Future Work
We have explored the idea of generating lexical features for NER out of Wikipedia data automatically annotated with fine-grained entity types. We used WiFiNE (, a Wikipedia dump annotated with fine entity type mentions, for training a vector space that jointly embeds words and named-entities. This vector space is used to compute a 120 dimensional vector per word, which encodes the similarity of the word to each of the entity types. Our results show that our proposed lexical representation, even though it is not adjusted at training time, matches state-of-the-art results compared to more complex approaches on the well-studied CONLL dataset, and delivers anew state-of-the-art F1 score of 87.95 on the more diversified ONTONOTES dataset. We further observe larger gains on collections with more unfrequent words.
The source code and the data we used in this work are publicly available at http://rali.iro. umontreal.ca/rali/en/wikipedia-lex-sim, with the hope that other researchers will report gains, when using our lexical representation. As a future work, we want to investigate the usefulness of our LS feature representation on other NER tasks, including NER in tweets where out-of-vocabulary and low-frequency words represent a challenge; as well as finer-grained NER which suffers from the lack of manually annotated training data.
