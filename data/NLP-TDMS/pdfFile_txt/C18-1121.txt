section: title
Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization
section: abstract
In this paper, we investigate the sentence summarization task that produces a summary from a source sentence. Neural sequence-to-sequence models have gained considerable success for this task, while most existing approaches only focus on improving word overlap between the generated summary and the reference, which ignore the correctness, i.e., the summary should not contain error messages with respect to the source sentence. We argue that correctness is an essential requirement for summarization systems. Considering a correct summary is semantically entailed by the source sentence, we incorporate entailment knowledge into abstractive summarization models. We propose an entailment-aware encoder under multi-task framework (i.e., summarization generation and entailment recognition) and an entailment-aware decoder by entailment Reward Augmented Maximum Likelihood (RAML) training. Experimental results demonstrate that our models significantly outperform baselines from the aspects of informative-ness and correctness.
section: Introduction
Sentence summarization is a well-studied task that creates a condensed version of along source sentence. Sequence-to-sequence (seq2seq) model that encodes a source sequence into a latent representation and outputs another sequence is the dominating framework for sentence summarization. Despite substantial improvements on this task, most of the existing researches typically aim to improve word overlap between the generated summary and the references, which is measured by n-gram matching metrics (e.g., ROUGE). Hence, it cannot guarantee the semantic correctness of the summary as a whole. Therefore, in some cases, the summary giving high matching scores may contain critical error messages, which makes the summary fail to capture the correct information with respect to the source sentence. Previous study shows that about 30% of the summaries generated by state-of-the-art seq2seq system are subject to this problem (. Here is an example (the digits are replaced by "#"):
Source sentence: franch won the gold medal at women 's epee team event of the fie #### world championships by beating china ##-## .
Reference: france beats china for women 's epee team gold State-of-the-art seq2seq model: canada wins women 's epee team event
For the example shown above, the seq2seq system produces a fluent summary which contains an obvious mistake. The true winner of the "women 's epee team event" is "france", while the summarization model wrongly generates "canada", which is probably due to similar word representations for country names. Though the word overlap between the generated summary and the reference is considerable, leading to high ROUGE scores, the summary is invalid.
We argue that correctness is an essential requirement for summarization systems, while most existing systems ignore it. Generally, a correct summary is semantically entailed by the source sentence, thus we believe entailment) knowledge is beneficial to avoid producing contradictory or unrelated information in the summary.
To incorporate entailment knowledge into abstractive summarization models, we propose in this work an entailment-aware encoder and an entailment-aware decoder. We share the encoder of the summarization generation system with the entailment recognition system, so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships. Furthermore, we propose an entailment Reward Augmented Maximum Likelihood (RAML) ( training that encourages the decoder of the summarization system to produce summary entailed by the source. Experimental results demonstrate that our models significantly outperform some solid baselines on objective evaluation for informativeness and manual evaluation for correctness. Further analysis suggests that our summarization model is aware of entailment knowledge.
Our main contributions are as follows:
• We incorporate entailment knowledge into summarization models to avoid producing unrelated information with respect to the source sentence.
• We propose an entailment-aware encoder by jointly modeling summarization generation and entailment recognition.
• We introduce an entailment-aware decoder via entailment RAML training.
section: Background: Seq2seq Learning
In this section, we describe the basic seq2seq learning framework. Given a dataset of input-output pairs,
, the seq2seq model maximizes the conditional probability of a target sequence y * : p(y * |x). Recurrent Neural Networks (RNN) encoder () reads and converts a variablelength input sequence x into a context representation c as follows:
where ht ∈ Rn is a hidden state at time t, and ct is a context vector generated from the sequence of the hidden states. f enc and f care nonlinear activation functions. The decoder generates wordy t given the context vector ct and the previously generated words:
where st is the hidden state of the decoder and f dec is a nonlinear activation function. The maximum likelihood (ML) framework tries to minimize negative log-likelihood of the parameters as follows:
3 Our Proposed Model
section: Overview
In order to avoid generating unrelated summary with respect to the source sentence, we propose two strategies to incorporate entailment knowledge into seq2seq summarization model. We first introduce an entailment-aware encoder using multi-task learning for summarization generation and entailment recognition. Then, we introduce an entailment-aware decoder by entailment RAML training.: The framework of our model. Entailment-aware encoder is learned by jointly training summarization generation (left part of (a), which is a seq2seq model) and entailment recognition (right part of (a), in which sentence pair in the entailment recognition corpus are encoded as u and v). Entailmentaware decoder is learned by entailment RAML training, in which the summary will be rewarded if it is entailed by the source sentence.
section: Entailment-aware Encoder
In this section, we propose a multi-task learning for abstractive summarization by sharing the encoder with the task of entailment recognition. By doing so, we can learn an entailment-aware encoder for sentence summarization task. In this way, we can improve the correctness aspect of the summarization model, while maintaining the salient information extraction aspects. Note that the training data for summarization and entailment task is from summarization and entailment corpus, respectively.
section: Shared Sentence Encoder
Given a source sentence x = (x 1 , · · · , x n ), we employ a bidirectional LSTM (BiLSTM) to build its hidden representation (h 1 , · · · , h n ). The BiLSTM encodes source sentence forwardly and backwardly to generate two sequences of the hidden states:
The final sentence representation hi is the concatenation of the forward and backward vectors:
section: Attention-based Summarization Decoder
At each time step t, the state of the decoder st is calculated as follows:
We compute the context vector ct as a weighted sum of the source annotations as follows:
where each vector is weighted by the attention weight α t,i , as calculated in Equations 10 and 11:
The probability for the next target wordy t is computed using hidden state st and the previously emitted wordy t−1 as follows:
where
and L y are model parameters. The summarization model is trained by minimizing negative log-likelihood loss as in Equation 4.
section: Matching-based Entailment Inference Model
To infer entailment relation, input sentence pairs from the entailment recognition corpus are fed into sentence encoder to obtain hidden representation
, respectively. Next, the absolute difference and the element-wise product for the tuple are concatenated with the original vectors u and v ( as follows:
We then feed q into a 3-layer multilayer perceptron (MLP) classifier. The 3-class softmax output layer is on top of MLP. The entailment recognition model is trained by minimizing cross-entropy loss.
section: Multi-Task Learning (MTL)
In our multi-task setup, we share the encoder parameters of both the tasks, as shown in(a). Traditional MTL considers equal contribution for all tasks. In our model, two tasks are significantly different. The summary generation task is much more complicated than entailment recognition, leading to different learning difficulties and convergence rates. Therefore, summarization generation is regarded as the main task and entailment recognition as the auxiliary task, and our goal is to optimize the main task with assistance of auxiliary task. To this end, we optimize the two loss functions alternatively during training. Let α be the number of mini-batches of training for entailment recognition after 100 mini-batches of training for summarization generation (. We adopt α = 10 and performance with different α is discussed in Section 6.6.3.
section: Entailment-aware Decoder
In order to encourage the decoder of the summarization system to produce summary entailed by the source sentence, we apply an entailment-aware decoder by entailment RAML training (Norouzi et al., 2016).
section: Reward Augmented Maximum Likelihood (RAML) Training
RAML provides a computationally efficient approach to optimize task-specific reward (loss) directly. In our work, we apply RAML to incorporate entailment-based reward into our summarization model, as shown in.
The RAML objective function is defined as follows:
q
where Y is the set of possible model outputs. r(x, y, y * ) denotes the reward function and τ is the regularization parameter.
section: Optimizing by Entailment-based Sampling
We can express the gradient of L RAML in terms of an expectation over samples from q (y|x, y * ; τ ):
RAML training adds a sampling step over typical ML objective. Instead of optimizing ML on training samples, given training input (x, y * ), RAML training first samples an output y proportionally to the reward. Then, RAML optimizes log-likelihood on such sample given the corresponding input. Thus, we need to sample auxiliary outputs from the exponentiated payoff distribution, q (y|x, y * ; τ ). In this work, we first use reward values defined by negative Hamming distance and then re-weight the reward based on entailment reward s(x, y, y * ). Particularly, given a sentence y * of length , we count the number of sentences within an edit distance d, where d ∈ {0, . . . , 2}. Then, we weight the counts by exp{−d/τ } and perform normalization. Finally, we apply importance sampling by the weight exp{(s(x, y, y * ) + d)/τ } and perform normalization, where the proposal distribution is Hamming distance sampling 2 .
We define entailment reward s(x, y, y * ) as follows:
where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of
section: Related work
Text summarization methods can be categorized into extraction-based methods ( solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descriptions. To the best of our knowledge, our work is the first to directly explore the correctness of summary without any preprocessing. Some previous work) has used textual entailment recognition to reduce redundancy for extractive summarization task. Our work is partially inspired by the models of with following differences: Pasunuru et al. (2017) model the entailment task as the seq2seq generation problem and enforce sharing of the same decoder between summarization and entailment. However, the entailment task is more reasonable to be considered as a multi-label classification problem rather than a generation problem. We thus design a multi-task learning framework in which the summarization generation task shares the same encoder with the entailment recognition task.
section: Dataset
We conduct experiments on English Gigaword and DUC 2004 datasets.
Gigaword Corpus. We use the annotated Gigaword corpus provided by. The dataset has about 3.8 million training pairs. Following , we use 8, 000 pairs as validation set and the test samples provided by and  as our test sets.
DUC 2004 Corpus. DUC-2004 corpus for tasks consists of 500 documents. Each document in these datasets has four human annotated summaries. For experiments on this corpus, we directly use the model trained on the Gigaword to test on the DUC 2004 corpus.
section: Experiment
section: Experimental Settings
Word embedding size is set to 300 and LSTM hidden state size is set to 512. We use the full source and target vocabularies collected from the training data, which have 119, 505 and 68, 885 words, respectively. Adam () optimizer is applied with the learning rate of 0.001, momentum parameters β 1 = 0.9 and β 1 = 0.999, and = 10 −8 . For RAML, τ = 0.85. The mini-batch size is set to 64. We test the model performance (ROUGE-2 F1 score) on validation set for every 2,000 batches. We halve the learning rate if the ROUGE-2 F1 score drops for twelve consecutive tests on validation set. We also apply gradient clipping () with range [−5, 5] during training. Our model with entailmentaware encode requires less than 300,000 training iterations to train with early stopping. To speedup the training for our RAML model, we continue the RAML training based on the pre-trained model with ML training with the current decayed learning rate. At test time, we use beam search with beam size 10 to generate the summary. We report ROUGE F1 score including ROUGE-1, ROUGE-2 and ROUGE-L for Gigaword corpus and ROUGE recall score for DUC 2004 corpus.
section: Comparative Methods
We compare a set of sentence summarization baselines. ABS. propose a neural machine translation model with two-layer LSTMs for the encoder-decoder. Seq2seq. This is a standard seq2seq model with attention mechanism. Seq2seq + MTL. This is our proposed model with entailment-aware encoder, which applies a multi-task learning (MTL) framework to seq2seq model. Seq2seq + MTL (Share decoder). propose a multi-task learning (MTL) framework in which the decoder is shared for summarization generation and entailment generation task. Seq2seq + ERAML. This is our proposed model with entailment-aware decoder, which conducts an Entailment Reward Augmented Maximum Likelihood (ERAML) training framework. Seq2seq + ROUGE-2 RAML. We apply ROUGE-2 RAML training for seq2seq model. Seq2seq + RL. We implement Reinforcement Learning (RL) models (policy gradient) with reward metrics of Entailment and ROUGE-2. Seq2seq + selective.  employ a selective encoding model to control the information flow from encoder to decoder. To verify the generalization of our entailment-based strategies, we adopt selective encoding mechanism to our seq2seq model and apply MTL and RAML to Seq2seq + selective model, which is denoted as the Seq2seq + selective + MTL + RAML model.: Experimental results (%) on the English Gigaword test set of . Our models perform significantly better than baselines by the 95% confidence interval measured by the official ROUGE script.
section: Experimental Results: Gigaword Corpus
In, we report the ROUGE F1 score of our model and the baseline methods on the English Gigaword test set provided by . Our entailment-aware models outperform all baseline models by a large margin. Our final model, Seq2seq + selective + MTL + ERAML, achieves the best results, which improves 2.52 (%) ROUGE-1, 2.32 ROUGE-2 and 2.33 ROUGE-L over seq2seq model. Our seq2seq model with entailment-aware encoder (Seq2seq + MTL) surpasses the state-of-the-art seq2seq model of 1.35 ROUGE-1, 1.59 ROUGE-2, 1.36 ROUGE-L, and entailment-aware decoder (Seq2seq + ER-AML) gains improvement of 0.95 ROUGE-1, 1.46 ROUGE-2, 0.97 ROUGE-L. Compared to the another MTL model via sharing decoder for entailment generation task (Seq2seq + MTL (Share decoder)), our MTL model (Seq2seq + MTL) has obvious ROUGE score gains. The Seq2seq + ROUGE-2 RAML model also shows promising performance, especially for ROUGE-2 score. RAML has a clear advantage over RL. In principle, RL samples from the model distribution, which slows down training and several tricks are needed to get better estimates of the gradient (. The comparison to the model of Seq2seq + selective shows that our entailment-aware strategies are also useful for seq2seq model with selective encoding framework, which demonstrates the good generalization of our method.
The results on English Gigaword test set provided by are shown in. Our model performs better than the previous works.
section: Experimental Results: DUC 2004 Test Corpus
We evaluate our model with the ROUGE recall score. The reference summaries of the DUC 2004 test set are fixed to 75 bytes and we set the maximum length of the summary to 18 following . In, experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models, surpassing Feats2s (Nallapati et al., 2016) by 0.98% ROUGE-1, 0.78% ROUGE-2 and 0.65% ROUGE-L without fine-tuning on DUC data.
section: Manual Evaluation
Next, we conduct a manual evaluation to inspect the correctness of the generated summaries. We randomly select 500 samples in the test set and employ five postgraduates to classify the generated summaries as correct (i.e., not contain wrong information) or not. As shown in, 60.6% of the summaries generated by seq2seq model are correct, and it rises to 69.4% and 74.2% for our model with selective encoding and entailment-aware strategies, respectively, which indicates the effectiveness of our model to generate a correct summary.: Manual evaluation for correctness.
section: Further Analysis
To further investigate the effectiveness of our model, we perform analysis on the entailment score improvement, the abstraction degree of our model and the impact for entailment recognition task.
section: Does our summarization model learn entailment knowledge?
The motivation of our work is to encourage summarization model to generate summaries that are entailed by the source sentences. To verify this goal, we investigate the entailment score for source-summary pairs for different models. For the test set of , the average entailment score for the reference is 0.72, while for the basic seq2seq model, the entailment score is only 0.46. When we adopt entailmentbased strategies, the entailment score rises to 0.63 for seq2seq model. Note that the entailment score is 0.57 for seq2seq model with selective encoding, and we believe that the selective mechanism can filter out secondary information in the input, which will reduce the possibility to generate irrelevant information. Entailment-aware selective model achieves a high entailment reward of 0.71. In part at least, we can conclude that our model has successfully learned entailment knowledge.
section: Is it less abstractive for our model?
We have shown that our entailment-aware model can generate correct summaries more frequently (Section 6.5). Intuitively, it is more likely to be correct if summary segments are directly extracted from the source. Thus, readers may wonder whether our model is less abstractive. shows that the seq2seq model produces more novel words (i.e., words that do not appear in the article) than our model, indicating a lower degree of abstraction for our model. However, when we exclude all the words not in the reference (these words may lead to wrong information), our model generates more novel words, suggesting that our model provides a compromise solution for informativeness and correctness. Thus, our model can generate summary with fewer mistakes.   6.6.3 Could the entailment recognition also be improved? Multi-task learning (MTL) involves sharing parameters between related tasks, whereby each task can benefit from extra information of other tasks in the training process. In this section, we explore whether the entailment recognition can benefit from summarization generation task. shows that our summarization model with MTL outperforms basic seq2seq model. As α increases, the accuracy of entailment recognition improves and finally exceeds that of the model without MTL, which reveals the advantage of MTL framework.
section: Case Study
We illustrate the examples of outputs in. As shown in the table, seq2seq model generates summaries that are not relevant to the source sentence, while the output of our model obtains higher entailment scores than those of seq2seq model. For the first example, seq2seq model regards the reason for "brazil stocks rise" as "consumer credit concerns", while in fact, "consumer" is not worried because "government said it would n't impose restraints on consumer credit". By contrast, since our model incorporates entailment knowledge, the true reason is captured and the output of our model is related to the source sentence. A similar problem happens in example 2, and seq2seq model generates a summary that is contradictory to the source. The "demonstration" is "denied" by the "authorities", while seq2seq model confirms the "demonstration". In Example 3, neither seq2seq nor our model performs satisfactorily. Seq2seq model again misunderstands the meaning of the source and outputs summary containing wrong information. Though the summary generated by our model is entailed by the source, the summary fails to produce an integrated sentence and misses the key points of the source, such as the object of the event, "queens taxi driver". A mixed reward, i.e., combining entailment and ROUGE-2, may address this issue. We leave it for our future work.
section: Conclusion
This paper investigates the correctness problem in abstractive summarization. We propose an entailmentaware encoder by jointly learning summarization generation and entailment recognition. We present an entailment-aware decoder by entailment reward augmented maximum likelihood training. By enriching the encoder and decoder with entailment information, our model makes the summary more likely be entailed by the source input. Experimental results on datasets demonstrate that our model achieves significant improvements over strong baselines on both informativeness and correctness. Our code is available online 4 .
