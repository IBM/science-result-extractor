<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supertagging with LSTMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 12-17, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
							<email>vaswani@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
							<email>ybisk@isi.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
							<email>sagae@kitt.ai</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
							<email>ramusa2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kitt</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Southern California</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Supertagging with LSTMs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of NAACL-HLT 2016</title>
						<meeting>NAACL-HLT 2016 <address><addrLine>San Diego, California</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="232" to="237"/>
							<date type="published">June 12-17, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we present new state-of-the-art performance on CCG supertagging and parsing. Our model outperforms existing approaches by an absolute gain of 1.5%. We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphosyntactic labels for words are commonly used in a variety of NLP applications. For this reason, part-of-speech (POS) tagging and supertagging have drawn significant attention from the community. Combinatory Categorial Grammar is a lexicalized grammar formalism that is widely used for syntactic and semantic parsing. Supertagging <ref type="bibr" target="#b6">(Clark, 2002;</ref><ref type="bibr" target="#b0">Bangalore and Joshi, 2010</ref>) assigns complex syntactic labels to words to enable fast and accurate parsing. The disambiguation of correctly labeling a word with one of over 1,200 CCG labels is difficult compared to choosing on of the 45 POS labels in the Penn Treebank ( <ref type="bibr" target="#b13">Marcus et al., 1993)</ref>. In addition to the large label space of CCG supertags, labeling a word correctly depends on knowledge of syntactic phenomena arbitrarily far in the sentence <ref type="bibr" target="#b8">(Hockenmaier and Steedman, 2007)</ref>. This is because supertags encode highly specific syntactic information (e.g. types and locations of arguments) about a word's usage in a sentence.</p><p>In this paper, we show that Bidirectional Long Short-Term Memory recurrent neural networks (biLSTMs) <ref type="bibr" target="#b7">(Graves, 2013;</ref><ref type="bibr" target="#b20">Zaremba et al., 2014</ref>), which can use information from the entire sentence, are a natural and powerful architecture for CCG supertagging. In addition to the bi-LSTM, we create a simple yet novel model that outperforms the previous state-of-the-art RNN model that uses handcrafted features ( ) by 1.5%. Concurrent to this work ( <ref type="bibr" target="#b11">Lewis et al., 2016</ref>) introduced a different training methodology for bi-LSTM for supertagging. We provide a detailed analysis of the quality of various LSTM architectures, forward, backward, and bi-directional, shedding light over the ability of the bi-LSTM to exploit rich sentential context necessary for performing supertagging. We also show that a baseline feed-forward neural network (NN) architecture significantly outperforms previous feed-forward NN baselines, with slightly fewer features, achieving better accuracy than the RNN model from ( .</p><p>Recently, bi-LSTMs have achieved high accuracies in a simpler sequence labeling task: partof-speech tagging ( <ref type="bibr" target="#b12">Wang et al., 2015;</ref><ref type="bibr" target="#b12">Ling et al., 2015</ref>) on the Penn treebank, with small improvements over local models. However, we achieve strong accuracies compared to ( <ref type="bibr" target="#b12">Wang et al., 2015</ref>) using feed-forward neural network model trained on local context, showing that this task does not require bi-LSTMs. Our strong feed-forward NN baselines show the power of feed-forward NNs for some tasks.</p><p>Our main contributions are the introduction of a new bi-LSTM model for CCG supertagging that achieves state-of-the-art, on both CCG supertagging and parsing, and a detailed analysis of our results, including a comparison of bi-LSTMs and simpler feed forward NN models for supertagging and POS tagging, which suggests that the added complexity of bi-LSTMs may not be necessary for POS tagging, where local contexts suffice to a much greater extent than in supertagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>232</head><p>We use feed-forward neural network models and bidirectional LSTM (bi-LSTM) based models in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feed-Forward</head><p>For both POS tagging and our baseline supertagging model, we use feed-forward neural networks with two hidden layers of rectified linear units <ref type="bibr" target="#b14">(Nair and Hinton, 2010)</ref>. For supertagging, we use a slightly smaller set than <ref type="bibr" target="#b9">Lewis and Steedman (2014a)</ref>, using a left and right 3-word window with suffix and capitalization features for the center word. However, unlike them, we train on the full set of supertag categories observed during training.</p><p>In POS tagging, when tagging word w i , we consider only features from a window of five words, with w i at the center. For each w j with i − 2 ≤ j ≤ i + 2, we add w j lowercased and a string that encodes the basic "word shape" of w j . This is computed by replacing all sequences of uppercase letters with A, all sequences of lowercase letters with a, all sequences of digits with 9, and all sequences of other characters with * . Finally, we add two and three letter suffixes and two letter prefix for w i only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LSTM models</head><p>We experiment with two kinds of bi-LSTM models. We train a basic bi-LSTM where the forward and backward LSTMs take input words w i and produce hidden state − → h i and ← − h i . For each position, we produce˜hduce˜ duce˜h i , where˜h</p><formula xml:id="formula_0">where˜ where˜h i = σ(W← − h ← − h T i + W− → h − → h T i ),<label>(1)</label></formula><p>where σ(x) = max(0, x) is a rectifier nonlinearity, and where W← − h and W− → h are parameters to be learned. The unnormalized likelihood of an output supertag is computed using supertag embeddings</p><formula xml:id="formula_1">D t i and biases b t i as p(t i | ˜ h i ) = D t i ˜ h T i + b t i .</formula><p>The final softmax layer computes normalized supertag probabilities.</p><p>Although bidirectional LSTMs can capture long distance interactions between words, each output label is predicted independently. To explicitly model supertag interactions, our next model combines two models, the bi-LSTM and a LSTM language model (LM) over the supertags <ref type="figure" target="#fig_1">(Figure 1</ref>   to h i similar to the combiner for˜hfor˜ for˜h i (Equation 1). Output supertag probabilities are computed just as before, replacing replacing˜hreplacing˜ replacing˜h i with h i . We refer to this model as bi-LSTM-LM. For all our LSTM models, we only use words as input features.</p><formula xml:id="formula_2">h LM i+1 h LM i+2 ← − hi ← − − hi+1 ← − − hi+2 − − → hi+2 − − → hi+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>We train our models to maximize the log-likelihood of the data with minibatch gradient ascent. Gradients of the models are computed with backpropagation ( <ref type="bibr" target="#b3">Chauvin and Rumelhart, 1995)</ref>. Since gold supertags are available during training time and not while decoding, a bi-LSTM-LM trained on gold supertags might not recover from errors caused by using incorrectly predicted supertags. This results in the bi-LSTM-LM slightly underperforming the bi-LSTM (we refer to training with gold supertags as g-train in <ref type="table" target="#tab_2">Table 1</ref>). To bridge this gap between training and testing we also experiment with a sampling training regime in addition to training.</p><p>Scheduled sampling: Following ( <ref type="bibr" target="#b1">Bengio et al., 2015;</ref><ref type="bibr" target="#b15">Ranzato et al., 2015)</ref>, for each output token, with some probability p, we use the most likely predicted supertag (arg max t i P (t i | h i )) from the model in position i−1 as input to the supertag LSTM LM in position i and use the gold supertag with probability 1 − p. We denote this training as sstrain-1. We also experiment with using the 5-best previous predicted supertags from the output distribution at position i − 1 and feed them to the LM as input in position i as a bit vector. Additionally, we  g-train ss-train-1 ss-train-5 񮽙 1 use their probabilities (re-normalized over the 5-best tags) and scale the input supertag embeddings with their re-normalized probability during look-up. We refer to this setting as ss-train-5. In this work, we use an inverse sigmoid schedule to compute p,</p><formula xml:id="formula_3">p = k k + e s k ,</formula><p>where s is the epoch number and k is a hyperparameter that is tuned. 1 In <ref type="figure" target="#fig_3">Figure 2</ref>, we see that for the development set training with scheduled sampling improves the perplexity of the gold supertag sequence when using predicted supertags, indicating better recovery from conditioning on erroneous supertags. For both ss-train and g-train, we use gold supertags for the output layer and train the model to maximize the log-likelihood of the data. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Architectures</head><p>Our feed-forward models use 2048 rectifier units in the first hidden layer, 50 and 128 rectifier units in the second hidden layer for POS tagging and Supertagging respectively, and 64 dim. input embeddings.</p><p>Our LSTM based models use 512 hidden states. We pre-train our word embeddings with a 7-gram feed-forward neural language model using the NPLM toolkit 3 on a concatenation of the BLLIP corpus ( <ref type="bibr" target="#b2">Charniak et al., 2000</ref>) and WSJ sections 02-21 of the Penn Treebank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supertag Accuracy Model</head><p>All Seen Novel % P  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Decoding</head><p>We perform greedy decoding. For each position i, we select the most probable supertag from the output distribution. For the bi-LSTM-LM models trained with g-train and ss-train-1, we feed the most likely supertag from the output distribution as LM input in the next position. We decode with beam search (size 12) for bi-LSTM-LMs trained with g-train and ss-train-1. For the bi-LSTM-LMs trained with ss-train-5, we perform greedy decoding similar to training, feeding the k-best supertags from the output supertag distribution in position i − 1 as input to the LM in position i, along with the renormalized probabilities. We don't perform beam decoding for ss-train-5, as the previous k-best inputs already capture different paths through the network. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>For supertagging, experiments were run with the standard splits of CCGbank. Unlike previous work no features were extracted for the LSTM models and rare categories were not thresholded. Words were lowercased and digits replaced with @.</p><p>CCGbank's training section contains 1,284 lexical categories (394 in Dev). The distribution of categories has a long tail, with only a third of those cate-LSTM Supertag F-For Forward Backward bi-LSTM +LM(g-train) ss-train-1 ss-train-5   gories having a frequency count ≥ 10 (the threshold used by existing literature). Following ( <ref type="bibr" target="#b10">Lewis and Steedman, 2014b)</ref>, we allow the model to predict all categories for a word, not just those with which the word was observed to co-occur in the training data. Accuracies on these unseen (word, cat) pairs are presented in the third column of <ref type="table" target="#tab_2">Table 1</ref>. <ref type="table" target="#tab_5">Table 3</ref> presents our Feed-Forward POS tagging results. We achieve 97.28% on the development set and 97.4% on test. Although slightly below state-ofthe-art, we approach existing work with bi-LSTMs, and our models are much simpler and faster to train. 5 <ref type="table" target="#tab_2">Table 1</ref> shows a steady increase in performance as the model is provided additional context. The forward and backward models are presented with information that may be arbitrarily far away in the sentence, but only in a specific direction. This yields weaker results than the Feed Forward model which can see in both directions within a small window. The real gains are achieved by the Bidirectional LSTM which incorporates knowledge from the entire sentence. Our addition of a language model and changes to training, further improve the perfor- <ref type="bibr">5</ref> We use train, dev, and test splits of WSJ sections 00-18, 19-21, and <ref type="bibr">22</ref>  mance. Our final model (bi-LSTM-LM+ss-train-1 model with beam decoding) has a test accuracy of 94.5%, 1.5% above state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parsing</head><p>Our primary goal in this paper was to demonstrate how a bi-LSTM captures new and different information from uni-directional or feed-forward approaches. This advantage also translates to gains in parsing. <ref type="table" target="#tab_7">Table 4</ref> presents new state-of-the-art parsing results for both ( ) and our bi-LSTM-LM +ss-train-1. These results were attained using our part-of-speech tags <ref type="table" target="#tab_5">(Table 3)</ref> and the Java implementation ) of the C&amp;C parser (Clark and Curran, 2007) 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Error Analysis</head><p>Our analysis indicates that the information following a word is more informative than what preceded it. <ref type="table" target="#tab_4">Table 2</ref> compares how well our models recover common and syntactically interesting supertags. In particular, the Forward and Backward models, motivate the need for a Bi-directional approach. <ref type="table">Table 5</ref>: "Neighbor" categories as determined by embedding-based vector similarity for each class of model. As expected for this category, the Backward model captures the argument preference while the Forward model correctly predicts the result.</p><formula xml:id="formula_4">(S[dcl]\NP)/(S[adj]\NP) Forward Backward Bidirectional ((S[dcl]\NP)/PP)/(S[adj]\NP) ((S[dcl]\NP)/PP)/(S[adj]\NP) (S[dcl]\NP)/(S[pss]\NP) ((S[dcl]\NP)/(S[to]\NP))/(S[adj]\NP) ((S[b]\NP)\NP)/(S[adj]\NP) (S[dcl]\NP)/PP)/(S[adj]\NP) ((S[dcl]\NP)/PP)/PP (S[dcl]\S[qem])/(S[adj]\NP) (S[b]\NP)\NP)/(S[adj]\NP) (S[dcl]\NP)/S ((S[dcl]\NP)/(S[to]\NP))/(S[adj]\NP) (S[dcl]\NP)/(S[to]\NP))/(S[adj]\NP) (S[dcl]\NP)/(S[pss]\NP) ((S[dcl]\NP)/(S[adj]\NP))/(S[adj]\NP) (S[dcl]\NP)/(S[adj]\NP))/(S[adj]\NP)</formula><p>The first two rows show prepositional phrase attachment decisions (noun and verb attaching categories are in rows one and two, respectively). Here the forward model outperforms the backward model, presumably because knowing the word to be modified and the preposition, is more important than observing the object of the prepositional phrase (the information available to the backward model).</p><p>Conversely, the backward model outperforms the forward model in most of the remaining categories. (Di-)transitive verbs (lines 4 &amp; 5) require knowledge of future arguments in the sentence (e.g. separated by a relative clause). Because English has strict SVO word-order, the presence of a subject is more predictable than the presence of an (in-)direct object. It is therefore not surprising that the backward model is often comparable to the Feed Forward model.</p><p>If the information missing from either the forward or backward models were local, the bidirectional model should perform the same as the Feed-Forward model, instead it surpasses it, often by a large margin. This implies there is long range information necessary for choosing a supertag.</p><p>Embeddings In addition, we can visualize the information captured by our models by investigating a category's nearest neighbors based on the learned embeddings. <ref type="table">Table 5</ref> shows nearest neighbor categories for (S[dcl]\NP)/(S[adj]\NP) under the Forward, Backward, and Bidirectional models.</p><p>We see see that the forward model learns internal structure with the query category, but the list of arguments is nearly random. In contrast, the backward model clusters categories primarily based on the final argument, perhaps sharing similarities in the subject argument only because of the predictable SVO nature of English text. However, due to its lack of forward context the model incorrectly associates categories with less-common first arguments (e.g. S <ref type="bibr">[qem]</ref>). Finally, the bidirectional embeddings appear to cleanly capture the strengths of both the forward and backward models.</p><p>Consistency and Internal Structure Because supertags are highly structured their co-occurence in a sentence must be permitted by the combinators of CCG. Without encoding this explicitly, the language model dramatically increases the percent of predicted sequences that result in a valid parse by up to 15% (last column of <ref type="table" target="#tab_4">Table 2</ref>).</p><p>Sparsity One consideration of our approach is that we do not threshold rare categories or use any tag dictionaries; our models are presented with the full space of CCG categories, despite the long tail. This did not did not hurt performance and the models learned to successfully use several categories which were outside the set of traditionally-thresholded frequent categories. Additionally, the total number of categories used correctly at least once by the bidirectional models was substantially higher than the other models (∼270 vs. ∼220 of 394), though the large number of unused categories (≥120) indicates that there is still substantial room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Because bi-LSTMs with a language model encode an entire sentence at decision time, we demonstrated large gains in supertagging and parsing. Future work will investigate improving performance on rare categories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We add a language model between supertags. i, the LM accepts an input supertag t i−1 producing hidden state h LM i , and a second combiner layer, parametrized by matrices W LM and W ˜ h transforms˜h transforms˜ transforms˜h i and h LM i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>50586</head><label>50586</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scheduled sampling improves the perplexity of the gold sequence under predicted tags. We see that the perplexity of the gold supertag sequence when using predicted tags for the LM is lower for ss-train-1 and ss-train-5 than with g-train.</figDesc><graphic url="image-1.png" coords="3,100.47,71.69,195.08,60.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>parseability</head><label></label><figDesc>of the sequence (%P). The numbers for bi-LSTM- LM + ss-train-1 and + g-train are with beam decoding. All others use greedy decoding. Interestingly, greedy decoding with ss-train-5 works as well as beam decoding with ss-train-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Accuracies on the development section. The language model provides a boost in performance, and large gains on the</figDesc><table>Lewis et al. (2014) 
91.30 
Wenduan et al. (2015) 
93.07 

Feed Forward + g-train 
93.29 93.77 91.53 70.3 
Forward LSTM + g-train 
83.70 85.76 46.22 20.7 
Backward LSTM + g-train 88.82 90.06 66.22 40.6 
bi-LSTM 
94.08 95.03 76.36 81.1 
bi-LSTM-LM + g-train 
93.89 94.93 76.83 96.5 
bi-LSTM-LM + ss-train-1 94.24 95.22 76.70 87.8 
bi-LSTM-LM + ss-train-5 94.23 95.20 76.62 94.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Prediction accuracy for our models on several common and difficult supertags. 

Architecture 
Test Acc 

Ling et al. (2015) 
Bi-LSTM 
97.36 
Wang et al. (2015) Bi-LSTM 
97.78 
Søgaard (2011) 
SCNN 
97.50 

This work 
Feed-Forward 
97.40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Our new POS tagging results show a strong Feed-</head><label>3</label><figDesc></figDesc><table>Forward baseline can perform as well as or better than more 

sophisticated models (e.g. Bi-LSTMs). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>- 24 , for POS tagging.</head><label>24</label><figDesc></figDesc><table>Dev F1 Test F1 

Wenduan et al. (2015) 
86.25 
87.04 
+ new POS Tags &amp; C&amp;C 
86.99 
87.50 
bi-LSTM-LM +ss-train-1 
87.75 
88.32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Parsing at 100% coverage with our new Feed-Forward 

POS tagger and the Java implementation of C&amp;C. We show both 

the published and improved results for Wenduan et al. 

</table></figure>

			<note place="foot" n="1"> The reader should refer to (Bengio et al., 2015) for details. 2 We use dropout for all our feed-forward (Srivastava, 2013) and bi-LSTM based models (Zaremba et al., 2014). We carry out a grid search over dropout probabilities and sampling schedules. We train the LSTMs for 25 epochs and the feed-forward models for 30 epochs, tuning on the development data. 3 http://nlg.isi.edu/software/nplm/</note>

			<note place="foot" n="4"> Code and supertags for our models can be downloaded here: https://bitbucket.org/ashish_vaswani/ lstm_supertagger</note>

			<note place="foot" n="6"> Results are presented on the standard development and test splits (Section 00 and 23), and with a beam threshold of 10 −6 . For a fair comparison to prior work we report results without the skimmer, so no partial credit is given to parse failures. The skimmer boosts performance to 87.91/88.39 for Dev and Test.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the U.S. DARPA LORELEI Program No. HR0011-15-C-0115. We would like to thank Wenduan Xu for his help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Supertagging: Using Complex Lexical Descriptions in Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Blaheta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Bllip 1987-89 wsj corpus release 1. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Backpropagation: theory, architectures, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Java Version of the C&amp;C Parser: Version 0.95</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supertagging for combinatory categorial grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+6)</title>
		<meeting>the 6th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+6)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A* ccg parsing with a supertag-factored model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved ccg parsing with semi-supervised supertagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LSTM CCG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter</title>
		<meeting>the 15th Annual Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<title level="m">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence Level Training with Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semisupervised condensed nearest neighbor for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="48" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving neural networks with dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging with bidirectional long short-term memory recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.06168</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ccg supertagging with a recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Short Papers</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
