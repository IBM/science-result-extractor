<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T01:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-10-09">9 Oct 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
							<email>r.i.k.van.noord@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Center for Language and Cognition Groningen (CLCG)</orgName>
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Center for Language and Cognition Groningen (CLCG)</orgName>
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">Bos@rug</forename><surname>Nl</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Center for Language and Cognition Groningen (CLCG)</orgName>
								<orgName type="institution">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-10-09">9 Oct 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We evaluate the character-level translation method for neural semantic parsing on a large corpus of sentences annotated with Abstract Meaning Representations (AMRs). Using a sequence-to-sequence model, and some trivial preprocessing and postprocessing of AMRs, we obtain a baseline accuracy of 53.1 (F-score on AMR-triples). We examine five different approaches to improve this baseline result: (i) reordering AMR branches to match the word order of the input sentence increases performance to 58.3; (ii) adding part-of-speech tags (automatically produced) to the input shows improvement as well (57.2); (iii) So does the introduction of super characters (conflating frequent sequences of characters to a single character), reaching 57.4; (iv) optimizing the training process by using pre-training and averaging a set of models increases performance to 58.7; (v) adding silver-standard training data obtained by an off-the-shelf parser yields the biggest improvement , resulting in an F-score of 64.0. Combining all five techniques leads to an F-score of 71.0 on holdout data, which is state-of-the-art in AMR parsing. This is remarkable because of the relative simplicity of the approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Various approaches to open-domain semantic parsing have been proposed in the last years. What we now could refer to as "traditional" approaches are semantic parsers that use supervised learning to create a syntactic analysis on which the meaning representations are constructed, usually in a compositional way. Research in this area comprises <ref type="bibr" target="#b8">Bos et al. (2004)</ref>, <ref type="bibr" target="#b15">Copestake et al. (2005)</ref>, <ref type="bibr" target="#b10">Butler (2010)</ref>, <ref type="bibr" target="#b26">Le and Zuidema (2012)</ref>, <ref type="bibr" target="#b27">Lewis and Steedman (2013)</ref>, <ref type="bibr" target="#b7">Bos (2015)</ref>, <ref type="bibr" target="#b1">Artzi et al. (2015)</ref>, and many others. Efforts to create datasets of sentences paired with meaning representations have stimulated research in semantic parsing ( <ref type="bibr" target="#b2">Banarescu et al. 2013a</ref>, especially those using the formalism of Abstract Meaning Representation (AMR), for which also shared tasks have been organized <ref type="bibr" target="#b29">(May 2016)</ref>. In this article, therefore, we concentrate on semantic parsing of AMRs, because large gold-standard datasets are available and various different approaches can be compared.</p><p>In contrast to the traditional approaches mentioned above, there have been interesting attempts recently to view semantic parsing as a translation task, mapping English expressions to logical forms under supervision of some deep learning method. <ref type="bibr" target="#b19">Dong and Lapata (2016)</ref> used sequenceto-sequence (seq2seq) and sequence-to-tree (seq2tree) neural translation models to produce logical forms from sentences for four different datasets (but not AMRs). <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref> used a similar method to produce AMRs in the context of the previously mentioned shared task, but the performance of their neural parser was still far below the state-of-the-art. Despite this, their method inspired other researchers to adopt this seq2seq approach ( <ref type="bibr" target="#b30">Peng et al. 2017</ref><ref type="bibr" target="#b25">, Konstas et al. 2017</ref>). But, even though they got substantial improvements over <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref>, their systems still did not come close to state-of-the-art. The neural approach of <ref type="bibr" target="#b22">Foland and Martin (2017)</ref> did reach state-of-the-art performance, but they used five bi-LSTM networks instead of a single seq2seq model.</p><p>What all these attempts have in common, and why they are fascinating, is that they completely avoid complex models of the syntactic and semantic parsing process and therefore do not rely on heavily engineered features. However, except for <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref>, they also only use word-level input. This is interesting, because <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref> obtained a substantial improvement for their character-level model over their word-level model. Character-embeddings, since they were introduced by <ref type="bibr" target="#b34">Sutskever et al. (2011)</ref>, have also shown improvements in a number of areas, such as POS-tagging <ref type="bibr" target="#b33">(Santos and</ref><ref type="bibr">Zadrozny 2014, Plank et al. 2016</ref>), text classification ( <ref type="bibr" target="#b38">Zhang et al. 2015</ref>), and, most importantly, Neural Machine Translation ( <ref type="bibr" target="#b12">Chung et al. 2016)</ref>.</p><p>The aim of this article is to find out how far we can push character-level neural semantic parsing: can we reach accuracy scores comparable with traditional approaches to semantic parsing? More specifically, our objectives are (1) try to reproduce the results of <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref>; <ref type="formula">(2)</ref> improve on their results by employing several novel techniques; and (3) investigate whether injecting linguistic knowledge can improve neural semantic parsing.</p><p>We make three main contributions. First, we introduce novel techniques to improve neural AMR parsing. Second, we show that linguistic knowledge can still contribute to neural semantic parsing. Third, we show that adding silver standard to the training data makes a considerable (positive) difference in terms of performance. Our final model reaches an F-score of 71.0, which is the current state-of-the-art in AMR parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method and Data</head><p>We first give a bit of background on AMRs. Then we outline the basic ideas of the character-based translation model with English sentences as input and AMRs as output. We then establish a baseline system with the aim to improve it in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Abstract Meaning Representations</head><p>In our experiments utilizing neural semantic parsing we will focus on parsing Abstract Meaning Representations (AMRs). AMRs were introduced by <ref type="bibr" target="#b3">Banarescu et al. (2013b)</ref> and are acyclic, directed graphs that represent the meaning of a sentence. There are, in fact, three ways to display an AMR: as a graph, as a set of triples, or as a tree. An example of an AMR is shown in <ref type="figure">Figure 1</ref>, here displayed as a tree, the format that is used in the annotated corpora. The corresponding triple representation is shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>(a / affect-01 :ARG0 (w / wave-04 :ARG1 (h2 / heat) :location (c / country :wiki "France" :name (n / name :op1 "France"))) :ARG1 (p / person :ARG0-of (s / strike-02 :mod (h / hunger-01 :ARG0 p))))</p><p>Figure 1: AMR representing the meaning of Hunger strikers were affected by France's heat wave.</p><p>An AMR consists of concepts that are linked to variable names with a slash. In the example above we have that a is an instance of the concept affect-01, and p is an instance of the concept person (note that the names of the variables are not important). Concepts can be related to each other by using two-place predicates, which are indicated by a colon. So, the first :ARG0 is an ordered relation between a and w. Inverse relations are denoted by the suffix -of. Note that, if one concept relates to more than one other concept (for instance, in the example above, the node a is related to w via :ARG0, and to p via :ARG1), the order of these relations within the AMR is not important. AMRs also allow for a re-occurrence of variables: the concept person with variable p stands in a relation with affect-01 as well as with hunger-01. The brackets are important, because they signal which relations belong to which concepts (the spacing used in <ref type="figure">Figure 1</ref> is optional and is only used to increase readability). Some of the concepts have a number as suffix that indicate a specific word sense. AMRs also include proper name reference resolution by including a link to a wikipedia entry (wikification).</p><p>For evaluation purposes, AMRs are converted into triples. The triples of the AMR in <ref type="figure">Figure 1</ref> are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The accuracy of an AMR parser is computed by precision and recall on matching triples between gold standard AMRs and system-produced AMRs, using the SMATCH system <ref type="bibr" target="#b11">(Cai and Knight 2013)</ref>.</p><p>For the evaluation of our experiments we use the sentences annotated with AMRs from LDC release LDC2016E25 1 , consisting of 36,521 training AMRs, 1,368 development AMRs and 1,371 test AMRs.</p><p>This release also includes the PropBank frameset and comes with pre-aligned AMRs and sentences. In all results shown in this article, the models are trained on the training data. As development and test data we use the designated dev and test set from LDC2016E25, which are the exact same sets that are used in LDC2015E89. We remove HTML-tags from the input sentences, but URLs are kept in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Basic Translation Model</head><p>To create our sequence-to-sequence translation model, we use the OpenNMT system ( <ref type="bibr" target="#b24">Klein et al. 2017)</ref>. In contrast to <ref type="bibr" target="#b30">Peng et al. (2017)</ref> and <ref type="bibr" target="#b25">Konstas et al. (2017)</ref>, who use word-level input, we use character-level input . <ref type="bibr">2</ref> We train a model with bidirectional encoding and general attention ( <ref type="bibr" target="#b28">Luong et al. 2015</ref>). Since training a full model takes two to three days on a GPU, we perform a heuristic parameter search instead of an exhaustive one. We started out with a default model and changed only one parameter value in separate experiments. If we improved over the default, the setting was kept and combined with other parameter settings that improved performance. All models were only tested on the development set. Ultimately, we arrived at the settings shown in <ref type="table" target="#tab_1">Table 2</ref>. All our described models in this paper are trained with these settings. Training is stopped 3 epochs after there is no improve-ment in validation perplexity on the development set anymore. The best performing model on the development set is then used to decode the test set.  <ref type="formula">(2016)</ref>, we do not want our model to learn the arbitrary characters that are used to represent variables. The characters itself do not carry any semantic information and are only necessary to indicate co-referring nodes. Therefore we remove all variables from the AMRs and simply duplicate co-referring nodes from the input. An example of such a preprocessed AMR is shown in <ref type="figure">Figure 2</ref>. Note that this means that we lose information, since the variables cannot be put back perfectly. We describe an approach to restore the co-referring nodes in the output in section 2.3.3. All wikification relations present in AMRs in the training set are also removed and restored in a post-processing step. Newlines present in an AMR are replaced by spaces, and multiple spaces are squeezed into single ones (so the input AMR is represented on a single line). :ARG1-of (use-01 :ARG2 (make-01 :ARG1 (heroin) :ARG2 (opium))))</p><p>Figure 2: Example of the original AMR (left) and the variable-free AMR (right) displaying the meaning of Opium is the raw material used to make heroin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Postprocessing and Restoring Information</head><p>The output of the seq2seq model is, of course, an AMR without variables, without wiki-links, and without co-occurrent variables. Furthermore, because of the character-based seq2seq model, it could well be that there are brackets in the output that do not match, or that some nodes representing concepts are incomplete. This, obviously, needs to be fixed. First, the variables in the AMRs are restored by assigning a unique variable to each concept. We also try to fix invalidly produced AMRs by applying a few heuristics, such as inserting parentheses and quotes, or by removing unfinished nodes. This is done by using the restoring script from <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref>. <ref type="bibr">4</ref> Then, we apply three methods to increase the quality of the AMRs. They are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Pruning</head><p>A problem with many deep learning approaches is the fact that the decoder does not keep track of what it has already produced. As a consequence, we sometimes end up with duplicated, redundant material in our generated AMRs. This hurts precision. We propose four different methods to remove this redundant material. This is done on node level, where nodes are defined as relation-concept pairs without children, e.g. :mod (raw) and :domain (opium). The statistics of applying these four methods on our baseline model (dev set) are shown in <ref type="table" target="#tab_2">Table 3</ref>. Note that all these processes are trade-offs: usually duplicates are correctly recognized as redundant and can be removed, but sometimes we erroneously remove actual re-occurrent nodes.</p><p>The first method simply removes all re-occurrent nodes and is already quite effective: F-score increases by 0.6. The second method is more careful and only removes duplicate nodes if they have the same parent. This helps, but only by a small margin. The third method does not consider parent nodes, but removes nodes if they occur more than twice in the full AMR. This method also increases the F-score, but does not outperform the first method yet. The fourth method is a combination of the second and third method. All re-occurrent nodes with the same parent are removed, but also nodes occurring more than twice are removed. This results in the best F-score, an increase of 0.7 over the baseline. Two example AMRs whose branches are pruned using the fourth method are shown in <ref type="figure">Figure 3</ref>. Figure 3: Example of pruned branches for the produced AMRs of Opium is the raw material used to make heroin. In the left AMR, the second occurrence of :mod (raw) is already removed, because both branches are children of material. However, in the right AMR, none of the :mod (raw) branches share the same parent, so only the third occurrence is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Wikification</head><p>Since we removed wikification relations in preprocessing, our model will never output such a link. We restore wiki links in the output AMR by using an off-the-shelf system ( <ref type="bibr" target="#b16">Daiber et al. 2013</ref>), following the method presented by <ref type="bibr" target="#b6">Bjerva et al. (2016)</ref>. They look at the :name relations in an AMR and try to find this name on Wikipedia. If it has a page, the corresponding link gets added; otherwise the AMR remains unaltered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Restoring co-referring nodes</head><p>Our system also tries to restore co-referring nodes. If we output a duplicate node (a node already produced for this AMR), it replaces the node by the variable name of the node encountered first. This can only happen once per unique node, since the third instance of such a node is already removed in the pruning phase. An example of how the co-referring nodes are restored is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Baseline Results</head><p>Our first objective was to reproduce the results obtained by <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref>. We did so, arriving at an F-score of 53.1 (see <ref type="table" target="#tab_3">Table 4</ref>). Compared to the F-score of 43.0 by <ref type="bibr" target="#b4">Barzdins and Gosko (2016)</ref>, our score is significantly higher. This is probably due to the higher amount of training data and the fact that they used Tensorflow instead of OpenNMT. We also reproduced their results by using the exact same data, software and parameter settings as they did, obtaining an F1-score of 42.3. 5 As is shown in <ref type="table" target="#tab_3">Table 4</ref>, concept pruning, restoring co-reference variables, and wikification all increase the F-score by about a percentage point each. This small gain of performance is what one could expect as each single operation has only a small impact on the overall contents of an AMR.</p><p>5. We did not possess their Wikification and coreference restoring scripts, so differences might be attributed to that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Improving the Basic Translation Model</head><p>In the previous section we outlined our basic method of producing AMRs using a seq2seq model based on characters. In this section, we look at five different techniques to move beyond the F-score that we obtain with our basic method, that we will consider in this section as baseline. Some of the techniques were already (briefly) introduced in van Noord and Bos (2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AMR Re-ordering</head><p>Although AMRs are unordered by definition, in our textual representation of the AMRs there is an order of the branches. However, these branches do not necessarily follow the word order in the corresponding English sentence. It has been shown that for (statistical) machine translation reordering improves translation quality ( <ref type="bibr" target="#b14">Collins et al. 2005</ref>). We use the provided alignments to permute the AMR in such a way that it best matches the word order. We do this both on sub-tree level and on individual node level. The best matching AMR is defined as the AMR in which the order of the nodes (when traversing over the AMR depth-first) is the closest to the order of the words in the English sentence, following the alignments. An example of an AMR with a branch order best matching the input sentence is shown in <ref type="figure">Figure 5</ref>. Figure 5: Example of a variable-free AMR before (left) and after re-ordering (right) for the sentence Opium is the raw material used to make heroin.</p><p>We are also able to use this approach to augment the training data, since each reordering of the AMR provides us with a new AMR-sentence pair. Due to the exponential increase, large AMRs often have thousands of possible orders. We performed a number of experiments to find out how we could best exploit this surplus of data. Ultimately, we found that it is most beneficial to "double" the training data by adding the best matching AMR to the existing data set. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Introducing Super Characters</head><p>We are not necessarily restricted to only using characters as input. For example, we can view the AMR relations (e.g. :ARG0, :mod) as atomic instead of a set of characters. This ensures that the characters for relations (e.g. m, o and d for :mod) do not influence the general character embeddings of the concepts, which might improve performance. This way, we create a hybrid model that is a combination of word and character level input. An example of the AMR and sentence level input using super characters is shown in <ref type="figure" target="#fig_4">Figure 6</ref> and <ref type="figure" target="#fig_5">Figure 7</ref>.</p><p>We also tried various ways to explicitly encode the tree structure by using super characters. In our basic model, the parentheses '(' and ')' are simply characters. This means that the model cannot <ref type="bibr">6</ref>. Instead of ordering the AMR nodes reflected by the word order of sentence, we also tried two different experiments based on consistency. The first experiment simply ordered the nodes alphabetically, without any other influence. This decreased the result of our baseline model by 2.0. Our second experiment was focused on fixing irregularities: if two nodes occur in a different order than they usually do (based on the full training set), we simply switch them around. This method did not change the order as considerably as the alphabetical ordering, but the result of the baseline model still decreased by 1.0. Hence we discarded both reordering techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMR, chars:</head><p>( t h i n g + : q u a n t + 1 + : p o l a r i t y + -) AMR, super chars: ( t h i n g + :quant + 1 + :polarity + -)  differentiate between a parenthesis that opens the full AMR and a parenthesis that opens, say, the fifth subtree of the AMR. One would expect it would help the model if it has this information explicitly encoded in the input. For example, in an experiment we replaced each parenthesis in the structure by a super character that also provides the subtree information (e.g., an opening parenthesis on the fifth level becomes *5*(, while a closing bracket on the third level becomes *3*).</p><p>However this resulted in an F-score lower than the baseline and we discarded the technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adding Part-of-Speech Information</head><p>We might still be able to benefit from syntactic information, even though we use a character-level neural semantic parser. To show this, we parse the sentences with the POS-tagger of the C&amp;C tools ( <ref type="bibr" target="#b13">Clark et al. 2003</ref>), employing the Penn POS tagset. Each tag is represented as a single character and placed after the last character representation of the word that matches the tag (see <ref type="figure" target="#fig_5">Figure 7</ref>). Put differently, we create a new super character for each unique tag and add this to the input sentence.</p><p>On the one hand, this will increase the size of the input. On the other hand, just a single character will add a lot of general, potentially useful, information. For example, proper nouns correlate with the :name relation, while adjectives correlate with the :mod relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adding Silver Standard Data</head><p>A problem with neural parsing approaches is data sparsity, since a lot of manual effort is required to create gold standard data. <ref type="bibr" target="#b30">Peng et al. (2017)</ref> tried to overcome this by extensive generalization of the training data, but did not get near state-of-the-art results. <ref type="bibr" target="#b25">Konstas et al. (2017)</ref> applied a similar method, but also used the GigaWord corpus to self-train their system. They use their own pre-trained parser to parse the previously unseen sentences and add those to the training data in a series of iterations. Ultimately, their system is trained on 20 million additional data AMR-sentence pairs and obtains an F-score of 62.1. Without this additional data, they obtain a score of 55.5, which is better than <ref type="bibr" target="#b30">Peng et al. (2017)</ref>, but not close to state-of-the-art performance.</p><p>Our method of obtaining new training data mainly differs from <ref type="bibr" target="#b25">Konstas et al. (2017)</ref> in two ways: (i) we use two off-the-shelf parsers to create the training data instead of self-training; (ii) we employ a method to exclude lower-quality AMRs instead of using all available data. We therefore refer to this data as "silver standard" data, by which we mean something in between unchecked automatically produced data and gold standard data.</p><p>Instead of self-training our parser, we use the off-the-shelf AMR parsers CAMR ( <ref type="bibr" target="#b37">Wang et al. 2015</ref>) and JAMR ( <ref type="bibr" target="#b21">Flanigan et al. 2014</ref>) to create silver standard data for our system. Both are non-neural, syntax-based parsers. CAMR works by first generating a dependency tree for the English sentence, after which it uses a transition-based algorithm to create the AMR graph. JAMR is the first published AMR parser and does the parsing in two stages: first identifying the concepts by using a semi-Markov model, and then identifying the relations between these concepts by searching for the maximum spanning connected subgraph.</p><p>Both systems are trained on the LDC2015E86 AMR corpus, which contains 16,833 training instances. We parse 1,303,419 sentences from the Groningen Meaning Bank ( <ref type="bibr" target="#b5">Basile et al. 2012</ref>), which mainly consists of newswire text. AMRs that are either invalid or include null-tag or null-edge (this is what the CAMR parser outputs when it is not able to find a suitable candidate parse) are removed.</p><p>We do not simply add the other AMRs to our data set. To ensure that the AMRs are at least of decent quality, we compare the produced AMRs with each other using SMATCH <ref type="bibr" target="#b11">(Cai and Knight 2013)</ref>. If their pairwise score does not exceed 55.0, the AMRs are not considered for adding to our training set. This value was picked to filter out AMRs that would only hurt the training process, but to also still include a large variety of AMRs and sentences. Our final set contained 530,450 sentences, that have both a CAMR and JAMR parse.</p><p>We now have to determine which AMR to add to our silver data set. CAMR produces higher quality AMRs in general (64.0 vs 55.0 on the test set), but it might be beneficial to introduce some variety by also adding JAMR-parsed AMRs. We never add both CAMR and JAMR for the same sentence. We performed five experiments in which we added 100k silver AMRs, either containing 100%, 75%, 67%, 50% or 0% CAMR-parsed AMRs. The results of testing on the development set are shown in <ref type="table">Table 5</ref>. <ref type="table">Table 5</ref>: F-scores on the dev set for adding different ratios of CAMR and JAMR parsed AMRs to our initial data set. All scores are without postprocessing improvement methods.</p><p># CAMR AMRs # JAMR AMRs F-score 100,000 0 65.8 75, <ref type="bibr">000</ref> 25,000 65.8 66, <ref type="bibr">667</ref> 33,333 65.7 50,000 50,000 65.3 0 100,000 61.4</p><p>As would be expected, we see that only adding CAMR scores considerably better than only adding JAMR. However, the scores for adding 67% and 75% CAMR are very similar to adding 100% CAMR. But, since this does not indicate that adding JAMR actually helps performance, we only add the CAMR-parsed AMRs in our silver data experiments. We randomly selected 20k, 50k, 75k, 100k and 500k instances for these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimizing training</head><p>Aside from the pre-and post-processing methods described, we can also optimize the training process itself. The first method we employ is pre-training on our full data set including silver AMRs, after which the model is fine-tuned on the gold data only. Both phases use the same parameter settings, as experiments with different learning rates resulted in lower performance. A similar procedure was used by <ref type="bibr" target="#b25">Konstas et al. (2017)</ref> and in general this is a method widely used in Neural Machine Translation <ref type="bibr" target="#b18">(Denkowski and Neubig 2017)</ref>.</p><p>The second method is averaging a set of models to decode the test set, instead of using a single model. This was first applied by <ref type="bibr" target="#b23">Junczys-Dowmunt et al. (2016)</ref> as an alternative to the usual ensembling of models, which is known to give substantial improvements in Neural Machine Translation ( <ref type="bibr" target="#b35">Sutskever et al. 2014</ref>). Ensembling, however, is very resource intensive, since the predictions of different models are averaged at decoding time. This as opposed to averaging, where the parameters of models are averaged to create a single model. This means that averaging, say, four models is four times faster than ensembling four models, while also using only a quarter of the memory the ensemble method uses. We tested with both ensembling and averaging and obtained similar results on the development set, thus opting to only use averaging in our experiments. <ref type="table" target="#tab_4">Table 6</ref> shows the results of our improvement methods in isolation, meaning that only that individual method is added to our baseline model. Re-ordering has a clear positive effect, both for using the best re-ordering (+2.0) and adding that re-ordering to the existing data set (+5.2). Constructing super characters and adding POS-tags both lead to a similar increase in performance. Pre-training and subsequently fine-tuning also results in a substantial improvement, but creating an average model only has a slight positive effect. The biggest improvement comes from adding silver standard data to our training set, reaching a maximum of 65.8 on the dev set. However, there is a limit with regards to adding silver data, since adding 500k silver AMRs performed worse than adding 50k, 75k or 100k silver AMRs. Finding the optimal number of silver AMRs is difficult due to the long training times and is therefore left for future work. Since the previous experiments were all in isolation, we now test whether a combination of our methods still increases performance. The tested combinations are shown in <ref type="table" target="#tab_5">Table 7</ref>. Even after adding the silver data, the addition of POS-tags and super characters still increased the performance, albeit by a smaller margin. Interestingly, the best result (71.0) was not obtained by combining all improvement methods, since re-ordering the AMRs does not show an increase anymore after adding POS-tags and super characters. The best model without using any silver data obtains an F-score of 64.0, which is considerably higher than the AMR-only score (55.5) of <ref type="bibr" target="#b25">Konstas et al. (2017)</ref>. <ref type="table" target="#tab_6">Table 8</ref> shows the results of the most notable previous AMR parsing systems. Our best model outperforms all these previous parsers and reaches state-of-the-art results. However, we are also the first approach that uses the LDC2016E25 data set, which contains slightly more than double the number of gold standard training instances compared to the LDC2015E86 data set. 7 Therefore, we also trained the best performing model in <ref type="table" target="#tab_5">Table 7</ref> on the LDC2015E86 data set, while still applying all our improvement methods. This model still obtains an F-score of 68.5, outperforming all previous AMR parsers, except for the parser of <ref type="bibr" target="#b22">Foland and Martin (2017)</ref>.  <ref type="formula">)</ref> presented a way to evaluate system output in a more detailed way, by focussing on various aspects that are present in an AMR: the role labelling, word sense disambiguation, named entity recognition, wikification, detecting negation, and so on. These detailed results of our best system are shown in <ref type="table">Table 9</ref>, in which the results of the other parsers are taken from <ref type="bibr" target="#b17">Damonte et al. (2017)</ref>. Unfortunately, Foland and Martin (2017) did not publish these specific scores. As the table shows, our system scores higher than the other parsers on five of the eight metrics other than Smatch. In general, our system is quite conservative, obtaining a higher precision than recall for each metric. Given the results in <ref type="table">Table 9</ref>, one would think that detecting negation and reentrancy would be ways to get an improvement in accuracy. Note that the other parsers score also relatively bad at these metrics. Compared to the other systems, our system scores worse on concepts, named entities, and wikification. A possible method to increase performance in the first two of those metrics 7. LDC2015E86 only contains 16,833 instances, as opposed to the 36,521 of LDC2016E25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>is to adopt an anonymization or generalization approach for named entities and concepts, similar to <ref type="bibr" target="#b30">Peng et al. (2017)</ref> or <ref type="bibr" target="#b25">Konstas et al. (2017)</ref>. <ref type="table">Table 9</ref>: Comparison with previous parsers using the evaluation script of <ref type="bibr" target="#b17">Damonte et al. (2017)</ref>. We also included precision and recall scores for our system.</p><p>CAMR JAMR-16 AMR- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>Applying re-ordering of AMR branches, introducing super characters, and adding POS-tags are techniques that substantially improve neural AMR parsing using a character-based seq2seq model. However, the biggest increase of performance is triggered by adding a large quantity of silver standard AMRs produced by existing (traditional) parsers. This is in line with the findings of <ref type="bibr" target="#b25">Konstas et al. (2017)</ref>, who used the Gigaword corpus to get extra training data, although their training method is different from ours. The obtained results are promising. Our best model, with an F-score of 71.0, outperformed any known previously published result on AMR parsing. This is remarkable, for traditional approaches are often based on extensive, manually crafted lexicons using linguistic knowledge. It should be noted, of course, that we use some linguistic knowledge in the form of POS-tags in our best models, and that we employ existing parsers trained on extensive linguistics annotations. In fact, one could consider the use of silver standard AMR data as a disadvantage, as there is still a need of an existing high-quality AMR parser to get the silver data in the first place. In our approach we rely even on two different off-the-shelf parsers. It would therefore be interesting to explore other opportunities, such as self-learning, as proposed by <ref type="bibr" target="#b25">Konstas et al. (2017)</ref>.</p><p>We have the feeling that there are still a lot of techniques that one could try to increase the performance of neural AMR parsing. From a more esthetical perspective, it would be nice if one could eliminate the AMR repair strategies that are used to resolve unbalanced brackets. An interesting candidate that could master this problem would be the seq2tree model presented by <ref type="bibr" target="#b19">Dong and Lapata (2016)</ref>. Similarly, a more principled approach to deal with co-occurring variables would be desirable.</p><p>Another possible next step in semantic parsing is to change the target meaning representation. AMRs are unscoped meaning representations, and have no quantifiers. It would be challenging to transfer the techniques of neural semantic parsing to scoped meaning representations, such as those used in the Groningen Meaning Bank ( <ref type="bibr" target="#b5">Basile et al. 2012</ref>) or the Parallel Meaning Bank ( <ref type="bibr" target="#b0">Abzianidze et al. 2017</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of how co-referring nodes are restored. On the left an example of a produced AMR, on the right the AMR with co-reference restored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Input for the AMR (t / thing :quant 1 :polarity -) representing the sentence Not one thing, with and without super characters. The +-symbols represent spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Input for the sentence I am not that rich, without and with POS-tags. POS-tags are inserted as super characters. The +-symbols represent spaces (word boundaries).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : The AMR of Hunger strikers were affected by France's heat wave. displayed as the set of instance, attribute and relation triples.</head><label>1</label><figDesc></figDesc><table>Instance 
Attribute 
Relation 

(instance, a, affect-01) 
(TOP, a, affect-01) (ARG0, a, w) 
(instance, w, wave-04) 
(wiki, c, France) 
(ARG1, a, p) 
(instance, h2, heat) 
(op1, n, France) 
(location, w, c) 
(instance, c, country) 
(ARG1, w, h2) 
(instance, n, name) 
(name, c, n) 
(instance, p, person) 
(ARG0, s, p) 
(instance, s, strike-02) 
(mod, s, h) 
(instance, h, hunger-01) 
(ARG0, h, p) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parameter settings of the seq2seq model. 
Parameter 
Value Parameter Value 

Layers 
2 
RNN type 
brnn 
Nodes 
500 
Dropout 
0.3 
Epochs 
20-25 Vocabulary 
100-200 
Optimizer 
sgd 
Max length 
750 
Learning rate 0.1 
Beam size 
5 
Decay 
0.7 
Replace unk true 

Following Barzdins and Gosko </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics of the different pruning methods. Methods were applied on the output of our 
baseline model on the dev set. 

Model 
Nodes pruned AMRs changed F-score 

Baseline 
0 
0 
54.8 

Removing all re-occurrent nodes 
1426 
689 
55.4 
Removing re-occurrent nodes with same parent 
135 
95 
55.0 
Removing re-occurrent nodes with frequency &gt;2 
427 
249 
55.3 
Removing all re-occurrent nodes with same 
parent, but also nodes with frequency &gt;2 
496 
302 
55.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Baseline Results Semantic Parsing on LDC2016E25. 
Type 
Dev Diff 
Test Diff 

Baseline 
seq2seq 
54.8 
53.1 

Post-processing 
Pruning 
55.5 + 0.7 53.7 + 0.6 
Restoring Co-reference 55.7 + 0.9 54.2 + 1.1 
Wikification 
55.8 + 1.0 54.1 + 1.0 

All post-processing 
57.3 +2.5 
55.5 + 2.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 6 : Results of the improvements in isolation (without post-processing).</head><label>6</label><figDesc></figDesc><table>Type 
Dev 
Diff 
Test 
Diff 

Baseline 
seq2seq 
54.8 
53.1 

AMR Re-ordering 
Best 
56.8 
+ 2.0 
55.1 
+ 2.0 
Doubling 
60.0 
+ 5.2 
58.3 
+ 5.2 

Introducing Super Characters Relations 
58.3 
+ 3.5 
57.4 
+ 4.3 

Adding POS Tags 
PTB 
58.2 
+ 3.4 
57.2 
+ 4.1 

Training optimization 
Averaging 
54.9 
+ 0.1 
53.4 
+ 0.3 
Pre-training 
59.4 
+ 4.6 
58.6 
+ 5.5 
Both 
59.5 
+ 4.7 
58.7 
+ 5.6 

Adding Silver Standard Data 
Adding 20k 
62.2 
+ 7.4 
60.0 
+ 6.9 
Adding 50k 
64.7 
+ 9.9 
62.9 
+ 9.8 
Adding 75k 
65.7 + 10.9 63.7 + 10.6 
Adding 100k 65.8 + 11.0 64.0 + 10.9 
Adding 500k 63.8 
+ 9.0 
62.1 
+ 9.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 7 : F-scores for our neural models, combining the different improvement methods.</head><label>7</label><figDesc></figDesc><table>Post-proc 
Adding 
100k Silver 
POS tags 
Super 
Chars 

Re-ordering 
Best 

Optimize 
Training 
Dev Test 

✖ 
✖ 
✖ 
✖ 
✖ 
✖ 
54.8 53.1 
✔ 
✖ 
✖ 
✖ 
✖ 
✖ 
57.3 55.5 
✔ 
✖ 
✔ 
✔ 
✔ 
✔ 
65.1 64.0 
✔ 
✔ 
✖ 
✖ 
✖ 
✖ 
68.0 66.4 
✔ 
✔ 
✔ 
✖ 
✖ 
✖ 
68.9 67.3 
✔ 
✔ 
✔ 
✔ 
✖ 
✖ 
70.4 69.0 
✔ 
✔ 
✔ 
✔ 
✔ 
✖ 
69.0 68.0 
✔ 
✔ 
✔ 
✔ 
✖ 
✔ 
71.9 71.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>F-scores for AMR parsing. Comparison with previously published results on the test set. 
Authors 
Model 
Train set (gold) F-score 

Flanigan et al. (2014) 
JAMR-14 
LDC2013E117 
58.0 
Damonte et al. (2017) 
AMR-eager 
LDC2015E86 
64.0 
Artzi et al. (2015) 
CCG parsing 
LDC2014T12 
66.3 
Wang et al. (2015) 
CAMR 
LDC2015E86 
66.5 
Flanigan et al. (2016) 
JAMR-16 
LDC2015E86 
67.0 
Pust et al. (2015) 
SBMT 
LDC2015E86 
67.1 

Barzdins and Gosko (2016) char-based seq2seq 
LDC2015E86 
43.0 
Peng et al. (2017) 
word-based seq2seq 
LDC2015E86 
52.0 
Konstas et al. (2017) 
word-based seq2seq 
LDC2015E86 
55.5 
Konstas et al. (2017) 
word-based seq2seq + giga 
LDC2015E86 
62.1 
Foland and Martin (2017) 
5 bi-LSTM networks (word-based) 
LDC2015E86 
70.7 

This article 
char-based seq2seq model + silver 
LDC2015E86 
68.5 
This article 
char-based seq2seq model + silver 
LDC2016E25 
71.0 

Damonte et al. (2017</table></figure>

			<note place="foot" n="2">. We did experiment with word-based models, but they never obtained F-scores higher than 30.0. This is in line with Peng et al. (2017) and Konstas et al. (2017), who only arrived at their final F-scores by applying extensive anonymization methods.</note>

			<note place="foot" n="3">. All pre-and post-processing scripts are available at https://github.com/RikVN/AMR 4. Taken from https://github.com/didzis/tensorflowAMR/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>First of all we would like to thank Antonio Toral and Lasha Abzianidze for helpful discussion on neural AMR parsing and machine translation. We thank the three anonymous reviewers for their comments. We would also like to thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine high performance computing cluster. We also used a Tesla K40 GPU, which was kindly donated to us by the NVIDIA Corporation. This work was funded by the NWO-VICI grant "Lost in Translation Found in Meaning" (288-89-003).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ludmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc-Duy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="242" to="247" />
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG Semantic Parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf˜hermjakobulf˜</forename><surname>Ulf˜hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-2322" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guntis</forename><surname>Barzdins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didzis</forename><surname>Gosko</surname></persName>
		</author>
		<title level="m">RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Developing a large semantically annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Valerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noortje</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venhuizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC 2012)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC 2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3196" to="3200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Meaning Factory at SemEval-2016 Task 8: Producing AMRs with Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessel</forename><surname>Haagsma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1179" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open-Domain Semantic Parsing with Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015)</title>
		<editor>Megyesi, Beáta</editor>
		<meeting>the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WideCoverage Semantic Representations from a CCG Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics (COLING &apos;04)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING &apos;04)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1240" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Groningen Meaning Bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noortje</forename><surname>Venhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Linguistic Annotation</title>
		<editor>Ide, Nancy and James Pustejovsky</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="463" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><surname>Butler</surname></persName>
		</author>
		<title level="m">The Semantics of Grammatical Dependencies</title>
		<imprint>
			<publisher>Emerald Group Publishing Limited</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Smatch: an Evaluation Metric for Semantic Feature Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NYU-MILA Neural Machine Translation Systems for WMT16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation, Association for Computational Linguistics</title>
		<meeting>the First Conference on Machine Translation, Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="268" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrapping POS taggers using unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="49" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kučerová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimal Recursion Semantics: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research on Language and Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="281" to="332" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving Efficiency and Accuracy in Multilingual Entity Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)</title>
		<meeting>the 9th International Conference on Semantic Systems (I-Semantics)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Incremental Parser for Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="536" to="546" />
		</imprint>
	</monogr>
	<note>Long Papers, Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stronger Baselines for Trustable Results in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language to Logical Form with Neural Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<title level="m">CMU at SemEval-2016 task 8: Graph-based AMR parsing with infinite ramp loss, Proceedings of SemEval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1202" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Discriminative Graph-Based Parser for the Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation Parsing using LSTM Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Foland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sennrich</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W16-2316" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="319" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02810</idno>
		<title level="m">Open-NMT: Open-Source Toolkit for Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08381</idno>
		<title level="m">Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>accepted in ACL-2017</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Compositional Semantics for Open Domain Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of COLING 2012, Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3350</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<title level="m">SemEval-2016 Task 8: Meaning Representation Parsing, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1063" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Addressing the Data Sparsity Issue in Neural AMR Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parsing English into abstract meaning representation using syntax-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="218" to="239" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<title level="m">Producing AMRs with Neural Semantic Parsing, Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="929" to="933" />
		</imprint>
	</monogr>
	<note>The Meaning Factory at SemEval-2017 Task</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Transition-based Algorithm for AMR Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
