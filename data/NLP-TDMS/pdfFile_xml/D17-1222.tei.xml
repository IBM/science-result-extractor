<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Recurrent Generative Decoder for Abstractive Text Summarization *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ‡ AI Lab</orgName>
								<orgName type="institution" key="instit2">Tencent Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ‡ AI Lab</orgName>
								<orgName type="institution" key="instit2">Tencent Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Lidong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ‡ AI Lab</orgName>
								<orgName type="institution" key="instit2">Tencent Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
							<email>zhwang@se.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong ‡ AI Lab</orgName>
								<orgName type="institution" key="instit2">Tencent Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory on High Confidence Software Technologies (Sub-Lab, CUHK)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Recurrent Generative Decoder for Abstractive Text Summarization *</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2091" to="2100"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new framework for ab-stractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the dis-criminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document <ref type="bibr" target="#b10">(Edmundson, 1969;</ref><ref type="bibr" target="#b27">Luhn, 1958;</ref><ref type="bibr" target="#b32">Nenkova and McKeown, 2012)</ref>. Different from the common extraction-based and compression-based methods, abstraction-based methods aim at constructing new sentences as summaries, thus they require a deeper understanding of the text and the capability of generating new sentences, which provide an obvious advantage in improving the focus of a summary, reducing the redundancy, and keeping a good compression rate ( <ref type="bibr" target="#b34">Rush et al., 2015;</ref><ref type="bibr" target="#b31">Nallapati et al., 2016</ref>). *</p><p>The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apple sues Qualcomm for nearly $1 billion</head><p>Twitter fixes botched @POTUS account transfer Track Trump's 100-day promises, Silicon Valley-style</p><p>The emergence of the 'cyber cold war' Tesla Autopilot not defective in fatal crash</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Twitter mostly meets modest diversity goals</head><p>Uber to pay $20 million for misleading drivers top stories_ <ref type="figure">Figure 1</ref>: Headlines of the top stories from the channel "Technology" of CNN.</p><p>Some previous research works show that human-written summaries are more abstractive <ref type="bibr" target="#b18">(Jing and McKeown, 2000</ref>). Moreover, our investigation reveals that people may naturally follow some inherent structures when they write the abstractive summaries. To illustrate this observation, we show some examples in <ref type="figure">Figure 1</ref>, which are some top story summaries or headlines from the channel "Technology" of CNN. After analyzing the summaries carefully, we can find some common structures from them, such as "What", "What-Happened" , "Who Action What", etc. For example, the summary "Apple sues Qualcomm for nearly $1 billion" can be structuralized as "Who (Apple) Action (sues) What (Qualcomm)". Similarly, the summaries " <ref type="bibr">[Twitter]</ref> [fixes] [botched @POTUS account transfer]", " <ref type="bibr">[Uber]</ref> [to pay] [$20 million] for misleading drivers", and "[Bipartisan bill] aims to <ref type="bibr">[reform]</ref> [H-1B visa system]" also follow the structure of "Who Action What". The summary "The emergence of the 'cyber cold war"' matches with the structure of "What", and the summary "St. Louis' public library computers hacked" follows the structure of "What-Happened".</p><p>Intuitively, if we can incorporate the latent structure information of summaries into the abstractive summarization model, it will improve the quality of the generated summaries. However, very few existing works specifically consider the latent structure information of summaries in their summarization models. Although a very popular neural network based sequence-to-sequence (seq2seq) framework has been proposed to tackle the abstractive summarization problem <ref type="bibr" target="#b26">(Lopyrev, 2015;</ref><ref type="bibr" target="#b34">Rush et al., 2015;</ref><ref type="bibr" target="#b31">Nallapati et al., 2016)</ref>, the calculation of the internal decoding states is entirely deterministic. The deterministic transformations in these discriminative models lead to limitations on the representation ability of the latent structure information. <ref type="bibr" target="#b28">Miao and Blunsom (2016)</ref> extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they did not consider the recurrent dependencies in their generative model leading to limited representation ability.</p><p>To tackle the above mentioned problems, we design a new framework based on sequenceto-sequence oriented encoder-decoder model equipped with a latent structure modeling component. We employ Variational Auto-Encoders (VAEs) ( <ref type="bibr" target="#b20">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b33">Rezende et al., 2014</ref>) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling. However, the standard framework of VAEs is not designed for sequence modeling related tasks. Inspired by <ref type="bibr" target="#b9">(Chung et al., 2015)</ref>, we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder (DRGD) for latent structure modeling.</p><p>Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework. The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information. All the neural parameters are learned by back-propagation in an end-to-end training paradigm.</p><p>The main contributions of our framework are summarized as follows: (1) We propose a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGD) to model and learn the latent structure information implied in the target summaries of the training data. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. (2) Both the generative latent structural information and the discriminative deterministic variables are jointly considered in the generation process of the abstractive summaries. (3) Experimental results on some benchmark datasets in different languages show that our framework achieves better performance than the state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document ( <ref type="bibr" target="#b32">Nenkova and McKeown, 2012)</ref>. Traditionally, the summarization methods can be classified into three categories: extraction-based methods ( <ref type="bibr" target="#b12">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b13">Goldstein et al., 2000;</ref><ref type="bibr" target="#b38">Wan et al., 2007;</ref><ref type="bibr" target="#b29">Min et al., 2012;</ref><ref type="bibr">Nalla- pati et al., 2017;</ref><ref type="bibr" target="#b6">Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b3">Cao et al., 2016;</ref><ref type="bibr" target="#b36">Song et al., 2017)</ref>, compression-based methods ( <ref type="bibr" target="#b22">Li et al., 2013;</ref><ref type="bibr" target="#b40">Wang et al., 2013;</ref><ref type="bibr" target="#b1">Li et al., 2015</ref>, and abstraction-based methods. In fact, previous investigations show that human-written summaries are more abstractive ( <ref type="bibr" target="#b0">Barzilay and McKeown, 2005;</ref>. Abstraction-based approaches can generate new sentences based on the facts from different source sentences. <ref type="bibr" target="#b0">Barzilay and McKeown (2005)</ref> employed sentence fusion to generate a new sentence.  proposed a more fine-grained fusion framework, where new sentences are generated by selecting and merging salient phrases. These methods can be regarded as a kind of indirect abstractive summarization, and complicated constraints are used to guarantee the linguistic quality.</p><p>Recently, some researchers employ neural network based framework to tackle the abstractive summarization problem. <ref type="bibr" target="#b34">Rush et al. (2015)</ref> proposed a neural network based model with local attention modeling, which is trained on the Gigaword corpus, but combined with an additional loglinear extractive summarization model with handcrafted features. <ref type="bibr" target="#b15">Gu et al. (2016)</ref> integrated a copying mechanism into a seq2seq framework to improve the quality of the generated summaries. <ref type="bibr" target="#b5">Chen et al. (2016)</ref> proposed a new attention mechanism that not only considers the important source segments, but also distracts them in the decoding step in order to better grasp the overall meaning of input documents. <ref type="bibr" target="#b31">Nallapati et al. (2016)</ref> utilized a trick to control the vocabulary size to improve the training efficiency. The calculations in these methods are all deterministic and the representation ability is limited. <ref type="bibr" target="#b28">Miao and Blunsom (2016)</ref> extended the seq2seq framework and proposed a generative model to capture the latent summary information, but they do not consider the recurrent dependencies in their generative model leading to limited representation ability.</p><p>Some research works employ topic models to capture the latent information from source documents or sentences. <ref type="bibr" target="#b39">Wang et al. (2009)</ref> proposed a new Bayesian sentence-based topic model by making use of both the term-document and termsentence associations to improve the performance of sentence selection. Celikyilmaz and HakkaniTur (2010) estimated scores for sentences based on their latent characteristics using a hierarchical topic model, and trained a regression model to extract sentences. However, they only use the latent topic information to conduct the sentence salience estimation for extractive summarization. In contrast, our purpose is to model and learn the latent structure information from the target summaries and use it to enhance the performance of abstractive summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the basic framework of our approach is a neural network based encoderdecoder framework for sequence-to-sequence learning. The input is a variable-length sequence X = {x 1 , x 2 , . . . , x m } representing the source text. The word embedding x t is initialized randomly and learned during the optimization process. The output is also a sequence Y = {y 1 , y 2 , . . . , y n }, which represents the generated abstractive summaries. Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>) is employed as the basic sequence modeling component for the encoder and the decoder. For latent structure modeling, we add historical dependencies on the latent variables of Variational Auto-Encoders (VAEs) and propose a deep recurrent generative decoder (DRGD) to distill the complex latent structures implied in the target summaries of the training data. Finally, the abstractive summaries will be decoded out based on both the discriminative deterministic variables H and the generative latent structural information Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent Generative Decoder</head><p>Assume that we have obtained the source text representation h e ∈ R k h . The purpose of the decoder is to translate this source code h e into a series of hidden states {h d 1 , h d 2 , . . . , h d n }, and then revert these hidden states to an actual word sequence and generate the summary.</p><p>For standard recurrent decoders, at each time step t, the hidden state h d t ∈ R k h is calculated using the dependent input symbol y t−1 ∈ R kw and the previous hidden state h d t−1 :</p><formula xml:id="formula_0">h d t = f (y t−1 , h d t−1 ) (1)</formula><p>where f (·) is a recurrent neural network such as vanilla RNN, Long Short-Term Memory (LSTM) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997)</ref>, and Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>). No matter which one we use for f (·), the common transformation operation is as follows:</p><formula xml:id="formula_1">h d t = g(W d yh y t−1 + W d hh h d t−1 + b d h )<label>(2)</label></formula><p>where W d yh ∈ R k h ×kw and W d hh ∈ R k h ×k h are the linear transformation matrices. b d h is the bias. k h is the dimension of the hidden layers, and k w is the dimension of the word embeddings. g(·) is the non-linear activation function. From Equation 2, we can see that all the transformations are deterministic, which leads to a deterministic recurrent hidden state h d t . From our investigations, we find that the representational power of such deterministic variables are limited. Some more complex latent structures in the target summaries, such as the high-level syntactic features and latent topics, cannot be modeled effectively by the deterministic operations and variables.</p><p>Recently, a generative model called Variational Auto-Encoders (VAEs) ( <ref type="bibr" target="#b20">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b33">Rezende et al., 2014</ref>) shows strong capability in modeling latent random variables and improves the performance of tasks in different fields such as sentence generation <ref type="bibr" target="#b2">(Bowman et al., 2016</ref>) and image generation ( <ref type="bibr" target="#b14">Gregor et al., 2015</ref>  introducing the historical latent variable dependencies to make it be capable of modeling sequence data. Our proposed latent structure modeling framework can be viewed as a sequence generative model which can be divided into two parts: inference (variational-encoder) and generation (variational-decoder). As shown in the decoder component of <ref type="figure" target="#fig_0">Figure 2</ref>, the input of the original VAEs only contains the observed variable y t , and the variational-encoder can map it to a latent variable z ∈ R kz , which can be used to reconstruct the original input. For the task of summarization, in the sequence decoder component, the previous latent structure information needs to be considered for constructing more effective representations for the generation of the next state. For the inference stage, the variational-encoder can map the observed variable y &lt;t and the previous latent structure information z &lt;t to the posterior probability distribution of the latent structure variable p θ (z t |y &lt;t , z &lt;t ). It is obvious that this is a recurrent inference process in which z t contains the historical dynamic latent structure information. Compared with the variational inference process p θ (z t |y t ) of the typical VAEs model, the recurrent framework can extract more complex and effective latent structure features implied in the sequence data.</p><p>For the generation process, based on the latent structure variable z t , the target word y t at the time step t is drawn from a conditional probability distribution p θ (y t |z t ). The target is to maximize the probability of each generated summary y = {y 1 , y 2 , . . . , y T } based on the generation process according to:</p><formula xml:id="formula_2">p θ (y) = T t=1 p θ (y t |z t )p θ (z t )dz t<label>(3)</label></formula><p>For the purpose of solving the intractable integral of the marginal likelihood as shown in Equation 3, a recognition model q φ (z t |y &lt;t , z &lt;t ) is introduced as an approximation to the intractable true posterior p θ (z t |y &lt;t , z &lt;t ). The recognition model parameters φ and the generative model parameters θ can be learned jointly. The aim is to reduce the Kulllback-Leibler divergence (KL) between q φ (z t |y &lt;t , z &lt;t ) and p θ (z t |y &lt;t , z &lt;t ):</p><formula xml:id="formula_3">D KL [q φ (z t |y &lt;t , z &lt;t )p θ (z t |y &lt;t , z &lt;t )] = z q φ (z t |y &lt;t , z &lt;t ) log q φ (z t |y &lt;t , z &lt;t ) p θ (z t |y &lt;t , z &lt;t ) dz = E q φ (zt|y&lt;t,z&lt;t) [log q φ (z t |·) − log p θ (z t |·)]</formula><p>where · denotes the conditional variables y &lt;t and z &lt;t . Bayes rule is applied to p θ (z t |y &lt;t , z &lt;t ), and we can extract log p θ (z) from the expectation, transfer the expectation term E q φ (zt|y&lt;t,z&lt;t) back to KL-divergence, and rearrange all the terms. Consequently the following holds:</p><formula xml:id="formula_4">log p θ (y &lt;t ) = D KL [q φ (z t |y &lt;t , z &lt;t )p θ (z t |y &lt;t , z &lt;t )] + E q φ (zt|y&lt;t,z&lt;t) [log p θ (y &lt;t |z t )] − D KL [q φ (z t |y &lt;t , z &lt;t )p θ (z t )]<label>(4)</label></formula><p>Let L(θ, φ; y) represent the last two terms from the right part of Equation 4:</p><formula xml:id="formula_5">L(θ, ϕ; y) = E q φ (zt|y&lt;t,z&lt;t) T t=1 log p θ (y t |z t ) − D KL [q φ (z t |y &lt;t , z &lt;t )p θ (z t )]<label>(5)</label></formula><p>Since the first KL-divergence term of Equation 4 is non-negative, we have log p θ (y &lt;t ) ≥ L(θ, φ; y) meaning that L(θ, φ; y) is a lower bound (the objective to be maximized) on the marginal likelihood. In order to differentiate and optimize the lower bound L(θ, φ; y), following the core idea of VAEs, we use a neural network framework for the probabilistic encoder q φ (z t |y &lt;t , z &lt;t ) for better approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Abstractive Summary Generation</head><p>We also design a neural network based framework to conduct the variational inference and generation for the recurrent generative decoder component similar to some design in previous works <ref type="bibr" target="#b20">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b33">Rezende et al., 2014;</ref><ref type="bibr" target="#b14">Gregor et al., 2015</ref>). The encoder component and the decoder component are integrated into a unified abstractive summarization framework. Considering that GRU has comparable performance but with less parameters and more efficient computation, we employ GRU as the basic recurrent model which updates the variables according to the following operations:</p><formula xml:id="formula_6">r t = σ(W xr x t + W hr h t−1 + b r ) z t = σ(W xz x t + W hz h t−1 + b z ) g t = tanh(W xh x t + W hh (r t h t−1 ) + b h ) h t = z t h t−1 + (1 − z t ) g t</formula><p>where r t is the reset gate, z t is the update gate. denotes the element-wise multiplication. tanh is the hyperbolic tangent activation function. As shown in the left block of <ref type="figure" target="#fig_0">Figure 2</ref>, the encoder is designed based on bidirectional recurrent neural networks. Let x t be the word embedding vector of the t-th word in the source sequence. GRU maps x t and the previous hidden state h t−1 to the current hidden state h t in feed-forward direction and back-forward direction respectively:</p><formula xml:id="formula_7">h t = GRU (x t , h t−1 ) h t = GRU (x t , h t−1 )<label>(6)</label></formula><p>Then the final hidden state h e t ∈ R 2k h is concatenated using the hidden states from the two directions: h e t = h t || h. As shown in the middle block of <ref type="figure" target="#fig_0">Figure 2</ref>, the decoder consists of two components: discriminative deterministic decoding and generative latent structure modeling.</p><p>The discriminative deterministic decoding is an improved attention modeling based recurrent sequence decoder. The first hidden state h d 1 is initialized using the average of all the source input states:</p><formula xml:id="formula_8">h d 1 = 1 T e T e t=1</formula><p>h e t , where h e t is the source input hidden state. T e is the input sequence length. The deterministic decoder hidden state h d t is calculated using two layers of GRUs. On the first layer, the hidden state is calculated only using the current input word embedding y t−1 and the previous hidden state h d 1 t−1 :</p><formula xml:id="formula_9">h d 1 t = GRU 1 (y t−1 , h d 1 t−1 )<label>(7)</label></formula><p>where the superscript d 1 denotes the first decoder GRU layer. Then the attention weights at the time step t are calculated based on the relationship of h d 1 t and all the source hidden states {h e t }. Let a i,j be the attention weight between h d 1 i and h e j , which can be calculated using the following formulation:</p><formula xml:id="formula_10">a i,j = exp(e i,j ) T e j =1 exp(e i,j ) e i,j = v T tanh(W d hh h d 1 i + W e hh h e j + b a )</formula><p>where</p><formula xml:id="formula_11">W d hh ∈ R k h ×k h , W e hh ∈ R k h ×2k h , b a ∈ R k h , and v ∈ R k h .</formula><p>The attention context is obtained by the weighted linear combination of all the source hidden states:</p><formula xml:id="formula_12">c t = T e j =1 a t,j h e j<label>(8)</label></formula><p>The final deterministic hidden state h d 2 t is the output of the second decoder GRU layer, jointly considering the word y t−1 , the previous hidden state h d 2 t−1 , and the attention context c t :</p><formula xml:id="formula_13">h d 2 t = GRU 2 (y t−1 , h d 2 t−1 , c t )<label>(9)</label></formula><p>For the component of recurrent generative model, inspired by some ideas in previous works <ref type="bibr" target="#b20">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b33">Rezende et al., 2014;</ref><ref type="bibr" target="#b14">Gregor et al., 2015)</ref>, we assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z t ) = N (0, I) and q φ (z t |y &lt;t , z &lt;t ) = N (z t ; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated via a multilayer perceptron. Precisely, given the word embedding y t−1 , the previous latent structure variable z t−1 , and the previous deterministic hidden state h d t−1 , we first project it to a new hidden space:</p><formula xml:id="formula_14">h ez t = g(W ez yh y t−1 +W ez zh z t−1 +W ez hh h d t−1 +b ez h )</formula><p>where</p><formula xml:id="formula_15">W ez yh ∈ R k h ×kw , W ez zh ∈ R k h ×kz , W ez hh ∈ R k h ×k h , and b ez h ∈ R k h . g is the sigmoid acti- vation function: σ(x) = 1/(1 + e −x )</formula><p>. Then the Gaussian parameters µ t ∈ R kz and σ t ∈ R kz can be obtained via a linear transformation based on h ez t :</p><formula xml:id="formula_16">µ t = W ez hµ h ez t + b ez µ log(σ 2 t ) = W hσ h ez t + b ez σ<label>(10)</label></formula><p>The latent structure variable z t ∈ R kz can be calculated using the reparameterization trick:</p><formula xml:id="formula_17">ε ∼ N (0, I), z t = µ t + σ t ⊗ ε<label>(11)</label></formula><p>where ε ∈ R kz is an auxiliary noise variable. The process of inference for finding z t based on neural networks can be teated as a variational encoding process.</p><p>To generate summaries precisely, we first integrate the recurrent generative decoding component with the discriminative deterministic decoding component, and map the latent structure variable z t and the deterministic decoding hidden state h d 2 t to a new hidden variable:</p><formula xml:id="formula_18">h dy t = tanh(W dy zh z t + W dz hh h d 2 t + b dy h ) (12)</formula><p>Given the combined decoding state h dy t at the time t, the probability of generating any target word y t is given as follows:</p><formula xml:id="formula_19">y t = ς(W d hy h dy t + b d hy )<label>(13)</label></formula><p>where W d hy ∈ R ky×k h and b d hy ∈ R ky . ς(·) is the softmax function. Finally, we use a beam search algorithm <ref type="bibr" target="#b21">(Koehn, 2004</ref>) for decoding and generating the best summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning</head><p>Although the proposed model contains a recurrent generative decoder, the whole framework is fully differentiable. As shown in Section 3.3, both the recurrent deterministic decoder and the recurrent generative decoder are designed based on neural networks. Therefore, all the parameters in our model can be optimized in an end-to-end paradigm using back-propagation. We use {X} N and {Y } N to denote the training source and target sequence. Generally, the objective of our framework consists of two terms. One term is the negative loglikelihood of the generated summaries, and the other one is the variational lower bound L(θ, φ; Y ) mentioned in Equation 5. Since the variational lower bound L(θ, φ; Y ) also contains a likelihood term, we can merge it with the likelihood term of summaries. The final objective function, which needs to be minimized, is formulated as follows:</p><formula xml:id="formula_20">J = 1 N N n=1 T t=1 − log p(y (n) t |y (n) &lt;t , X (n) ) +DKL q φ (z (n) t |y (n) &lt;t , z (n) &lt;t )p θ (z (n) t )<label>(14)</label></formula><p>4 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datesets</head><p>We train and evaluate our framework on three popular datasets. Gigawords is an English sentence summarization dataset prepared based on Annotated Gigawords 1 by extracting the first sentence from articles with the headline to form a sourcesummary pair. We directly download the prepared dataset used in ( <ref type="bibr" target="#b34">Rush et al., 2015</ref>  <ref type="bibr">2015</ref>). We take Part-I as the training set, Part-II as the development set, and Part-III as the test set.</p><p>There is a score in range 1 ∼ 5 labeled by human to indicate how relevant an article and its summary is. We only reserve those pairs with scores no less than 3. The size of the three sets are 2.4M, 8.7k, and 725 respectively. In our experiments, we only take Chinese character sequence as input, without performing word segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We use ROUGE score <ref type="bibr" target="#b25">(Lin, 2004</ref>) as our evaluation metric with standard options. The basic idea of ROUGE is to count the number of overlapping units between generated summaries and the reference summaries, such as overlapped n-grams, word sequences, and word pairs. F-measures of ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (R-L) and ROUGE-SU4 (R-SU4) are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparative Methods</head><p>We compare our model with some baselines and state-of-the-art methods. Because the datasets are quite standard, so we just extract the results from their papers. Therefore the baseline methods on different datasets may be slightly different.</p><p>• TOPIARY ( <ref type="bibr" target="#b41">Zajic et al., 2004</ref>) is the best on DUC2004 Task-1 for compressive text summarization. It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization.</p><p>• MOSES+ (Rush et al., 2015) uses a phrasebased statistical machine translation system trained on Gigaword to produce summaries. It also augments the phrase table with "deletion" rulesto improve the baseline performance, and MERT is also used to improve the quality of generated summaries.</p><p>• ABS and ABS+ (Rush et al., 2015) are both the neural network based models with local attention modeling for abstractive sentence summarization. ABS+ is trained on the Gigaword corpus, but combined with an additional log-linear extractive summarization model with handcrafted features.</p><p>• RNN and RNN-context (Hu et al., 2015) are two seq2seq architectures. RNN-context integrates attention mechanism to model the context.</p><p>• CopyNet ( <ref type="bibr" target="#b15">Gu et al., 2016</ref>) integrates a copying mechanism into the sequence-tosequence framework.</p><p>• RNN-distract ( <ref type="bibr" target="#b5">Chen et al., 2016</ref>) uses a new attention mechanism by distracting the historical attention in the decoding steps.</p><p>• RAS-LSTM and RAS-Elman (Chopra et al., 2016) both consider words and word positions as input and use convolutional encoders to handle the source information. For the attention based sequence decoding process, RAS-Elman selects Elman RNN <ref type="bibr">(El- man, 1990</ref>) as decoder, and RAS-LSTM selects Long Short-Term Memory architecture <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997</ref>).</p><p>• LenEmb ( <ref type="bibr" target="#b19">Kikuchi et al., 2016</ref>) uses a mechanism to control the summary length by considering the length embedding vector as the input.</p><p>• ASC+FSC 1 ( <ref type="bibr" target="#b28">Miao and Blunsom, 2016</ref>) uses a generative model with attention mechanism to conduct the sentence compression problem. The model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary.</p><p>• lvt2k-1sent and lvt5k-1sent ( <ref type="bibr" target="#b31">Nallapati et al., 2016</ref>) utilize a trick to control the vocabulary size to improve the training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Settings</head><p>For the experiments on the English dataset Gigawords, we set the dimension of word embeddings to 300, and the dimension of hidden states and latent variables to 500. The maximum length of documents and summaries is 100 and 50 respectively. The batch size of mini-batch training is 256. For DUC-2004, the maximum length of summaries is 75 bytes. For the dataset of LCSTS, the dimension of word embeddings is 350. We also set the dimension of hidden states and latent variables to 500. The maximum length of documents and summaries is 120 and 25 respectively, and the batch size is also 256. The beam size of the decoder was set to be 10. Adadelta (Schmidhuber, 2015) with hyperparameter ρ = 0.95 and = 1e − 6 is used for gradient based optimization. Our neural network based framework is implemented using Theano (Theano Development Team, 2016). We first depict the performance of our model DRGD by comparing to the standard decoders (StanD) of our own implementation. The comparison results on the validation datasets of Gigawords and LCSTS are shown in <ref type="table" target="#tab_2">Table 1</ref>. From the results we can see that our proposed generative decoders DRGD can obtain obvious improvements on abstractive summarization than the standard decoders. Actually, the performance of the standard   The results on the English datasets of Gigawords and DUC-2004 are shown in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref> respectively. Our model DRGD achieves the best summarization performance on all the ROUGE metrics. Although ASC+FSC 1 also uses a generative method to model the latent summary variables, the representation ability is limited and it cannot bring in noticeable improvements. It is worth noting that the methods lvt2k-1sent and lvt5k-1sent ( <ref type="bibr" target="#b31">Nallapati et al., 2016</ref>) utilize linguistic features such as parts-of-speech tags, namedentity tags, and TF and IDF statistics of the words as part of the document representation. In fact, extracting all such features is a time consuming work, especially on large-scale datasets such as Gigawords. lvt2k and lvt5k are not end-to-end style models and are more complicated than our model in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ROUGE Evaluation</head><p>The results on the Chinese dataset LCSTS are shown in <ref type="table" target="#tab_5">Table 4</ref>. Our model DRGD also achieves the best performance. Although CopyNet employs a copying mechanism to improve the summary quality and RNN-distract considers attention information diversity in their decoders, our model is still better than those two methods demonstrating that the latent structure information learned from target summaries indeed plays a role in abstractive summarization. We also believe that integrating the copying mechanism and coverage diversity in our framework will further improve the summarization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Summary Case Analysis</head><p>In order to analyze the reasons of improving the performance, we compare the generated summaries by DRGD and the standard decoders StanD used in some other works such as ( <ref type="bibr" target="#b8">Chopra et al., 2016)</ref>. The source texts, golden summaries, and the generated summaries are shown in <ref type="table">Table 5</ref>. From the cases we can observe that DRGD can indeed capture some latent structures which are consistent with the golden summaries. For example, our result for S(1) "Wuhan wins men's soccer title at Chinese city games" matches the "Who Action What" structure. However, the standard decoder StanD ignores the latent structures and generates some loose sentences, such as the results for S(1) "Results of men's volleyball at Chinese city games" does not catch the main points. The reason is that the recurrent variational auto-encoders used in our framework have better representation ability and can capture more effective and complicated latent structures from the sequence data. Therefore, the summaries generated by DRGD have consistent latent structures with the ground truth, leading to a better ROUGE evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a deep recurrent generative decoder (DRGD) to improve the abstractive summarization performance. The model is a sequenceto-sequence oriented encoder-decoder framework equipped with a latent structure modeling component. Abstractive summaries are generated based on both the latent variables and the deterministic states. Extensive experiments on benchmark <ref type="table">Table 5</ref>: Examples of the generated summaries. S(1): hosts wuhan won the men 's soccer title by beating beijing shunyi #-# here at the #th chinese city games on friday. Golden: hosts wuhan wins men 's soccer title at chinese city games. StanD: results of men 's volleyball at chinese city games. DRGD: wuhan wins men 's soccer title at chinese city games. S(2): UNK and the china meteorological administration tuesday signed an agreement here on long -and short-term cooperation in projects involving meteorological satellites and satellite meteorology. Golden: UNK china to cooperate in meteorology. StanD: weather forecast for major chinese cities. DRGD: china to cooperate in meteorological satellites. S(3): the rand gained ground against the dollar at the opening here wednesday , to #.# to the greenback from #.# at the close tuesday. Golden: rand gains ground. StanD: rand slightly higher against dollar. DRGD: rand gains ground against dollar. S(4): new zealand women are having more children and the country 's birth rate reached its highest level in ## years , statistics new zealand said on wednesday. Golden: new zealand birth rate reaches ##-year high. StanD: new zealand women are having more children birth rate hits highest level in ## years. DRGD: new zealand 's birth rate hits ##-year high. datasets show that DRGD achieves improvements over the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our deep recurrent generative decoder (DRGD) for latent structure modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>2 y 1 y 2 y 񮽙 2 log񮽙 񮽙 񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 2 [ ( , )|| (0, )] KL D N u N I 񮽙 1 x 2 x 3 x Attention input output z 1 z 2 z 3 z Encoder</head><label></label><figDesc></figDesc><table>). 
However, the standard VAEs is not designed for 
modeling sequence directly. Inspired by (Chung 
et al., 2015), we extend the standard VAEs by &lt;eos&gt; 

1 

y 

Decoder 
Variational Auto-Encoders 

&lt;eos&gt; 

4 

x 

variational-encoder 

variational-decoder 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : ROUGE-F1 on validation sets</head><label>1</label><figDesc></figDesc><table>Dataset System R-1 
R-2 
R-L 
GIGA 
StanD 32.69 15.29 30.60 
DRGD 36.25 17.61 33.55 
LCSTS 
StanD 33.88 21.49 31.05 
DRGD 36.71 24.00 34.10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : ROUGE-F1 on Gigawords</head><label>2</label><figDesc></figDesc><table>System 
R-1 
R-2 
R-L 
ABS 
29.55 11.32 26.42 
ABS+ 
29.78 11.89 26.97 
RAS-LSTM 
32.55 14.70 30.03 
RAS-Elman 
33.78 15.97 31.15 
ASC + FSC 1 
34.17 15.94 31.92 
lvt2k-1sent 
32.67 15.59 30.64 
lvt5k-1sent 
35.30 16.64 32.62 
DRGD 
36.27 17.57 33.62 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : ROUGE-Recall on DUC2004</head><label>3</label><figDesc></figDesc><table>System 
R-1 
R-2 
R-L 
TOPIARY 
25.12 6.46 20.12 
MOSES+ 
26.50 8.13 22.85 
ABS 
26.55 7.06 22.05 
ABS+ 
28.18 8.49 23.81 
RAS-Elman 
28.97 8.26 24.06 
RAS-LSTM 
27.41 7.69 23.06 
LenEmb 
26.73 8.39 23.88 
lvt2k-1sen 
28.35 9.46 24.59 
lvt5k-1sen 
28.61 9.42 25.24 
DRGD 
31.79 10.75 27.48 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : ROUGE-F1 on LCSTS</head><label>4</label><figDesc></figDesc><table>System 
R-1 
R-2 
R-L 
RNN 
21.50 8.90 18.60 
RNN-context 
29.90 17.40 27.20 
CopyNet 
34.40 21.60 31.30 
RNN-distract 
35.20 22.60 32.50 
DRGD 
36.99 24.15 34.21 

decoders is similar with those mentioned popular 
baseline methods. 
</table></figure>

			<note place="foot" n="1"> https://catalog.ldc.upenn.edu/ldc2012t21 2 http://duc.nist.gov/duc2004 3 http://www.weibo.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentence fusion for multidocument news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="328" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstractive multidocument summarization via phrase selection and merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1587" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoNLL</title>
		<imprint>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Attsum: Joint learning of focusing and summarization with neural attention. COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hybrid hierarchical model for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2754" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NAACL-HLT</publisher>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New methods in automatic extracting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold P Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-document summarization by sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kantrowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACLANLPWorkshop</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lcsts: A large scale chinese short text summarization dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangze</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1962" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cut and paste based text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pharaoh: a beam search decoder for phrase-based statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Machine Translation in the Americas</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="490" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reader-aware multi-document summarization via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1270" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Salience estimation via variational auto-encoders for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3497" to="3503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generating news headlines with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01712</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The automatic creation of literature abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Peter Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of research and development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting category-specific information for multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><forename type="middle">Kan</forename><surname>Ziheng Lin Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lim</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLING</title>
		<imprint>
			<biblScope unit="page" from="2903" to="2108" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of text summarization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Summarizing answers in non-factoid community question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongya</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Manifold-ranking based topic-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2903" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-document summarization using sentence-based topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="297" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
	<note>Radu Florian, and Claire Cardie</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<title level="m">Bbn/umd at duc-2004: Topiary. In HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
