<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word-Context Character Embeddings for Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing University &amp; Toutiao AI Lab</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<orgName type="institution" key="instit3">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit4">Nanjing University</orgName>
								<orgName type="institution" key="instit5">Nanjing University</orgName>
								<orgName type="institution" key="instit6">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenting</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing University &amp; Toutiao AI Lab</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<orgName type="institution" key="instit3">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit4">Nanjing University</orgName>
								<orgName type="institution" key="instit5">Nanjing University</orgName>
								<orgName type="institution" key="instit6">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>yue_zhang@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing University &amp; Toutiao AI Lab</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<orgName type="institution" key="instit3">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit4">Nanjing University</orgName>
								<orgName type="institution" key="instit5">Nanjing University</orgName>
								<orgName type="institution" key="instit6">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
							<email>huangsj@nlp.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing University &amp; Toutiao AI Lab</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<orgName type="institution" key="instit3">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit4">Nanjing University</orgName>
								<orgName type="institution" key="instit5">Nanjing University</orgName>
								<orgName type="institution" key="instit6">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
							<email>daixinyu@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing University &amp; Toutiao AI Lab</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<orgName type="institution" key="instit3">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit4">Nanjing University</orgName>
								<orgName type="institution" key="instit5">Nanjing University</orgName>
								<orgName type="institution" key="instit6">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
							<email>chenjj@nlp.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Nanjing University &amp; Toutiao AI Lab</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<orgName type="institution" key="instit3">Singapore University of Technology and Design</orgName>
								<orgName type="institution" key="instit4">Nanjing University</orgName>
								<orgName type="institution" key="instit5">Nanjing University</orgName>
								<orgName type="institution" key="instit6">Nanjing University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Word-Context Character Embeddings for Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="760" to="766"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural parsers have benefited from automatically labeled data via dependency-context word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tri-training baselines for leveraging auto-segmented data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network Chinese word segmentation (CWS) models ( <ref type="bibr" target="#b13">Zhang et al., 2016;</ref><ref type="bibr">Liu et al., 2016;</ref><ref type="bibr" target="#b1">Cai and Zhao, 2016</ref>) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features ( <ref type="bibr" target="#b16">Zheng et al., 2013;</ref><ref type="bibr">Pei et al., 2014;</ref><ref type="bibr">Ma and Hinrichs, 2015;</ref><ref type="bibr">Chen et al., 2015a</ref>). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies.</p><p>Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training <ref type="bibr" target="#b10">(Yarowsky, 1995;</ref><ref type="bibr">McClosky et al., 2006;</ref><ref type="bibr">Huang et al., 2010</ref>; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training ( <ref type="bibr" target="#b17">Zhou and Li, 2005;</ref><ref type="bibr">Li et al., 2014</ref>) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. * Equal contributions Tri-training has been used in neural parsing, giving considerable improvements for both of dependency ( <ref type="bibr" target="#b7">Weiss et al., 2015</ref>) and constituent parsing ( <ref type="bibr" target="#b5">Vinyals et al., 2015;</ref><ref type="bibr">Choe and Charniak, 2016)</ref>. Knowledge from auto-labeled data has also been used for parsing ( <ref type="bibr" target="#b0">Bansal et al., 2014;</ref><ref type="bibr">Melamud et al., 2016)</ref>, where word embeddings are trained on automatic dependency tree context. Such knowledge has also been proved effective in conventional discrete CWS models, such as label distribution information ( <ref type="bibr" target="#b6">Wang et al., 2011;</ref>). However, it has not been investigated for neural CWS.</p><p>We propose word-context character embeddings (WCC), using segmentation label information in the pre-training of unigram and bigram character embeddings. The method packs the label distribution information into the embeddings, which could be regarded as a way for knowledge parameterization. Our idea follows <ref type="bibr">Levy and Goldberg (2014)</ref>, who use dependency contexts to train word embeddings. Additionally, motivated by co-training, we propose multi-view wordcontext character embeddings for cross-domain segmentation, which pre-trains two types of embedding for in-domain and out-of-domain data, respectively. In-domain embeddings are used for solving data sparseness, and out-of-domain embeddings are used for domain adaptation.</p><p>Our proposed model is simple, efficient and effective, giving average 1% accuracy improvement on in-domain data and 3.5% on out-of-domain data, respectively, significantly out-performing self-training and tri-training methods for leveraging auto-segmented data. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation <ref type="bibr" target="#b9">(Xue, 2003;</ref><ref type="bibr">Low et al., 2005;</ref><ref type="bibr" target="#b15">Zhao et al., 2006</ref>). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word.</p><p>Following <ref type="bibr">Chen et al. (2015b)</ref>, a standard bi-LSTM model <ref type="bibr">(Graves, 2008</ref>) is used to assign segmentation label for each character. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w 1 , w 2 , w 3 , · · · , w N }, where w i is the i th character in the sentence, and N is the sentence length, we have a corresponding embedding e w i and e w i−1 w i for each character unigram w i and character bigram w i−1 w i , respectively. A forward word representation e f i is calculated as follows:  Given the representation r i , we use a scoring unit to score for each potential segment label. Given r i , the score of segment label M is:</p><formula xml:id="formula_0">e f i = concat 1 (e w i , e w i−1 w i ), = tanh(W 1 [e w i ; e w i−1 w i ])</formula><formula xml:id="formula_1">f i M = W M h, where h = concat 3 (r i , e M ), = tanh(W 3 [r i ; e M ])</formula><p>W M is the score matrix for label M, and e M is the label embedding for label M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word-Context Character Embeddings</head><p>Our model structure is a derivation from the skipgram model ( <ref type="bibr">Mikolov et al., 2013</ref>), similar to <ref type="bibr">Levy and Goldberg (2014)</ref>. Given a sentence with length n: {w 1 , w 2 , w 3 , · · · w n } and its corresponding segment labels: {l 1 , l 2 , l 3 , · · · l n }, the pre-training context of current character w t is the around characters in the windows with size c, together with their corresponding segment labels ( <ref type="figure">Figure 2</ref>). Characters w i and labels l i in the context are represented by vectors e c w i ∈ R d and e c l i ∈ R d , respectively, where d is the embedding dimensionality.</p><p>The word-context embedding of character w t is represented as e wt ∈ R d , which is trained by predicting the surrounding context representations e c w ′ and e c l i , parameterizing the labeled segmentation information in the embedding parameters. To capture order information ( <ref type="bibr">Ling et al., 2015)</ref>, we use different embedding matrices for context embedding in different context positions, training different embeddings for the same word when they reside on different locations as the context word. In particular, our context window size is five. As a result, each word has four different versions of e c , namely e c −1 , e c −2 , e c +1 , and e c +2 , each taking a distinct embedding matrix. Given the context window [w −2 , w −1 , w, w +1 , w +2 ], w −1 is the left first context word of the focus word w, e c −1,w i will be selected from embedding matrix E −1 , and w +1 is the right first word of w, e c +1,w i will be selected from embedding matrix E +1 .</p><p>Note that each character has two types of embeddings, where e w i is the embedding form of w i when w i is the focus word, and e c w i is the embedding form of w i when w i is used as a surrounding context word. We do not have e l i because l i only acts as the surrounding context. After pre-training, e w i will be used as the WCC embeddings.</p><p>The objective of our model is to maximize the average log probability of the context:</p><formula xml:id="formula_2">1 T T ∑ t=1 ∑ −c≤j≤c,j̸ =0</formula><p>log p(w t+j |w t ) + log p(t t+j |w t ) Negative sampling ( <ref type="bibr">Mikolov et al., 2013</ref>) is used, where log p(w t+j |w t ) and log p(t t+j |w t ) are computed as:</p><formula xml:id="formula_3">p(w t+j |w t ) = log σ(e c w t+j ⊤ e wt ) + k ∑ i=1 E w i ∼Pn(w) [log σ(−e c w i ⊤ e wt )]</formula><p>and</p><formula xml:id="formula_4">p(t t+j |w t ) = log σ(e c l t+j ⊤ e wt ) + k ∑ i=1 E l i ∼Pn(l) [log σ(−e c l i ⊤ e wt )],</formula><p>respectively, where P n (w) and P n (l) is the noise distributions and k is the size of negative samples for each data sample. Bigram embeddings are trained in the same way as unigram character embeddings. For out-ofdomain segmentation, we pre-train two embeddings for each token, extracting knowledge from the two domains, respectively.</p><formula xml:id="formula_5">上 来 了 马 在 马 上 骑 来 了 ride on horse up come le 他 E S S B S he immedially come le E S S B S S</formula><p>Figure 2: Word-context for the character ' 上' in two different sentences. The windows size c = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Set-up</head><p>We perform experiments on three standard datasets for Chinese word segmentation: PKU and MSR from the second SIGHAN bakeoff shared task, and Chinese Treebank 6.0 (CTB6). For PKU and MSR, 10% of the training data are randomly selected as development data. We follow <ref type="bibr" target="#b13">Zhang et al. (2016)</ref> to split the CTB6 corpus into training, development and testing sections. For evaluating cross-domain performance, we also experiment on Chinese novel data. Following <ref type="bibr" target="#b12">Zhang et al. (2014)</ref>, the training set of CTB5 is selected for training, and the manually annotated sentences of free Internet novel 'Zhuxian' (ZX) are selected as the development and test data (Liu and Zhang, 2012) <ref type="bibr">1</ref> . Chinese Gigaword (LDC2011T13, 4M) is used for in-domain unlabeled data. For out-of-domain data, 20K raw sentences of Zhuxian is used. We take self-training and tri-training as baselines, which also use large-scale auto-segmented data. For self-training, skip-gram pre-training and word-context character embedding, unlabeled corpus is segmented automatically by our baseline model. For tri-training, we additionally use the ZPar ( <ref type="bibr" target="#b14">Zhang and Clark, 2007)</ref> and ICTCLAS 2 as our base classifiers .</p><p>We use F1 to evaluate segmentation accuracy. The recalls of in-vocabulary (IV) and out-ofvocabulary (OOV) are also measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyper-Parameters</head><p>The hyper-parameters used in this work are listed in <ref type="table" target="#tab_0">Table 1. The values are selected according unigram dimension   50  bigram dimesion  50  label embedding dimention 32  LSTM hidden size  100  LSTM input size  100  learning rate</ref> 0.1 windows size 5  to the development set of CTB6. Many previous character-based CWS models use a transition matrix to model the tag dependency and CRF for structured inference ( <ref type="bibr">Pei et al., 2014;</ref><ref type="bibr">Chen et al., 2015a</ref>). However, we find that, the greedy model obtains comparable segmentation accuracies across CTB6, PKU and MSR, yet giving much fast speed <ref type="table" target="#tab_1">(Table 2)</ref>. Hence we adopt the greedy model as our baseline segmentation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Utilizing Varying-Scale Data</head><p>The results of self-training and tri-training with varying-scale training data are list in <ref type="table" target="#tab_3">Table 3</ref>, where +4X means adding 4 times the size of supervised training data into the training set. We find that self-training does not work well, and tritraining with 16X gives a 0.5% accuracy improvement. We adopt this setting for our baseline in the remaining experiments 3 . We also try to choose more effective examples for self-training and tri-training, by selecting training instances according to the base segmentation model score. However, the segmentation performances do not get improved. A possible reason is that the training instances with higher confidence are always shorter than the original sampled sentences, which may not be very helpful for semispervised segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">In-Domain Results</head><p>As shown in <ref type="table" target="#tab_4">Table 4</ref>, pre-training with conventional skip-gram embeddings gives only small improvements, which is consistent as findings of previous work <ref type="bibr">(Chen et al., 2015a</ref>   We compare our model with other state-of-theart segmentation models 5 , which are grouped into 3 classes, namely traditional segmentation models (non-nn), neural segmentation models (nn), and the combination of both neural and traditional discrete features (comb). Our simple model gives top accuracies compared with related work. <ref type="bibr">Liu et al. (2016)</ref>, <ref type="bibr" target="#b1">Cai and Zhao (2016)</ref> and <ref type="bibr" target="#b13">Zhang et al. (2016)</ref> propose to incorporate word embedding features in the neural CWS, pre-training the word embeddings in the large-scale labeled data. Different to them, we employ a simpler character level model containing word information, yet obtaining higher F1 scores. </p><formula xml:id="formula_6">若在 (if)_ 鬼王 (guiwang)_ 手上 (hand) 夺下 (wrest)_ 七星剑 (qixin sword)，_ 我 (I) 必 (must)_ 器重 (think highly of)_ 于 (at) 你 (you)</formula><p>baseline:</p><formula xml:id="formula_7">若在 (if)_ 鬼 (gui)_ 王 (king)_ 手上 (hand) 夺下 (wrest)_ 七 (seven)_ 星 (star)_ 剑 (sword)， 我 (I)_ 必 (must)_ 器 (ware)_ 重 (heavy) 于 (at)_ 你 (you)</formula><p>Figure 3: Case studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Out-of-Domain Results</head><p>We test out-of-domain performance of our model on the ZX dataset. We also use the multi-view word-context character embeddings (WCC) for cross domain segmentation, which uses two types of embeddings by simple vector concatenation. One type of embeddings is pre-trained on indomain data, and the other type is pre-trained on out-of-domain data. In such case, the multi-view embeddings includes cross-domain information, which may enhance the cross-domain segmentation performance ( <ref type="bibr">Mou et al., 2016</ref>). As shown in <ref type="table" target="#tab_6">Table 5</ref>, using word-context character (WCC) embeddings and multi-view wordcontext character embeddings both give significantly higher accuracy improvements compared with other semi-supervised methods. Additionally, we find that multi-view WCC embeddings give an extra 1% F1 score improvement over WCC embeddings. Our proposed model also significantly improves the OOV recall (ROOV) and IV recall (RIV). By studying the cases of segmented output <ref type="figure">(Figure 3)</ref>, we find that our model can recognize OOV words such as '鬼王', '七星剑' and the IV word '器重', which are incorrectly labeled by the baseline. This confirms that our proposed model is helpful for the data sparseness problem on closed domain and domain adaptation on across domain.</p><p>We also list the results of Zhang et al. (2014) and <ref type="bibr" target="#b12">Liu et al. (2014)</ref> on this dataset. <ref type="bibr" target="#b12">Liu et al. (2014)</ref> obtains better out-of-domain performance than our model. However, their results cannot be compared directly with ours because they use partial labeled URL link data from Chinese Wikipedia data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed word-context character embeddings for semi-supervised neural CWS, which makes the segmentation model more accurate on in-domain  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Baseline model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>backward representation e b i can be obtained in the same way. Then e f i and e b i are fed into forward and backward LSTM units at current position, ob- taining the corresponding forward and backward LSTM representations r lstm−f i and r lstm−b i , re- spectively. In the scoring layer, we first obtain a linear com- bination of r lstm−f i and r lstm−b i , which is the final representation at the i th position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Hyper-parameters.</head><label>1</label><figDesc></figDesc><table>System CTB6 PKU MSR Speed 
Greedy 
94.9 
95.0 
97.2 
14.7 
CRF 
95.0 
95.1 
97.2 
3.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparisons between greedy and CRF 
segmentation. Speed: tokens per millisecond. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>; Ma and Hinrichs,</head><label></label><figDesc></figDesc><table>Systems 
+4X +8X +16X +32X 
baseline 
94.9 
self-training 95.0 94.9 
94.9 
94.8 
tri-training 
95.2 95.3 
95.4 
95.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Results of self-training and tri-training on CTB6 with varying scaled training data.</head><label>3</label><figDesc></figDesc><table>Type 
System 
CTB6 PKU MSR 

non-nn 

Tseng et al. (2005) 
-
95.0 
96.4 
Sun et al. (2009) 
-
95.2 
97.3 
Wang et al. (2011) 
95.8 
-
-
Zhang et al. (2013) 
-
96.1 
97.5 

nn 

Zheng et al. (2013) 
-
92.4 
93.3 
Pei et al. (2014) 
-
95.2 
97.2 
Kong et al. (2015) 
-
90.6 
90.7 
Ma and Hinrichs (2015) 
-
95.1 
96.6 
Chen et al. (2015c) † 
-
94.8 
95.6 
Xu and Sun (2016) 
95.8 
96.1 
96.3 
Liu et al. (2016) 
95.5 
95.7 
97.6 
Zhang et al. (2016) 
95.4 
95.1 
97.0 
Cai and Zhao (2016) 
-
95.5 
96.5 
comb 
Zhang et al. (2016) 
96.0 
95.7 
97.7 

Ours 

baseline 
94.9 
95.0 
97.2 
+ self-training 
95.0 
94.8 
97.0 
+ tri-training 
95.5 
95.5 
97.4 
+ skip-gram embeddings 
95.3 
95.5 
97.4 
+ WCC embeddings 
96.2 
96.0 
97.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison with other models. 

2015; Cai and Zhao, 2016). Segmentation with 
self-training even shows accuracy drops on PKU 
and MSR. We speculate that the self-training by the 
neural CWS baseline is sensitive to the segmenta-
tion errors of the auto-labeled data. On average, 
our method obtains an absolute 1% accuracy im-
provement over the baseline, outperforming other 
semi-supervised method significantly 4 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on the out-of-domain data. Mod-
els with  † do not use large-scale data, models with 
 ‡ use in-domain large-scale data, and models with 
♯ use both in-domain, and out-of-domain large-

scale data. 

data, and more robust on the out-of-domain data. 
Our segmentation model is simple yet effective, 
achieving state-of-the-art segmentation accuracies 
on standard benchmarks. It can also be use-
ful for other NLP tasks with small labeled train-
ing data, but a large unlabeled data. Our code 
could be downloaded at https://github.com/ 
zhouh/WCC-Segmentation. </table></figure>

			<note place="foot" n="1"> http://zhangmeishan.github.io/ eacl14mszhang.zip 2 http://ictclas.nlpir.org/</note>

			<note place="foot" n="3"> For out-of-domain experiments, we include both the +16X and the 20K out-of-domain data for self-training and tri-training.</note>

			<note place="foot" n="4"> The p-values are below 0.01 using pairwise t-test. 5 Results with † are obtained from Cai and Zhao (2016), because results in the original paper use dictionary resources.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We would like to thank the anonymous reviewers for their insightful comments. We also thank Ji Ma and Meishan Zhang for their helpful discussions. This work was partially founded by the Natural Science Foundation of China (61672277, 71503124) and the China National 973 project 2014CB340301.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="809" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural word segmentation learning for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised feature transformation for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1303" to="1313" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving chinese word segmentation and pos tagging with semisupervised methods using large auto-analyzed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="309" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06158</idno>
		<title level="m">Structured training for neural network transition-based parsing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dependency-based gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 33rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="311" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Type-supervised domain adaptation for joint segmentation and pos-tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transition-based neural word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="840" to="847" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An improved chinese word segmentation system with conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="volume">1082117</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tri-training: Exploiting unlabeled data using three classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1529" to="1541" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and accurate shift-reduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
