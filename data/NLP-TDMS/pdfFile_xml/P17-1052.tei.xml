<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Pyramid Convolutional Neural Networks for Text Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30 -August 4, 2017. July 30 -August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
							<email>riejohnson@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab Shenzhen</orgName>
								<orgName type="institution">RJ Research Consulting Tarrytown</orgName>
								<address>
									<region>NY</region>
									<country>USA, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab Shenzhen</orgName>
								<orgName type="institution">RJ Research Consulting Tarrytown</orgName>
								<address>
									<region>NY</region>
									<country>USA, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Pyramid Convolutional Neural Networks for Text Categorization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="562" to="570"/>
							<date type="published">July 30 -August 4, 2017. July 30 -August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/P17-1052</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text catego-rization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover , it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers out-performs the previous best models on six benchmark datasets for sentiment classification and topic categorization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text categorization is an important task whose applications include spam detection, sentiment classification, and topic classification. In recent years, neural networks that can make use of word order have been shown to be effective for text categorization. While simple and shallow convolutional neural networks (CNNs) <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr">John- son and Zhang, 2015a</ref>) were proposed for this task earlier, more recently, deep and more complex neural networks have also been studied, assuming availability of relatively large amounts of training data (e.g., one million documents). Examples are deep character-level CNNs ( <ref type="bibr" target="#b0">Conneau et al., 2016</ref>), a complex combination of CNNs and recurrent neural networks (RNNs) ( <ref type="bibr" target="#b12">Tang et al., 2015)</ref>, and RNNs in a wordsentence hierarchy <ref type="bibr" target="#b13">(Yang et al., 2016)</ref>.</p><p>A CNN is a feedforward network with convolution layers interleaved with pooling layers. Essentially, a convolution layer converts to a vector every small patch of data (either the original data such as text or image or the output of the previous layer) at every location (e.g., 3-word windows around every word), which can be processed in parallel. By contrast, an RNN has connections that form a cycle. In its typical application to text, a recurrent unit takes words one by one as well as its own output on the previous word, which is parallel-processing unfriendly. While both CNNs and RNNs can take advantage of word order, the simple nature and parallel-processing friendliness of CNNs make them attractive particularly when large training data causes computational challenges.</p><p>There have been several recent studies of CNN for text categorization in the large training data setting. For example, in ( <ref type="bibr" target="#b0">Conneau et al., 2016)</ref>, very deep 32-layer character-level CNNs were shown to outperform deep 9-layer character-level CNNs of ( ). However, in <ref type="bibr" target="#b7">(Johnson and Zhang, 2016)</ref>, very shallow 1-layer word-level CNNs were shown to be more accurate and much faster than the very deep characterlevel CNNs of ( <ref type="bibr" target="#b0">Conneau et al., 2016)</ref>. Although character-level approaches have merit in not having to deal with millions of distinct words, shallow word-level CNNs turned out to be superior even when used with only a manageable number (30K) of the most frequent words. This demonstrates the basic fact -knowledge of word leads to a powerful representation. These results motivate us to pursue an effective and efficient design of deep wordlevel CNNs for text categorization. Note, however, that it is not as simple as merely replacing characters with words in character-level CNNs; doing so rather degraded accuracy in ( .</p><p>We carefully studied deepening of word-level CNNs in the large-data setting and found a deep but low-complexity network architecture with which the best accuracy can be obtained by increasing the depth but not the order of computation time -the total computation time is bounded by a constant. We call it deep pyramid CNN (DPCNN), as the computation time per layer decreases exponentially in a 'pyramid shape'. After converting discrete text to continuous representation, the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size (as well as per-layer computation) shrinks in a pyramid shape. The network depth can be treated as a meta-parameter. The computational complexity of this network is bounded to be no more than twice that of one convolution block. At the same time, as described later, the 'pyramid' enables efficient discovery of long-range associations in the text (and so more global information), as the network is deepened. This is why DPCNN can achieve better accuracy than the shallow CNN mentioned above (hereafter ShallowCNN), which can use only short-range associations. Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b</ref>) and later tested with large datasets in <ref type="bibr" target="#b7">(Johnson and Zhang, 2016)</ref>.</p><p>We show that DPCNN with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic classification.</p><p>2 Word-level deep pyramid CNN (DPCNN) for text categorization Overview of DPCNN: DPCNN is illustrated in <ref type="figure" target="#fig_0">Figure 1a</ref>. The first layer performs text region embedding, which generalizes commonly used word embedding to the embedding of text regions covering one or more words. It is followed by stacking of convolution blocks (two convolution layers and a shortcut) interleaved with pooling layers with stride 2 for downsampling. The final pooling layer aggregates internal data for each document into one vector. We use max pooling for all pooling layers. The key features of DPCNN are as follows.</p><p>• Downsampling without increasing the number of feature maps (dimensionality of layer output, 250 in <ref type="figure" target="#fig_0">Figure 1a</ref>). Downsampling enables efficient representation of long-range associations (and so more global information) in the text. By keeping the same number of feature maps, every 2-stride downsampling reduces the per-block computation by half and thus the total computation time is bounded by a constant.</p><p>• Shortcut connections with pre-activation and identity mapping (He et al., 2016) for enabling training of deep networks.</p><p>• Text region embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) (Johnson and Zhang, 2015b) for improving accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network architecture</head><p>Downsampling with the number of feature maps fixed After each convolution block, we perform max-pooling with size 3 and stride 2. That is, the pooling layer produces a new internal representation of a document by taking the component-wise maximum over 3 contiguous internal vectors, representing 3 overlapping text regions, but it does this only for every other possible triplet (stride 2) instead of all the possible triplets (stride 1). This 2-stride downsampling reduces the size of the internal representation of each document by half.</p><p>A number of models ( <ref type="bibr">Simonyan and Zisser- man, 2015;</ref><ref type="bibr" target="#b1">He et al., 2015</ref><ref type="bibr" target="#b2">He et al., , 2016</ref><ref type="bibr" target="#b0">Conneau et al., 2016)</ref> increase the number of feature maps whenever downsampling is performed, causing the total computational complexity to be a function of the depth. In contrast, we fix the number of feature maps, as we found that increasing the number of feature maps only does harm -increasing computation time substantially without accuracy improvement, as shown later in the experiments. With the number of feature maps fixed, the computation time for each convolution layer is halved (as the data size is halved) whenever 2-stride downsampling is performed, thus, forming a 'pyramid'. Therefore, with DPCNNs, the total computation time is bounded by a constant -twice the computation time of a single block, which makes our deep pyramid networks computationally attractive.</p><p>In addition, downsampling with stride 2 essentially doubles the effective coverage (i.e., coverage in the original document) of the convolution kernel; therefore, after going through downsampling L times, associations among words within a distance in the order of 2 L can be represented. Thus, deep pyramid CNN is computationally efficient for representing long-range associations and so more global information.</p><p>Shortcut connections with pre-activation To enable training of deep networks, we use additive shortcut connections with identity mapping, which can be written as z + f (z) where f represents the skipped layers ( <ref type="bibr" target="#b2">He et al., 2016)</ref>. In DPCNN, the skipped layers f (z) are two convolution layers with pre-activation. Here, pre-activation refers to activation being done before weighting instead of after as is typically done. That is, in the convolution layer of DPCNN, Wσ(x) + b is computed at every location of each document where a column vector x represents a small region (overlapping with each other) of input at each location, σ(·) is a component-wise nonlinear activation, and weights W and biases b (unique to each layer) are the parameters to be trained. The number of W's rows is the number of feature maps (also called the number of filters ( <ref type="bibr" target="#b1">He et al., 2015)</ref>) of this layer. We set activation σ(·) to the rectifier σ(x) = max(x, 0). In our implementation, we fixed the number of feature maps to 250 and the kernel size (the size of the small region covered by x) to 3, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>.</p><p>With pre-activation, it is the results of linear weighting (Wσ(x) + b) that travel through the shortcut, and what is added to them at a ⊕ ( <ref type="figure" target="#fig_0">Figure  1a</ref>) is also the results of linear weighting, instead of the results of nonlinear activation (σ(Wx + b)). Intuitively, such 'linearity' eases training of deep networks, similar to the role of constant error carousels in LSTM <ref type="bibr">(Hochreiter and Schmid- huder, 1997</ref>). We empirically observed that preactivation indeed outperformed 'post-activation', which is in line with the image results ( <ref type="bibr" target="#b2">He et al., 2016)</ref>.</p><p>No need for dimension matching Although the shortcut with pre-activation was adopted from the improved ResNet of ( <ref type="bibr" target="#b2">He et al., 2016)</ref>, our model is simpler than ResNet <ref type="figure" target="#fig_0">(Figure 1c</ref>), as all the shortcuts are exactly simple identity mapping (i.e., passing data exactly as it is) without any complication for dimension matching. When a shortcut meets the 'main street', the data from two paths need to have the same dimensionality so that they can be added; therefore, if a shortcut skips a layer that changes the dimensionality, e.g., by downsampling or by use of a different number of feature maps, then a shortcut must perform dimension matching. Dimension matching for increased number of feature maps, in particular, is typically done by projection, introducing more weight parameters to be trained. We eliminate the complication of dimension matching by not letting any shortcut skip a downsampling layer, and by fixing the number of feature maps throughout the network. The latter also substantially saves computation time as mentioned above, and we will show later in our experiments that on our tasks, we do not sacrifice anything for such a substantial efficiency gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text region embedding</head><p>A CNN for text categorization typically starts with converting each word in the text to a word vector (word embedding). We take a more general viewpoint as in (Johnson and Zhang, 2015b) and consider text region embedding -embedding of a region of text covering one or more words.</p><p>Basic region embedding We start with the basic setting where there is no unsupervised embedding. In the region embedding layer we compute Wx + b for each word of a document where input x represents a k-word region (i.e., window) around the word in some straightforward manner, and weights W and bias b are trained with the parameters of other layers. Activation is delayed to the pre-activation of the next layer. Now let v be the size of vocabulary, and let us consider the following three types of straightforward representation of a k-word region for x: (1) sequential input: the kv-dimensional concatenation of k one-hot vectors; (2) bow input: a v-dimensional bag-of-word (bow) vector; and (3) bag-of-n-gram input: e.g., a bag of word uni, bi, and trigrams contained in the region. Setting the region size k = 1, they all become word embedding.</p><p>A region embedding layer with the sequential input is equivalent to a convolution layer applied to a sequence of one-hot vectors representing a document, and this viewpoint was taken to describe the first layer of ShallowCNN in <ref type="bibr">(Johnson and Zhang, 2015a,b)</ref>. From the region embedding viewpoint, ShallowCNN is DPCNN's special case in which a region embedding layer is directly followed by the final pooling layer <ref type="figure" target="#fig_0">(Figure 1b)</ref>.</p><p>A region embedding layer with region size k &gt; 1 seeks to capture more complex concepts than single words in one weight layer, whereas a network with word embedding uses multiple weight layers to do this, e.g., word embedding followed by a convolution layer. In general, having fewer layers has a practical advantage of easier optimization. Beyond that, the optimum input type and the optimum region size can only be determined empirically. Our preliminary experiments indicated that when used with DPCNN (but not ShallowCNN), the sequential input has no advantage over the bow input -comparable accuracy with k times more weight parameters; therefore, we excluded the sequential input from our experiments 2 . The n-gram input turned out to be prone to overfitting in the supervised setting, likely due to its high representation power, but it is very useful as the input to unsupervised embeddings, which we discuss next.</p><p>Enhancing region embedding with unsupervised embeddings In ( <ref type="bibr">Zhang, 2015b, 2016)</ref>, it was shown that accuracy was substantially improved by extending ShallowCNN with unsupervised embeddings obtained by tvembedding training ('tv' stands for two views). We found that accuracy of DPCNN can also be improved in this manner. Below we briefly review tv-embedding training and then describe how we use the resulting unsupervised embeddings with DPCNN.</p><p>The tv-embedding training requires two views. For text categorization, we define a region of text as view-1 and its adjacent regions as view-2. Then using unlabeled data, we train a neural network of one hidden layer with an artificial task of predicting view-2 from view-1. The obtained hidden layer, which is an embedding function that takes view-1 as input, serves as an unsupervised embedding function in the model for text categorization. In (Johnson and Zhang, 2015b), we showed theoretical conditions on views and labels under which <ref type="table" target="#tab_0">of training documents 120K 450K  560K  560K 650K 1.4M  3M  3.6M  # of test documents  7.6K  60K  70K  38K  50K  60K  650K 400K  # of classes  4  5  14  2  5  10  5  2  Average #words  45  578  55  153  155  112  93  91</ref>  </p><note type="other">AG Sogou Dbpedia Yelp.p Yelp.f Yahoo Ama.f Ama.p #</note><note type="other">embeddings in this manner, which differ from one another in the region size and the vector representations of view-1 (input region) so that we can benefit from diversity. The region embedding layer of DPCNN computes Wx +</note><formula xml:id="formula_0">u∈U W (u) z (u) + b</formula><p>, where x is the discrete input as in the basic region embedding, and z (u) is the output of an unsupervised embedding function indexed by u. We will show below that use of unsupervised embeddings in this way consistently improves the accuracy of DPCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We report the experiments with DPCNNs in comparison with previous models and alternatives. The code is publicly available on the internet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>Data and data preprocessing To facilitate comparisons with previous results, we used the eight datasets compiled by , summarized in <ref type="table" target="#tab_0">Table 1</ref>. AG and Sogou are news. Dbpedia is an ontology. Yahoo consists of questions and answers from the 'Yahoo! Answers' website. Yelp and Amazon ('Ama') are reviews where '.p' (polarity) in the names indicates that labels are binary (positive/negative), and '.f' (full) indicates that labels are the number of stars. Sogou is in Romanized Chinese, and the others are in English. Classes are balanced on all the datasets. Data preprocessing was done as in <ref type="bibr" target="#b7">(Johnson and Zhang, 2016)</ref>. That is, upper-case letters were converted to lower-case letters. Unlike <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr" target="#b0">Conneau et al., 2016)</ref>, variable-sized documents were handled as variable-sized without any shortening or padding; however, the vocabulary size was limited to 30K words. For example, as also mentioned in <ref type="bibr" target="#b7">(Johnson and Zhang, 2016)</ref>, the complete vocabulary of the Ama.p training set contains 1.3M words. A vocabulary of 30K words is only a small portion of it, but it covers about 98% of the text and produced good accuracy as reported below.</p><p>Training protocol We held out 10K documents from the training data for use as a validation set on each dataset, and meta-parameter tuning was done based on the performance on the validation set.</p><p>To minimize a log loss with softmax, minibatch SGD with momentum 0.9 was conducted for n epochs (n was fixed to 50 for AG, 30 for Yelp.f/p and Dbpedia, and 15 for the rest) while the learning rate was set to η for the first <ref type="bibr">4</ref> 5 n epochs and then 0.1η for the rest <ref type="bibr">3</ref> . The initial learning rate η was considered to be a meta-parameter. The minibatch size was fixed to 100. Regularization was done by weight decay with the parameter 0.0001 and by optional dropout (Hinton et al., 2012) with 0.5 applied to the input to the top layer. In some cases overfitting was observed, and so we performed early stopping, based on the validation performance, after reducing the learning rate to 0.1η. Weights were initialized by the Gaussian distribution with zero mean and standard deviation 0.01. The discrete input to the region embedding layer was fixed to the bow input, and the region size was chosen from {1,3,5}, while fixing output dimensionality to 250 (same as convolution layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of unsupervised embedding training</head><p>To facilitate comparison with ShallowCNN, we matched our unsupervised embedding setting exactly with that of <ref type="bibr" target="#b7">(Johnson and Zhang, 2016)</ref>. That is, we trained the same four types of tvembeddings, which are embeddings of 5-and 9-word regions, each of which represents the input regions by either 30K-dim bow or 200K-dim  <ref type="table">Table 2</ref>: Error rates (%) on larger datasets in comparison with previous models. The previous results are roughly sorted in the order of error rates (best to worst). The best results and the second best are shown in bold and italic, respectively. 'tv' stands for tv-embeddings. 'w2v' stands for word2vec. '(w2v)' in row 7 indicates that the best results among those with and without word2vec pretraining are shown. Note that 'best' in rows 4&amp;6-8 indicates that we are giving an 'unfair' advantage to these models by choosing the best test error rate among a number of variations presented in the respective papers. bags of {1,2,3}-grams, retaining only the most frequent 30K words or 200K {1,2,3}-grams. Training was done on the labeled data (disregarding the labels), setting the training objectives to the prediction of adjacent regions of the same size as the input region (i.e., 5 or 9). Weighted square loss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><formula xml:id="formula_1">i,j α i,j (z i [j] − p i [j]) 2</formula><p>was minimized where i goes through instances, z represents the target regions by bow, p is the model output, and the weights α i,j were set to achieve the negative sampling effect. The dimensionality of unsupervised embeddings was set to 300 unless otherwise specified. Unsupervised embeddings were fixed during the supervised training -no fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>In the results below, the depth of DPCNN was fixed to 15 unless otherwise specified. Making it deeper did not substantially improve or degrade accuracy. Note that we count as depth the number of hidden weight layers including the region embedding layer but excluding unsupervised embeddings, therefore, 15 means 7 convolution blocks of 2 layers plus 1 layer for region embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Main results</head><p>Large data results We first report the error rates of our full model (DPCNN with 15 weight layers plus unsupervised embeddings) on the larger five datasets <ref type="table">(Table 2)</ref>. To put it into perspective, we also show the previous results in the literature.</p><p>The previous results are roughly sorted in the order of error rates from best to worst. On all the five datasets, DPCNN outperforms all of the previous results, which validates the effectiveness of our approach.</p><p>DPCNN can be regarded as a deep extension of ShallowCNN (row 2), sharing region embedding enhancement with diverse unsupervised embeddings. Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in <ref type="bibr" target="#b6">(Johnson and Zhang, 2015b</ref>) as a semi-supervised extension of <ref type="bibr" target="#b5">(Johnson and Zhang, 2015a)</ref>, and then it was tested on the large datasets in <ref type="bibr" target="#b7">(Johnson and Zhang, 2016)</ref>. The performance improvements of DPCNN over ShallowCNN indicates that the added depth is indeed useful, capturing more global information.    Though shown on one particular dataset Yelp.f, the trend is the same on the other four large datasets. els are word-level and therefore use the knowledge of word boundaries which character-level models have no access to. While this is arguably not an apple-to-apple comparison, since word boundaries can be obtained for free in many languages, we view our model as much more useful in practice. Row 7 shows the performance of deep wordlevel CNN from ( , which was designed to match their character-level models in complexity. Its relatively poor performance shows that it is not easy to design a high-performance deep word-level CNN.</p><p>Computation time In <ref type="figure" target="#fig_5">Figure 2</ref>, we plot error rates in relation to the computation time -the time spent for categorizing 10K documents using our implementation on a GPU. The right figure is a close-up of x ∈ <ref type="bibr">[0,</ref><ref type="bibr">20]</ref> of the left <ref type="figure">figure.</ref> It stands out in the left figure that the character-level CNN of ( <ref type="bibr" target="#b0">Conneau et al., 2016</ref>) is much slower than DPCNNs. This is partly because it increases the number of feature maps with downsampling (i.e., no pyramid) while it is deeper (32 weight layers), and partly because it deals with characters -there are more characters than words in each document. DPCNNs are more accurate than ShallowCNNs at the expense of more computation time due to the depth (15 layers vs. 1 layer). Nevertheless, their computation time is comparable -the points of both fit in the same range <ref type="bibr">[0,</ref><ref type="bibr">20]</ref>. The efficiency of DPCNNs is due to the exponential decrease of per-layer computation due to downsampling with the number of feature maps being fixed.</p><p>Comparison with non-pyramid variants Furthermore, we tested the following two 'nonpyramid' models for comparison. The first model doubles the number of feature maps at every other downsampling so that per-layer computation is  Small data results Now we turn to the results on the three smaller datasets in <ref type="table" target="#tab_5">Table 3</ref>. Again, the previous models are roughly sorted from best to worst. For these small datasets, the DPCNN performances with 100-dim unsupervised embed-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep</head><p>Unsup   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Empirical studies</head><p>We present some empirical results to validate the design choices. For this purpose, the larger five datasets were used to avoid the paucity of training data.</p><p>Depth <ref type="figure" target="#fig_7">Figure 4</ref> shows error rates of DPCNNs with 3, 7, and 15 weight layers (blue circles from left to right). For comparison, the ShallowCNN results (green 'x') from (Johnson and Zhang, 2016) are also shown. The x-axis represents the computation time (seconds for categorizing 10K documents on a GPU). For simplicity, the results without unsupervised embeddings are shown for all. The error rate improves as the depth increases. The results confirm the effectiveness of our strategy of deepening the network.</p><p>Unsupervised embeddings To study the effectiveness of unsupervised embeddings, we experimented with variations of DPCNN that differ only in whether/how to use unsupervised embeddings <ref type="table">(Table 4)</ref>. First, we compare DPCNNs with and without unsupervised embeddings. The model with unsupervised embeddings (row 1, copied from <ref type="table">Table 2</ref> for easy comparison) clearly outperforms the one without them (row 4), which confirms the effectiveness of the use of unsupervised embeddings. Second, in the proposed model (row 1), a region embedding layer receives two types of input, the output of unsupervised embedding functions and the high-dimensional discrete input such as a bow vector. Row 2 shows the results obtained by using unsupervised embeddings to produce sole input (i.e., no discrete vectors provided to the region embedding layer). Degradations of error rates are up to 0.32%, small but consistent. Since the discrete input add almost no computation cost due to its sparseness, its use is desirable. Third, a number of previous studies used unsupervised word embedding to initialize word embedding in neural networks and then fine-tune it as training proceeds (pretraining). The model in row 3 does this with DPCNN using word2vec ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>). The word2vec training was done on the training data (ignoring the labels),  <ref type="table">Table 4</ref>: Error rates (%) of DPCNN variations that differ in use of unsupervised embeddings. The rows are roughly sorted from best to worst.</p><p>same as tv-embedding training. This model (row 3) underperformed our proposed model (row 1).</p><p>We attribute the superiority of the proposed model to its use of richer information than a word embedding. These results support our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper tackled the problem of designing highperformance deep word-level CNNs for text categorization in the large training data setting. We proposed a deep pyramid CNN model which has low computational complexity, and can efficiently represent long-range associations in text and so more global information. It was shown to outperform the previous best models on six benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Our proposed model DPCNN. (b,c) Previous models for comparison. ⊕ indicates addition. The dotted red shortcuts in (c) perform dimension matching. DPCNN is dimension-matching free.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Computation per layer is halved after every pooling. Computation per layer is halved after every pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[</head><label></label><figDesc>JZ16]: Johnson and Zhang (2016), [YYDHSH16]: Yang et al. (2016), [CSBL16]: Conneau et al. (2016), [ZZL15]: Zhang et al. (2015)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Yang et al. (2016)'s hierarchical attention network (row 3) consists of RNNs in the word level and the sentence level. It is more complex than DPCNN due to the use of RNNs and linguistic knowledge for sentence segmentation. Similarly, Tang et al. (2015) pro- posed to use CNN or LSTM to represent each sen- tence in documents and then use RNNs. Although we do not have direct comparison with Tang et al.'s model, Yang et al. (2016) reports that their model outperformed Tang et al.'s model. Conneau et al. (2016) and Zhang et al. (2015) proposed deep character-level CNNs (row 4&amp;6). Their models underperform our DPCNN with relatively large differences in spite of their deepness. Our mod- 567</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>30</head><label>30</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Error rates and computation time. DPCNN, ShallowCNN, and Conneau et al. (2016)'s character-level CNN. The x-axis is the time in seconds spent for categorizing 10K documents using our implementation on Tesla M2070. The right figure is a close-up of x ∈ [0, 20] of the left figure. Though shown on one particular dataset Yelp.f, the trend is the same on the other four large datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison with non-pyramid models. Models of depth 11 and 15 are shown. No unsupervised embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Error rates of DPCNNs with various depths (3, 7, and 15). The x-axis is computation time. No unsupervised embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Data. Note that Yelp.f is a balanced subset of Yelp 2015. The results on these two datasets are not comparable.</head><label>1</label><figDesc></figDesc><table>unsupervised embeddings obtained this way are 
useful for classification. 
For use with DPCNN, we train several unsu-
pervised </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Error rates (%) on smaller datasets in comparison with previous models. The previous results 
are roughly sorted in the order of error rates (best to worst). Notation follows that of Table 2. 

3.2 

3.3 

3.4 

3.5 

0 2 4 6 8 
Error rate (%) 

Time 

Yelp.p 

</table></figure>

			<note place="foot" n="1"> Previous deep CNNs (either on image or text) tend to be more complex and irregular, having occasional increase of the number of feature maps.</note>

			<note place="foot" n="2"> This differs from ShallowCNN where the sequential input is often superior to bow input. We conjecture that when bow input is used in DPCNN, convolution layers following region embedding make up for the loss of local word order caused by bow input, as they use word order.</note>

			<note place="foot" n="3"> This learning rate scheduling method was used also in (Johnson and Zhang, 2015a,b, 2016). It was meant to reduce learning rate when error plateaus, as is often done on image tasks, e.g., (He et al., 2015), though for simplicity, the timing of reduction was fixed for each dataset.</note>

			<note place="foot" n="4"> Note that if we double the number of feature maps, it would increase the computation cost of the next layer by 4 times as it doubles the dimensionality of both input and output. On image, downsampling with stride 2 cancels it out as it makes data 4 times smaller by shrinking both horizontally and vertically, but text is one dimensional, and so downsampling with stride 2 merely halves data. That is why we doubled the number of feature maps at every other downsampling instead of at every downsampling to avoid exponential increase of computation time.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01781v1</idno>
		<title level="m">Very deep convolutional networks for natural language processing</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional neural networks for text categorization via region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00718</idno>
		<title level="m">Convolutional neural networks for text categorization: Shallow word-level vs. deep character-level</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01795v3</idno>
		<imprint>
			<date type="published" when="2016-08-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
