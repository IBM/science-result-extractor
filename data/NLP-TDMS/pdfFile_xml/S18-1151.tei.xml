<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 5-6, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Maldonado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ADAPT Centre Dublin Institute of Technology</orgName>
								<orgName type="institution">ADAPT Centre Trinity College Dublin</orgName>
								<address>
									<country>Ireland, Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Klubička</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ADAPT Centre Dublin Institute of Technology</orgName>
								<orgName type="institution">ADAPT Centre Trinity College Dublin</orgName>
								<address>
									<country>Ireland, Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
						<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018) <address><addrLine>New Orleans, Louisiana</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="924" to="927"/>
							<date type="published">June 5-6, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper describes a simple but competitive unsupervised system for hypernym discovery. The system uses skip-gram word embeddings with negative sampling, trained on specialised corpora. Candidate hypernyms for an input word are predicted based on cosine similarity scores. Two sets of word embedding models were trained separately on two specialised corpora: a medical corpus and a music industry corpus. Our system scored highest in the medical domain among the competing unsu-pervised systems but performed poorly on the music industry domain. Our approach does not depend on any external data other than raw specialised corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The SemEval-2018 shared task on Hypernymy Discovery sought to study approaches for identifying words that hold a hypernymic relation <ref type="bibr" target="#b2">(Camacho-Collados et al., 2018)</ref>. Two words have a hypernymic relation if one of the words belongs to a taxonomical class that is more general than that of the other word. For example, the word vehicle belongs to a more general taxonomical class than car does, as car is a type of vehicle. Hypernymy can be seen as an is-a relationship. Hypernymy has been studied from different angles in the natural language processing literature as it is related to the human cognitive ability of generalisation.</p><p>This shared task differs from recent taxonomy evaluation tasks ( <ref type="bibr" target="#b0">Bordea et al., 2015</ref><ref type="bibr" target="#b1">Bordea et al., , 2016</ref>) by concentrating on Hypernym Discovery: the task of predicting (discovering) n hypernym candidates for a given input word, within the vocabulary of a specific domain <ref type="bibr" target="#b3">(Espinosa-Anke et al., 2016)</ref>. This shared task provided a general language domain vocabulary and two specialised domain vocabularies in English: medical and music industry. For each vocabulary, a reference corpus was also supplied. In addition to these English vocabularies, general language domain vocabularies for Spanish and Italian were also provided. The ADAPT team focused on the two specialised domain English subtasks by developing an unsupervised system that builds word embeddings from the supplied reference corpora for these domains.</p><p>Word embeddings trained on large corpora have been shown to capture semantic relations between words <ref type="bibr">(Mikolov et al., 2013a,b)</ref>, including hypernym-hyponym relations. The word embeddings built and used by the system presented here exploit this property. Although these word embeddings do not distinguish one semantic relation from another, we expect that true hypernyms will constitute a significant proportion of the predicted candidate hypernyms. Indeed, we show that for the medical domain subtask, our system beats the other unsupervised systems, although it still ranks behind the supervised systems.</p><p>Even though unsupervised systems tend to rank behind supervised systems in NLP tasks in general, our motivation to focus on an unsupervised approach is derived from the fact that they do not require explicit hand-annotated data, and from the expectation that they are able to generalise more easily to unseen hypernym-hyponym pairs.</p><p>The rest of this system description paper is organised as follows: Section 2 briefly surveys the relevant literature and explains the reasons for choosing to use a particular flavour of word embeddings. Section 3 describes the components of the system and its settings. Section 4 summarises the results and offers some insights behind the numbers. Section 5 concludes and proposes avenues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Modern neural methods for natural language processing (NLP) use pre-trained word embeddings as fixed-sized vector representations of lexical units in running text as input data <ref type="bibr">(Goldberg, 2017, ch. 10)</ref>. However, as mentioned previously, word embedding vectors can be used on their own to measure semantic relations between words in an unsupervised manner by, for example, taking the cosine similarity of two word embedding vectors for which semantic similarity is to be measured.</p><p>There are several competing approaches for producing word embedding vectors. One such approach is skip-gram with negative sampling (SGNS), introduced by <ref type="bibr">Mikolov et al. (2013a,b)</ref> as part of their Word2Vec software package. The skip-gram approach assumes that a focus word occurring in text depends on its context words (the words the focus word co-occurs with inside a fixed-sized window), but that those context words occur independently of each other. This conditional independence assumption in the context words makes computation more efficient and produces vectors that work well in practice. The negative sampling portion of the algorithm is a way of producing "negative" context words for the focus word by simply drawing random words from the corpus. These random words are assumed to be "bad" context words for the focus word. The positive and negative examples are used by an objective function that seeks to maximise the probability that the positive examples came from the corpus whilst the negative examples did not.</p><p>Cosine measures on word embeddings pairs (or even on other distributional lexical semantic representations) give an indication of the overall semantic relatedness of the word pairs they represent <ref type="bibr" target="#b11">(Turney and Pantel, 2010)</ref>, without specifying the type(s) of semantic relation(s) the two words hold. There have been endeavours to train word embeddings that emphasise one semantic relation over another. For example, <ref type="bibr" target="#b10">Nguyen et al. (2016)</ref> modified the skip-gram objective function to train word embeddings that distinguished synonymy from antonymy. In a similar vein, <ref type="bibr" target="#b9">Nguyen et al. (2017)</ref> developed an algorithm called Hypervec by adapting the skip-gram objective function to emphasise the non-symmetric hypernymhyponym relations.</p><p>Our team indeed implemented a variant of the Hypervec method but failed to obtain better performance scores on the training set than those obtained by using traditional SGNS (see Section 4). Whilst it is possible that a software bug in our implementation could be the cause of this lower performance, we decided to submit the SGNS results to the official shared task due to time constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Description</head><p>Our system consists of two components: a trainer that learns word vectors using an implementation of the Skip-Gram with Negative Sampling algorithm, and a predictor that outputs (predicts) the top 10 hypernyms of an input word based on the trained vectors. These two components and their settings are described here.</p><p>Trainer The trainer is a modification of PyTorch SGNS 1 , a freely available implementation of the Skip-Gram with Negative Sampling algorithm. One set of vectors per specialised corpus (medicine and music industry) were trained on a vocabulary that consists of the 100,000 most frequent words in each corpus, using a word window of 5 words to the left and 5 words to the right of a sliding focus word. The windows do not cross sentence boundaries. For negative sampling, 20 words were randomly selected from the vocabulary based on their frequency 2 . All vectors had a dimensionality of 300.</p><p>Predictor For each input word in the test file, the predictor attempts to produce 10 candidate hypernyms based on the vectors it learned during training. If there is no vector for an input word, no output for that word is given. If the input word is a multiword expression, then the learned vectors for the individual component words are retrieved and averaged together. This averaged vector is interpreted to represent the input multiword expression. After a vector is retrieved (or computed, in the case of averaged multiword expressions), pairwise cosine similarities are taken between this vector and all other vectors (i.e. the vectors corresponding to the other 99,999 most frequent words). The words represented by the 10 highest ranking cosine similarities are output as the 10 candidate hypernyms for the input word or multiword expression. As can be seen, our system is completely unsupervised as it does not require corpora with tagged examples of words holding hypernym-hyponym relations or any external linguistic or taxonomical resources. <ref type="table">Table 1</ref> shows the results for our SGNS-based approach, which was submitted to the official shared task (SGNS), and for our Hypervec variant (HV), which was not submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Our official submission ranked at eleven out of eighteen on the medical domain subtask with a Mean Average Precision (MAP) of 8.13. However, it ranked first place among all the unsupervised systems on this subtask. On the music industry domain subtask, our system ranked 13th out of 16 places with a MAP of 1.88, ranking 4th among the unsupervised systems. We believe that one reason why the music industry scores are so much lower than the medical results is due to our system not producing an output for 233 of the music industry input words (45% of the total), compared to the 128 medical input words (26%) it failed to predict.</p><p>Another aspect that seems to work against our system is its simplistic way of handling multiword expressions, namely by averaging together the individual word's vectors. The total number of multiword expressions in the medical test set is 264, slightly higher than in the music test set, which contains 220 multiword expressions. Similarly, our system does not have a way of predicting multiword expressions as hypernym candidates, as it can only output the unigrams for which it has vector representations. 82% of the medical domain input words have at least one hypernym that is a multi-word expression, whilst 92% of the music industry domain input words have multi-word expression hypernyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We presented a simple but competitive unsupervised system to predict hypernym candidates for input words, based on cosine similarity scores of word embedding vectors trained on specialised corpora.</p><p>Unsupervised systems in general tend to have lower performance than supervised systems as they lack explicit information to train on. So we are encouraged that our system beat other unsupervised systems on one corpus, as this gives us more avenues to explore.</p><p>One such avenue is to revisit our Hypervec implementation. We suspect that it might require more training epochs than the traditional SGNS method in order to achieve reasonable results. We also seek to experiment with refining pre-trained SGNS word embeddings with Hypervec, rather than training word embeddings from scratch using Hypervec directly.</p><p>Another avenue to explore involves incorporating taxonomical information into our word embeddings. One way to achieve this is by retrofitting pre-trained SGNS word embeddings with information derived from existing taxonomies like WordNet ( <ref type="bibr" target="#b4">Faruqui et al., 2015)</ref>. Another way of incorporating taxonomical information is by generating a pseudo-corpus via a random walk over such a taxonomy and then learn SGNS word embeddings in the usual way ( <ref type="bibr" target="#b5">Goikoetxea et al., 2015)</ref>.</p><p>These approaches (Hypervec, retrofitting and taxonomy random-walk) however, would relax the unsupervised constraint we followed in our implementation. So yet another avenue to explore is to instead apply different similarity functions that might be more sensitive to the one-way, generalspecific nature of hypernymic relationships between words.</p></div>
			<note place="foot" n="1"> https://github.com/theeluwin/pytorch-sgns 2 The frequencies were smoothed by raising them to the power of 0.75 before dividing by the total.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank our anonymous reviewers for their input. The ADAPT Centre for Digital Content Tech-926 nology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<title level="m">Semeval-2015 task 17: Taxonomy extraction evaluation (texeval). In Proceedings of the 9th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 13: Taxonomy extraction evaluation (texeval-2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vered Shwartz, Roberto Navigli, and Horacio Saggion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
		<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018)<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018 Task 9: Hypernym Discovery</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised Distributional Hypernym Discovery via Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Retrofitting Word Vectors to Semantic Lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random Walks and Neural Network Language Models on Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josu</forename><surname>Goikoetxea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2015 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>CO</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1434" to="1439" />
		</imprint>
	</monogr>
	<note>Denver</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural Network Methods for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR 2013)</title>
		<meeting>the International Conference on Learning Representations (ICLR 2013)<address><addrLine>Scottsdale, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Stutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Annual Conference on Neural Information Processing Systems (NIPS) In Advances in Neural Information Processing Systems</title>
		<meeting>the Twenty-Seventh Annual Conference on Neural Information Processing Systems (NIPS) In Advances in Neural Information Processing Systems<address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical Embeddings for Hypernymy Detection and Directionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Kim Anh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Köper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Kim Anh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: Vector Space Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
