<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 16-17, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
							<email>jflanigan@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of SemEval-2016</title>
						<meeting>SemEval-2016 <address><addrLine>San Diego, California</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1202" to="1206"/>
							<date type="published">June 16-17, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present improvements to the JAMR parser as part of the SemEval 2016 Shared Task 8 on AMR parsing. The major contributions are: improved concept coverage using external resources and features, an improved aligner, and a novel loss function for structured prediction called infinite ramp, which is a generalization of the structured SVM to problems with un-reachable training instances.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our entry to the SemEval 2016 Shared Task 8 is a set of improvements to the system presented in <ref type="bibr" target="#b6">Flanigan et al. (2014)</ref>. The improvements are: a novel training loss function for structured prediction, which we call "infinite ramp," new sources for concepts, improved features, and improvements to the rule-based aligner in <ref type="bibr" target="#b6">Flanigan et al. (2014)</ref>. The overall architecture of the system and the decoding algorithms for concept identification and relation identification are unchanged from <ref type="bibr" target="#b6">Flanigan et al. (2014)</ref>, and we refer readers seeking a complete understanding of the system to that paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">New Concept Fragment Sources and Features</head><p>The concept identification stage relies on a function called clex in Section 3 of <ref type="bibr" target="#b6">Flanigan et al. (2014)</ref> to provide candidate concept fragments. In that work, clex has three sources of concept fragments: a lexicon extracted from the training data, rules for named entities identified by the named entity tagger, and rules for time expressions. We augment these sources with five additional sources:</p><p>• Frame file lookup: for every word in the input sentence, if the lemma matches the name of a frame in the AMR frame files (with sense tag removed), we add the lemma concatenated with "-01" as a candidate concept fragment.</p><p>• Lemma: for every word in the input sentence, we add the lemma of the word as a candidate concept fragment.</p><p>• Verb pass-through: for every word in the input sentence, if the word is a verb, we add the lemma concatenated with "-00" as a candidate concept fragment.</p><p>• Named entity pass-through: for every span of words of length 1 until 7 in the input, we add the concept fragment "(thing :name (name :op1 word1 . . . :opn wordn)" as a candidate concept fragment, where n is the length of the span, and "word1" and "wordn" are the first and last words in the fragment.</p><p>We use the following features for concept identification:</p><p>• Fragment given words: Relative frequency estimates of the probability of a concept fragment given the sequence of words in the span.</p><p>• Length of the matching span (number of tokens).</p><p>• Bias: 1 for any concept graph fragment.</p><p>• First match: 1 if this is the first place in the sentence that matches the span.</p><p>• Number: 1 if the span is length 1 and matches the regular expression "[0-9]+".</p><p>• Short concept: 1 if the length of the concept fragment string is less than 3 and contains only upper or lowercase letters.</p><p>• Sentence match: 1 if the span matches the entire input sentence.</p><p>• ; list: 1 if the span consists of the single word ";" and the input sentence is a ";" separated list.</p><p>• POS: the sequence of POS tags in the span.</p><p>• POS and event: same as above but with an indicator if the concept fragment is an event concept (matches the regex ".*-[0-9][0-9]").</p><p>• Span: the sequence of words in the span if the words have occurred more than 10 times in the training data as a phrase with no gaps.</p><p>• Span and concept: same as above concatenated with the concept fragment in PENMAN notation.</p><p>• Span and concept with POS: same as above concatenated with the sequence of POS tags in the span.</p><p>• Concept fragment source: indicator for the source of the concept fragment (corpus, NER tagger, date expression, frame files, lemma, verb-pass through, or NE pass-through).</p><p>• No match from corpus: 1 if there is no matching concept fragment for this span in the rules extracted from the corpus.</p><p>The new sources of concepts complicate concept identification training. The new sources improve concept coverage on held-out data but they do not improve coverage on the training data since one of the concept sources is a lexicon extracted from the training data. Thus correctly balancing use of the training data lexicon versus the additional sources to prevent overfitting is a challenge.</p><p>To balance the training data lexicon with the other sources, we use a variant of cross-validation. During training, when processing a training example in the training data, we exclude concept fragments extracted from the same section of the training data. This is accomplished by keeping track of the training instances each phrase-concept fragment pair was extracted from, and excluding all phrase-concept fragment pairs within a window of the current training instance. In our submission the window is set to 20.</p><p>While excluding phrase-concept fragment pairs allows the learning algorithm to balance the use of the training data lexicon versus the other concept sources, it creates another problem: some of the gold standard training instances may be unreachable (cannot be produced), because of the phrase-concept pair need to produce the example has been excluded. This can cause problems during learning. To handle this, we use a generalization of structured SVMs which we call "infinite ramp." We discuss this in the general framework of structured prediction in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Infinite Ramp Loss</head><p>The infinite ramp is a new loss function for structured prediction problems. It is useful when the training data contains outputs that the decoder cannot produce given their inputs (we refer to these as "unreachable examples"). It is a direct generalization of the SVM loss and latent SVM loss.</p><p>Let x be the input, Y(x) be the space of all possible outputs given the input x, andˆyandˆ andˆy be the predicted output. Let f (x, y ) denote the feature vector for the output y with the input x, which is the sum of the local features. (In concept identification, the local features are the features computed for each span, and f is the sum of the features for each span.) Let w be the parameters of a linear model, used to make predictions as follows:</p><formula xml:id="formula_0">ˆ y = arg max y ∈Y(x) w · f (x, y )</formula><p>To train the model parameters w, a function of the training data is minimized with respect to w. This function is a sum of individual training examples' losses L, plus a regularizer:</p><formula xml:id="formula_1">L(D; w) = (x i ,y i )∈D L(x i , y i ; w) + λw 2 L(x i , y i ; w) = −    lim α→∞ max y∈Y(x i )   w · f (x i , y) + α ·    C(x i ,y i ) min y ∈Y(x i ) cost(y i , y ) −cost(y i , y)          + max y ∈Y(x i ) w · f (x i , y ) + cost(y i , y )<label>(1)</label></formula><p>Figure 1: Infinite ramp loss.</p><p>Typical loss functions are the structured perceptron loss <ref type="bibr" target="#b3">(Collins, 2002)</ref>:</p><formula xml:id="formula_2">L(x i , y i ; w) = −w · f (x i , y i ) + max y∈Y(x i ) w · f (x i , y)<label>(2)</label></formula><p>and the structured SVM loss ( <ref type="bibr" target="#b9">Taskar et al., 2003;</ref><ref type="bibr" target="#b10">Tsochantaridis et al., 2004</ref>), which incorporates margin using a cost function: 1</p><formula xml:id="formula_3">L(x i , y i ; w) = −w · f (x i , y i ) + max y∈Y(x i ) w · f (x i , y) + cost(y i , y)<label>(3)</label></formula><p>Both <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref> are problematic if example i is unreachable, i.e., y i / ∈ Y(x i ), due to imperfect data or an imperfect definition of Y. In this case, the model is trying to learn an output it cannot produce. In some applications, the features f (x i , y i ) cannot even be computed for these examples. This problem is well known in machine translation: some examples cannot be produced by the phrase-table or grammar. It also occurs in AMR parsing.</p><p>To handle unreachable training examples, we modify (3), introducing the infinite ramp loss, shown in Eq. 1 in <ref type="figure">Fig. 1</ref>. The term labeled C(x i , y i ) is present only to make the limit well-defined in case min y∈Y(x i ) cost(y i , y) = 0. In practice, we set α to be a very large number <ref type="bibr">(10 12</ref> ) instead of taking a proper limit, and set C(x i , y i ) = 0.</p><p>The intuition behind Eq. 1 is the following: for very large α, the first max picks a y that minimizes cost(y i , y), using the model score w · f (x i , y) to break any ties. This is what the model updates towards in subgradient descent-style updates, called the "hope derivation" by <ref type="bibr" target="#b2">Chiang (2012)</ref>. The second max is the usual cost augmented decoding that gives a margin in the SVM loss, and is what the model updates away from in subgradient descent, called the "fear derivation" by <ref type="bibr" target="#b2">Chiang (2012)</ref>.</p><p>Eq. 1 generalizes the structured SVM loss. If y i is reachable and the minimum over y ∈ Y(x i ) of cost(y, y i ) occurs when y = y i , then the first max in Eq. 1 picks out y = y i and Eq. 1 reduces to the structured SVM loss.</p><p>The infinite ramp is also a generalization of the latent structured SVM ( <ref type="bibr" target="#b11">Yu and Joachims, 2009)</ref>, which is a generalization of the structured SVM for hidden variables. This loss can be used when the output can be written y i = (˜ y i , h i ), where˜ywhere˜ where˜y i is observed output and h i is latent (even at training time). Let˜YLet˜ Let˜Y(x i ) be the space of all possible observed outputs and H(x i ) be the hidden space for the example x i . Let˜cLet˜ Let˜c be the cost function for the observed output. The latent structured SVM loss is:</p><formula xml:id="formula_4">L(x i , y i ; w) = − max h∈H(x i ) w · f (x i , ˜ y i , h) + max˜y∈˜Y max˜ max˜y∈max˜y∈˜ max˜y∈˜Y(x i ) max h ∈H(x i ) w · f (x i , ˜ y, h) + ˜ c(˜ y i , ˜ y)<label>(4)</label></formula><p>If we set cost(y i , y) = ˜ c(˜ y i , ˜ y) in Eq. 1, and the minimum of˜cof˜ of˜c(˜ y i , ˜ y) occurs wheñ y = ˜ y i , then minimizing Eq. 1 is equivalent to minimizing Eq. 4.</p><p>Eq. 1 is related to ramp loss ( <ref type="bibr" target="#b4">Collobert et al., 2006;</ref><ref type="bibr" target="#b1">Chapelle et al., 2009;</ref><ref type="bibr" target="#b8">Keshet and McAllester, 2011)</ref>:</p><formula xml:id="formula_5">L(x i , y i ; w) = − max y∈Y(x i ) w · f (x i , y) − α · cost(y i , y) + max y ∈Y(x i ) w · f (x i , y ) + cost(y i , y ) (5)</formula><p>The parameter α is often set to zero, and controls the "height" of the ramp, which is α + 1. Taking α → ∞ in Eq. 5 corresponds roughly to Eq. 1, hence the name "infinite ramp loss". However, Eq. 1 also includes C(x i , y i ) term to make the limit well defined even when min y∈Y(x i ) cost(y i , y) = 0. Like infinite ramp loss, ramp loss also handles unreachable training examples ( <ref type="bibr" target="#b7">Gimpel and Smith, 2012</ref>), but we have found ramp loss to be more difficult to optimize than infinite ramp loss in practice due to local minima. Both loss functions are nonconvex. However, infinite ramp loss is convex if arg min y∈Y(x i ) cost(y i , y) is unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>We train the concept identification stage using infinite ramp loss (1) with AdaGrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>). We process examples in the training data ((x 1 , y 1 ), . . . , (x N , y N )) one at a time. At time t, we decode with the current parameters and the cost function as an additional local factor to get the two outputs:</p><formula xml:id="formula_6">h t = arg max y ∈Y(xt) w t · f (x t , y ) − α · cost(y i , y)<label>(6)</label></formula><formula xml:id="formula_7">f t = arg max y ∈Y(xt) w t · f (x t , y ) + cost(y i , y)<label>(7)</label></formula><p>and compute the subgradient:</p><formula xml:id="formula_8">s t = f (x t , h t ) − f (x t , f t ) − 2λw t</formula><p>We then update the parameters and go to the next example. Each component i of the parameters gets updated as:</p><formula xml:id="formula_9">w t+1 i = w t i − η t t =1 s t i s t i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate using Smatch <ref type="bibr" target="#b0">(Cai and Knight, 2013)</ref>. Following the recommended train/dev./test split of LDC2015E86, our parser achieves 70% precision, 65% recall, and 67% F 1 Smatch on the LDC2015E86 test set. The JAMR baseline on this same dataset is 55% F 1 Smatch, so the improvements are quite substantial. On the SemEval 2016 Task 8 test set, our improved parser achieves 56% F 1 Smatch. We hypothesize that the lower performance of the parser on the SemEval Task 8 test set is due to drift in the AMR annotation scheme between the production of the LDC2015E86 training data and the SemEval test set. During that time, there were changes to the concept senses and the concept frame files. Because the improvements in our parser were due to boosting recall in concept identification (and using the frame files to our advantage), our approach does not show as large improvements on the SemEval test set as on the LDC2015E86 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented improvements to the JAMR parser as part of the SemEval 2016 Shared Task on AMR parsing, showing substantial improvements over the baseline JAMR parser. As part of these improvements, we introduced infinite ramp loss, which generalizes the structured SVM to handle training data with unreachable training examples. We hope this loss function will be useful in other application areas as well.</p></div>
			<note place="foot" n="1"> cost(yi, y) returns the cost of mistaking y for correct output yi.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the U.S. Army Research Office under grant number W911NF-10-1-0533. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the U.S. Army Research Office or the United States Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tighter bounds for structured estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuong</forename><forename type="middle">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hope and fear for discriminative training of statistical translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1159" to="1187" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trading convexity for scalability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured ramp loss minimization for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generalization bounds and consistency for latent structural probit and ramp loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Max-margin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support vector machine learning for interdependent and structured output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning structural SVMs with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
