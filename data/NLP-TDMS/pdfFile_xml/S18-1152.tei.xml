<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 5-6, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Berend</surname></persName>
							<email>berendg@inf.u-szeged.hu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Márton</forename><surname>Makrai</surname></persName>
							<email>makrai.marton@nytud.mta.hu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Péter</forename><surname>Földiák</surname></persName>
							<email>peter.foldiak@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="laboratory">Institute for Linguistics Hungarian Academy of Sciences Benczúr u. 33</orgName>
								<orgName type="institution">University of Szeged Árpád</orgName>
								<address>
									<addrLine>tér 2</addrLine>
									<postCode>H6720, H1068</postCode>
									<settlement>Szeged, Budapest</settlement>
									<country>Hungary, Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Secret Sauce Partners</orgName>
								<address>
									<addrLine>657 Mission Suite 410</addrLine>
									<postCode>94105</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
						<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018) <address><addrLine>New Orleans, Louisiana</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="928" to="934"/>
							<date type="published">June 5-6, 2018</date>
						</imprint>
					</monogr>
					<note>300-sparsans at SemEval-2018 Task 9: Hypernymy as interaction of sparse attributes</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper describes 300-sparsans&apos; participation in SemEval-2018 Task 9: Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings. Our system took first place in subtasks (1B) Italian (all and entities), (1C) Spanish entities, and (2B) music entities.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language phenomena are extremely sparse by their nature, whereas continuous word embeddings employ dense representations of words. Turning these dense representations into a much sparser form can help in focusing on most salient parts of word representations <ref type="bibr" target="#b1">Berend, 2017;</ref><ref type="bibr" target="#b13">Subramanian et al., 2018)</ref>.</p><p>Sparsity-based techniques often involve the coding of a large number of signals over the same dictionary <ref type="bibr" target="#b12">(Rubinstein et al., 2008)</ref>. Sparse, overcomplete representations have been motivated in various domains as a way to increase separability and interpretability <ref type="bibr" target="#b10">(Olshausen and Field, 1997)</ref> and stability in the presence of noise.</p><p>Non-negativity has also been argued to be advantageous for interpretability <ref type="bibr" target="#b7">Fyshe et al., 2015;</ref><ref type="bibr" target="#b0">Arora et al., 2016)</ref>. As <ref type="bibr" target="#b13">Subramanian et al. (2018)</ref> illustrates this in the language domain, where sparse features are interpreted as lexical attributes, "to describe the city of Pittsburgh, one might talk about phenomena typical of the city, like erratic weather and large bridges. It is redundant and inefficient to list negative properties, like the absence of the Statue of Liberty". Berend (2018) utilizes non-negative sparse coding for word translation by training sparse word vectors for the two languages such that coding bases correspond to each other.</p><p>Here we apply sparse feature pairs to hypernym extraction. The role of an attribute pair i, j ∈ φ(q) × φ(h) (where q is the query word, h is the hypernym candidate, and φ(w) is the index of a non-zero component in the sparse representations of w) is similar to interaction terms in regression, see section 2 for details.</p><p>Sparse representation is related to hypernymy in various natural ways. One of them is through Formal concept Analysis (FCA). The idea of acquiring concept hierarchies from a text corpus with the tools of Formal concept Analysis (FCA) is relatively new ( <ref type="bibr" target="#b4">Cimiano et al., 2005</ref>). Our submissions experiment with formal concept analysis tool by <ref type="bibr" target="#b5">Endres et al. (2010)</ref>. See the next section for a description of formal concept lattices, and how hypernyms can be found in them.</p><p>Another natural formulation is related to hierarchical sparse coding ( <ref type="bibr" target="#b15">Zhao et al., 2009)</ref>, where trees describe the order in which variables "enter the model" (i.e., take non-zero values). A node may take a non-zero value only if its ancestors also do: the dimensions that correspond to top level nodes should focus on "general" meaning components that are present in most words. <ref type="bibr" target="#b14">Yogatama et al. (2015)</ref> offer an implementation that is efficient for gigaword corpora. Exploiting the correspondence between the variable tree and the hypernym hierarchy offers itself as a natural choice.</p><p>The task <ref type="bibr" target="#b3">(Camacho-Collados et al., 2018</ref>) evaluated systems on their ability to extract hypernyms for query words in five subtasks (three languages, English, Italian, and Spanish, and two domains, medical and music). Queries have been categorized as concepts or entities. Results were reported for each category separately as well as in combined form, thus resulting in 5 × 3 combinations. Our system took first place in subtasks (1B) Italian (all and entities), (1C) Spanish entities, and (2B) music entities. Detailed results for our system appear in section 3. Our source code is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Formal concept analysis</head><p>Formal concept Analysis (FCA) is the mathematization of concept and conceptual hierarchy <ref type="bibr">(Gan- ter and Wille, 2012;</ref><ref type="bibr" target="#b5">Endres et al., 2010)</ref>. In FCA terminology, a context is a set of objects O, a set of attributes A, and a binary incidence relation I ⊆ O × A between members of O and A. In our application, I associates a word w ∈ O to the indices of its non-zero sparse coding coordinates i ∈ A. There is an order defined in the context: if A 1 , B 1 and A 2 , B 2 are concepts in C, A 1 , B 1 is a subconcept of A 2 , B 2 if A 1 ⊆ A 2 which is equivalent to B 1 ⊇ B 2 . The concept order forms a lattice. The smallest concept whose extent contains a word is said to introduce the object. We expect that h will be a hypernym of q iff n(q) ≤ n(h) where n(w) denotes the node in the concept lattice that introduces w.</p><p>The closedness of extents and intents has an important structural consequence. Adding attributes to A (e.g. responses of additional neurons) will very probably grow the model. However, the original concepts will be embedded as a substructure in the larger lattice, with their ordering relationships preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our approach</head><p>Now we describe our system that is based on sparse non-negative word representations and FCA besides more traditional features.</p><p>We use the popular skip-gram (SG) approach ( <ref type="bibr" target="#b9">Mikolov et al., 2013</ref>) to train d = 100 dimensional dense distributed word representations for each sub-corpus. The word embeddings are trained over the text corpora provided by the shared task organizers with the default training parameters of word2vec (w2v), i.e. a window size of 10 and 25 negative samples for each positive context.</p><p>We derived multi-token units by relying on the word2phrase software accompanying the <ref type="bibr">w2v</ref> toolkit. An additional source for identifying multitoken units in the training corpora was the list of potential hypernyms released for each subtask by the shared task organizers.</p><p>Given the dense embedding matrix W x ∈ R d×|Vx| , for some subcorpus of the shared task x ∈ {1A, 1B, 1C, 2A, 2B}, where |V x | is the size of the vocabulary and d is set to 100. As a subsequent step, we turn W x into sparse word vectors akin to <ref type="bibr" target="#b1">Berend (2017)</ref> by solving for</p><formula xml:id="formula_0">min D∈C,α∈R ≥0 Dα − W x F + λα 1 ,<label>(1)</label></formula><p>where C refers to the convex set of R d×k matrices consisting of d-dimensional columns vectors with norm at most 1, and α contains the sparse coefficients for the elements of the vocabulary. The only difference compared to <ref type="bibr" target="#b1">Berend (2017)</ref> is that here we ensure a non-negativity constraint over the elements of α.</p><p>For the elements of the vocabulary we ran the formal concept analysis tool of <ref type="bibr" target="#b5">Endres et al. (2010)</ref>  <ref type="bibr">2</ref> . In order to keep the size of the DAG outputted by the FCA algorithm manageable, we only included the query words and those hypernyms in the analysis which occur in the training dataset for the corpora. As we will see in the next section, this restriction turns out to be very useful.</p><p>Next, we determine a handful of features for a pair of expressions (q, h) consisting of a query q and its potential hypernym h. <ref type="table" target="#tab_0">Table 1</ref> provides an overview of the features employed for a pair (q, h). We denote with q and h the 100-dimensional dense vectorial representations of q and h. Additionally, we denote with Q and H the sequence of tokens constituting the query and hypernym phrases. Finally, we refer to the set of basis vectors (in the FCA terminology, attributes) Core feature name The features employed in our classifier. M F 50 (q.type) refers to the set of top-50 most frequent hypernyms for a given query type.</p><formula xml:id="formula_1">cosine q h q 2 h 2 difference q − h 2 normRatio q 2 h 2 qureyBeginsWith Q[0] = h queryEndsWith Q[−1] = h hasCommonWord Q ∩ H = ∅ sameFirstWord Q[0] = H[0] sameLastWord Q[−1] = H[−1] logFrequencyRatio log 10 count(q) count(h) isFrequentHypernym 3 c ∈ M F 50 (q.type) sameConcept n(h) = n(q) parent n(q) n(h) child n(h) n(q) overlappingBasis φ(q) ∩ φ(h) = ∅ sparseDifference q\h |φ(q) − φ(h)| sparseDifference h\q |φ(h) − φ(q)| attributePair ij i, j ∈ φ(q) × φ(h)</formula><p>which are assigned non-zero weights in the reconstruction of the vectorial representation of q and h as φ(q) and φ(h). It is also considered as a feature (isFrequentHypernym) whether a particular candidate hypernym h belongs to the top-50 most frequent hypernyms for the category of q (i.e. concept or entity). Modeling the two categories separately played an important role in the success of our systems. Three additional features are defined for incorporating the concept lattice output by FCA. With n(w) denoting the concept that introduces w, i.e. the most specific location within the DAG for w, our features indicate whether n(q) (1) coincides with that of h, (2) is the parent (immediate successor) for that of h, or (3) is the child (immediate predictions) for that of h. Parents, and even the inverse relation, proved to be more predictive than the conceptually motivated q ≤ h. In Table 1, n 1 n 2 denotes that n 1 is an immediate predecessor of n 2 . We will see in post-evaluation ablation experiments, where we refer to the above three features as the FCA features, that they were not useful in our submissions. The attributePair ij s above, our most important features, are indicator features for every possible interaction term between the sparse coefficients in α. That means that for a pair of words (q, h) we defined φ(q) × φ(h), i.e. candidates get assigned with the Cartesian product derived from the indices of the non-zero coefficients in α. Note that this feature template induces k 2</p><note type="other">features, with k being the number of basis vectors introduced in the dictionary matrix D according to Eq. 1. In order to rank potential hypernym candidates over the test set we trained a logistic regression classifier for concepts and entities utilizing the sklearn package (Pedregosa et al., 2011) 4 with the regularization parameter defaulting to 1.0.</note><p>For each appropriate (q, h) pair of words for which h is a hypernym of q, we generated a number of negative samples (q, h ), such that the training data does not include h as a valid hypernym for q. For a given query q, belonging to either of the concept or entity category, we sampled h from those hypernyms which were included as a valid hypernym in the training data with respect to some q = q query phrase.</p><p>When making predictions for the hypernyms of a query, we relied on our query type sensitive logistic regression model to determine the ranking of the hypernym candidates. In our official submission we treated such phrases to rank which were included in the training data for being a proper hypernym at least once.</p><p>After the appropriate model ranked the hypernym candidates, we selected the top 15 ranked candidates and applied a post-ranking heuristic over them, i.e. reordered them according to their background frequency from the training corpus.</p><p>Our assumption here is that more frequent words tend to refer to more general concepts and more general hypernymy relations potentially tend to be more easily detectable than more specialized ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our submissions</head><p>Our submissions were based on k = 200 dimensional sparse vectors computed from unit-normed 100-dimensional dense vectors with λ = .3.   Baseline results, most frequent training hypernyms. We (upper) consider the most frequent hypernym in the given query type (concept or entity). For comparison, we also show the MFH baseline provided by the organizers (lower) that is based on the most frequent hypernyms in general.</p><p>submissions involved attribute pairs, the other not. Both submissions used the conceptually motivated but practically harmful FCA-based features.  <ref type="table">Table 4</ref>: Number of in-vocabulary (and out-ofvocabulary, OOV) queries per query type. The ratio of the latter is also shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query type sensitive baselining</head><p>Our submission with attribute pairs achieved first place in categories (1B) Italian (all and entities), (1C) Spanish entities, and (2B) music entities. This is in part due to our good choice of a fallback solution in the case of OOV queries: we applied a category-sensitive baseline returning the most frequent train hypernym in the corresponding query type (concept or entity). <ref type="table">Table 4</ref> shows how frequently we had to rely on this fallback, and <ref type="table" target="#tab_3">Table 3</ref> shows the corresponding pure baseline results.   <ref type="table">Table 6</ref>: Ablation experiments, on the 1A dataset with k = 200, ns = 50 (and the implementation of isFreqHyp fixed). The first two columns indicate whether attributePair ij and FCA-derived features are utilized, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post-evaluation analysis</head><p>After the evaluation closed, we conducted ablation experiments the results of which are included in <ref type="table">Table 6</ref>. In these experiments, we investigated the contribution of the features derived from sparse attribute pairs and FCA. These ablation experiments corroborate the importance of features derived from sparse attribute pairs and reveal that turning off FCA-based features does not hurt performance at all. For this reason -even though our official shared task submission included FCArelated features -we no longer employed them in our post-evaluation experiments.  Test results of an oracle system which uses candidate filtering.</p><p>training. In our post evaluation experiments we investigated the effects of generating more negative samples, i.e. we regarded all the valid hypernyms over the training set -not being a proper hypernym for q -as h upon the creation of the (q, h ) negative training instances. This latter strategy is referenced as ns = all in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>In our official submission we regarded only those hypernyms as potential candidates to rank during test time which occurred at least once as a correct hypernym in the training data. We call this strategy as candidate filtering. Historically, we applied this restriction to speed up the FCA algorithm because this way the size of the concept lattice could be made smaller. As there are valid hypernyms on the test set which never occurred in the training data, our official submission would not be able to obtain a perfect score even in theory. Table 7 contains the best possible metrics on the test set that we could achieve when candidate filtering is applied. In our post evaluation experiments we also investigated the effects of turning this kind of filtering step off. As <ref type="table" target="#tab_6">Table 5</ref> illustrates, however, our scores degrade after turning candidate filtering off.</p><p>Our post evaluation experiments in  gest that it is advantageous to apply sparse representation of more expressive power (i.e. a higher number of basis vectors). Generating more negative samples also provides some additional performance boost. These previous observations hold irrespective whether candidate filtering is employed or not, however, their effects are more pronounced when hypernym candidates are not filtered. Finally, we report our post-evaluation results for all the subtasks and compare them to the official scores of the best performing systems in <ref type="table" target="#tab_10">Table 8</ref>. It can be seen from these enhanced results for category "all" (concepts and entities mixed) that we would win (1B) Italian and (1C) Spanish. Our post-evaluation system -which only differs from our participating system that it fixes the calculation of a features, does not rely on FCA-based features and uses k = 1000 -would also place third in the rest of the subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we experimented with the integration of sparse word representations into the task of hypernymy discovery. We strived to utilize sparse word representations in two ways, i.e. via building concept lattices using formal concept analysis and modeling the hypernymy relation with the help of interaction terms. While our former approach for deriving formal concepts from sparse word representations was not successful, the interaction terms derived from sparse word representations proved to be highly beneficial.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>FCA finds formal concepts, pairs O, A of object sets and attribute sets (O ⊆ O, A ⊆ A) such that A consists of the shared attributes of ob- jects in O (and no more), and O consists of the objects in O that have all the attributes in A (and no more). (There is a closure-operator related to each FCA context, for which O and A are closed sets iff O, A is a concept.) O is called the extent and A is the intent of the concept.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>in the training dataset. One of our without attribute pairs with attribute pairs MAP MRR P@1 P@3 P@5 P@15 MAP MRR P@1 P@3 P@5 P@15</head><label></label><figDesc>The sum of the two dimensions motivates our group name. For training the regression model with neg- ative samples, 50 false hypernyms were sampled for each query q</figDesc><table>1A offic 
8.6 
18.0 13.0 
8.9 
8.2 
7.9 
8.9 
19.4 14.9 
9.3 
8.6 
8.1 
1A reprd 9.07 
18.7 13.5 
9.4 
8.8 
8.5 
9.2 
19.9 14.9 
9.5 
8.7 
8.4 
1B offic 
9.4 
19.9 13.2 
9.5 
9.3 
8.8 
12.1 
25.1 17.6 12.9 11.7 
11.2 
1B reprd 
9.2 
19.5 12.8 
8.9 
8.9 
8.7 
12.8 
26.7 18.9 13.6 12.4 
11.9 
1C offic 12.5 
25.9 16.6 13.6 12.6 
11.5 
17.9 
37.6 27.8 19.7 17.1 
16.3 
1C reprd 12.9 
26.0 16.2 13.9 13.0 
11.9 
18.3 
38.4 28.5 20.2 17.4 
16.6 
2A offic 15.0 
32.2 24.8 17.7 15.8 
11.6 
20.8 
40.6 31.6 23.5 21.4 
17.1 
2A reprd 15.1 
32.4 24.4 18.0 16.2 
11.8 
21.5 
43.7 35.6 25.3 21.8 
17.0 
2B offic 19.1 
36.7 27.2 23.0 20.1 
15.4 
29.5 
46.4 33.0 31.9 28.9 
27.7 
2B reprd 21.5 
40.9 29.6 25.6 22.1 
18.0 
30.4 
46.8 33.8 31.8 29.5 
28.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Our submissions results: official and those that can be reproduced with the code in the 
project repo (with the isFrequentHypernym feature being turned off). 

MAP MRR P@1 P@3 P@5 P@10 

1A 9.8 
22.6 
19.8 10.0 9.0 
8.6 
1A 8.8 
21.4 
19.8 8.9 
7.8 
7.5 

1B 8.9 
21.2 
17.1 9.1 
8.3 
7.9 
1B 7.8 
19.4 
17.1 8.3 
6.8 
6.5 

1C 16.4 
33.3 
24.6 17.5 16.1 14.9 
1C 12.2 
29.8 
24.6 12.0 11.3 11.0 

2A 29.0 
35.9 
32.6 34.3 34.2 21.7 
2A 28.9 
35.8 
32.6 34.3 34.2 21.4 

2B 40.2 
58.8 
50.6 44.6 40.3 35.5 
2B 33.3 
51.5 
36.2 40.1 35.8 28.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 shows</head><label>2</label><figDesc>submission results. The figures that can be reproduced with the code in the project repo (reprd) is slightly different from our official submissions (offic) for two reasons: because the implementation of isFreqHyp contained a bug, and because of the natural randomness in negative sampling. For reproducibility, we report result without the isFreqHyp feature. The ran- domness introduced by negative sampling is now factored out by random seeding.</figDesc><table>Train 
Test 

1A 975(4) 
0.41% 1055(4) 0.38% 
1B 709(1) 
0.14% 
767(2) 
0.26% 
1C 776(2) 
0.26% 
625(2) 
0.32% 
2A 442(58) 11.60% 433(67) 13.40% 
2B 366(21) 5.43% 341(17) 4.75% 

(a) concept 

Train 
Test 

1A 379(142) 27.26% 344(99) 22.35% 
1B 249(41) 14.14% 205(26) 11.26% 
1C 184(38) 17.12% 328(45) 12.06% 
2A 
0(0) 
-
0(0) 
-
2B 
79(34) 
30.09% 102(40) 28.17% 

(b) entity 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :and ns denotes the number of basis vectors and negative samples generated during training per each positive (q, h) pair. Best results obtained for each metric are marked as bold.</head><label>5</label><figDesc></figDesc><table>Post evaluation results on the 1A dataset investigating the effect of various hyperparameter 
choices. k MAP MRR P@1 P@3 P@5 P@15 

off off 10.3 
21.3 15.0 10.6 10.1 
9.6 
off on 10.1 
21.1 14.9 10.5 
9.9 
9.5 
on off 12.1 
25.4 18.9 12.9 11.6 
10.9 
on on 12.1 
25.3 18.7 13.0 11.6 
11.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 includes</head><label>5</label><figDesc></figDesc><table>the detailed behavior of our 
model on subtask 1A with respect three distinct 
factors, that is 

1. the number of basis vectors employed during 
sparse coding (k ∈ {200, 300, 1000}), 

2. the number of negative training samples per 
positive sample (ns ∈ {50, all}), 

3. candidate filtering being turned on/off. 

In our original submission we generated 50 neg-
ative samples (ns) generated per query q during 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 sug-</head><label>5</label><figDesc></figDesc><table>932 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 : Post evaluation results for the different subtasks using k = 1000, ns = 50</head><label>8</label><figDesc></figDesc><table>and hypernym 
candidate filtering. Upper: our system, lower: 
subtask winner. 

</table></figure>

			<note place="foot" n="1"> https://github.com/begab/fca_ hypernymy</note>

			<note place="foot" n="2"> www.compsens.uni-tuebingen.de/pub/ pages/personals/3/concepts.py</note>

			<note place="foot" n="3"> At submission time, this feature did not work properly.</note>

			<note place="foot" n="4"> scikit-learn.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank András Kornai for useful comments on negative sampling. This research was supported by the project Integrated program for training new generation of scientists in the fields of computer science, no. EFOP-3.6.3-VEKOP-16-2017-0002. The project has been supported by the European Union and co-funded by the European Social Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Linear algebraic structure of word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03764v1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>with applications to polysemy</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse coding of neural word embeddings for multilingual sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Berend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="247" to="261" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards cross-lingual utilization of sparse word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Berend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSZNY2018, XVI. Magyar Számítóg´pesSzámítóg´pes Nyelvészeti Konferencia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vered Shwartz, Roberto Navigli, and Horacio Saggion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
		<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018)<address><addrLine>New Orleans, LA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018 Task 9: Hypernym Discovery</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning concept hierarchies from text corpora using formal concept analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="305" to="339" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Application of Formal Concept Analysis to Semantic Neural Decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Földiák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Priss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="233" to="248" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Reviewed</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2015. Best Student Paper Award</title>
		<meeting>NAACL 2015. Best Student Paper Award</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A compositional and interpretable semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Formal concept analysis: mathematical foundations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Ganter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Wille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David J</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient implementation of the k-svd algorithm and the batch-omp method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zibulevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Technion, Israel</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anant</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<title level="m">Spine: Sparse interpretable neural embeddings. AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning word representations with hierarchical sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. Previous version in NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The composite and absolute penalties for grouped and hierarchical variable selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gulherme</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3468" to="3497" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
