<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T01:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CCG Supertagging with a Recurrent Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Computer Laboratory</orgName>
								<orgName type="institution" key="instit1">University of Cambridge Computer Laboratory</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<email>michaelauli@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Computer Laboratory</orgName>
								<orgName type="institution" key="instit1">University of Cambridge Computer Laboratory</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Computer Laboratory</orgName>
								<orgName type="institution" key="instit1">University of Cambridge Computer Laboratory</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CCG Supertagging with a Recurrent Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="250" to="255"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work on supertagging using a feed-forward neural network achieved significant improvements for CCG supertagging and parsing (Lewis and Steedman, 2014). However, their architecture is limited to considering local contexts and does not naturally model sequences of arbitrary length. In this paper, we show how directly capturing sequence information using a recurrent neural network leads to further accuracy improvements for both su-pertagging (up to 1.9%) and parsing (up to 1% F1), on CCGBank, Wikipedia and biomedical text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Combinatory Categorial Grammar (CCG; <ref type="bibr">Steed- man, 2000</ref>) is a highly lexicalized formalism; the standard parsing model of  uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging ( <ref type="bibr" target="#b1">Bangalore and Joshi, 1999</ref>). <ref type="bibr" target="#b2">Clark and Curran (2004)</ref> show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased.</p><p>In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy ( <ref type="bibr" target="#b4">Curran et al., 2006;</ref><ref type="bibr" target="#b9">Kummerfeld et al., 2010)</ref>, since the derivation space of the parser is determined by the supertagger, at both train-*All work was completed before the author joined <ref type="bibr">Face- book.</ref> ing and test time.  enhanced supertagging using a so-called adaptive strategy, such that additional categories are supplied to the parser only if a spanning analysis cannot be found. This strategy is used in the de facto C&amp;C parser ( , and the two-stage CCG parsing pipeline (supertagging and parsing) continues to be the choice for most recent CCG parsers ( <ref type="bibr" target="#b21">Zhang and Clark, 2011;</ref><ref type="bibr" target="#b0">Auli and Lopez, 2011;</ref><ref type="bibr" target="#b20">Xu et al., 2014)</ref>.</p><p>Despite the effectiveness of supertagging, the most widely used model for this task ) has a number of drawbacks. First, it relies too heavily on POS tags, which leads to lower accuracy on out-of-domain data ( <ref type="bibr" target="#b15">Rimell and Clark, 2008)</ref>. Second, due to the sparse, indicator feature sets mainly based on raw words and POS tags, it shows pronounced performance degradation in the presence of rare and unseen words ( <ref type="bibr" target="#b15">Rimell and Clark, 2008;</ref><ref type="bibr">Lewis and Steed- man, 2014)</ref>. And third, in order to reduce computational requirements and feature sparsity, each tagging decision is made without considering any potentially useful contextual information beyond a local context window. <ref type="bibr" target="#b11">Lewis and Steedman (2014)</ref> introduced a feedforward neural network to supertagging, and addressed the first two problems mentioned above. However, their attempt to tackle the third problem by pairing a conditional random field with their feed-forward tagger provided little accuracy improvement and vastly increased computational complexity, incurring a large efficiency penalty.</p><p>We introduce a recurrent neural network-based (RNN) supertagging model to tackle all the above problems, with an emphasis on the third one. RNNs are powerful models for sequential data, which can potentially capture long-term dependencies, based on an unbounded history of previous words ( §2); similar to <ref type="bibr" target="#b11">Lewis and Steedman (2014)</ref> we only use distributed word representa-tions ( §2.2). Our model is highly accurate, and by integrating it with the C&amp;C parser as its adaptive supertagger, we obtain substantial accuracy improvements, outperforming the feed-forward setup on both supertagging and parsing.</p><p>2 Supertagging with a RNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>We use an Elman recurrent neural network <ref type="bibr">(El- man, 1990</ref>) which consists of an input layer x t , a hidden state (layer) h t with a recurrent connection to the previous hidden state h t−1 and an output layer y t . The input layer is a vector representing the surrounding context of the current word at position t, whose supertag is being predicted. <ref type="bibr">1</ref> The hidden state h t−1 keeps a representation of all context history up to the current word. The current hidden state h t is computed using the current input x t and hidden state h t−1 from the previous position. The output layer represents probability scores of all possible supertags, with the size of the output layer being equal to the size of the lexical category set.</p><p>The parameterization of the network consists of three matrices which are learned during supervised training. Matrix U contains weights between the input and hidden layers, V contains weights between the hidden and output layers, and W contains weights between the previous hidden state and the current hidden state. The following recurrence 2 is used to compute the activations of the hidden state at word position t:</p><formula xml:id="formula_0">h t = f (x t U + h t−1 W),<label>(1)</label></formula><p>where f is a non-linear activation function; here we use the sigmoid function f (z) = 1 1+e −z . The output activations are calculated as:</p><formula xml:id="formula_1">y t = g(h t V),<label>(2)</label></formula><p>where g is the softmax activation function g(z i ) = e z i j e z j that squeezes raw output activations into a probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Embeddings</head><p>Our RNN supertagger only uses continuous vector representations for features and each feature type has an associated look-up table, which maps a feature to its distributed representation. In total, three feature types are used. The first type is word embeddings: given a sentence of N words, (w 1 , w 2 , . . . , w N ), the embedding feature of w t (for 1 ≤ t ≤ N ) is obtained by projecting it onto a n-dimensional vector space through the look-up table L w ∈ R |w|×n , where |w| is the size of the vocabulary. Algebraically, the projection operation is a simple vector-matrix product where a one-hot vector b j ∈ R 1×|w| (with zeros everywhere except at the jth position) is multiplied with L w :</p><formula xml:id="formula_2">e wt = b j L w ∈ R 1×n ,<label>(3)</label></formula><p>where j is the look-up index for w t .</p><p>In addition, as in <ref type="bibr" target="#b11">Lewis and Steedman (2014)</ref>, for every word we also include its 2-character suffix and capitalization as features. Two more lookup tables are used for these features. L s ∈ R |s|×m is the look-up table for suffix embeddings, where |s| is the suffix vocabulary size. L c ∈ R 2×m is the look-up table for the capitalization embeddings. L c contains only two embeddings, representing whether or not a given word is capitalized.</p><p>We extract features from a context window surrounding the current word to make a tagging decision. Concretely, with a context window of size k, k/2 words either side of the target word are included. For a word w t , its continuous feature representation is:</p><formula xml:id="formula_3">f wt = [e wt ; s wt ; c wt ],<label>(4)</label></formula><p>where e wt ∈ R 1×n , s wt ∈ R 1×m and c wt ∈ R 1×m are the output vectors from the three different look-up tables, and [e wt ; s wt ; c wt ] denotes the concatenation of three vectors and hence f wt ∈ R 1×(n+2m) . At word position t, the input layer of the network x t is:</p><formula xml:id="formula_4">x t = [f w t−−k/2 ; . . . f wt ; . . . ; f w t+k/2 ],<label>(5)</label></formula><p>where x t ∈ R 1×k(n+2m) and the right-hand side is the concatenation of all feature representations in a size k context window. We use pre-trained word embeddings from <ref type="bibr" target="#b19">Turian et al. (2010)</ref> to initialize lookup table L w , and we apply a set of word pre-processing techniques at both training and test time to reduce sparsity. All words are first lower-cased, and all numbers are collapsed into a single digit '0'. If a lower-cased hyphenated word does not have an entry in the pre-trained word embeddings, we attempt to back-off to the substring after the last hyphen. For compound words and numbers delimited by "\/", we attempt to back-off to the substring after the delimiter. After pre-processing, the Turian embeddings have a coverage of 94.25% on the training data; for out-of-vocabulary words, three separate randomly initialized embeddings are used for lower-case alphanumeric words, upper-case alphanumeric words, and non-alphanumeric symbols.</p><p>For padding at the start and end of a sentence, the "unknown" entry from the pre-trained embeddings is used. Look-up tables L s and L c are also randomly initialized, and all look-up tables are modified during supervised training using backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets and Baseline. We follow the standard splits of <ref type="bibr">CCGBank (Hockenmaier and Steedman, 2007)</ref> for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from <ref type="bibr" target="#b8">Honnibal et al. (2009)</ref> and the Bioinfer corpus ( <ref type="bibr" target="#b14">Pyysalo et al., 2007)</ref> are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C&amp;C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C&amp;C parser. We use the same 425 supertag set used in both C&amp;C and NN.</p><p>Hyperparameters and Training. For L w , we use the scaled 50-dimensional Turian embeddings (n = 50 for L w ) as initialization. We have experimented during development with using 100-dimensional embeddings and found no improvements in the resulting model. Out , and are then scaled by their corresponding input vector size. We experimented with context window sizes of 3, 5, 7, 9 and 11 during development and found a window size of 7 gives the best performing model on the dev set. We use a fixed learning rate of 0.0025 and a hidden state size of 200.  To train the model, we optimize cross-entropy loss with stochastic gradient descent using minibatched backpropagation through time <ref type="bibr">(BPTT;</ref><ref type="bibr" target="#b16">Rumelhart et al., 1988;</ref><ref type="bibr" target="#b13">Mikolov, 2012)</ref>; the minibatch size for BPTT, again tuned on the dev set, is set to 9.</p><p>Embedding Dropout Regularization. Without any regularization, we found cross-entropy error on the dev set started to increase while the error on the training set was continuously driven to a very small value <ref type="figure">(Fig. 1a)</ref>. With the suspicion of overfitting, we experimented with l 1 and l 2 regularization and learning rate decay but none of these techniques gave any noticeable improvements for our model. Following <ref type="bibr" target="#b10">Legrand and Collobert (2014)</ref>, we instead implemented word embedding dropout as a regularization for all the look-up tables, since the capacity of our tagging model mainly comes from the look-up tables, as in their system. We observed more stable learning and better generalization of the trained model with dropout. Similar to other forms of droput ( <ref type="bibr" target="#b17">Srivastava et al., 2014</ref>), we randomly drop units and their connections to other units at training time. Concretely, we apply a binary dropout mask to x t , with a dropout rate of 0.25, and at test time no mask is applied, but the input to the network, x t , at each word position is scaled by 0.75. We experimented during development with different dropout rates, but found the above choice to be optimal in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Supertagging Results</head><p>We use the RNN model which gives the highest 1-best supertagging accuracy on the dev set as the final model for all experiments. Without any form of regularization, the best model was obtained at the 20th epoch, and it took 35 epochs for the dropout model to peak <ref type="figure">(Fig. 1b)</ref>. We use the dropout model for all experiments and, unlike the C&amp;C supertagger, no tag dictionaries are used.     ger drops about 1% with automatically assigned POS tags, while our RNN model gives higher accuracy (+0.47%) than the C&amp;C supertagger with gold POS tags. All timing values are obtained on a single Intel i7-4790k core, and all implementations are in C++ except NN which is implemented using Torch and Java, and therefore we believe the efficiency of NN could be vastly improved with an implementation with a lower-level language. <ref type="table" target="#tab_3">Table 2</ref> compares different supertagging models for multi-tagging accuracy at the default β levels used by the C&amp;C parser on the dev set. The β parameter determines the average number of supertags assigned to each word (ambiguity) by a supertagger when integrated with the parser; categories whose probabilities are not within β times the probability of the 1-best category are pruned. At the first β level (0.075), the three supertagging models give very close ambiguity levels, but our RNN model clearly outperforms NN and C&amp;C (auto POS) in both word (WORD) and sentence (SENT) level accuracies, giving similar word-level accuracy as C&amp;C (gold POS). For other β levels (except β = 0.001), the RNN model gives comparable ambiguity levels to the C&amp;C model which uses a tagdict, while being much more accurate than both the other two models.    <ref type="figure">Fig. 1c</ref> compares multi-tagging accuracies of all the models on the dev set. For all models, the same β levels are used (ranging from 0.075 to 10 −4 , and all C&amp;C default values are included). The RNN model consistently outperforms other models across different ambiguity levels. <ref type="table" target="#tab_4">Table 3</ref> shows 1-best accuracies of all models on the test data sets (Bio-GENIA gold-standard CCG lexical category data from <ref type="bibr" target="#b15">Rimell and Clark (2008)</ref> are used, since no gold categories are available in the Bioinfer data). With gold-standard POS tags, the C&amp;C model outperforms both the NN and RNN models on CCGBank and Bio-GENIA; with auto POS, the accuracy of the C&amp;C model drops significantly, due to its high reliance on POS tags. <ref type="figure">Fig. 2</ref> shows multi-tagging accuracies on all test data (using β levels ranging from 0.075 to 10 −6 , and all C&amp;C default values are included). On CCGBank, the RNN model has a clear accuracy advantage, while on the other two data sets, the accuracies given by the NN model are closer to the RNN model at some ambiguity levels, representing these data sets are still more challenging than CCGBank. However, both the NN and RNN models are more robust than the C&amp;C model on the two out-of-domain data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parsing Results</head><p>We integrate our supertagging model into the C&amp;C parser, at both training and test time, using all default parser settings; C&amp;C hybrid model is used for CCGBank and Wikipedia; the normal-form model is used for the Bioinfer data, in line with <ref type="bibr" target="#b11">Lewis and Steedman (2014)</ref> and <ref type="bibr" target="#b15">Rimell and Clark (2008)</ref>. Parsing development results are shown in <ref type="table" target="#tab_6">Table 4</ref>; for out-of-domain data sets, no separate development experiments were done. Final results are shown in <ref type="table" target="#tab_7">Table 5</ref>, and we substantially improve parsing accuracies on CCGBank and Wikipedia. The accuracy of our model on CCGBank represents a F1 score improvement of 1.53%/1.85% over the C&amp;C baseline, which is comparable to the best known accuracy reported in <ref type="bibr" target="#b0">Auli and Lopez (2011)</ref>. However, our RNN-supertaggingbased model is conceptually much simpler, with no change to the parsing model required at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented a RNN-based model for CCG supertagging, which brings significant accuracy improvements for supertagging and parsing, on both in-and out-of-domain data sets. Our supertagger is fast and well-suited for large scale processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-of-vocabulary embedding values in L w and all embedding values in L s and L c are initialized with a uniform distri- bution in the interval [−2.0, 2.0]. The embedding dimension size m of L s and L c is set to 5. Other parameters of the network {U, V, W} are initial- ized with values drawn uniformly from the inter- val [−2.0, 2.0]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Learning curve and 1-best tagging accuracy of the RNN model on CCGBank Section 00. Plot (c) shows ambiguity vs. multi-tagging accuracy for all supertaggers (auto POS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 shows 1-best supertagging accuracies on the dev set. The accuracy of the C&amp;C supertag-</head><label>1</label><figDesc></figDesc><table>252 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Multi-tagging accuracy and ambiguity comparison (supertags/word) at the default C&amp;C β levels 
on CCGBank Section 00. 

Model 
Section 23 Wiki 
Bio 
C&amp;C (gold POS) 93.32 
88.80 91.85 
C&amp;C (auto POS) 92.02 
88.80 89.08 
NN 
91.57 
89.00 88.16 
RNN 
93.00 
90.00 88.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>1-best tagging accuracy compari-
son on CCGBank Section 23 (2,407 sentences), 
Wikipedia (200 sentences) and Bio-GENIA (1,000 
sentences). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Parsing development results on CCGBank Section 00 (auto POS). 

CCGBank Section 23 
Wikipedia 
Bioinfer 
LP 
LR 
LF 
cov. 
LP 
LR 
LF 
cov. 
LP 
LR 
LF 
cov. 

C&amp;C 

86.24 84.85 85.54 99.42 81.58 80.08 80.83 99.50 77.78 76.07 76.91 95.40 
C&amp;C (+ NN) 
86.71 85.56 86.13 99.92 82.65 81.36 82.00 100 
79.77 78.62 79.19 97.40 
C&amp;C (+ RNN) 87.68 86.47 87.07 99.96 83.22 81.78 82.49 100 
80.10 78.21 79.14 97.80 

C&amp;C 

86.24 84.17 85.19 100 
81.58 79.48 80.52 100 
77.78 71.44 74.47 100 
C&amp;C (+ NN) 
86.71 85.40 86.05 100 
-
-
-
-
79.77 75.35 77.50 100 
C&amp;C (+ RNN) 87.68 86.41 87.04 100 
-
-
-
-
80.10 75.52 77.74 100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Parsing test results on all three domains (auto POS). We evaluate on all sentences (100% 
coverage) as well as on only those sentences that returned spanning analyses (% cov.). RNN and NN 
both have 100% coverage on the Wikipedia data. 

</table></figure>

			<note place="foot" n="1"> This is different from some RNN models (e.g., Mikolov et al. (2010)) where the input is a one-hot vector. 2 We assume the input to any layer is a row vector unless otherwise stated.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The first author acknowledges the Carnegie Trust for the Universities of Scotland and the Cambridge Trusts for providing funding. SC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1. We also thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training a log-linear parser with loss functions via softmaxmargin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="333" to="343" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supertagging: An approach to almost parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="265" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The importance of supertagging for wide-coverage CCG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James R Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics, page 282. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics, page 282. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Widecoverage efficient statistical parsing with CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-tagging for lexicalized-grammar parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James R Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale NLP with C&amp;C and Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James R Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating a statistical CCG parser on wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources</title>
		<meeting>the 2009 Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="38" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Faster parsing by supertagger adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessika</forename><surname>Jonathan K Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Dawborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haggerty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James R Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="345" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7028</idno>
		<title level="m">Joint RNNbased greedy parsing and word composition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved CCG parsing with semi-supervised supertagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="338" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IN-TERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Statistical Language Models Based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bioinfer: a corpus for information extraction in the biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Heimonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jari</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Järvinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapting a lexicalized-grammar parser to contrasting domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning representations by backpropagating errors. Cognitive modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shift-reduce CCG parsing with a dependency model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACL Conference</title>
		<meeting>the 2014 ACL Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shift-reduce CCG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL 2011</title>
		<meeting>ACL 2011<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="683" to="692" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
