<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as conference paper at ICLR 2015 EMBEDDING ENTITIES AND RELATIONS FOR LEARN- ING AND INFERENCE IN KNOWLEDGE BASES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
							<email>bishan@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<postCode>14850</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">1˚,</forename><surname>1˚,</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<postCode>14850</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
							<email>deng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Published as conference paper at ICLR 2015 EMBEDDING ENTITIES AND RELATIONS FOR LEARN- ING AND INFERENCE IN KNOWLEDGE BASES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCitypa, bq ^ CityInCountrypb, cq ùñ N ationalitypa, cq. We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have witnessed a rapid growth of knowledge bases (KBs) such as Freebase 1 , DBPedia ( <ref type="bibr" target="#b0">Auer et al., 2007)</ref>, and YAGO ( <ref type="bibr" target="#b21">Suchanek et al., 2007)</ref>. These KBs store facts about real-world entities (e.g. people, places, and things) in the form of RDF triples 2 (i.e. (subject, predicate, object)). Today's KBs are large in size. For instance, Freebase contains millions of entities and billions of facts (triples) involving a large variety of predicates (relation types). Such large-scale multirelational data provide an excellent potential for improving a wide range of tasks, from information retrieval, question answering to biological data mining.</p><p>Recently, much effort has been invested in relational learning methods that can scale to large knowledge bases. Tensor factorization (e.g. ( <ref type="bibr" target="#b10">Nickel et al., 2011;</ref>) and neural-embedding-based models (e.g. ( <ref type="bibr" target="#b2">Bordes et al., 2013a;</ref><ref type="bibr" target="#b20">Socher et al., 2013)</ref>) are two popular kinds of approaches that learn to encode relational information using low-dimensional representations of entities and relations. These representation learning methods have shown good scalability and reasoning ability in terms of validating unseen facts given the existing KB.</p><p>In this work, we focus on the study of neural-embedding models, where the representations are learned using neural networks with energy-based objectives. Recent embedding models TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013b</ref>) and NTN ( <ref type="bibr" target="#b20">Socher et al., 2013</ref>) have shown state-of-the-art prediction performance compared to tensor factorization methods such as RESCAL ( <ref type="bibr" target="#b11">Nickel et al., 2012)</ref>. They are similar in model forms with slight differences on the choices of entity and relation representations. Without careful comparison, it is not clear how different design choices affect the˚Work the˚the˚Work conducted while interning at Microsoft Research. 1 http://freebase.com 2 http://www. <ref type="bibr">w3</ref>.org/TR/rdf11-concepts/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multi-relational learning has been an active research area for the past couple of years. Traditional statistical learning approaches <ref type="bibr">(Getoor &amp; Taskar, 2007</ref>) such as Markov-logic networks <ref type="bibr">(Richard- son &amp; Domingos, 2006</ref>) usually suffer from scalability issues. More recently, various types of representation learning methods have been proposed to embed multi-relational knowledge into lowdimensional representations of entities and relations, including tensor/matrix factorization <ref type="bibr" target="#b18">(Singh &amp; Gordon, 2008;</ref><ref type="bibr" target="#b10">Nickel et al., 2011;</ref>, Bayesian clustering framework ( <ref type="bibr" target="#b8">Kemp et al., 2006;</ref><ref type="bibr" target="#b22">Sutskever et al., 2009)</ref>, and neural networks <ref type="bibr" target="#b12">(Paccanaro &amp; Hinton, 2001;</ref><ref type="bibr" target="#b2">Bordes et al., 2013a;</ref><ref type="bibr" target="#b20">Socher et al., 2013)</ref>. Our work focuses on the study of neural-embedding models as they have shown good scalability and strong generalizability on large-scale KBs.</p><p>Existing neural embedding models ( <ref type="bibr" target="#b2">Bordes et al., 2013a;</ref><ref type="bibr" target="#b20">Socher et al., 2013</ref>) all represent entities as low-dimensional vectors and represent relations as operators that combine the representations of two entities. They differ in different parametrization of relation operators. For instance, given two entity vectors, the model of Neural Tensor Network (NTN) <ref type="bibr" target="#b20">(Socher et al., 2013)</ref> represents each relation as a bilinear tensor operator followed by a linear matrix operator. The model of <ref type="bibr">TransE (Bordes et al., 2013b</ref>), on the other hand, represents each relation as a single vector that linearly interacts with the entity vectors. Likewise, variations on entity representations also exist. Most methods represent each entity as a unit vector while <ref type="bibr">NTN (Socher et al., 2013</ref>) represent entities as an average of word vectors and initializing word vectors with pre-trained vectors from external text corpora. There has not been work that closely examines the effectiveness of these different design choices.</p><p>Our work on embedding-based rule extraction presented in part of this paper is related to the earlier study on logical inference with learned continuous-space representations. Much existing work along this line focuses on learning logic-based representations for natural language sentences. For example, <ref type="bibr" target="#b19">Socher et al. (2012)</ref> builds a neural network that recursively combines word representations based on parse tree structures and shows that such neural network can simulate the behavior of conjunction and negation. <ref type="bibr">Bowman (2014)</ref> further demonstrates that recursive neural network can capture certain aspects of natural logical reasoning on examples involving quantifiers like some and all. Recently, <ref type="bibr">Grefenstette (2013)</ref> shows that in theory most aspects of predicate logic can be simulated using tensor calculus. <ref type="bibr" target="#b14">Rocktäschel et al. (2014)</ref> further implements the idea by introducing a supervised objective that trains embeddings to be consistent with given logical rules. The evaluation was conducted on toy data and uses limited logical forms. Different from these earlier studies, we propose a novel approach to utilizing embeddings learned without explicit logical constraints to directly mine logical rules from KBs. We demonstrate that the learned embeddings of relations can capture the compositional semantics of relations. Moreover, we systematically evaluate our approach and compare it favorably with a state-of-the-art rule mining approach on the rule extraction task on Freebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-RELATIONAL REPRESENTATION LEARNING</head><p>In this section, we present a general neural network framework for multi-relational representation learning. We discuss different design choices for the representations of entities and relations which will be empirically compared in Section 4.</p><p>Given a KB that is represented as a list of relation triplets pe 1 , r, e 2 q (denoting e 1 (the subject) and e 2 (the object) that are in a certain relationship r), we want to learn representations for entities and relations such that valid triplets receive high scores (or low energies). The embeddings can be learned via a neural network. The first layer projects a pair of input entities to low dimensional vectors, and the second layer combines these two vectors to a scalar for comparison via a scoring function with relation-specific parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ENTITY REPRESENTATIONS</head><p>Each input entity corresponds to a high-dimensional vector, either a "one-hot" index vector or a "n-hot" feature vector. Denote by x e1 and x e2 the input vectors for entity e 1 and e 2 , respectively. Denote by W the first layer projection matrix. The learned entity representations, y e1 and y e2 can be written as</p><formula xml:id="formula_0">y e1 " f ` Wx e1 ˘ , y e2 " f ` Wx e2</formula><p>˘ where f can be a linear or non-linear function, and W is a parameter matrix, which can be randomly initialized or initialized using pre-trained vectors.</p><p>Most existing embedding models adopt the "one-hot" input vectors except for NTN <ref type="bibr" target="#b20">(Socher et al., 2013</ref>) which represents each entity as an average of its word vectors. This can be viewed as adopting "bag-of-words" vectors as input and learning a projection matrix consisting of word vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RELATION REPRESENTATIONS</head><p>The choice of relation representations reflects in the form of the scoring function. Most of the existing scoring functions in the literature can be unified based on a basic linear transformation g a r , a bilinear transformation g b r or their combination, where g a r and g b r are defined as</p><formula xml:id="formula_1">g a r py e1 , y e2 q " A T r ˆ y e1 y e2 ˙ and g b r py e1 , y e2 q " y T e1 B r y e2 ,<label>(1)</label></formula><p>which A r and B r are relation-specific parameters.</p><formula xml:id="formula_2">Models Br A T r Scoring Function Distance (Bordes et al., 2011) - ` Q T r 1 ´Q T r 2 ˘ ´||g a r pye 1 , ye 2 q||1 Single Layer (Socher et al., 2013) - ` Q T r1 Q T r2 ˘ u T r tanhpg a r pye 1 , ye 2 qq TransE (Bordes et al., 2013b) I ` V T r ´V T r ˘ ´p2g a r pye 1 , ye 2 q ´ 2g b r pye 1 , ye 2 q ` ||Vr|| 2 2 q NTN (Socher et al., 2013) Tr`Q Tr` Tr`Q T r1 Q T r2 ˘ u T r tanh`g</formula><p>tanh` tanh`g a r pye 1 , ye 2 q ` g b r pye 1 , ye 2 q ˘ <ref type="table">Table 1</ref>: Comparisons among several multi-relational models in their scoring functions.</p><p>In <ref type="table">Table 1</ref>, we summarize several popular scoring functions in the literature for a relation triplet pe 1 , r, e 2 q, reformulated in terms of the above two functions. Denote by y e1 , y e2 P R n two entity vectors. Denote by Q r1 , Q r2 P R nˆm and V r P R n matrix or vector parameters for linear transformation g a r . Denote by T r P R nˆnˆm tensor parameters for bilinear transformation g b r . I P R n is an identity matrix. u r P R m is an additional parameter for relation r. The scoring function for TransE (L2 formulation) is derived from ||y e1´ye1´e1´y e2`Ve2`e2`V r || 2 2 " 2V T r py e1´ye1´e1´y e2 q ´ 2y T e1 y e2`||Ve2`e2`||V r || 2 2 ` ||y e1 || 2 2 ` ||y e2 || 2 2 , where y e1 and y e2 are unit vectors. Note that NTN is the most expressive model as it contains both linear and bilinear relation operators as special cases. In terms of the number of parameters, TransE is the simplest model which only parametrizes the linear relation operators with one-dimensional vectors.</p><p>In this paper, we also consider the basic bilinear scoring function:</p><formula xml:id="formula_3">g b r py e1 , y e2 q " y T e1 M r y e2 (2)</formula><p>which is a special case of NTN without the non-linear layer and the linear operator, and uses a 2-d matrix operator M r P R nˆn instead of a tensor operator. Such bilinear formulation has been used in other matrix factorization models such as in <ref type="bibr" target="#b10">(Nickel et al., 2011;</ref><ref type="bibr" target="#b7">Jenatton et al., 2012;</ref><ref type="bibr">García-Durán et al., 2014</ref>) with different forms of regularization. Here, we consider a simple way to reduce the number of relation parameters by restricting M r to be a diagonal matrix. This results in the same number of relation parameters as TransE. Our experiments in Section 4 demonstrate that this simple formulation enjoys the same scalable property as TransE and it achieves superior performance over TransE and other more expressive models on the task of link prediction.</p><p>This general framework for relationship modeling also applies to the recent deep-structured semantic model ( <ref type="bibr" target="#b5">Huang et al., 2013;</ref><ref type="bibr" target="#b16">Shen et al., 2014a;</ref><ref type="bibr">Gao et al., 2014;</ref><ref type="bibr" target="#b24">Yih et al., 2014</ref>), which learns the relevance or a single relation between a pair of word sequences. The framework above applies when using multiple neural network layers to project entities and using a relation-independent scoring function G r ` y e1 , y e2 ˘ " cosry e1 pW r q, y e2 pW r qs. The cosine scoring function is a special case of g b r with normalized y e1 , y e2 and with B r " I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PARAMETER LEARNING</head><p>The neural network parameters of all the models discussed above can be learned by minimizing a margin-based ranking objective , which encourages the scores of positive relationships (triplets) to be higher than the scores of any negative relationships (triplets). Usually only positive triplets are observed in the data. Given a set of positive triplets T , we can construct a set of "negative" triplets T 1 by corrupting either one of the relation arguments,</p><formula xml:id="formula_4">T 1 " tpe 1 1 , r, e 2 q|e 1 1 P E, pe 1 1 , r, e 2 q R T u Y tpe 1 , r, e 1 2 q|e 1 2 P E, pe 1 , r, e 1 2 q R T u.</formula><p>Denote the scoring function for triplet pe 1 , r, e 2 q as S pe1,r,e2q . The training objective is to minimize the margin-based ranking loss</p><formula xml:id="formula_5">LpΩq " ÿ pe1,r,e2qPT ÿ pe 1 1 ,r,e 1 2 qPT 1 maxtS pe 1 1 ,r,e 1 2 q ´ S pe1,r,e2q`1e2q`e2q`1, 0u<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INFERENCE TASK I: LINK PREDICTION</head><p>We first conduct a comparison study of different embedding models on the canonical link prediction task, which is to predict the correctness of unseen triplets. As in (Bordes et al., 2013b), we formulate link prediction as an entity ranking task. For each triplet in the test data, we treat each entity as the target entity to be predicted in turn. Scores are computed for the correct entity and all the corrupted entities in the dictionary and are ranked in descending order. We consider Mean Reciprocal Rank (MRR) (an average of the reciprocal rank of an answered entity over all test triplets), HITS@10 (top-10 accuracy), and Mean Average Precision (MAP) (as used in ( <ref type="bibr">Chang et al., 2014)</ref>) as the evaluation metrics.</p><p>We examine five embedding models in decreasing order of complexity: (1) NTN with 4 tensor slices as in <ref type="bibr" target="#b20">(Socher et al., 2013)</ref>; (2) Bilinear+Linear, NTN with 1 tensor slice and without the non-linear layer; (3) TransE, a special case of Bilinear+Linear (see <ref type="table">Table 1</ref> Implementation details All the models were implemented in C# and using GPU. Training was implemented using mini-batch stochastic gradient descent with AdaGrad ( <ref type="bibr">Duchi et al., 2011</ref>). At each gradient step, we sampled for each positive triplet two negative triplets, one with a corrupted subject entity and one with a corrupted object entity. The entity vectors are renormalized to have unit length after each gradient step (it is an effective technique that empirically improved all the models).</p><p>For the relation parameters, we used standard L2 regularization. For all models, we set the number of mini-batches to 10, the dimensionality of the entity vector d " 100, the regularization parameter 0.0001, and the number of training epochs T " 100 on FB15k and FB15k-401 and T " 300 on WN (T was determined based on the learning curves where the performance of all models plateaued.)</p><p>The learning rate was initially set to 0.1 and then adapted during training by AdaGrad.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FB15k</head><p>FB15k    <ref type="figure" target="#fig_0">(HITS@10)</ref>. We attribute such discrepancy mainly to the different choice of SGD optimization: AdaGrad vs. constant learning rate. We also found that Bilinear consistently provides comparable or better performance than TransE, especially on WN. Note that WN contains much more entities than FB, it may require the parametrization of relations to be more expressive to better handle the richness of entities. Interestingly, we found that a simple variant of Bilinear -BILINEAR-DIAG, clearly outperforms all baselines on FB and achieves comparable performance to Bilinear on WN. Note that BILINEAR-DIAG has the limitation of encoding the difference between a relation and its inverse. Still, as there is a large variety of relations in FB and the average number of training examples seen by each relation is relatively small (compared to WN), the simple form of BILINEAR-DIAG is able to provide good prediction performance.</p><p>Multiplicative vs. Additive Interactions Note that BILINEAR-DIAG and TRANSE have the same number of model parameters and their difference can be viewed as the operational choices of the composition of two entity vectors -BILINEAR-DIAG uses weighted element-wise dot product (multiplicative operation) and TRANSE uses element-wise subtraction with a bias (additive operation). To highlight the difference, here we use DISTMULT and DISTADD to refer to BILINEAR-DIAG and TRANSE, respectively. Comparisons between these two models can provide us more insights on the effect of two common choices of compositional operations -multiplication and addition for modeling entity relations. Overall, we observed superior performance of DISTMULT on all the datasets in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="table">Table 3</ref> shows the HITS@10 score on four types of relation categories (as defined in (Bordes et al., 2013b)) on FB15k-401 when predicting the subject entity and the object entity respectively. We can see that DISTMULT significantly outperforms DISTADD in almost all the categories.</p><p>Predicting subject entities Predicting object entities 1-to-1 1-to-n n-to-1 n-to-n 1-to-1 1-to-n n-to-1 n-to-n DISTADD 70.0 76.  <ref type="table">Table 3</ref>: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many</p><p>Initialization of Entity Vectors In the following, we examine the learning of entity representations and introduce two further improvements: using non-linear projection and initializing entity vectors with pre-trained vectors. We focus on DISTMULT as our baseline and compare it with the two modifications DISTMULT-tanh (using f " tanh for entity projection ) and DISTMULT-tanh-EV-init (initializing the entity parameters with the 1000-dimensional pre-trained entity vectors released by <ref type="bibr">word2vec (Mikolov et al., 2013)</ref>) on FB15k-401. We also reimplemented the initialization technique introduced in <ref type="bibr" target="#b20">(Socher et al., 2013</ref>) -each entity is represented as an average of its word vectors and the word vectors are initialized using the 300-dimensional pre-trained word vectors released by word2vec. We denote this method as DISTMULT-tanh-WV-init. Inspired by <ref type="bibr">(Chang et al., 2014</ref>), we design a new evaluation setting where the predicted entities are automatically filtered according to "entity types" (entities that appear as the subjects/objects of a relation have the same type defined by that relation). This provides us with better understanding of the model performance when some entity type information is provided.  <ref type="table">Table 4</ref>: Evaluation with pre-trained vectors</p><p>In <ref type="table">Table 4</ref>, we can see that DISTMULT-tanh-EV-init provides the best performance on all the metrics. Surprisingly, we observed performance drops by DISTMULT-tanh-WV-init. We suspect that this is because word vectors are not appropriate for modeling entities described by non-compositional phrases (more than 73% of the entities in FB15k-401 are person names, locations, organizations and films). The promising performance of DISTMULT-tanh-EV-init suggests that the embedding model can greatly benefit from pre-trained entity-level vectors using external textual resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INFERENCE TASK II: RULE EXTRACTION</head><p>In this section, we focus on a complementary inference task, where we utilize the learned embeddings to extract logical rules from the KB. For example, given the fact that a person was born in New York and New York is a city of the United States, then the person's nationality is the United States:</p><p>BornInCitypa, bq ^ CityOf Countrypb, cq ùñ N ationalitypa, cq</p><p>Such logical rules can serve four important purposes. First, they can help deduce new facts and complete the existing KBs. Second, they can help optimize data storage by storing only rules instead of large amounts of extensional data, and generate facts only at inference time. Third, they can support complex reasoning. Lastly, they can provide explanations for inference results, e.g. we may infer that people's professions usually involve the specialization of the field they study, etc.</p><p>The key problem of extracting Horn rules like the aforementioned example is how to effectively explore the search space. Traditional rule mining approaches directly operate on the KB graphthey search for possible rules (i.e. closed-paths in the graph) by pruning rules with low statistical significance and relevance ( <ref type="bibr" target="#b15">Schoenmackers et al., 2010</ref>). These approaches often fail on large KB graphs due to scalability issues. In the following, we introduce a novel embedding-based rule mining approach whose efficiency is not affected by the size of the KB graph but rather by the number of distinct types of relations in the KB (which is usually relatively small). It can also mine better rules due to its strong generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BACKGROUND AND NOTATIONS</head><p>For a better illustration, we adopt the graph view of KB. Each binary relation rpa, bq is a directed edge from node a to node b and with link type r. We are interested in extracting Horn rules that consist of a head relation H and a sequence of body relations B 1 , ..., B n : B 1 pa 1 , a 2 q ^ B 2 pa 2 , a 3 q ^ ... ^ B n pa n , a n`1 q ùñ Hpa 1 , a n`1 q</p><p>where a i are variables that can be substituted by entities. We constrain the body relations B 1 , ..., B n to form a directed path in the graph and the head relation H to from a directed edge that close the path (from the start of the path to the end of the path). We denote such property as the closed-path property. For consecutive relations that share one variable but do not form a path, e,g, B i´1 pa, bq ^ B i pa, cq, we can replace one of the relations with its inverse relation, so that the relations are connected by an object and an subject, e.g. B ´1 i´1 pb, aq ^ B i pa, cq. We are interested in mining rules that reflect relationships among different relation types, therefore we also constrain B 1 , ..., B n , H to have distinct relation types. A rule is instantiated when all variables are substituted by entities. We denote the length of the rule as the number of body relations. In general longer rules are harder to extract due to the exponential search space. In our experiments, we focus on extracting rules of length 2 and 3.</p><p>In KBs, entities usually have types and relations often can only take arguments of certain types. For example, BornInCity relation can only take a person as the subject and a location as the object. For each relation r, we denote the domain of its subject argument (the set of entities that can appear in the subject position) as X r and similarly the domain of its object argument as Y r . Such domain information can be extremely useful in restricting the search space of logical rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EMBEDDING-BASED RULE EXTRACTION</head><p>For simplicity, we consider Horn rules of length 2 (longer rules can be easily derived from this case):</p><formula xml:id="formula_7">B 1 pa, bq ^ B 2 pb, cq ùñ Hpa, cq<label>(5)</label></formula><p>Note that the body of the rule can be viewed as the composition of relations B 1 and B 2 , which is a new relation that has the property that entities a and c are in a relation if and only if there is an entity b which simultaneously satisfies two relations B 1 pa, bq and B 2 pb, cq.</p><p>We model relation composition as multiplication or addition of two relation embeddings. Here we focus on relation embeddings that are in the form of vectors (as in TRANSE) and matrices (as in BILINEAR and its variants). The composition results in a new embedding that lies in the same relation space. Specifically, we use addition for relation vector embeddings and multiplication for relation matrix embeddings. This is inspired by two different properties: (1) if a relation corresponds to a translation vector V and assume y a `V´y b « 0 when Bpa, bq holds, then we have the property that y a ` V 1 « y b and y b ` V 2 « y c implies y a ` pV 1 ` V 2 q « y c ; (2) if a relation corresponds to a matrix M in the bilinear transformation and assume y T a My b « 1 when Bpa, bq holds, also y a and y b are unit vectors and y T a M is still a unit vector 3 , then we have the property that y T a M 1 « y T b and y T b M 2 « y T c implies y T a pM 1 M 2 q « y T c . To simulate the implication in 5, we want the composition result of relation B 1 and B 2 to demonstrate similar behavior to the embedding of relation H. We assume the similarity between relation embeddings is related to the Euclidean distance if the embeddings are vectors and to the Frobenius norm if the embeddings are matrices. This distance metric allows us to rank possible pairs of relations with respect to the relevance of their composition to the target relation.</p><p>Note that we do not need to enumerate all possible pairs of relations in the KB. For example, if the relation in the head is r, then we are only interested in relation pairs pp, qq that satisfy the type constraints, namely:</p><formula xml:id="formula_8">(1) Y p X X q ‰ H; (2) X p X X r ‰ H; (3) Y q X Y r ‰ H.</formula><p>As mentioned before, the arguments (entities) of relations are usually strongly typed in KBs. Applying such domain constraints can effectively reduce the search space.</p><p>In Algorithm 1, we describe our rule extraction algorithm for general closed-path Horn rules as in Eq. (4). In Step 7, ˝ denotes vector addition or matrix multiplication. We apply a global threshold value δ in our experiments to filter candidate sequences for each relation r, and then automatically select the top remaining sequences by applying a heuristic thresholding strategy based on the difference of the distance scores: sort the sequences by increasing distance d 1 , ..., d T and set the cut-off point to be the j-th sequence where j " arg max i pd i`1 ´ d i q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EXPERIMENTS</head><p>We evaluate our rule extraction method (denoted as EMBEDRULE) on the FB15k-401 dataset. In our experiments, we remove the equivalence relations and relations whose domains have cardinality Algorithm 1 EMBEDRULE 1: Input: KB " tpe 1 , r, e 2 qu, relation set R 2: Output: Candidate rules Q 3: for each r in R do 4:</p><p>Select the set of start relations S " ts : X s X X r ‰ Hu</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Select the set of end relations T " tt : Y t X Y r ‰ Hu</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Find all possible relation sequences 7:</p><p>Select the K-NN sequences P 1 Ď P for r based on distpM r , M p1 ˝ ¨ ¨ ¨ ˝ M pn q 8:</p><p>Form candidate rules using P 1 where r is the head relation and p P P 1 is the body in a rule 9:</p><p>Add the candidate rules into Q 10: end for 1 since rules involving these relations are not interesting. This results in training data that contains 485,741 facts, 14,417 entities, and 373 relations. Our EMBEDRULE algorithm identifies 60,020 possible length-2 relation sequences and 2,156,391 possible length-3 relation sequences. We then apply the thresholding method described in Section 5.2 to further select top "3.9K length-2 rules and "2K length-3 rules <ref type="bibr">4</ref> . By default all the extracted rules are ranked by decreasing confidence, which is computed as the ratio of the correct predictions to the total number of predictions, where predictions are triplets that are derived from the instantiated rules where the body relations are observed.</p><p>We implemented four versions of EMBEDRULE using embeddings trained from TRANSE (DIS-TADD), BILINEAR, BILINEAR-DIAG (DISTMULT) and DISTMULT-tanh-EV-init with corresponding composition functions. We also compare our approaches to AMIE ( <ref type="bibr">Galárraga et al., 2013</ref>), a state-of-the-art rule mining system that can efficiently search for Horn rules in large-scale KBs by using novel measurements of support and confidence. The system extracts close rules -a superset of the rules we consider in this paper: every relation in the body is connected to the following relation by sharing an entity variable, and every entity variable in the rule appears at least twice. We run AMIE with the default setting on the same training set. It extracts 2,201 possible length-1 rules and 46,975 possible length-2 rules, among which 3,952 rules have the close-path property. We compare these length-2 rules with the similar number of length-2 rules extracted by EMBEDRULE. By default AMIE ranks rules by PCA confidence (a normalized confidence that takes into account the incompleteness of KBs). However we found that ranking by the standard confidence gives better performance than the PCA confidence on the Freebase dataset we use.</p><p>For computational cost, EmbedRule mines length-2 rules in 2 minutes and mines length-3 rules in 20 minutes (the computational time is similar when using different types of embeddings). AMIE mines rules of length ď 2 in 9 minutes. All methods are evaluated on a machine with a 64-bit processor, 2 CPUs and 8GB memory.</p><p>We consider precision as the evaluation metric, which is the ratio of predictions that are in the test (unseen) data to all the generated unseen predictions. Note that this is an estimation, since a prediction is not necessarily "incorrect" if it is not seen. <ref type="bibr">Galárraga et al. (2013)</ref> suggested to identify incorrect predictions based on the functional property of relations. However, we find that most relations in our datasets are not functional. For a better estimation, we manually labeled the top 30 unseen facts predicted by each method by checking Wikipedia. We also remove rules where the head relations are hard to justified due to dynamic factors (i.e. involving the word "current"). <ref type="figure" target="#fig_0">Figure 1</ref> compares the predictions generated by the length-2 rules extracted by different methods. We plot the aggregated precision of the top rules that produce up to 10K predictions in total. From left to right, the n-th data point represents the total number of predictions of the top n rules and the estimated precision of these predictions. We can see that EMBEDRULE that uses embeddings trained from the bilinear objective (BILINEAR, DISTMULT and DISTMULT-TANH-EV-INIT) consistently outperforms AMIE. This suggests that the bilinear embeddings contain good amount of information about relations which allows for effective rule selection without looking at the entities. For example, AMIE fails to extract T V P rogramCountryof Originpa, bq ^ CountryOf f icialLanguagepb, cq ùñ T V P rogramLanguagepa, cq by relying on the instantiations of the rule occurred in the observed KB while all the bilinear variants of EMBEDRULE successfully extract the rule purely based on the embeddings of the three involved relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">RESULTS</head><p>We can also see that in general, using multiplicative composition of matrix embeddings (from DIST-MULT and BILINEAR) results in better performance compared to using additive composition of vector embeddings (from DISTADD). We found many examples where DISTADD fails to retrieve rules because it assigns large distance between the composition of the body relations and the head relation in the embedding space while its multiplicative counterpart DISTMULT ranks the composition result much closer to the head relation. For example, DISTADD prunes the possible composition F ilmDistributorInRegion^RegionGDP Currency for relation F ilmBudgetCurrency while DISTMULT ranks the composition as the nearest neighbor of F ilmBudgetCurrency. We also look at the results for length-3 rules generated by different implementations of EMBEDRULE in <ref type="figure">Figure 2</ref>. We can see that the initial length-3 rules extracted by EMBEDRULE can provide very good precision in general. We can also see that BILINEAR consistently outperforms DISTMULT and DISTADD on the top 1K predictions and DISTMULT-TANH-EV-INIT tends to outperform the other methods as more predictions are generated. We think that the fact that BILINEAR starts to show more advantage over DISTMULT in extracting longer rules confirm the limitation of representing relations by diagonal matrices, as longer rules requires the modeling of more complex relation semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present a general framework for learning representations of entities and relations in KBs. Under the framework, we empirically evaluate different embedding models on knowledge inference tasks. We show that a simple formulation of bilinear model can outperform the state-ofthe-art embedding models for link prediction on Freebase. Furthermore, we examine the learned embeddings by utilizing them to extract logical rules from KBs. We show that embeddings learned from the bilinear objective can capture compositional semantics of relations and be successfully used to extract Horn rules that involve compositional reasoning. For future work, we aim to exploit deep structure in the neural network framework. As learning representations using deep networks has shown great success in various applications ( <ref type="bibr" target="#b4">Hinton et al., 2012;</ref><ref type="bibr" target="#b23">Vinyals et al., 2012;</ref>, it may also help capturing hierarchical structure hidden in the multi-relational data. Further, tensor constructs have been usefully applied to some deep learning architectures ( <ref type="bibr" target="#b25">Yu et al., 2013;</ref><ref type="bibr" target="#b6">Hutchinson et al., 2013)</ref>. Related constructs and architectures may help improve multi-relational learning and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A EXAMPLES OF THE EXTRACTED HORN RULES</head><p>Examples of length-2 rules extracted by EMBEDRULE with embeddings learned from DISTMULTtanh-EV-init: Grefenstette, Edward. Towards a formal distributional semantics: Simulating logical calculi with tensors. In *SEM, 2013.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Aggregated precision of top length-2 rules extracted by different methods</figDesc><graphic url="image-1.png" coords="9,205.76,261.35,198.00,148.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Relation embeddings (DISTADD)</figDesc><graphic url="image-4.png" coords="11,166.16,264.62,277.17,146.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Performance comparisons among different embedding models</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the results of all compared methods on all the datasets. In general, we observe 
that the performance increases as the complexity of the model decreases on FB. NTN, the most 
complex model, provides the worst performance on both FB and WN, which suggests overfitting. 
Compared to the previously published results of TransE (Bordes et al., 2013b), our implementation 
achieves much better results (53.9% vs. 47.1% on FB15k and 90.9% vs. 89.2% on WN) using the 
same evaluation metric </table></figure>

			<note place="foot" n="3"> These assumptions may not hold in our implementations. The intuition still leads to surprisingly good empirical performance on Horn rule extraction. How to effectively enforce these constraints is worth investigating in future work.</note>

			<note place="foot" n="4"> We consider K=100 nearest-neighbor sequences for each method, and set δ to 9.2, 36.3, 1.9 and 3.4 for DISTMULT-TANH-EV-INIT, DISTMULT, BILINEAR and DISTADD respectively for length-2 rules, and set it to 9.1, 48.8, 2.9, and 1.1 for lengh-3 rules.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sören</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oksana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for Web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Po-Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tensor deep stacking networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning systems of concepts with an infinite relational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hans-Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hans-Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts using linear relational embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Paccanaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="232" to="244" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-dimensional embeddings of logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bošnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Semantic Parsing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning first-order horn clauses from web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for Web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajit</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modelling relational data using Bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning with recursive perceptual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><forename type="middle">-</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The deep tensor neural network with applications to large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech and Language Proc</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="388" to="396" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
