<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaltParser: A language-independent system for data-driven dependency parsing A T A N A S C H A N E V</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Cambridge University Press</publisher>
				<availability status="unknown"><p>Copyright Cambridge University Press</p>
				</availability>
				<date type="published" when="2007">2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>A K I M N I V R E</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>S A N D R A K ¨ U B L</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>V E T O S L A V M A R I N O V</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Systems Engineering</orgName>
								<orgName type="institution">Växjö University</orgName>
								<address>
									<postCode>35195</postCode>
									<settlement>Växjö</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics and Philology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<postBox>Box 635</postBox>
									<postCode>75126</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Mathematics and Systems Engineering</orgName>
								<orgName type="institution">Växjö University</orgName>
								<address>
									<postCode>35195</postCode>
									<settlement>Växjö</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Dept. of Cognitive Sciences</orgName>
								<orgName type="department" key="dep2">ITC-irst</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<postCode>38068, 38055</postCode>
									<settlement>Rovereto, Povo-Trento</settlement>
									<country>Italy, Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Dept. of Computer Engineering</orgName>
								<orgName type="institution">Istanbul Technical University</orgName>
								<address>
									<postCode>34469</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<addrLine>Seminar für Sprachwissenschaft, Wilhelmstr. 19</addrLine>
									<postCode>72074</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">School of Humanities and Informatics</orgName>
								<orgName type="institution">University of Skövde</orgName>
								<address>
									<postBox>Box 408</postBox>
									<postCode>54128</postCode>
									<settlement>Skövde</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Faculty of Arts</orgName>
								<orgName type="laboratory">E R W I N M A R S I Tilburg University, Communication and Cognition</orgName>
								<orgName type="institution">Göteborg University</orgName>
								<address>
									<postBox>Box 200, Box 90153</postBox>
									<postCode>40530, 5000 LE</postCode>
									<settlement>Göteborg, Tilburg</settlement>
									<country>Sweden, The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MaltParser: A language-independent system for data-driven dependency parsing A T A N A S C H A N E V</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Natural Language Engineering</title>
						<imprint>
							<publisher>Cambridge University Press</publisher>
							<biblScope unit="volume">13</biblScope>
							<biblScope unit="issue">2</biblScope>
							<biblScope unit="page" from="95" to="135"/>
							<date type="published" when="2007">2007</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1017/S1351324906004505</idno>
					<note type="submission">(Received 16 February 2006; revised 15 August 2006)</note>
					<note>Printed in the United Kingdom 95 J O H A N H A L L, J E N S N I L S S O N G ¨ U L S ¸ E N E R Y ˙ I ˇ G ˙ I T</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Parsing unrestricted text is useful for many language technology applications but requires parsing methods that are both robust and efficient. MaltParser is a language-independent system for data-driven dependency parsing that can be used to induce a parser for a new language from a treebank sample in a simple yet flexible manner. Experimental evaluation confirms that MaltParser can achieve robust, efficient and accurate parsing for a wide range of languages without language-specific enhancements and with rather limited amounts of training data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the potential advantages of data-driven approaches to natural language processing is that they can be ported to new languages, provided that the necessary linguistic data resources are available. In practice, this advantage can be hard to realize if models are overfitted to a particular language or linguistic annotation scheme. Thus, several studies have reported a substantial increase in error rate when applying state-of-the-art statistical parsers developed for English to other languages, such as Czech ( <ref type="bibr" target="#b22">Collins et al. 1999</ref>), <ref type="bibr">Chinese (Bikel and Chiang 2000;</ref><ref type="bibr" target="#b41">Levy and Manning 2003)</ref>, German ( <ref type="bibr" target="#b28">Dubey and Keller 2003)</ref>, and Italian ( <ref type="bibr" target="#b24">Corazza et al. 2004</ref>). Another potential obstacle to successful reuse is that data-driven models may require large amounts of annotated training data to give good performance, while for most languages the availability of such resources is relatively limited. This is also a problem when porting parsers to new domains, even for languages where large amounts of annotated data are available <ref type="bibr" target="#b67">(Titov and Henderson 2006)</ref>. Given that approaches based on completely unsupervised learning are still vastly inferior in terms of accuracy, there is consequently a need for supervised approaches that are resilient against data sparseness.</p><p>In this article, we present a data-driven approach to dependency parsing that has been applied to a range of different languages, consistently giving a dependency accuracy in the range 80-90%, usually with less than a 5% increase in error rate compared to state-of-the-art parsers for the language in question. All these results have been obtained without any language-specific enhancements and in most cases with fairly modest data resources.</p><p>The methodology is based on three essential techniques:</p><p>1. Deterministic parsing algorithms for building dependency graphs ( <ref type="bibr" target="#b39">Kudo and Matsumoto 2002;</ref><ref type="bibr" target="#b72">Yamada and Matsumoto 2003;</ref><ref type="bibr" target="#b50">Nivre 2003</ref>) 2. History-based feature models for predicting the next parser action ( <ref type="bibr" target="#b4">Black et al. 1992</ref>; Magerman 1995; Ratnaparkhi 1997; Collins 1999) 3. Discriminative machine learning to map histories to parser actions ( <ref type="bibr" target="#b69">Veenstra and Daelemans 2000;</ref><ref type="bibr" target="#b39">Kudo and Matsumoto 2002;</ref><ref type="bibr" target="#b72">Yamada and Matsumoto 2003;</ref>)</p><p>The system uses no grammar but relies completely on inductive learning from treebank data for the analysis of new sentences and on deterministic parsing for disambiguation. This combination of methods guarantees that the parser is both robust, producing a well-formed analysis for every input sentence, and efficient, deriving this analysis in time that is linear or quadratic in the length of the sentence (depending on the particular algorithm used). This methodology has been implemented in the MaltParser system, which can be applied to a labeled dependency treebank in order to induce a labeled dependency parser for the language represented by the treebank. MaltParser is freely available for research and educational purposes <ref type="bibr">1</ref> and has been designed primarily as a tool for research on data-driven dependency parsing, allowing users to flexibly combine different parsing algorithms, feature models, and learning algorithms. However, given that the necessary data resources are available, MaltParser can also be used for rapid development of robust and efficient dependency parsers, which can be used in language technology applications that require parsing of unrestricted text.</p><p>In this article, we begin by describing the general methodology of deterministic dependency parsing with history-based feature models and discriminative machine learning (section 2). We then describe the implemented MaltParser system, focusing on its functionality with respect to parsing algorithms, feature models, and learning algorithms (section 3). To support our claims about language-independence and resilience against data sparseness, we then present an experimental evaluation based on data from ten different languages, with treebanks of different sizes and with different annotation schemes (section 4). Finally, we draw some general conclusions and make some suggestions for future work (section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Inductive dependency parsing</head><p>Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser. This methodology is exemplified by the influential parsers of <ref type="bibr" target="#b18">Collins (1997;</ref><ref type="bibr" target="#b19">1999)</ref> and <ref type="bibr" target="#b13">Charniak (2000)</ref>, among others. The accuracy of these parsers can be further improved by reranking the analyses output by the parser, typically using a discriminative model with global features that are beyond the scope of the underlying generative model <ref type="bibr" target="#b35">(Johnson et al. 1999;</ref><ref type="bibr" target="#b20">Collins 2000;</ref><ref type="bibr" target="#b21">Collins and Duffy 2002;</ref><ref type="bibr">Collins and Koo 2005;</ref><ref type="bibr" target="#b14">Charniak and Johnson 2005)</ref>.</p><p>A radically different approach is to perform disambiguation deterministically, using a greedy parsing algorithm that approximates a globally optimal solution by making a series of locally optimal choices, guided by a classifier trained on gold standard derivation sequences derived from a treebank. Although this may seem like a futile strategy for a complex task like parsing, it has recently been used with some success especially in dependency-based parsing. <ref type="bibr">2</ref> It was first applied to unlabeled dependency parsing by <ref type="bibr" target="#b39">Kudo and Matsumoto (2002)</ref> (for Japanese) and by <ref type="bibr" target="#b72">Yamada and Matsumoto (2003)</ref> (for English). It was later extended to labeled dependency parsing by  (for Swedish) and <ref type="bibr" target="#b57">Nivre and Scholz (2004)</ref> (for English). More recently, it has also been applied with good results to lexicalized phrase structure parsing by <ref type="bibr" target="#b60">Sagae and Lavie (2005)</ref>.</p><p>One of the advantages of the deterministic, classifier-based approach is that it is straightforward to implement and has a very attractive time complexity, with parsing time being linear or at worst quadratic in the size of the input, although the constant associated with the classifier can sometimes become quite large. Moreover, while the accuracy of a deterministic parser is normally a bit lower than what can be attained with a more complex statistical model, trained and tuned on large amounts of data, the deterministic parser will often have a much steeper learning curve, which means that it may in fact give higher accuracy with small training data sets. This is a natural consequence of the fact that the deterministic model has a much smaller parameter space, where only the mode of the distribution for each distinct history needs to be estimated, whereas a traditional generative model requires a complete probability distribution. Finally, and for essentially the same reason, the deterministic model can be less sensitive to differences in linguistic structure and annotation style across languages and should therefore be more easily portable without substantial adaptation.</p><p>In this study, we investigate these issues by applying the deterministic, classifierbased approach, as implemented in the MaltParser system for inductive dependency parsing, to a wide range of languages with varying annotation schemes and with data sets of varying sizes. By way of background, this section presents the theoretical foundations of inductive dependency parsing, defining syntactic representations, parsing algorithms, feature models, and learning algorithms. <ref type="bibr">3</ref> In section 3, we then describe the implemented MaltParser system that has been used for the experiments reported in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dependency graphs</head><p>In dependency parsing, the syntactic analysis of a sentence is represented by a dependency graph, which we define as a labeled directed graph, the nodes of which are indices corresponding to the tokens of a sentence. Formally:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1</head><p>Given a set R of dependency types (arc labels), a dependency graph for a sentence x = (w 1 , . . . , w n ) is a labeled directed graph G = (V , E, L), where:</p><formula xml:id="formula_0">1. V = Z n+1 2. E ⊆ V × V 3. L : E → R Definition 2</formula><p>A dependency graph G is well-formed if and only if:</p><p>1. The node 0 is a root (Root). 2. G is connected (Connectedness). <ref type="bibr">4</ref> The set V of nodes (or vertices) is the set Z n+1 = {0, 1, 2, . . . , n} (n ∈ Z + ), i.e., the set of non-negative integers up to and including n. This means that every token index i of the sentence is a node (1 ≤ i ≤ n) and that there is a special node 0, which does not correspond to any token of the sentence and which will always be a root of the dependency graph (normally the only root). In the following, we will reserve the term token node for a node that corresponds to a token of the sentence, and we will use the symbol V + to denote the set of token nodes of a sentence for which the set of nodes is V , i.e., V + = V − {0}. When necessary, we will write V x and V + x to indicate that V and V + are the nodes corresponding to a particular sentence x = (w 1 , . . . , w n ). Note, however, that the only requirement imposed by x is that the number of nodes matches the length of x, i.e., |V + | = n and |V | = n + 1.</p><p>The set E of arcs (or edges) is a set of ordered pairs <ref type="bibr">(i, j)</ref>, where i and j are nodes. Since arcs are used to represent dependency relations, we will say that i is the head and j is the dependent of the arc (i, j). As usual, we will use the notation i → j to mean that there is an arc connecting i and j (i.e., (i, j) ∈ E) and we will use the notation i → * j for the reflexive and transitive closure of the arc relation E (i.e., i → * j if and only if i = j or there is a path of arcs connecting i to j).</p><p>The function L assigns a dependency type (arc label) r ∈ R to every arc e ∈ E. We will use the notation i r → j to mean that there is an arc labeled r connecting i to j (i.e., (i, j) ∈ E and L((i, j)) = r). <ref type="figure" target="#fig_0">Figure 1</ref> shows a Czech sentence from the Prague Dependency Treebank with a well-formed dependency graph according to Definition 1-2. Note that the use of a special root node (0) is crucial for the satisfaction of Connectedness, since the graph would otherwise have consisted of two connected components rooted at nodes 3 and 8, respectively. The use of a special root node is thus a convenient way of ensuring Connectedness, regardless of whether a particular annotation scheme requires that a single token node should dominate all the others. More importantly, it is a way of achieving robustness in parsing, since there will always be a single entry point into the graph even if the parser produces fragmented output.</p><p>The only conditions so far imposed on dependency graphs is that the special node 0 be a root and that the graph be connected. Here are three further constraints that are common in the literature:</p><p>3. Every node has at most one head, i.e., if i → j then there is no node k such that k 񮽙 = i and k → j (Single-Head).</p><p>4. The graph G is acyclic, i.e., if i → j then not j → * i (Acyclicity). 5. The graph G is projective, i.e., if i → j then i → * k, for every node k such that i &lt; k &lt; j or j &lt; k &lt; i (Projectivity).</p><p>The Single-Head constraint, together with the basic well-formedness conditions, entails that the graph is a tree rooted at the node 0, which means that any wellformed graph satisfying Single-Head also satisfies Acyclicity. And whereas it is possible to require Acyclicity without Single-Head, the two conditions are jointly assumed in almost all versions of dependency grammar, especially in computational systems. By contrast, Projectivity is much more controversial. Broadly speaking, we can say that whereas most practical systems for dependency parsing do assume projectivity, most dependency-based linguistic theories do not. More precisely, most theoretical formulations of dependency grammar regard projectivity as the norm but also recognize the need for non-projective representations to capture non-local dependencies and discontinuities arising from free or flexible word order <ref type="bibr" target="#b47">(Mel'ˇ cuk 1988;</ref><ref type="bibr" target="#b34">Hudson 1990</ref>). This theoretical preference for non-projective dependency graphs is usually carried over into treebank annotation schemes, so that virtually all treebanks annotated with dependency graphs contain non-projective structures. This is true, for example, of the Prague Dependency Treebank of <ref type="bibr">Czech (Hajič et al. 2001)</ref>, the Danish Dependency Treebank ( <ref type="bibr" target="#b37">Kromann 2003)</ref>, and the Turkish Treebank ( <ref type="bibr" target="#b58">Oflazer et al. 2003</ref>), all of which are used in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deterministic parsing algorithms</head><p>The most commonly used deterministic algorithms for dependency parsing can be seen as variants of the basic shift-reduce algorithm, analyzing the input from left to right using two main data structures, a queue of remaining input tokens and a stack storing partially processed tokens. One example is the arc-eager algorithm introduced in <ref type="bibr" target="#b50">Nivre (2003)</ref>, which is used in all the experiments in this article and which we describe in detail in this section. Like most of the algorithms used for practical dependency parsing, this algorithm is restricted to projective dependency graphs. We begin by defining a parser configuration for a sentence x = (w 1 , . . . , w n ), relative to a set R of dependency types (including a special symbol r 0 for dependents of the root):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3</head><p>Given a set R = {r 0 , r 1 , . . . r m } of dependency types and a sentence x = (w 1 , . . . , w n ), a parser configuration for x is a quadruple c = (σ, τ, h, d), where:</p><p>1. σ is a stack of token nodes i (1 ≤ i ≤ j for some j ≤ n). 2. τ is a sorted sequence of token nodes i (j &lt; i ≤ n). 3. h : V + x → V x is a function from token nodes to nodes.</p><formula xml:id="formula_1">4. d : V +</formula><p>x → R is a function from token nodes to dependency types. 5. For every token node i ∈ V + x , d(i) = r 0 only if h(i) = 0. The idea is that the sequence τ represents the remaining input tokens in a left-toright pass over the input sentence x; the stack σ contains partially processed nodes that are still candidates for dependency arcs, either as heads or dependents; and the functions h and d represent the (partially constructed) dependency graph for the input sentence x.</p><p>Representing the graph by means of two functions in this way is possible if we assume the Single-Head constraint. Since, for every token node j, there is at most one arc (i, j), we can represent this arc by letting h(j) = i. Strictly speaking, h should be a partial function, to allow the possibility that there is no arc (i, j) for a given node j, but we will avoid this complication by assuming that every node j for which there is no token node i such that i → j is headed by the special root node 0, i.e., h(j) = 0. Formally, we establish the connection between configurations and dependency graphs as follows:</p><formula xml:id="formula_2">Definition 4 A configuration c = (σ, τ, h, d) for x = (w 1 , . . . , w n ) defines the dependency graph G c = (V x , E c , L c )</formula><p>, where:</p><formula xml:id="formula_3">1. E c = {(i, j) | h(j) = i} 2. L c = {((i, j), r) | h(j) = i, d(j) = r}</formula><p>We use the following notational conventions for the components of a configuration:</p><p>1. Both the stack σ and the sequence of input tokens τ will be represented as lists, although the stack σ will have its head (or top) to the right for reasons of perspicuity. Thus, σ|i represents a stack with top i and tail σ, while j|τ represents a list of input tokens with head j and tail τ, and the operator | is taken to be left-associative for the stack and right-associative for the input list. We use 񮽙 to represent the empty list/stack. 2. For the functions h and d, we will use the notation f[x 񮽙 → y], given a specific function f, to denote the function g such that g(x) = y and g(z) = f(z) for all</p><formula xml:id="formula_4">z 񮽙 = x. More formally, if f(x) = y 񮽙 , then f[x 񮽙 → y] = (f − {(x, y 񮽙 )}) ∪ {(x, y)}.</formula><p>Initial and terminal parser configurations are defined in the following way:</p><p>Definition 5 A configuration c for x = (w 1 , . . . , w n ) is initial if and only if it has the form c = (񮽙, (1, . . . , n), h 0 , d 0 ), where:</p><formula xml:id="formula_5">1. h 0 (i) = 0 for every i ∈ V + x . 2. d 0 (i) = r 0 for every i ∈ V + x .</formula><p>A configuration c for x = (w 1 , . . . , w n ) is terminal if and only if it has the form c = (σ, , h, d) (for arbitrary σ, h and d).</p><p>In other words, we initialize the parser with an empty stack, with all the token nodes of the sentence remaining to be processed, and with a dependency graph where all token nodes are dependents of the special root node 0 and all arcs are labeled with the special label r 0 , and we terminate whenever the list of input tokens is empty, which happens when we have completed one left-to-right pass over the sentence. We use C for the set of all possible configurations (relative to some set R of dependency types) and C n for the set of non-terminal configurations, i.e., any configuration c = (σ, τ, h, d) where τ 񮽙 = 񮽙. A transition is a partial function t : C n → C. In other words, a transition maps non-terminal configurations to new configurations but may be undefined for some non-terminal configurations. The parsing algorithm uses four transitions, two of which are parameterized by a dependency type r ∈ R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 6</head><p>Given a set of dependency types R, the following transitions are possible for every r ∈ R:</p><formula xml:id="formula_6">1. Left-Arc(r): (σ|i, j|τ, h, d) → (σ, j|τ, h[i 񮽙 → j], d[i 񮽙 → r]) if h(i) = 0 2. Right-Arc(r): (σ|i, j|τ, h, d) → (σ|i|j, τ, h[j 񮽙 → i], d[j 񮽙 → r]) if h(j) = 0 3. Reduce: (σ|i, τ, h, d) → (σ, τ, h, d) if h(i) 񮽙 = 0 4. Shift: (σ, i|τ, h, d) → (σ|i, τ, h, d)</formula><p>The transition Left-Arc(r) makes the top token i a (left) dependent of the next token j with dependency type r, i.e., j r → i, and immediately pops the stack. This transition can apply only if h(i) = 0, i.e., if the top token is previously attached to the root 0. The node i is popped from the stack because it must be complete with respect to left and right dependents at this point (given the assumption of projectivity). The transition Right-Arc(r) makes the next token j a (right) dependent of the top token i with dependency type r, i.e., i r → j, and immediately pushes j onto the stack. This transition can apply only if h(j) = 0, i.e., if the next token is previously attached to the root 0. <ref type="bibr">5</ref> The node j is pushed onto the stack since it must be complete with respect to its left dependents at this point, but it cannot be popped because it may still need new dependents to the right.</p><p>The transition Reduce pops the stack. This transition can apply only if h(i) 񮽙 = 0, i.e., if the top token i is already attached to a token node. This transition is needed for popping a node that was pushed in a Right-Arc(r) transition and which has since found all its right dependents.</p><p>The transition Shift pushes the next token i onto the stack. This transition can apply unconditionally as long as there are input tokens remaining. It is needed for processing nodes that have their heads to the right, as well as nodes that are to remain attached to the special root node.</p><p>The transition system just defined is nondeterministic in itself, since there is normally more than one transition applicable to a given configuration. In order to perform deterministic parsing, the transition system needs to be supplemented with a mechanism for predicting the next transition at each nondeterministic choice point, as well as choosing a dependency type r for the transitions Left-Arc(r) and Right-Arc(r). Such a mechanism can be called an oracle <ref type="bibr" target="#b36">(Kay 2000)</ref>. Assuming that we have an oracle o : C n → (C n → C), the algorithm for deterministic dependency parsing is very simple and straightforward:</p><formula xml:id="formula_7">Parse(x = (w 1 , . . . , w n )) 1 c ← (񮽙, (1, . . . , n), h 0 , d 0 ) 2 while c = (σ, τ, h, d) is not terminal 3 if σ = 񮽙 4 c ← Shift(c) 5 else 6 c ← [o(c)](c) 7 G ← (V x , E c , L c ) 8 return G</formula><p>As long as the parser remains in a non-terminal configuration, it applies the Shift transition if the stack is empty and otherwise the transition o(c) predicted by the oracle. When a terminal configuration is reached, the dependency graph defined by this configuration is returned.</p><p>The notion of an oracle is useful for the theoretical analysis of parsing algorithms and allows us to show, for example, that the parsing algorithm just described derives a well-formed projective dependency graph for any input sentence in time that is linear in the length of the input, and that any projective dependency graph can be derived by the algorithm <ref type="bibr" target="#b52">(Nivre 2006</ref>). In practice, the oracle can only be approximated, but the fundamental idea in inductive dependency parsing is that we can achieve a good approximation using history-based feature models and discriminative machine learning, as described in the following subsections.</p><p>An alternative to the algorithm described in this section is to use an arc-standard strategy, more directly corresponding to the strict bottom-up processing in traditional shift-reduce parsing. In this scheme, the Right-Arc(r) and Reduce transitions are merged into a single transition that immediately pops the dependent in the same way as Left-Arc(r), which means that right dependents can only be attached after they have found all their descendants. This is the strategy used by <ref type="bibr" target="#b39">Kudo and Matsumoto (2002)</ref>, <ref type="bibr" target="#b72">Yamada and Matsumoto (2003)</ref> and , although they also modify the algorithm by allowing multiple passes over the input. There are few studies comparing the performance of different algorithms, but <ref type="bibr" target="#b17">Cheng et al. (2005)</ref> found consistently better accuracy for the arc-eager, single-pass strategy (over the arc-standard, multi-pass algorithm) in parsing the CKIP Treebank of Chinese.</p><p>A somewhat different approach is to use the incremental algorithms described by <ref type="bibr" target="#b25">Covington (2001)</ref>, where the stack is replaced by an open list where any token can be linked to the next input token. This allows non-projective graphs to be derived at the cost of making parsing time quadratic in the length of the input. This is a technique that has not yet been evaluated on a large scale, and attempts at recovering non-projective dependencies within this tradition have so far relied on post-processing of projective dependency graphs, e.g., using the pseudo-projective technique proposed by <ref type="bibr" target="#b56">Nivre and Nilsson (2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">History-based feature models</head><p>History-based models for natural language processing were first introduced by <ref type="bibr" target="#b4">Black et al. (1992)</ref> and have been used extensively for part-of-speech tagging and syntactic parsing. The basic idea is to map each pair (x, y) of an input string x and an analysis y to a sequence of decisions D = (d 1 , . . . , d n ). In a generative probabilistic model, the joint probability P (x, y) can then be expressed using the chain rule of probabilities as follows:</p><formula xml:id="formula_8">P (x, y) = P (d 1 , . . . , d n ) = n 񮽙 i=1 P (d i | d 1 , . . . , d i−1 ) (1)</formula><p>The conditioning context for each d i , (d 1 , . . . , d i−1 ), is referred to as the history and usually corresponds to some partially built structure. In order to get a tractable learning problem, histories are grouped into equivalence classes by a function Φ:</p><formula xml:id="formula_9">P (x, y) = P (d 1 , . . . , d n ) = n 񮽙 i=1 P (d i | Φ(d 1 , . . . , d i−1 )) (2)</formula><p>Early versions of this scheme were integrated into grammar-driven systems. For example, <ref type="bibr" target="#b3">Black et al. (1993)</ref> used a standard PCFG but could improve parsing performance considerably by using a history-based model for bottom-up construction of leftmost derivations. In more recent developments, the history-based model has replaced the grammar completely, as in the parsers of <ref type="bibr" target="#b18">Collins (1997;</ref><ref type="bibr" target="#b19">1999)</ref> and <ref type="bibr" target="#b13">Charniak (2000)</ref>.</p><p>With a generative probabilistic model, the parameters that need to be estimated are the conditional probabilities P (d i | Φ(d 1 , . . . , d i−1 )), for every possible decision d i and non-equivalent history H i = Φ(d 1 , . . . , d i−1 ). With a deterministic parsing strategy, we only need to estimate the mode of each conditional distribution, i.e., arg max</p><formula xml:id="formula_10">d i P (d i | Φ(d 1 , . . . , d i−1 )</formula><p>). This reduces the parameter estimation problem to that of learning a classifier, where the classes are the possible decisions of the parser, e.g., the possible transitions of the algorithm described in the previous section.</p><p>Distinct parser histories are normally represented as sequences of attributes, socalled feature vectors, and the function Φ, referred to as the feature model, can therefore be defined in terms of a sequence Φ 1,p = (φ 1 , . . . , φ p ) of feature functions, where each function φ i identifies some relevant feature of the history. The most important features in dependency parsing are the attributes of input tokens, such as their word form, part-of-speech or dependency type, and we will in fact limit ourselves in this article to features that can be defined as simple attributes of tokens.</p><p>Token attributes can be divided into static and dynamic attributes, where static attributes are properties that remain constant during the parsing of a sentence. This primarily includes the actual word form of a token, but also any kind of annotation that is the result of preprocessing, such as part-of-speech tag, lemma, or word sense annotation. In this article, we restrict our attention to two kinds of static attributes, word form and part-of-speech. Given a sentence x = (w 1 , . . . , w n ), with part-of-speech annotation, we use w(i) and p(i) to refer to the word form and part-of-speech, respectively, of the ith token. We will also make use of fixed-length suffixes of word forms and write s m (w(i)) for the m-character suffix of w(i) (where s m (w(i)) = w(i) if w(i) has length l ≤ m).</p><p>Dynamic attributes, by contrast, are attributes that are defined by the partially built dependency graph, which in this article will be limited to the dependency type by which a token is related to its head, given by the function d of the current parser configuration c = (σ, τ, h, d).</p><p>To define complex history-based feature models, we need to refer to attributes of arbitrary tokens in the parser history, represented by the current parser configuration. For this purpose, we introduce a set of address functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 7</head><p>Given a sentence x = (w 1 , . . . , w n ) and a parser configuration c = (σ, τ, h, d) for x:</p><p>1. σ i is the ith token from the top of the stack (starting at index 0). 2. τ i is the ith token in the remaining input (starting at index 0). 3. h(i) is the head of token i in the graph defined by h. 4. l (i) is the leftmost child of token i in the graph defined by h. 5. r(i) is the rightmost child of token i in the graph defined by h.</p><p>By combining these functions, we can define arbitrarily complex functions that identify tokens relative to a given parser configuration c. For example, while σ 0 is the token on top of the stack, h(σ 0 ) is the head of the token on top of the stack, and l (h(σ 0 )) is the leftmost dependent of the head of the token on top of the stack. It should be noted that these functions are generally partial functions on token nodes, which means that if one of the inner functions in a chain of applications returns 0 (because h(i) = 0) or is undefined (because the stack is empty, or a token does not have a leftmost child, etc.), then the outermost function is always undefined.</p><p>Finally, we can now define feature functions by applying attribute functions to complex combinations of address functions. For example, p(τ 0 ) is the part-ofspeech of the next input token, while d(h(σ 0 )) is the dependency type of the head of the token on top of the stack, which may or may not be defined in a given configuration. Any feature function that is undefined for a given configuration, because the complex address function fails to identify a token, is assigned a special nil value. Feature models used for inductive dependency parsing typically combine static part-of-speech features and lexical features (or suffix features) with dynamic dependency type features. The kind of models used in the experiments later on are described in section 3.2 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Discriminative machine learning</head><p>Given a function approximation problem with labeled training data from target function f : X → Y , discriminative learning methods attempt to optimize the mapping from inputs x ∈ X to outputs y ∈ Y directly, without estimating a full generative model of the joint distribution of X and Y . Discriminatively trained models have in recent years been shown to outperform generative models for many problems in natural language processing, including syntactic parsing, by directly estimating a conditional probability distribution P (Y |X) <ref type="bibr" target="#b35">(Johnson et al. 1999;</ref><ref type="bibr" target="#b20">Collins 2000;</ref><ref type="bibr" target="#b21">Collins and Duffy 2002;</ref><ref type="bibr">Collins and Koo 2005;</ref><ref type="bibr" target="#b14">Charniak and Johnson 2005)</ref>. With a deterministic parsing strategy, the learning problem can be further reduced to a pure classification problem, where the input instances are histories (represented by feature vectors) and the output classes are parsing decisions.</p><p>Thus, the training data for the learner consists of pairs (Φ(c), t), where Φ(c) is the representation of a parser configuration defined by the feature model Φ(c) and t is the correct transition out of c. Such data can be generated from a treebank of gold standard dependency graphs, by reconstructing the correct transition sequence for each dependency graph in the treebank and extracting the appropriate feature vectors for each configuration, as described in detail by <ref type="bibr" target="#b52">Nivre (2006)</ref> for the parsing algorithm discussed in section 2.2.</p><p>Although in principle any learning algorithm capable of inducing a classifier from labeled training data can be used to solve the learning problem posed by inductive dependency parsing, most of the work done in this area has been based on support vector machines (SVM) and memory-based learning (MBL). <ref type="bibr">6</ref> SVM is a hyperplane classifier that relies on the maximum margin strategy introduced by <ref type="bibr" target="#b68">Vapnik (1995)</ref>. Furthermore, it allows the use of kernel functions to map the original feature space to a higher-dimensional space, where the classification problem may be (more) linearly separable. In dependency parsing, SVM has been used primarily by Matsumoto and colleagues <ref type="bibr" target="#b39">(Kudo and Matsumoto 2002;</ref><ref type="bibr" target="#b72">Yamada and Matsumoto 2003;</ref><ref type="bibr" target="#b17">Cheng et al. 2005</ref>).</p><p>MBL is a lazy learning method, based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems <ref type="bibr" target="#b26">(Daelemans and Van den Bosch 2005</ref>). In essence, this is a k nearest neighbor approach to classification, although a variety of sophisticated techniques, including different distance metrics and feature weighting schemes can be used to improve classification accuracy. In dependency parsing, MBL has been used primarily by Nivre and colleagues <ref type="bibr" target="#b57">Nivre and Scholz 2004;</ref><ref type="bibr" target="#b56">Nivre and Nilsson 2005)</ref>, and it is also the learning method that is used for the experiments in this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MaltParser</head><p>MaltParser is an implementation of inductive dependency parsing, as described in the previous section, where the syntactic analysis of a sentence amounts to the deterministic derivation of a dependency graph, and where discriminative machine learning is used to guide the parser at nondeterministic choice points, based on a history-based feature model. MaltParser can also be characterized as a data-driven parser-generator. While a traditional parser-generator constructs a parser given a grammar, a data-driven parser-generator constructs a parser given a treebank.</p><p>The system can be run in two basic modes. In learning mode, it takes as input a (training) set of sentences with dependency graph annotations, derives training data by reconstructing the correct transition sequences, and trains a classifier on this data set according to the specifications of the user. In parsing mode, it takes as input a (test) set of sentences and a previously trained classifier and parses the sentences using the classifier as a guide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parsing algorithms</head><p>MaltParser provides two main parsing algorithms, each with several options:</p><p>• The linear-time algorithm of Nivre <ref type="formula">(2003)</ref> can be run in arc-eager or arcstandard mode. The arc-standard version is similar to but not identical to the algorithm of <ref type="bibr" target="#b72">Yamada and Matsumoto (2003)</ref>, since the latter also uses multiple passes over the input <ref type="bibr" target="#b51">(Nivre 2004</ref>). In both versions, this algorithm is limited to projective dependency graphs.</p><p>• The incremental algorithm of <ref type="bibr" target="#b25">Covington (2001)</ref> can be run in projective or non-projective mode. In the latter case, graphs are still guaranteed to obey the constraints Root, Connectedness, Single-Head and Acyclicity.</p><p>The experiments reported in this article are all based on the arc-eager version of Nivre's algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature models</head><p>MaltParser allows the user to define arbitrarily complex feature models, using address functions and attribute functions as described in section 2.3. 7 The standard model used in most of the experiments reported below combines part-of-speech features, lexical features and dependency type features in the following way:</p><formula xml:id="formula_11">p(σ 1 ) w(h(σ 0 )) d(l(σ 0 )) p(σ 0 ) w(σ 0 ) d(σ 0 ) p(τ 0 ) w(τ 0 )) d(r(σ 0 )) p(τ 1 ) w(τ 1 ) d(l(τ 0 )) p(τ 2 ) p(τ 3 )</formula><p>This model includes six part-of-speech features, defined by the part-of-speech of the two topmost stack tokens (p(σ 0 ), p(σ 1 )) and by the first four tokens of the remaining input (p(τ 0 ), p(τ 1 ), p(τ 2 ), p(τ 3 )). The dependency type features involve the top token on the stack (d(σ 0 )), its leftmost and rightmost dependent (d(l(σ 0 )), d(r(σ 0 ))), and the leftmost child of the next input token (d(l(τ 0 ))). 8 Finally, the standard model includes four lexical features, defined by the word form of the top token on the stack (w(σ 0 )), the head of the top token (w(h(σ 0 ))), and the next two input tokens (w(τ 0 ), w(τ 1 )).</p><p>The standard model can be seen as the prototypical feature model used in the experiments reported below, although the tuned models for some languages deviate from it by adding or omitting features, or by replacing lexical features by suffix features (the latter not being used at all in the standard model). Deviations from the standard model are specified in table 3 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning algorithms</head><p>MaltParser provides two main learning algorithms, each with a variety of options:</p><p>• Memory-based learning (MBL) using TiMBL, a software package for memorybased learning and classification developed by Daelemans, Van den Bosch and colleagues at the Universities of Tilburg and Antwerp (Daelemans and Van den Bosch 2005).</p><p>• Support vector machines (SVM) using LIBSVM, a library for SVM learning and classification developed by Chang and Lin at National Taiwan University ( <ref type="bibr" target="#b12">Chang and Lin 2001)</ref>.</p><p>The experiments reported in this paper are all based on MBL and make crucial use of the following features of TiMBL:</p><p>• Varying the number k of nearest neighbors • Using the Modified Value Difference Metric (MVDM) for distances between feature values (for values seen more than l times)</p><p>• Distance-weighted class voting for determining the majority class</p><p>The optimal values for these parameters vary for different feature models, languages and data sets, but typical values are k = 5, MVDM down to l = 3 (with the simple Overlap metric for lower frequencies), and class voting weighted by inverse distance (ID). For more information about these and other TiMBL features, we refer to Daelemans and Van den Bosch (2005).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Auxiliary tools</head><p>MaltParser is supported by a suite of freely available tools for, among other things, parser evaluation and treebank conversion. Of special interest in this context are the tools for pseudo-projective dependency parsing <ref type="bibr" target="#b56">(Nivre and Nilsson 2005)</ref>. This is a method for recovering non-projective dependencies through a combination of data-driven projective dependency parsing and graph transformation techniques in the following way:</p><p>1. Dependency graphs in the training data sets are transformed (if necessary) to projective dependency graphs, by minimally moving non-projective arcs upwards towards the root and encoding information about these transformations in arc labels. 2. The projective parser is trained as usual, except that the dependency graphs in the training set are labeled with the enriched arc labels. 3. New sentences are parsed into projective dependency graphs with enriched arc labels. 4. Dependency graphs produced by the parser are transformed (if possible) to non-projective dependency graphs, using an inverse transformation guided by information in the arc labels.</p><p>This methodology has been used in a few of the experiments reported below, in particular for the parsing of Czech (section 4.2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental evaluation</head><p>In this section, we summarize experiments with the MaltParser system on data from ten different languages: Bulgarian, Chinese, Czech, Danish, Dutch, English, German, Italian, Swedish and Turkish. <ref type="bibr">9</ref> Although the group is dominated by IndoEuropean languages, in particular Germanic languages, the languages nevertheless represent fairly different language types, ranging from Chinese and English, with very reduced morphology and relatively inflexible word order, to languages like Czech and Turkish, with rich morphology and flexible word order, and with Bulgarian, Danish, Dutch, German, Italian and Swedish somewhere in the middle. In addition, the treebank annotation schemes used to analyze these languages differ considerably. Whereas the treebanks for Czech, Danish, Italian and Turkish are proper dependency treebanks, albeit couched in different theoretical frameworks, the annotation schemes for the remaining treebanks are based on constituency in combination with grammatical functions, which necessitates a conversion from constituent structures to dependency structures. Below we first describe the general methodology used to evaluate the system, in particular the evaluation metrics used to assess parsing accuracy, and give an overview of the different data sets and experiments performed for different languages (section 4.1). This is followed by a presentation of the results (section 4.2), with specific subsections for each language (section 4.2.1-4.2.10), where we also give a more detailed description of the respective treebanks and the specific settings <ref type="table">Table 1</ref>. Data sets. AS = Annotation scheme (C = Constituency, D = Dependency, G = Grammatical functions); Pro = Projective; #D = Number of dependency types; #P = Number of PoS tags; TA =Tagging accuracy; #W = Number of words; #S = Number of sentences; SL = Mean sentence length; EM = Evaluation method (T = Held-out test set, CV k = k-fold cross-validation) used for individual experiments, followed by a general discussion, where we bring together the results from different languages and try to discern some general trends (section 4.3). <ref type="table">Table 1</ref> gives an overview of the data sets for the ten languages. The first column characterizes the annotation scheme and the second indicates whether the (possibly converted) annotation is restricted to projective dependency graphs. The next two columns contain the number of distinct dependency types and part-of-speech tags, respectively, where the latter refers to the tagset actually used in parsing, which may be a reduced version of the tagset used in the original treebank annotation. The fifth column gives the mean accuracy of the part-of-speech tagging given as input to the parser, where 100.0 indicates that experiments have been performed using gold standard tags (i.e., manually assigned or corrected tags) rather than the output of an automatic tagger. The next three columns give the number of tokens and sentences, and the mean number of words per sentence. These figures refer in each case to the complete treebank, of which at most 90% has been used for training and at least 10% for testing (possibly using k-fold cross-validation). <ref type="table">Table 2</ref> gives a little more information about the syntactic analysis adopted in the different treebank annotation schemes. Whereas all the schemes agree on basic structures such as verbs taking their core arguments as dependents and adjuncts being dependents of the heads they modify, there are a number of constructions that have competing analyses with respect to their dependency structure. This holds in particular for constructions involving function words, such as auxiliary verbs, prepositions, determiners, and complementizers, but also for the ubiquitous phenomenon of coordination. <ref type="table">Table 2</ref> shows the choices made for each of these cases in the different treebanks, and we see that there is a fair amount of variation <ref type="table">Table 2</ref>. Annotation style (choice of head). VG = Verb group (Aux = Auxiliary verb, MV = Main verb); AP = Adpositional phrase (Ad = Adposition, N = Nominal head); NP = Noun phrase (Det = Determiner, N = Noun); SC = Subordinate clause (Comp = Complementizer, V = Verb); Coord = Coordination (CC = Coordinating conjunction, Conj 1 = First conjunct, Conj n = Last conjunct); NA = Not applicable especially with respect to verb groups and coordination. 10 It is worth noting that for Turkish, which is a richly inflected, agglutinative language, some of the distinctions are not applicable, since the relevant construction is encoded morphologically rather than syntactically. 11 It is also important to remember that, for the treebanks that are not originally annotated with dependency structures, the analysis adopted here only represents one conversion out of several possible alternatives. More information about the conversions are given for each language below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method</head><p>All the experiments reported in this article have been performed with the parsing algorithm described in <ref type="bibr" target="#b50">Nivre (2003;</ref>) and with memory-based learning and classification as implemented in the TiMBL software package by Daelemans and Van den <ref type="bibr" target="#b26">Bosch (2005)</ref>. A variety of feature models have been tested, but we only report results for the optimal model for each language, which is characterized in relation to the standard model defined in section 3.2. Standard settings for the TiMBL learner include k = 5 (number of nearest distances), MVDM metric down to a threshold of l = 3, and distance weighted class voting with Inverse Distance weights (ID).</p><p>Final evaluation has been performed using either k-fold cross-validation or a held-out test set, as shown in the last column in table 1. Evaluation on held-out data has in turn been preceded by a tuning phase using either k-fold cross-validation or a development test set, as described for each language below. The diversity in evaluation methods is partly a result of practical circumstances and partly motivated by the concern to make results comparable to previously published results for a <ref type="bibr">10</ref> The notation Conj 1 /Conj n under Coord for Chinese and English signifies that coordination is analyzed as a head-initial or head-final construction depending on whether the underlying phrase type is head-initial (e.g., verb phrases) or head-final (e.g., noun phrases). 11 Whereas postpositions generally appear as suffixes on nouns, there are marginal cases where they occur as separate words and are then treated as heads. Hence, the brackets around Ad in the AP column for Turkish.</p><p>given language. Thus, while results on the Penn Treebank are customarily obtained by training on sections 2-21 and testing on section 23 (using any of the remaining sections as a development test set), results on the Turkish Treebank have so far been based on ten-fold cross-validation, which is well motivated by the limited amount of data available. It should also be noted that the amount of work devoted to model selection and parameter optimization varies considerably between the languages, with Swedish and English being most thoroughly investigated while the results for other languages, notably Dutch, German and Turkish, are still preliminary and can probably be improved substantially. The evaluation metrics used throughout are the unlabeled attachment score AS U , which is the proportion of tokens that are assigned the correct head (regardless of dependency type), and the labeled attachment score AS L , which is the proportion of tokens that are assigned the correct head and the correct dependency type, following the proposal of <ref type="bibr" target="#b42">Lin (1998)</ref>. All results are presented as mean scores per token, with punctuation tokens excluded from all counts. <ref type="bibr">12</ref> For each language, we also provide a more detailed breakdown with (unlabeled) attachment score, precision, recall and F measure for individual dependency types.</p><p>Before we turn to the experimental results, a caveat is in order concerning their interpretation and in particular about cross-linguistic comparability. The main point of the experimental evaluation is to corroborate the claim that MaltParser is language-independent enough to achieve reasonably accurate parsing for a wide variety of languages, where the level of accuracy is related, whenever possible, to previously obtained results for that language. In order to facilitate this kind of comparison, we have sometimes had to sacrifice comparability between languages, notably by using training sets of different size or different procedures for obtaining accuracy scores as explained earlier. This means that, even though we sometimes compare results across languages, such comparisons must be taken with a pinch of salt. Although a more controlled cross-linguistic comparison would be very interesting, it is also very difficult to achieve given that available resources are very diverse with respect to standards of annotation, the amount of annotated data available, the existence of accurate part-of-speech taggers, etc. Faced with this diversity, we have done our best to come up with a reasonable compromise between the conflicting requirements of ensuring cross-linguistic comparability and being faithful to existing theoretical and practical traditions for specific languages and treebanks. This means, for example, that we retain the original arc labels for all treebanks, so that users of these treebanks can easily relate our results to theirs, even though this has the consequence that, e.g., subjects will be denoted by a variety of labels such as SUB, SBJ, SUBJ and Sb, but all arc labels will be accompanied by descriptions that should make them understandable also for readers who are not familiar with a given treebank annotation scheme.   <ref type="table" target="#tab_2">Table 3</ref> gives an overview of the results, summarizing for each language the optimal feature model and TiMBL parameter settings, as well as the best unlabeled and labeled attachment scores. In the following subsections, we analyze the results for each language in a little more detail, making state-of-the-art comparisons where this is possible. The earliest experiments were those performed on Swedish and English and the standard models and settings are mainly based on the results of these experiments. It is therefore natural to treat Swedish and English first, with the remaining languages following in alphabetical order.</p><note type="other">. Overview of results. Model = Best feature model (− = omitted, + = added, → = replaced</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Swedish</head><p>The Swedish data come from Talbanken (Einarsson 1976), a manually annotated corpus of both written and spoken Swedish, created at Lund University in the 1970s. We use the professional prose section, consisting of material taken from textbooks, newspapers and information brochures. Although the original annotation scheme is an eclectic combination of constituent structure, dependency structure, and topological fields (Teleman 1974), it has been possible to convert the annotated sentences to dependency graphs with very high accuracy. In the conversion process, we have reduced the original fine-grained classification of grammatical functions to a more restricted set of 17 dependency types, mainly corresponding to traditional grammatical functions such as subject, object and adverbial. We have used a pseudorandomized data split, dividing the data into 10 sections by allocating sentence i to section i mod 10. We have used sections 1-9 for 9-fold cross-validation during development and section 0 for final evaluation. The overall accuracy scores for Swedish, obtained with the standard model and standard settings, are AS U = 86.3% and AS L = 82.0%. <ref type="table" target="#tab_4">Table 4</ref> gives unlabeled attachment score (AS U ), labeled precision (P), recall (R) and F measure (F) for individual dependency types in the Swedish data. These types can be divided into three groups according to accuracy. In the high-accuracy set, with a labeled F  measure from 84% to 98%, we find all dependency types where the head is a closed class word: IM (marker → infinitive), PR (preposition → noun), UK (complementizer → verb) and VC (auxiliary verb → main verb). We also find the type DET (noun → determiner), which has similar characteristics although the determiner is not treated as the head in the Swedish annotation. The high-accuracy set also includes the central dependency types ROOT and SUB, which normally identify the finite verb of the main clause and the grammatical subject, respectively. In the medium-accuracy set, with a labeled F measure in the range of 75-80%, we find the remaining major dependency types, ADV (adverbial), ATT (nominal modifier), CC (coordination), OBJ (object) and PRD (predicative). However, this set can be divided into two subsets, the first consisting of ADV, ATT and CC, which have an unlabeled attachment score not much above the labeled F measure, indicating that parsing errors are mainly due to incorrect attachment. This is plausible since ADV and ATT are the dependency types typically involved in modifier attachment ambiguities and since coordination is also a source of attachment ambiguities. The second subset contains OBJ and PRD, which both have an unlabeled attachment score close to 90%, which means that they are often correctly attached but may be incorrectly labeled. This is again plausible, since these types identify nominal arguments of the verb (other than the subject), which can often occur in the same structural positions.</p><note type="other">. Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for</note><p>Finally, we have a low-accuracy set, with a labeled F measure below 70%, where the common denominator is mainly low frequency: INF (infinitive complements), APP (appositions), XX (unclassifiable). The only exception to this generalization is the type ID (idiom constituent), which is not that rare but which is rather special for other reasons. All types in this set except APP have a relatively high unlabeled attachment score, but their labels are seldom used correctly. Relating the Swedish results to the state of the art is rather difficult, since there is no comparable evaluation reported in the literature, let alone based on the same data. <ref type="bibr" target="#b70">Voutilainen (2001)</ref> presents a partial and informal evaluation of a Swedish FDG parser, based on manually checked parses of about 400 sentences from newspaper text, and reports F measures of 95% for subjects and 92% for objects. These results clearly indicate a higher level of accuracy than that attained in the experiments reported here, but without knowing the details of the data selection and evaluation procedure it is very difficult to draw any precise conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">English</head><p>The data set used for English is the standard data set from the Wall Street Journal section of the Penn Treebank, with sections 2-21 used for training and section 23 for testing (with section 00 as the development test set). The data has been converted to dependency trees using the head percolation table of <ref type="bibr" target="#b72">Yamada and Matsumoto (2003)</ref>, and dependency type labels have been inferred using a variation of the scheme employed by <ref type="bibr" target="#b19">Collins (1999)</ref>, which makes use of the nonterminal labels on the head daughter, non-head daughter and parent corresponding to a given dependency relation. However, instead of simply concatenating these labels, as in the Collins scheme, we use a set of rules to map these complex categories onto a set of 10 dependency types, including traditional grammatical functions such as subject, object, etc. More details about the conversion can be found in <ref type="bibr" target="#b52">Nivre (2006)</ref>.</p><p>The best performing model for English is the standard model and the TiMBL parameter settings deviate from the standard ones only by having a higher k value (k = 7) and a higher threshold for MVDM (l = 5). The overall accuracy scores for English are AS U = 88.1% and AS L = 86.3%. The relatively narrow gap between unlabeled and labeled accuracy is probably due mainly to the coarse-grained nature of the dependency type set and perhaps also to the fact that these labels have been inferred automatically from phrase structure representations. <ref type="table" target="#tab_6">Table 5</ref> shows the accuracy for individual dependency types in the same way as for Swedish in table 4, and again we can divide dependency types according to accuracy into three sets. In the high-accuracy set, with a labeled F measure from 86% to 95%, we find SBJ (subject) and three dependency types where the head is a closed class word: PMOD (preposition → complement/modifier), VC (auxiliary verb → main verb) and SBAR <ref type="bibr">(complementizer → verb)</ref>. In addition, this set includes the type NMOD, which includes the noun-determiner relation as an important subtype.</p><p>In the medium-accuracy set, with a labeled F measure from 74% to 82%, we find the types AMOD, VMOD, OBJ, PRD and ROOT. The former two dependency types mostly cover adverbial functions, and have a labeled accuracy not too far below their unlabeled attachment score, which is an indication that the main difficulty lies in finding the correct head. By contrast, the argument functions OBJ and PRD have a much better unlabeled attachment score, which shows that they are often attached to the correct head but misclassified. This tendency is especially pronounced for the PRD type, where the difference is more than 15 percentage points, which can probably be explained by the fact that this type is relatively infrequent in the annotated English data. The low-accuracy set for English only includes the default classification DEP. The very low accuracy for this dependency type can be explained by the fact that it is both a heterogeneous category and the least frequent dependency type in the data.</p><p>Compared to the state of the art, the unlabeled attachment score is about 4% lower than the best reported results, obtained with the parser of Charniak <ref type="formula">(2000)</ref> and reported in <ref type="bibr" target="#b72">Yamada and Matsumoto (2003)</ref>. <ref type="bibr">13</ref> For the labeled attachment score, we are not aware of any strictly comparable results, but <ref type="bibr" target="#b5">Blaheta and Charniak (2000)</ref> report an F measure of 98.9% for the assignment of grammatical role labels to phrases that were correctly parsed by the parser described in <ref type="bibr" target="#b13">Charniak (2000)</ref>, using the same data set. If null labels are excluded, the F score drops to 95.6%. The corresponding F measures for MaltParser are 98.0% and 97.8%, treating the default label DEP as the equivalent of a null label. The experiments are not strictly comparable, since they involve different sets of functional categories (where only the labels SBJ and PRD are equivalent) and one is based on phrase structure and the other on dependency structure, but it nevertheless seems fair to conclude that MaltParser's labeling accuracy is close to the state of the art, even if its capacity to derive correct structures is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Bulgarian</head><p>For the current experiments we used a subset of BulTreeBank ( <ref type="bibr" target="#b61">Simov et al. 2002)</ref>, since the complete treebank is not officially released and still under development. The set contains 71703 words of Bulgarian text from different sources, annotated with constituent structure. Although the annotation scheme is meant to be compatible with the framework of HPSG, syntactic heads are not explicitly annotated, which <ref type="table">Table 6</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for Bulgarian (mean of 8-fold cross-validation, frequency counts rounded to whole integers) means that the treebank must be converted to dependency structures using the same kind of head percolation tables and inference rules that were used for the English data, except that for Bulgarian the converted treebank also contains non-projective dependencies. In most cases, these involve subordinate da-clauses, where we often find subject-to-object or object-to-object raising. In these cases, we have taken da to be the head of the subordinate clause with the main verb dependent on da and the raised subject or object dependent on the main verb. More details about the conversion can be found in <ref type="bibr" target="#b44">Marinov and Nivre (2005)</ref>. Experiments were performed with several models but the highest accuracy was achieved with a variant of the standard model, where all lexical features are based on suffixes of length 6, rather than the full word forms. That is, every lexical feature w(a) (with address function a) is replaced by s 6 (w(a)) (cf. section 2.3). The overall accuracy scores for Bulgarian are 81.3% (AS U ) and 73.6% (AS L ). Using suffixes instead of full forms makes the data less sparse, which can be an advantage for languages with limited amounts of data, especially if the endings of content words can be expected to carry syntactically relevant information. The optimal suffix length can be determined using cross-validation, and a length of 6 seems to work well for several languages, presumably because it captures the informative endings of content words while leaving most function words intact. <ref type="table">Table 6</ref> gives accuracy, precision, recall and balanced F measures for individual dependency types. The overall trend is the same as for Swedish and English in that dependency relations involving function words tend to have higher accuracy than relations holding primarily between content words. Thus, the highest ranking dependency types with respect to the F measure are PR (preposition → noun) and UK (complementizer → verb), together with ID (multi-word unit), which in the Bulgarian data includes verbs taking the reflexive/possessive pronouns se and si. Further down the list we find as expected the major verb complement types OBJ (object) and PRD (predicative complement) but also SUBJ (subject), which has considerably lower accuracy than the corresponding type in Swedish and English. This is a reflection of the more flexible word order in Bulgarian.</p><p>Other dependency types that are ranked lower for Bulgarian than for the other languages considered so far are DET (noun → determiner) and VC (auxiliary verb ← main verb). In the former case, since Bulgarian lacks free-standing determiners like English the, this category was reserved for demonstratives (this, that, etc.), which occurred infrequently. In the latter case, this again seems to be related to word order properties, allowing the verbs to be separated by adverbials or even subordinate clauses (which will also lead the parser to erroneously connect verbs that belong to different clauses). Finally, we note that coordinate structures (CC) and adverbials (ADV) have very low accuracy (with an F measure below 60%). For adverbials, one possible error source is the fact that many adverbs coincide in form with the third person singular form of adjectives.</p><p>There are no other published results for parsing Bulgarian, except for a paper by <ref type="bibr" target="#b64">Tanev and Mitkov (2002)</ref>, who report precision and recall in the low 60s for a rule-based parser. However, this parser has only been tested on 600 syntactic phrases, as compared to the 5080 sentences used in the present study, so it is very difficult to draw any conclusions about the relative quality of the parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Chinese</head><p>The Chinese data are taken from the Penn Chinese Treebank (CTB), version 5.1 ( <ref type="bibr" target="#b71">Xue et al. 2005)</ref>, and the texts are mostly from Xinhua newswire, Sinorama news magazine and Hong Kong News. The annotation of CTB is based on a combination of constituent structure and grammatical functions and has been converted in the same way as the data for English and Bulgarian, with a head percolation table created by a native speaker for the purpose of machine translation. Dependency type labels have been inferred using an adapted version of the rules developed for English, which is possible given that the treebank annotation scheme for CTB is modeled after that for the English Penn Treebank. More details about the conversion can be found in <ref type="bibr" target="#b32">Hall (2006)</ref>.</p><p>One often underestimated parameter in parser evaluation is the division of data into training, development and evaluation sets. <ref type="bibr" target="#b41">Levy and Manning (2003)</ref> report up to 10% difference in parsing accuracy for different splits of CTB 2.0. We have used the same pseudo-randomized split as for Swedish (cf. section 4.2.1), with sections 1-8 for training, section 9 for validation, and section 0 for final evaluation. The results presented in this article are based on gold-standard word segmentation and part-of-speech tagging.</p><p>The best performing model for Chinese is the standard one and the same goes for TiMBL settings except that k = 6 and l = 8. <ref type="table">Table 7</ref> presents the unlabeled attachment score (AS U ), labeled precision (P), recall (R) and F measure (F) for individual dependency types in the Chinese data. We see that the overall accuracy scores for Chinese are AS U = 81.1% and AS L = 79.2%, and the difference between labeled and unlabeled accuracy is generally very small also on the level of individual dependency types, with a few notable exceptions. Both SBJ (subject) and VC (verb <ref type="table">Table 7</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for Chinese (held-out test set, section 0) chain) have considerably lower labeled F measure than unlabeled attachment score, which indicates that these relations are difficult to classify correctly even if the headdependent relations are assigned correctly. For the special ROOT label, we find a very low precision, which reflects fragmentation in the output (since too many tokens remain attached to the special root node), but even the recall is substantially lower than for any other language considered so far. This may indicate that the feature model has not yet been properly optimized for Chinese, but it may also indicate a problem with the arc-eager parsing strategy adopted in all the experiments. It is rather difficult to compare results on parsing accuracy for Chinese because of different data sets, word segmentation strategies, dependency conversion methods, and data splits. But the unlabeled attachment score obtained in our experiments is within 5% of the best reported results for CTB ( <ref type="bibr" target="#b17">Cheng et al. 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Czech</head><p>The Prague Dependency Treebank (PDT) consists of 1.5M words of newspaper text, annotated on three levels, the morphological, analytical and tectogrammatical levels <ref type="bibr" target="#b31">(Hajič et al. 2001)</ref>. Our experiments all concern the analytical annotation, which uses a set of 28 surface-oriented grammatical functions <ref type="bibr">(BöhmovBöhmov´Böhmová et al. 2003)</ref>. Unlike the treebanks discussed so far, PDT is a genuine dependency treebank also including non-projective dependencies.</p><p>The best results for Czech are again based on the standard model with standard settings, although it should be acknowledged that the sheer size of the Czech data sets makes it hard to perform extensive optimization of feature model and learning algorithm parameters. The experiments are based on the designated training and development sets in the treebank distribution, with final evaluation on the separate test set ( <ref type="bibr" target="#b31">Hajič et al. 2001)</ref>.</p><p>Although less than 2% of all arcs in the training data are non-projective, they are distributed over as many as 23% of the sentences. It follows that the configuration of <ref type="table">Table 8</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure for selected dependency types for Czech (held-out test set, etest section) MaltParser used for all languages, constructing only projective graphs, cannot even in theory achieve an exact match for these sentences. To cope with non-projectivity, the concept of pseudo-projective parsing was introduced and evaluated in Nivre and <ref type="bibr" target="#b56">Nilsson (2005)</ref>. An overview of this approach is given in section 3.4. Using non-projective training data, i.e., without applying any tree transformations and encodings, the overall accuracy scores are AS U = 78.5% and AS L = 71.3%. By simply transforming all non-projective sentences to projective, without encoding the transformations in dependency type labels (baseline), an improvement is achieved for both AS U = 79.1% and AS L = 72.0%. This indicates that it helps to make the input conform to the definition of projectivity, despite the fact that the trees are distorted and that it is not possible to recover non-projective arcs in the output of the parser.</p><p>In <ref type="bibr" target="#b56">Nivre and Nilsson (2005)</ref>, three types of encoding schemes were evaluated in order to recover the non-projective structure by an inverse transformation. The encodings increase the burden on the parser, since it now also has to distinguish between pseudo-projective arcs and the original projective arcs. The differences between different encodings are small and not statistically significant, but all three encodings increase both labeled and unlabeled attachment score in comparison both to the projectivized baseline and to the use of non-projective training data (all differences being significant beyond the 0.01 level according to McNemar's test). Compared to the projectivized baseline, the improvement is as high as 1 precentage point for AS U = 80.1% and 0.8 percentage points for AS L = 72.8%.</p><p>A closer look at the 13 most frequent dependency types in table 8 reveals a larger drop from unlabeled to labeled accuracy compared to other languages such as English and Chinese. This is partly a result of the more fine-grained set of dependency types for Czech, but the more flexible word order for major clause constituents like Sb (subject) and Obj (object) is probably important as well. On the other hand, dependents of the types AuxC (subordinate conjunction), AuxP (preposition), AuxV (auxiliary verb) or Coord (conjunction) actually have a higher F measure than AS U , due to higher precision. In contrast to Sb and Obj, these dependents all come from closed word classes, which often uniquely identifies the dependency type. In addition, it is worth noting the surprisingly low accuracy for Coord, lower than for most other languages. This may indicate that the analysis of coordination in PDT, treating the coordinating conjunction as the head, does not interact well with the parsing strategy and/or feature models adopted in the experiments. <ref type="bibr">14</ref> We are not aware of any published results for labeled accuracy, but the unlabeled attachment score obtained is about 5% lower than the best results reported for a single parser, using the parser of Charniak <ref type="formula">(2000)</ref>, adapted for Czech, with corrective post-processing to recover non-projective dependencies (Hall and NovákNov´Novák 2005).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6">Danish</head><p>The Danish experiments are based on the Danish Dependency Treebank (DDT), which is based on a subset of the Danish PAROLE corpus and annotated according to the theory of Discontinuous Grammar <ref type="bibr" target="#b37">(Kromann 2003)</ref>. This annotation involves primary dependencies, capturing grammatical functions, and secondary dependencies, capturing other relations such as co-reference. Our experiments only concern primary dependencies, since including secondary dependencies as well would have violated the Single-Head constraint (cf. section 2.1), but the dependency type set is still the most fine-grained of all, with 54 distinct dependency types. The annotation is not restricted to projective dependency graphs, and while only about 1% of all dependencies are non-projective, the proportion of sentences that contain at least one non-projective dependency is as high as 15%.</p><p>The treebank has been divided into training, validation and test sets using the same pseudo-randomized splitting method described earlier for Swedish and Chinese. The training data for the experiments have been projectivized in the same way as the Czech data, with a similar improvement compared to the use of non-projective training data. However, none of the encoding schemes for recovering non-projective dependencies in the output of the parser led to any improvement in accuracy (nor to any degradation), which is probably due to the fact that the training data for non-projective dependencies are much more sparse than for Czech.</p><p>The best performing model for Danish is a modification of the standard model, where the feature w(τ 1 ) (the word form of the first lookahead token) is omitted, and the feature w(h(σ 0 )) (the word form of the head of the top token) is replaced by the suffix feature s 6 (w(h(σ 0 ))). The TiMBL settings are standard. The overall accuracy scores for Danish are AS U = 85.6% and AS L = 79.5%. <ref type="bibr">15</ref> The relatively wide gap between unlabeled and labeled accuracy is probably due mainly to the fine-grained <ref type="bibr">14</ref> In more recent work, <ref type="bibr" target="#b49">Nilsson et al. (2006)</ref> have shown how parsing accuracy for coordination in Czech can be improved by transforming the representations so that coordinating conjunctions are not treated as heads internally. <ref type="bibr">15</ref> The labeled attachment score is slightly lower than the one published in <ref type="bibr" target="#b53">Nivre and Hall (2005)</ref>, where results were based on the development test set. <ref type="table">Table 9</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for Danish, n ≥ 10 (held-out test set, section 0) nature of the dependency type set in combination with a relatively small training data set. <ref type="table">Table 9</ref> shows the unlabeled attachment score (AS U ), precision (P), recall (R) and F measure (F) for dependency types occurring at least 10 times in the test set. It is clear that low-frequency types (n &lt; 100) generally have very low labeled precision and recall, despite sometimes having a quite high unlabeled accuracy. A striking example is indirect object (IOBJ), which has perfect unlabeled accuracy but only 15% labeled recall. Concentrating on types that occur at least 100 times in the test set, we see a pattern that is very similar to the one observed for the closely related language Swedish, despite important differences in the style of annotation. Thus, we can observe a very high accuracy (F ≥ 90) for dependencies involving function words, notably VOBJ, which includes dependencies linking verbs to function words (auxiliary verb → main verb, marker → infinitive, complementizer → verb), and NOBJ, which includes dependencies linking prepositions and determiners to nominals, but also for subjects, both normal subjects (SUBJ) and the much less frequent expletive subjects (EXPL), and roots (ROOT). Furthermore, we see that other arguments of the verb (DOBJ, IOBJ, LOBJ, PRED) have a high unlabeled accuracy but (sometimes substantially) lower labeled accuracy, while the generic adjunct type MOD has lower accuracy, both labeled and unlabeled. Finally, both Danish and Swedish have comparatively high accuracy for coordination, which in Danish is split into CC (conjunct → coordinator) and COORD (conjunct i → conjunct i+1 ). Compared to the results for Czech, this indicates that an analysis of coordination where a conjunct, rather than the coordinator, is treated as the head is easier to cope with for the parser. <ref type="bibr" target="#b46">McDonald and Pereira (2006)</ref> report an unlabeled attachment score for primary dependency types in DDT of 86.8%. 16 However, these results are based on gold standard part-of-speech tags, whereas our experiments use automatically assigned tags with an accuracy rate of 96.3%. Replicating the experiment with gold standard tags, using the same feature model and parameter settings, results in an unlabeled attachment score of 87.3%, which indicates that MaltParser gives state-of-the-art performance for Danish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.7">Dutch</head><p>The Dutch experiments are based on the Alpino Treebank ( <ref type="bibr" target="#b0">Beek et al. 2003</ref>). The text material (186k non-punctuation tokens) consists primarily of two sections of newspaper text (125k and 21k), plus two smaller segments containing questions (21k) and (in part) manually constructed sentences for parser development and annotation guide examples (19k). As the latter type of material is atypical, it is only used for training purposes, whereas the smaller newspaper text section is used as held out material for final testing.</p><p>The syntactic annotation of the Alpino Treebank is a mix of constituent structure and dependency relations, nearly identical to the syntactic annotation of the Spoken Dutch Corpus ( <ref type="bibr" target="#b70">Wouden et al. 2002)</ref>. It was converted to a pure dependency structure employing a head percolation table, removing secondary relations as indicated by traces. Multi-word units, consisting of a sequence of words without any further syntactic analysis, were concatenated into a single word using underscores. Finally, non-projective structures were projectivized using the same baseline procedure as for Danish (i.e., without extending the dependency type labels or attempting to recover non-projective dependencies in the output of the parser). Since the original partof-speech tags in the Alpino Treebank are coarse-grained and lack any additional feature information besides the word class, all tokens were retagged with the memorybased tagger for Dutch ( <ref type="bibr" target="#b27">Daelemans et al. 2003)</ref>.</p><p>Ten-fold cross-validation was used to manually optimize the TiMBL settings. Experimentation confirmed that the standard settings with MVDM, no feature weighting, and distance weighted class voting generally performs best. However, choosing a higher value for k (k = 10) usually gives an improvement of one to two percentage points for Dutch. The results obtained on held out data using the standard model are AS U = 84.7% and AS L = 79.2%. The relatively large <ref type="table">Table 10</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for Dutch (held-out test set) gap between the labeled and unlabeled scores may be attributed to the relatively fine-grained set of dependency labels. <ref type="table">Table 10</ref> gives unlabeled attachment score (AS U ), labeled precision (P), recall (R) and F measure (F) for individual dependency types. We can observe a general trend towards better scores for the more frequent dependency labels, but there are notable exceptions such as the relatively high score for the infrequently occurring SE (reflexive object) and the low score on the more frequent PC (prepositional complement) and LD (locative/directional complement). As for several other languages, we can distinguish three groups with high, medium and low F measures respectively. The high score set (F &gt; 80%) includes the dependency relations indicated by closed class words: DET (determiner → noun), VC (auxiliary verb → main verb), and BODY (complementizer → verb). Somewhat surprisingly, this group also includes OBJ1 (direct object), perhaps because this is the second most frequent dependency relation.</p><p>The low score group (F &lt; 60%) includes the rather infrequent suppletive subject (SUP) and object (POBJ1). Furthermore, it involves four classes which are canonically expressed in the form of a prepositional phrase -PC (prepositional complement), OBJ2 (indirect object), LD (locative/directional complement) and PREDM (predicate modifier) -and where the sometimes subtle distinction is often of a semantic rather than a syntactic nature. The fact that coordinator (CRD) is also in the low score group is somewhat counter-intuitive, because it is indicated by a closed word class, normally the word en 'and', but the result is consistent with the low accuracy for coordination in Czech, given that both treebanks treat the coordinating conjunction as the head of a coordinate structure.</p><p>The remaining 11 types belong to the medium score group (60% &lt; F &lt; 80%), which includes the by far most frequent class MOD (modifier). It is interesting to note that the scores for a conceptually difficult class like APP (apposition) are still quite good. The same goes for the potentially highly ambiguous CONJ (conjunct), although there seems to be a trade-off here with the low scores noted for CRD earlier.</p><p>The Alpino parser is a rule-based, HPSG-style parser that is currently the stateof-art parser for Dutch ( <ref type="bibr" target="#b8">Bouma et al. 2001)</ref>. It has an extensive and detailed lexicon (including, e.g., subcategorization information) and a MaxEnt-based disambiguation module. Its output is in the same format as the Alpino Treebank. We used it to parse the held out material and converted the parse trees to dependency structures, using exactly the same procedure as for converting the treebank, which includes transforming non-projective to projective structures. Evaluation resulted in the scores AS U = 93.2% and AS L = 91.2%. Clearly, there is still a substantial gap between the two parsers. Also, the Alpino parser provides additional information, e.g., traces and non-projective analyses, which is ignored here. Yet, given all the effort invested in building the Alpino grammar, lexicon, and disambiguation strategy, it is interesting to see that its performance can be approximated by a purely inductive approach using fairly limited amounts of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.8">German</head><p>The experiments for German are based on the Tübingen Treebank of Written German (TüBa-D/Z) ( <ref type="bibr" target="#b66">Telljohann et al. 2005</ref>). The treebank is based on issues of the German daily newspaper 'die tageszeitung' (taz) that appeared in April and May of 1999. The annotation of the treebank is constituency-based, but it is augmented by function-argument structure on all levels, which allows a straightforward conversion to dependencies for most phenomena. Heuristics are used only for apposition, embedded infinitive clauses, and nominal postmodifications. Long-distance relations, which are annotated in the constituency model via special labels, are translated into non-projective dependencies. The set of dependency types is modeled after the one used for the Constraint Dependency Grammar for German ( <ref type="bibr" target="#b30">Foth et al. 2004</ref>), a manually written dependency grammar for German.</p><p>The best performing model for German modifies the standard model by omitting the two lexical features w(h(σ 0 )) and w(τ 1 ) and by adding the part-of-speech of an additional stack token p(σ 2 ). The TiMBL settings for German deviate from the standard settings by using k = 13 and voting based on inverse linear weighting (IL).</p><p>The overall accuracy scores for German are AS U = 88.1% and AS L = 83.4%. The (unlabeled) results are comparable to results by <ref type="bibr" target="#b30">Foth et al. (2004)</ref>, who reached 89.0% accuracy when parsing the NEGRA treebank <ref type="bibr" target="#b63">(Skut et al. 1997)</ref>, another treebank for German, which is also based on newspaper texts (but which uses a different constituency-based annotation scheme). The labeled results are considerably <ref type="table">Table 11</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure for selected dependency types for German (mean of 10-fold cross-validation, frequency counts rounded to whole integers) higher than constituency parsing results reported for German, which reach a labeled F measure of 75.3% when constituent nodes also include grammatical functions ( <ref type="bibr">Kübler et al. 2006</ref>). <ref type="table">Table 11</ref> gives unlabeled attachment scores (AS U ), labeled precision (P), recall (R), and F measure (F) for selected dependency types. The overall trends are very similar to what we have observed for other languages, notably Germanic languages like Swedish and Danish. For example, both determiners (DET) and adverbials (ADV) have labeled and unlabeled accuracy at about the same level (although considerably higher for DET than for ADV), while arguments of the verb (AccOBJ, DatOBJ, GenOBJ, PRED and PPOBJ) have substantially better unlabeled than labeled accuracy. One difference, compared to Danish and Swedish, is that the lower labeled accuracy also affects subjects (SUBJ), which is probably a reflection of the fact that German exhibits freer word order thanks to case marking. The relatively low labeled accuracy for different case-marked arguments is also an indication that the parser would benefit from morphological information, which is currently not included in the German part-of-speech tags.</p><p>Contrary to expectations that, with growing data size, adding more lexical features would improve performance, experiments with all the lexical features of the standard model showed a decrease in performance by 1.5 percentage points. The hypothesis that this decrease is due to data sparseness is refuted by experiments with only 2000 sentences for training, where the decrease in performance is only 7.5%. These results are consistent with those of <ref type="bibr" target="#b28">Dubey and Keller (2003)</ref>, who found that lexicalizing a PCFG grammar for NEGRA results in a decrease in performance, although it should be remembered that the first two lexical features are beneficial in the case of MaltParser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.9">Italian</head><p>The Italian treebank used in the experiments is the Turin University Treebank (TUT) <ref type="bibr" target="#b7">(Bosco 2004</ref>), consisting of 1500 sentences and 41771 tokens. It is balanced <ref type="table">Table 12</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for Italian (mean of 10-fold cross-validation, frequency counts rounded to whole integers) over genres with 60% newspaper text, 30% legal text, and 10% from novels and academic literature. The dependency annotation involves traces in order to avoid non-projective structures, although there is in fact a certain number of non-projective trees in the treebank. The treebank has been converted to the format required by MaltParser without significant loss of linguistic information, as described in <ref type="bibr" target="#b11">Chanev (2005)</ref>, replacing traces if necessary by (possibly non-projective) dependency arcs. The dependency tag set was reduced from 283 to 17 distinct tags, keeping only information about syntactic dependency relations. The training data were projectivized using the same procedure as for Danish and Dutch and tagged for part-of-speech using TnT <ref type="bibr" target="#b9">(Brants 2000)</ref>. All experiments were performed using 10-fold cross-validation with a randomized split.</p><p>The best performing feature model for Italian is the standard model, although several simpler models give nearly the same results. The accuracy scores for Italian are AS U = 82.9% and AS L = 75.7%, and table 12 shows the accuracy obtained for different dependency types. It is striking that there are only two types that obtain a really high accuracy in the Italian data, the type ARG, which is usually used for relations between articles and nouns or prepositions and articles, and the type AUX, which is used for auxiliary verbs. While these two types have a labeled F measure well above 90%, no other type has a score higher than 70%. There is also a set of low-frequency types that all have zero recall and precision. The relatively low labeled accuracy for most dependency types in Italian is undoubtedly due partly to sparse data, but it is also relevant that the inventory of dependency types is more semantically oriented than for most other languages.</p><p>For Italian there are not any published results for statistical dependency parsing except the preliminary results for MaltParser reported in <ref type="bibr" target="#b11">Chanev (2005)</ref>. Compared to <ref type="bibr" target="#b24">Corazza et al. (2004)</ref>, where state-of-the-art constituency parsers were tested on the Italian Syntactic-Semantic Treebank ( <ref type="bibr" target="#b48">Montemagni et al. 2003)</ref>, an improvement seems to have been achieved, although it is not straightforward to compare evaluation metrics for constituency and dependency parsing. A more relevant comparison is the rule-based parser of <ref type="bibr" target="#b40">Lesmo et al. (2002)</ref>, which uses the TUT dependency type set and which has been reported to achieve a labeled attachment score of 76.65% when evaluated during the development of the treebank. Since this is within a percentage point of the results reported in this article and the evaluation is based on the same kind of data, it seems clear that MaltParser achieves highly competitive results for Italian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.10">Turkish</head><p>The Turkish Treebank ( <ref type="bibr" target="#b58">Oflazer et al. 2003</ref>), created by Metu and Sabancı Universities is used in the experiments for Turkish. This treebank is composed of 5635 sentences, annotated with dependency structures, of which 7.2% are non-projective (not counting punctuation that is not connected to a head). As can be seen from table 1, even though the number of sentences in the Turkish Treebank is in the same range as for Danish, Swedish and Bulgarian, the number of words is considerably smaller (54k as opposed to 70-100k for the other treebanks). This significant difference arises from the very rich morphological structure of the language due to which a word may sometimes correspond to a whole sentence in another language.</p><p>As a result of their agglutinative morphology, Turkish words can change their main part-of-speech after the concatenation of multiple suffixes. This structure is represented in the treebank by dividing words into inflectional groups (IG). The root and derived forms of a word are represented by different IGs separated from each other by derivational boundaries (DB). Each IG is annotated with its own part-of-speech and inflectional features, as illustrated in the following example: 17 okulunuzdaydı (he was at your school) okulunuzda DB ydı okul +Noun+A3sg+P2pl+Loc</p><formula xml:id="formula_12">񮽙 񮽙񮽙 񮽙 IG 1 DB +Verb+Zero+Past+A3sg 񮽙 񮽙񮽙 񮽙 IG 2</formula><p>The part-of-speech of the stem of the word okulunuzdaydı is a noun, from which a verb is derived in a separate IG. In the treebank, dependencies hold between specific IGs of the dependent and head word. For the parsing experiments, we have concatenated IGs into word forms to get a word-based tokenization and used a reduced version of the part-of-speech tagset given by the treebank, very similar to the reduced tagset used in the parser of <ref type="bibr">Eryi˘ git and Oflazer (2006)</ref>. For each word, we use the part-of-speech of each IG and in addition include the case and possessive information if the stem is a noun or pronoun. Using this approach, the tag of the word okulunuzdaydı becomes <ref type="table" target="#tab_2">Table 13</ref>. Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for Turkish (mean of 10-fold cross-validation, frequency counts rounded to whole integers) Noun+P2pl+Loc+Verb. Even after this reduction, the tagset contains 484 distinct tags, making it by far the biggest tagset used in the experiments. The best performing model for Turkish omits five of the features of the standard model, three part-of-speech features (p(σ 1 ), p(τ 2 ), p(τ 3 )) and two lexical features (w(h(σ 0 )), w(τ 1 )). In addition, the stem of a word is used as its word form in lexical features. This leads to an accuracy of AS U = 81.6% and AS L = 69.0%. These are the mean results obtained after 10-fold cross-validation. <ref type="table" target="#tab_2">Table 13</ref> gives unlabeled attachment scores (AS U ), labeled precision (P), recall (R), and F measure (F) for individual dependency types. First of all, we see that types with a frequency below 5 in the test set have very low labeled accuracy, which is consistent with results reported for other languages earlier. Secondly, we may note that the frequency of tokens analyzed as roots (ROOT) is very low, which is a consequence of the fact that punctuation tokens are excluded in evaluation, since final punctuation is generally treated as the root node of a sentence in the Turkish Treebank. <ref type="bibr">18</ref> Therefore, the closest correspondent to ROOT for other languages is SENTENCE, which is the type assigned to a token dependent on the final punctuation token (normally the final verb of the sentence) and which has a very high accuracy, on a par with the ROOT type for most other languages. Finally, there is a clear tendency that dependency types with high accuracy (INTENSI-FIER, QUESTION.PARTICLE, RELATIVIZER, SENTENCE, DETERMINER, NEGATIVE.PARTICLE) are types that are generally adjacent to their head, whereas types with lower accuracy (COORDINATION, SENTENCE.MODIFIER, APPOSITION, COLLOCATION, VOCATIVE) are types that are either more distant or hard to differentiate from other types.</p><p>The only comparable results for Turkish are for the unlexicalized dependency parser of <ref type="bibr">Eryi˘ git and Oflazer (2006)</ref>. These results are based on a selected subset of the treebank sentences containing only projective dependencies with the heads residing on the right side of the dependents and the main evaluation metrics are based on IGs rather than words, but word-based scores are presented for the purpose of comparison with a top score of AS U = 81.2%. Applying MaltParser with the best feature model to the same subset of the treebank resulted in an unlabeled attachment score of 84.0%, which is a substantial improvement. <ref type="bibr">19</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Although MaltParser achieves an unlabeled dependency accuracy above 80% for all languages, there is also a considerable range of variation, which seems to correlate fairly well with the linguistic dimensions of morphological richness and word order flexibility, exemplified by high accuracy for English and lower accuracy for Czech, which represent extreme positions on these scales. Given that English and Czech are also the languages with the largest data sets, the linguistic properties seem to be more important than the amount of data available. Another influencing factor is the level of detail of the dependency annotation, as given by the number of dependency types used, where Czech has a more fine-grained classification than English. However, Danish has an even more fine-grained classification but still comes out with higher parsing accuracy than Czech, despite a much smaller training data set.</p><p>If morphological richness and word order flexibility are indeed the most important factors determining parsing accuracy, the results for German are surprisingly good, given that German has both richer morphology and freer word order than English. On the other hand, the results for Chinese are on the low side. This points to another important factor, namely the complexity of the sentences included in the treebank data, which can be roughly approximated by considering the mean sentence length in the sample. Here we see that Chinese has the second highest value of all languages, while the sentence length for German is at least considerably lower than for English. At the same time, we have to remember that the number of words per sentence is not strictly comparable between languages with different morphological properties, as illustrated especially by the data for Turkish (cf. section 4.2.10).</p><p>Comparing individual dependency types across languages is very difficult, given the diversity in annotation, but a few recurrent patterns are clearly discernible. The first is that dependencies involving function words generally have the highest accuracy. The second is that core arguments of the verb often have high unlabeled accuracy but lower labeled accuracy, with the possible exception of subjects, which have high labeled accuracy in languages where they are distinguished configurationally. The third is that the parsing accuracy for coordinate structures tends to be higher if the dependency analysis treats conjuncts, rather than coordinating conjunctions, as heads.</p><p>Needless to say, a more detailed error analysis will be needed before we can draw any reliable conclusions about the influence of different factors, so the tentative conclusions advanced here are best regarded as conjectures to be corroborated or refuted by future research. However, given the fact that unlabeled dependency accuracy is consistently above 80%, the parsing methodology has proven to be relatively insensitive to differences in language typology as well as in annotation schemes. Moreover, respectable results can be obtained also with fairly limited amounts of data, as illustrated in particular by the results for Italian and Turkish.</p><p>Finally, we note that MaltParser achieves state-of-the-art performance for most of the languages investigated in this article, although the possibility of comparison differs widely between languages. For English, Chinese, Czech and Dutch, parsing accuracy does not quite reach the highest level, but the difference is never more than about 5% (slightly more for Dutch). <ref type="bibr">20</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented MaltParser, a data-driven system for dependency parsing that can be used to construct syntactic parsers for research purposes or for practical language technology applications. Experimental evaluation using data from ten different languages shows that MaltParser generally achieves good parsing accuracy without language-specific enhancements and with fairly limited amounts of training data. Unlabeled dependency accuracy is consistently above 80% and the best results are normally within a 5% margin from the best performing parsers, where such comparisons are possible. MaltParser is freely available for research and educational purposes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Dependency graph for Czech sentence from the Prague Dependency Treebank.</figDesc><graphic url="image-1.pbm" coords="5,68.86,54.31,339.84,147.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>by); Settings = TiMBL settings; AS U = Unlabeled attachment score; AS L = Labeled attachment score</head><label></label><figDesc></figDesc><table>Language 
Model 
Settings 
AS U 
AS L 

Bulgarian 
∀a[w(a) → s 6 (w(a)))] 
Standard 
81.3 
73.6 
Chinese 
Standard 
k = 6, l = 8 
81.1 
79.2 
Czech 
Standard 
Standard 
80.1 
72.8 
Danish 
[w(h(σ 0 )) → s 6 (w(h(σ 0 ))); −w(τ 1 )] 
Standard 
85.6 
79.5 
Dutch 
Standard 
k = 10 
84.7 
79.2 
English 
Standard 
k = 7, l = 5 
88.1 
86.3 
German 
[−w(h(σ 0 )); −w(τ 1 ); +p(σ 2 )] 
k = 13, IL 
88.1 
83.4 
Italian 
Standard 
Standard 
82.9 
75.7 
Swedish 
Standard 
Standard 
86.3 
82.0 
Turkish 
[−p(σ 1 ); −p(τ 2 ); −p(τ 3 ); −w(h(σ 0 )); −w(τ 1 )] 
Standard 
81.6 
69.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Swedish (held-out test set, section 0)</head><label></label><figDesc></figDesc><table>Dependency Type 
n 
AS U 
P 
R 
F 

Adverbial (ADV) 
1607 
79.8 
75.8 
76.8 
76.3 
Apposition (APP) 
42 
23.8 
38.1 
19.0 
25.4 
Attribute (ATT) 
950 
81.3 
79.9 
78.5 
79.2 
Coordination (CC) 
963 
82.5 
78.1 
79.8 
78.9 
Determiner (DET) 
947 
92.6 
88.9 
90.2 
89.5 
Idiom (ID) 
254 
72.0 
72.5 
58.3 
64.6 
Infinitive marker (IM) 
133 
98.5 
98.5 
98.5 
98.5 
Infinitive complement (INF) 
10 
100.0 
100.0 
30.0 
46.2 
Object (OBJ) 
585 
88.0 
78.2 
77.3 
77.7 
Preposition dependent (PR) 
985 
94.2 
88.6 
92.7 
90.6 
Predicative (PRD) 
244 
90.6 
76.7 
77.0 
76.8 
Root (ROOT) 
607 
91.3 
84.6 
91.3 
87.8 
Subject (SUB) 
957 
89.8 
86.7 
82.5 
84.5 
Complementizer dependent (UK) 
213 
85.0 
89.4 
83.6 
86.4 
Verb group (VC) 
238 
93.7 
82.1 
90.6 
86.1 
Other (XX) 
29 
82.8 
85.7 
20.7 
33.3 

Total 
8764 
86.3 
82.0 
82.0 
82.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 . Attachment score (AS U ), precision (P), recall (R) and F measure per dependency type for English (held-out test set, section 23) Dependency Type n AS U P R F</head><label>5</label><figDesc></figDesc><table>Adjective/adverb modifier (AMOD) 
2072 
78.2 80.7 73.0 76.7 
Other (DEP) 
259 
42.9 56.5 30.1 39.3 
Noun modifier (NMOD) 
21002 
91.2 91.1 90.8 91.0 
Object (OBJ) 
1960 
86.5 78.9 83.5 81.1 
Preposition modifier (PMOD) 
5593 
90.2 87.7 89.5 88.6 
Predicative (PRD) 
832 
90.0 75.9 71.8 73.8 
Root (ROOT) 
2401 
86.4 78.8 86.4 82.4 
Complementizer dependent (SBAR) 
1195 
86.0 87.1 85.1 86.1 
Subject (SBJ) 
4108 
90.0 90.6 88.1 89.3 
Verb group (VC) 
1771 
98.8 93.4 96.6 95.0 
Adverbial (VMOD) 
8175 
80.3 76.5 77.1 76.8 

Total 
49368 
88.1 86.3 86.3 86.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dependency Type n AS U P R F</head><label></label><figDesc></figDesc><table>Adverbial (ADV) 
914 
67.2 
59.4 
51.2 
55.0 
Apposition (APP) 
120 
65.5 
54.1 
49.0 
51.9 
Attribute (ATT) 
1297 
79.6 
74.0 
75.4 
74.7 
Coordination (CC) 
555 
53.6 
52.8 
48.5 
50.6 
Determiner (DET) 
259 
82.9 
80.2 
76.5 
78.3 
Idiom (ID) 
214 
94.6 
90.2 
89.5 
89.8 
Object (OBJ) 
949 
85.9 
66.9 
70.4 
68.6 
Preposition dependent (PR) 
1137 
93.6 
91.8 
93.2 
92.5 
Predicative (PRD) 
254 
89.8 
65.7 
73.3 
69.3 
Root (ROOT) 
635 
88.7 
76.8 
88.7 
82.3 
Subject (SUBJ) 
600 
82.7 
68.9 
66.8 
67.8 
Complementizer dependent (UK) 
418 
88.1 
87.5 
88.7 
88.1 
Verb group (VC) 
397 
79.8 
71.2 
72.5 
71.8 

Total 
7748 
81.3 
73.6 
73.6 
73.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Dependency Type n AS U P R F</head><label></label><figDesc></figDesc><table>Adjective/adverb modifier (AMOD) 
1503 
95.2 
95.8 
94.5 
95.1 
Other (DEP) 
2999 
90.5 
92.4 
89.5 
90.9 
Noun modifier (NMOD) 
13046 
85.4 
86.3 
85.2 
85.7 
Object (OBJ) 
2802 
86.0 
82.8 
85.3 
84.0 
Preposition modifier (PMOD) 
1839 
77.3 
81.3 
77.2 
79.2 
Predicative (PRD) 
467 
78.8 
81.4 
76.0 
78.6 
Root (ROOT) 
1880 
70.5 
55.4 
70.5 
62.0 
Complementizer dependent (SBAR) 
1296 
83.6 
83.6 
83.3 
83.4 
Subject (SBJ) 
3242 
83.2 
73.3 
78.5 
75.8 
Verb group (VC) 
940 
80.0 
76.0 
75.1 
75.5 
Adverbial (VMOD) 
12043 
72.6 
71.3 
68.8 
70.0 

Total 
42057 
81.1 
79.2 
79.2 
79.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Dependency Type n AS U P R F</head><label></label><figDesc></figDesc><table>Apposition (APPOSITION) 
69 
44.4 
54.2 
47.8 
50.8 
Argument (ARG) 
1351 
95.0 
92.7 
94.5 
93.6 
Auxiliary verb (AUX) 
96 
92.1 
90.5 
94.2 
92.3 
Part of expression (CONTIN) 
75 
86.9 
78.2 
54.4 
64.2 
Coordination (COORDINATOR) 
271 
66.6 
63.6 
63.6 
63.6 
Other (DEPENDENT) 
1 
40.0 
0.0 
0.0 
-
Reflexive complement (EMPTYCOMPL) 
15 
94.5 
35.7 
50.0 
41.7 
Indirect complement (INDCOMPL) 
82 
85.9 
70.4 
47.5 
56.7 
Indirect object (INDOBJ) 
18 
81.5 
33.3 
33.3 
33.3 
Interjection (INTERJECTION) 
1 
20.0 
0.0 
0.0 
-
Object (OBJ) 
222 
84.9 
33.3 
33.3 
33.3 
Predicative complement (PREDCOMPL) 
52 
78.4 
54.3 
37.3 
44.2 
Restrictive modifier (RMOD) 
1013 
74.3 
69.5 
70.2 
69.8 
Subject (SUBJ) 
256 
75.5 
64.8 
58.6 
61.5 
Root (TOP) 
150 
75.5 
63.5 
77.2 
69.7 
Adverbial extraction (VISITOR) 
13 
74.6 
0.0 
0.0 
-

Total 
3683 
82.9 
75.7 
75.7 
75.7 

</table></figure>

			<note place="foot" n="1"> URL: http://www.msi.vxu.se/users/nivre/research/MaltParser.html.</note>

			<note place="foot" n="2"> In fact, essentially the same methodology has been proposed earlier for other frameworks by Berwick (1985), Simmons and Yu (1992), Zelle and Mooney (1993) and Veenstra and Daelemans (2000), among others, although these approaches have typically been evaluated only on artificially generated or very small data sets.</note>

			<note place="foot" n="3"> For an in-depth discussion of inductive dependency parsing and its relation to other parsing methods, see Nivre (2006). 4 Strictly speaking, we require the graph to be weakly connected, which entails that the corresponding undirected graph is connected, whereas a strongly connected graph has a directed path between any pair of nodes.</note>

			<note place="foot" n="5"> This condition is in fact superfluous, since it is impossible for the next input token to be attached to any other node, but it is included for symmetry.</note>

			<note place="foot" n="6"> In addition, maximum entropy modeling was used in the comparative evaluation of Cheng et al. (2005).</note>

			<note place="foot" n="7"> The feature models supported by MaltParser are in fact slightly more general in that they also allow address functions that refer to siblings. This option is not exploited in the experiments reported below and has therefore been excluded from the presentation in section 2.3.</note>

			<note place="foot" n="8"> It is worth pointing out that, given the nature of the arc-eager parsing algorithm, the dependency type of the next input token and its rightmost child will always be undefined at decision time (hence their omission in the standard model and all other models).</note>

			<note place="foot" n="9"> Results have been published previously for Swedish (Nivre et al. 2004; Nivre 2006), English (Nivre and Scholz 2004; Nivre 2006), Czech (Nivre and Nilsson 2005), Bulgarian (Marinov and Nivre 2005), Danish (Nivre and Hall 2005) and Italian (Chanev 2005) but not for Chinese, German and Turkish.</note>

			<note place="foot" n="12"> Although punctuation tokens are excluded in the calculation of accuracy scores, they are included during parsing. No changes have been made to the tokenization or sentence segmentation found in the respective treebanks, except for Turkish (see section 4.2.10).</note>

			<note place="foot" n="13"> The score for the Charniak parser has been obtained by converting the output of the parser to dependency structures using the same conversion as in our experiments, which means that the comparison is as exact as possible. For further comparisons, see Nivre (2006).</note>

			<note place="foot" n="16"> It should be pointed out that McDonald and Pereira (2006) also consider secondary dependency arcs, which are beyond the reach of MaltParser in its current configuration, and that the result reported is actually the highest precision of their parser when restricted to primary dependencies.</note>

			<note place="foot" n="17"> A3sg = 3sg number agreement, P2pl = 2pl possessive agreement, Loc = locative case.</note>

			<note place="foot" n="18"> The few roots that do occur are unconnected words that give rise to non-projective dependency structures.</note>

			<note place="foot" n="19"> Strictly speaking, the subset used by Eryi˘ git and Oflazer (2006) only contains non-crossing dependencies, although it does contain punctuation that is not connected to other tokens. In order to make these graphs projective, the punctuation tokens were attached to the immediately following word. However, since punctuation is excluded in all evaluation scores, this nevertheless seems like a fair comparison.</note>

			<note place="foot" n="20"> More recent work using SVM, rather than MBL, for discriminative learning has shown that this gap can be closed, and in the recent shared task of multilingual dependency parsing at the Tenth Conference on Computational Natural Language Learning (CoNLL-X), MaltParser was one of the two top performing systems (Buchholz and Marsi 2006; Nivre et al. 2006; Hall 2006).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to express our gratitude for assistance with data sets, conversions and many other things to Christina Bosco, Yuchang Cheng, Yuan Ding, Jan Hajič,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matthias Trautner Kromann, Alberto Lavelli, Haitao Liu, Yuji Matsumoto, Ryan McDonald, Kemal Oflazer, Petya Osenova, Kiril Simov, Yannick Versley, Hiroyasu Yamada, and Daniel Zeman. We are also grateful for the support of GSLT (Swedish National Graduate School of Language Technology), T ¨ UB ˙ ITAK (The Scientific and Technical Research Council of Turkey), and The Swedish Research Council.</head><p>Finally, we want to thank our three anonymous reviewers for insightful comments and suggestions that helped us improve the final version of the article.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Alpino Dependency Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Beek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Malouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics in the Netherlands 2002. Selected Papers from the Thirteenth CLIN Meeting</title>
		<editor>Gaustad, T.</editor>
		<meeting><address><addrLine>Rodopi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="8" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Acquisition of Syntactic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Berwick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two statistical parsing models applied to the Chinese Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Chinese Language Processing Workshop</title>
		<meeting>the Second Chinese Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leech</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Rodopi</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards history-based grammars: Using richer models for probabilistic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th DARPA Speech and Natural Language Workshop</title>
		<meeting>the 5th DARPA Speech and Natural Language Workshop</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="31" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Assigning function tags to parsed text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blaheta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="234" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Prague Dependency Treebank: A three-level annotation scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Böhmovböhmov´böhmová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hajičovhajičov´hajičová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hladkáhladk´hladká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Treebanks: Building and Using Parsed Corpora</title>
		<editor>Abeillé, A.</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="103" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Grammatical Relation System for Treebank Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Turin University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Alpino: Wide-coverage computational analysis of Dutch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Malouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics in the Netherlands 2000. Selected Papers from the Eleventh CLIN Meeting</title>
		<editor>Daelemans, W., Sima&apos;an, K., Veenstra, J. and Zavrel, J.</editor>
		<meeting><address><addrLine>Rodopi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="45" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TnT -a statistical part-of-speech tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP&apos;2000)</title>
		<meeting>the Sixth Applied Natural Language Processing Conference (ANLP&apos;2000)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="224" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Portability of dependency parsing algorithms -an application for Italian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT)</title>
		<meeting>the Fourth Workshop on Treebanks and Linguistic Theories (TLT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="29" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Maximum-Entropy-Inspired Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coarse-to-fine n-best parsing and discriminative MaxEnt reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deterministic dependency structure analyzer for Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the First International Joint Conference on Natural Language Processing (IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="500" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine learning-based dependency analyzer for Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Chinese Computing (ICCC)</title>
		<meeting>International Conference on Chinese Computing (ICCC)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="66" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chinese deterministic dependency analyzer: Examining effects of global features and root node finder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 35th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning</title>
		<meeting>the 17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A statistical parser for Czech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analyzing an Italian treebank with state-of-the-art statistical parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Corazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zanoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Treebanks and Linguistic Theories (TLT)</title>
		<meeting>the Third Workshop on Treebanks and Linguistic Theories (TLT)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A fundamental algorithm for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Covington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual ACM Southeast Conference</title>
		<meeting>the 39th Annual ACM Southeast Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Memory-Based Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Bosch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">MBT: Memory Based Tagger, version 2.0, Reference Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zavrel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Der Sloot</surname></persName>
		</author>
		<idno>03-13</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Tilburg University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">ILK Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic parsing for German using sisterhead dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistical dependency parsing of Turkish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">;</forename><surname>Einarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter</title>
		<meeting>the 11th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1976" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
		<respStmt>
			<orgName>Talbankens skriftspråkskonkordans. Lund University, Department of Scandinavian Languages. Eryi˘ git</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A broad-coverage parser for German based on defeasible constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Foth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KONVENS 2004, Beiträge zur 7. Konferenz zur Verarbeitung natürlicher Sprache</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vidova Hladka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Panevovápanevov´panevová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hajičovhajičov´hajičová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sgall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pajas</surname></persName>
		</author>
		<idno>1.0. LDC</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">Prague Dependency Treebank</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">MaltParser -An Architecture for Labeled Inductive Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Växjö University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Licentitate thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Corrective modeling for non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nováknov´novák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 9th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="42" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">English Word Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimators for stochastic &quot;unification-based&quot; grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Guides and oracles for linear-time parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 6th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="6" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Danish Dependency Treebank and the DTAG treebank tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Kromann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT)</title>
		<meeting>the Second Workshop on Treebanks and Linguistic Theories (TLT)</meeting>
		<imprint>
			<publisher>Växjö University Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Is it really that difficult to parse German?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Japanese dependency analysis using cascaded chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Computational Language Learning (CoNLL)</title>
		<meeting>the Sixth Workshop on Computational Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="63" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Treebank development: The TUT approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lesmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
		<editor>Sangal, R. and Bendre, S. M.</editor>
		<meeting><address><addrLine>New Delhi</addrLine></address></meeting>
		<imprint>
			<publisher>Vikas Publishing House</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Is it harder to parse Chinese, or the Chinese Treebank?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="439" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A dependency-based method for evaluating broad-coverage parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="97" to="114" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical decision-tree models for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Magerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 33rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="276" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A data-driven parser for Bulgarian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT)</title>
		<meeting>the Fourth Workshop on Treebanks and Linguistic Theories (TLT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structural disambiguation with constraint propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maruyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 28th Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online Learning of Approximate Dependency Parsing Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Dependency Syntax: Theory and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mel&amp;apos;ˇ Cuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
		<respStmt>
			<orgName>State University of New York Press</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Building and Using Syntactically Annotated Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barsotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Battista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Corazzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zampolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fanciulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Massetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raffaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Pazienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saracino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pianesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Delmonte</surname></persName>
		</author>
		<editor>Anne Abeillé</editor>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="189" to="210" />
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
	<note>Building the Italian syntactic-semantic treebank</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph transformations in data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL)</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Inductive Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MaltParser: A language-independent system for data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT)</title>
		<meeting>the Fourth Workshop on Treebanks and Linguistic Theories (TLT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="137" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Memory-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 8th Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Labeled pseudo-projective dependency parsing with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eryi˘ Git</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="221" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pseudo-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deterministic dependency parsing of English text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="64" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Building a Turkish treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Say</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Treebanks: Building and Using Parsed Corpora</title>
		<editor>Abeillé, A.</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="261" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A linear observed time statistical parser based on maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Second Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A classifier-based parser with linear run-time complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 9th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">HPSG-based syntactic treebank of Bulgarian (BulTreeBank)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Rainbow of Corpora: Corpus Linguistics and the Languages of the World</title>
		<editor>Wilson, A., Rayson, P. and McEnery, T.</editor>
		<imprint>
			<publisher>Lincon-Europa</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The acquisition and use of context-dependent grammars for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="391" to="418" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An annotation scheme for free word order languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Applied Natural Language Processing</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing<address><addrLine>Washington, D.C</addrLine></address></meeting>
		<imprint>
			<publisher>ANLP</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Shallow language processing architecture for Bulgarian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 17th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="995" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Manual för grammatisk beskrivning av talad och skriven svenska</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Teleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<publisher>Lund: Studentlitteratur</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Stylebook for the Tübingen Treebank of Written German (TüBa-D/Z)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Telljohann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zinsmeister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Seminar für Sprachwissenschaft</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Universität Tübingen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Porting statistical parsers with data-defined kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">A memory-based alternative for connectionist shiftreduce parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veenstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<idno>ILK-0012</idno>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>University of Tilburg</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Syntactic analysis in the spoken Dutch corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voutilainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Der Wouden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moortgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Renmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schuurman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parsing Swedish. Extended Abstract for the 13th Nordic Conference of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2001-05-20" />
			<biblScope unit="page" from="768" to="773" />
		</imprint>
		<respStmt>
			<orgName>Uppsala University</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the Third International Conference on Language Resources and Evaluation</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The Penn Chinese Treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-D</forename><surname>Fei Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning semantic grammars with constructive inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh National Conference of the American Association for Artificial Intelligence (AAAI)</title>
		<meeting>the Eleventh National Conference of the American Association for Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="817" to="899" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
