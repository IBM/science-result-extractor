<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Segment Representations for Neural Segmentation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Wanxiang</roleName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
							<email>yjliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><forename type="middle">⇤</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
							<email>jguo@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>qinb@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Segment Representations for Neural Segmentation Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many natural language processing (NLP) tasks can be generalized into segmentation problem. In this paper, we combine semi-CRF with neural network to solve NLP segmentation tasks. Our model represents a segment both by composing the input units and embedding the entire segment. We thoroughly study different composition functions and different segment embeddings. We conduct extensive experiments on two typical segmentation tasks: named entity recognition (NER) and Chinese word seg-mentation (CWS). Experimental results show that our neural semi-CRF model benefits from representing the entire segment and achieves the state-of-the-art performance on CWS benchmark dataset and competitive results on the CoNLL03 dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given an input sequence, segmentation is the problem of identifying and assigning tags to its subsequences. Many natural language processing (NLP) tasks can be cast into the segmentation problem, like named entity recognition <ref type="bibr">[Okanohara et al., 2006]</ref>, opinion extraction <ref type="bibr">[Yang and Cardie, 2012]</ref>, and Chinese word segmentation <ref type="bibr" target="#b1">[Andrew, 2006]</ref>. Properly representing segment is critical for good segmentation performance. Widely used sequence labeling models like conditional random fields <ref type="bibr" target="#b2">[Lafferty et al., 2001]</ref> represent the contextual information of the segment boundary as a proxy to entire segment and achieve segmentation by labeling input units (e.g. words or characters) with boundary tags. Compared with sequence labeling model, models that directly represent segment are attractive because they are not bounded by local tag dependencies and can effectively adopt segment-level information. Semi-Markov CRF (or semi-CRF) <ref type="bibr" target="#b4">[Sarawagi and Cohen, 2004]</ref> is one of the models that directly represent the entire segment. In semi-CRF, the conditional probability of a semi-Markov chain on the input sequence is explicitly modeled, whose each state corresponds to a subsequence of input units, which makes semi-CRF a natural choice for segmentation problem.</p><p>⇤ Email corresponding.</p><p>However, to achieve good segmentation performance, conventional semi-CRF models require carefully hand-crafted features to represent the segment. Recent years witness a trend of applying neural network models to NLP tasks. The key strengths of neural approaches in NLP are their ability for modeling the compositionality of language and learning distributed representation from large-scale unlabeled data. Representing a segment with neural network is appealing in semi-CRF because various neural network structures <ref type="bibr">[Hochreiter and Schmidhuber, 1997]</ref> have been proposed to compose sequential inputs of a segment and the well-studied word embedding methods <ref type="bibr" target="#b3">[Mikolov et al., 2013]</ref> make it possible to learn entire segment representation from unlabeled data.</p><p>In this paper, we combine neural network with semi-CRF and make a thorough study on the problem of representing a segment in neural semi-CRF. <ref type="bibr" target="#b2">Kong et al. [2015]</ref> proposed a segmental recurrent neural network (SRNN) which represents a segment by composing input units with RNN. We study alternative network structures besides the SRNN. We also study segment-level representation using segment embedding which encodes the entire segment explicitly. We conduct extensive experiments on two typical NLP segmentation tasks: named entity recognition (NER) and Chinese word segmentation (CWS). Experimental results show that our concatenation alternative achieves comparable performance with the original SRNN but runs 1.7 times faster and our neural semi-CRF greatly benefits from the segment embeddings. In the NER experiments, our neural semi-CRF model with segment embeddings achieves an improvement of 0.7 F-score over the baseline and the result is competitive with state-of-the-art systems. In the CWS experiments, our model achieves more than 2.0 F-score improvements on average. On the PKU and MSR datasets, state-of-the-art F-scores of 95.67% and 97.58% are achieved respectively. We release our code at https://github.com/ExpResults/segrep-for-nn-semicrf. <ref type="figure">Figure 1</ref> shows examples of named entity recognition and Chinese word segmentation. For the input word sequence in the NER example, its segments ("Michael Jordan":PER, "is":NONE, "a":NONE, "professor":NONE, "at":NONE, "Berkeley":ORG) reveal that "Michaels Jordan" is a person name and "Berkeley" is an organization. In the CWS example, the subsequences (" /Pudong", " /development",</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>None</head><p>Organization 浦东开发与建设 浦东 / 开发 / 与 / 建设 Pudong development and construction <ref type="figure">Figure 1</ref>: Examples for named entity recognition (above) and Chinese word segmentation (below).</p><p>" /and", " /construction") of the input character sequence are recognized as words. Both NER and CWS take an input sequence and partition it into disjoint subsequences.</p><p>Formally, for an input sequence x = (x 1 , .., x |x| ) of length |x|, let x a:b denote its subsequence (x a , ..., x b ). A segment of x is defined as (u, v, y) which means the subsequence x u:v is associated with label y. A segmentation of x is a segment sequence s = (s 1 , .., s p ), where s j = (u j , v j , y j ) and u j+1 = v j + 1. Given an input sequence x, the segmentation problem can be defined as the problem of finding x's most probable segment sequence s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Semi-Markov CRF</head><p>Semi-Markov CRF (or semi-CRF, <ref type="figure" target="#fig_0">Figure 2a</ref>) <ref type="bibr" target="#b4">[Sarawagi and Cohen, 2004</ref>] models the conditional probability of s on x as</p><formula xml:id="formula_0">p(s|x) = 1 Z(x) exp{W · G(x, s)}</formula><p>where G(x, s) is the feature function, W is the weight vector and Z(x) = P s 0 2S exp{W ·G(x, s 0 )} is the normalize factor of all possible segmentations S over x.</p><p>By restricting the scope of feature function within a segment and ignoring label transition between segments (0-order semi-CRF), G(x, s) can be decomposed as G(x, s) = P p j=1 g(x, s j ) where g(x, s j ) maps segment s j into its representation. Such decomposition allows using efficient dynamic programming algorithm for inference. To find the best segmentation in semi-CRF, let ↵ j denote the best segmentation ends with j th input and ↵ j is recursively calculated as</p><formula xml:id="formula_1">↵ j = max l=1..L,y (j 񮽙 l, j, y) + ↵ j񮽙l񮽙1</formula><p>where L is the maximum length manually defined and (j 񮽙 l, j, y) is the transition weight for s = (j 񮽙 l, j, y) in which</p><formula xml:id="formula_2">(j 񮽙 l, j, y) = W · g(x, s).</formula><p>Previous semi-CRF works <ref type="bibr" target="#b4">[Sarawagi and Cohen, 2004;</ref><ref type="bibr">Okanohara et al., 2006;</ref><ref type="bibr" target="#b1">Andrew, 2006;</ref><ref type="bibr">Yang and Cardie, 2012]</ref> parameterize g(x, s) as a sparse vector, each dimension of which represents the value of corresponding feature function. Generally, these feature functions fall into two types: 1) the CRF style features which represent input unit-level information such as "the specific words at location i" 2) the semi-CRF style features which represent segment-level information such as "the length of the segment". <ref type="bibr" target="#b2">Kong et al. [2015]</ref> proposed the segmental recurrent neural network model (SRNN, see <ref type="figure" target="#fig_0">Figure 2b</ref>) which combines the semi-CRF and the neural network model. In <ref type="bibr">SRNN, g(x, s)</ref> is parameterized as a bidirectional LSTM (bi-LSTM). For a segment s j = (u j , v j , y j ), each input unit x in subsequence (x uj , .., x vj ) is encoded as embedding and fed into the bi-LSTM. The rectified linear combination of the final hidden layers from bi-LSTM is used as g(x, s). <ref type="bibr" target="#b2">Kong et al. [2015]</ref> pioneers in representing a segment in neural semi-CRF. Bi-LSTM can be regarded as "neuralized" CRF style features which model the input unit-level compositionality. However, in the SRNN work, only the bi-LSTM was employed without considering other input unit-level composition functions. What is more, the semi-CRF styled segment-level information as an important representation was not studied. In the following sections, we first study alternative input unit-level composition functions (3.1). Then, we study the problem of representing a segment at segment-level (3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Alternative Seg-Rep. via Input Composition</head><p>Segmental CNN Besides recurrent neural network (RNN) and its variants, another widely used neural network architecture for composing and representing variable-length input is the convolutional neural network (CNN) <ref type="bibr" target="#b1">[Collobert et al., 2011]</ref>. In CNN, one or more filter functions are employed to convert a fix-width segment in sequence into one vector. With filter function "sliding" over the input sequence, contextual information is encoded. Finally, a pooling function is used to merge the vectors into one. In this paper, we use a filter function of width 2 and max-pooling function to compose input units of a segment. Following SRNN, we name our CNN segment representation as SCNN (see <ref type="figure" target="#fig_0">Figure 2c</ref>).</p><p>However, one problem of using CNN to compose input units into segment representation lies in the fact that the maxpooling function is insensitive to input position. Two different segments sharing the same vocabulary can be treated without difference. In a CWS example, " " (racket for sell) and " " (ball audition) will be encoded into the same vector in SCNN if the vector of "</p><p>" that produced by filter function is always preserved by max-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmental Concatenation</head><p>Concatenation is also widely used in neural network models to represent fixed-length input. Although not designed to handle variable-length input, we see that in the inference of semi-CRF, a maximum length L is adopted, which make it possible to use padding technique to transform the variable-length representation problem into fixed-length of L. Meanwhile, concatenation preserves the positions of inputs because they are directly mapped into the certain positions in the resulting vector. In this paper, we study an alternative concatenation function to compose input units into segment representation, namely the SCONCATE model (see <ref type="figure" target="#fig_0">Figure 2d</ref>). Compared with SRNN, SCONCATE requires less computation when representing one segment, thus can speed up the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Seg-Rep. via Segment Embeddings</head><p>For segmentation problems, a segment is generally considered more informative and less ambiguous than an individual input. Incorporating segment-level features usually lead  performance improvement in previous semi-CRF work. Segment representations in Section 3.1 only model the composition of input units. It can be expected that the segment embedding which encodes an entire subsequence as a vector can be an effective way for representing a segment.</p><p>In this paper, we treat the segment embedding as a lookupbased representation, which retrieves the embedding table with the surface string of entire segment. With the entire segment properly embed, it is straightforward to combine the segment embedding with the composed vector from the input so that multi-level information of a segment is used in our model (see <ref type="figure">Figure 3)</ref>. However, how to obtain such embeddings is not a trivial problem.</p><p>A natural solution for obtaining the segment embeddings can be collecting all the "correct" segments from training data into a lexicon and learning their embeddings as model parameters. However, the in-lexicon segment is a strong clue for a subsequence being a correct segment, which makes our model vulnerable to overfitting. Unsupervised pre-training has been proved an effective technique for improving the robustness of neural network model <ref type="bibr" target="#b2">[Erhan et al., 2010]</ref>. To mitigate the overfitting problem, we initialize our segment embeddings with the pre-trained one.</p><p>Word embedding gains a lot of research interest in recent years <ref type="bibr" target="#b3">[Mikolov et al., 2013]</ref> and is mainly carried on English texts which are naturally segmented. Different from the word embedding works, our segment embedding requires largescale segmented data, which cannot be directly obtained. Following Wang et al. <ref type="bibr">[2011]</ref> which utilize automatically segmented data to enhance their model, we obtain the autosegmented data with our neural semi-CRF baselines (SRNN, SCNN, and SCONCATE) and use the auto-segmented data to  learn our segment embeddings. Another line of research shows that machine learning algorithms can be boosted by ensembling heterogeneous models. Our neural semi-CRF model can take knowledge from heterogeneous models by using the segment embeddings learned on the data segmented by the heterogeneous models. In this paper, we also obtain the auto-segmented data from a conventional CRF model which utilizes hand-crafted sparse features. Once obtaining the auto-segmented data, we learn the segment embeddings in the same with word embeddings.</p><p>A problem that arises is the fine-tuning of segment embeddings. Fine-tuning can learn a task-specific segment embeddings for the segments that occur in the training data, but it breaks their relations with the un-tuned out-of-vocabulary segments. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates this problem. Since OOV segments can affect the testing performance, we also try learning our model without fine-tuning the segment embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model details</head><p>In this section, we describe the detailed architecture for our neural semi-CRF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Unit Representation</head><p>Following <ref type="bibr" target="#b2">Kong et al. [2015]</ref>, we use a bi-LSTM to represent the input sequence. To obtain the input unit representation, we use the technique in Dyer et al. <ref type="bibr">[2015]</ref> and separately use two parts of input unit embeddings: the pre-trained embeddings E p without fine-tuning and fine-tuned embeddings E t . For the ith input, E p i and E t i are merged together through linear combination and form the input unit representation</p><formula xml:id="formula_3">I i = RELU(W I [E p i ; E t i ] + b I ) fixed</formula><note type="other">input unit embedding E p i size 100 fine tuned input unit embedding E t i size 32 input unit representation Ii size 100</note><p>LSTM hidden layer Hi size 100 seg-rep via input composition SCOMP 64 seg-rep via segment embedding SEMB 50 label embedding E Y y i size 20 final segment representation Si size 100 <ref type="table">Table 1</ref>: Hyper-parameter settings where the notation of W [X 1 ; ..; X n ] equals to X 1 , .., X n 's linear combination W 1 X 1 + .. + W n X n and b I is the bias. After obtaining the representation for each input unit, a sequence (I 1 , ..., I |x| ) is fed to a bi-LSTM. The hidden layer of forward LSTM 񮽙 ! H i and backward LSTM H i are combined as</p><formula xml:id="formula_4">H i = RELU(W H [ 񮽙 ! H i ; H i ] + b H )</formula><p>and used as the i th input unit's final representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segment Representation</head><formula xml:id="formula_5">S j = RELU(W S [SCOMP j ; SEMB j ; E Y yj ] + b S )</formula><p>where E Y is the embedding for the label of a segment.</p><p>Throughout this paper, we use the same hyper-parameters for different experiments as listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Procedure</head><p>In this paper, negative log-likelihood is used as learning objective. We follow  and use stochastic gradient descent to optimize model parameters. Initial learning rate is set as ⌘ 0 = 0.1 and updated as ⌘ t = ⌘ 0 /(1 + 0.1t) on each epoch t. Best training iteration is determined by the evaluation score on development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We conduct our experiments on two NLP segmentation tasks: named entity recognition and Chinese word segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Word Embedding</head><p>For NER, we use the CoNLL03 dataset which is widely adopted for evaluating NER models' performance. F-score is used as evaluation metric. <ref type="bibr">1</ref> For CWS, we follow previous study and use three Simplified Chinese datasets: PKU and MSR from 2 nd SIGHAN 1 conlleval script in CoNLL03 shared task is used. bakeoff and Chinese Treebank 6.0 (CTB6). For the PKU and MSR datasets, last 10% of the training data are used as development data as <ref type="bibr" target="#b4">[Pei et al., 2014]</ref> does. For CTB6 data, recommended data split is used. We convert all the double byte digits and letters in the PKU data into single byte. Like NER, CWS performance is evaluated by F-score. <ref type="bibr">2</ref> Unlabeled data are used to learn both the input unit embeddings (word embedding for NER, character embedding for CWS) and segment embeddings. For NER, we use RCV1 data as our unlabeled English data. For CWS, Chinese gigawords is used as unlabeled Chinese data. Throughout this paper, we use the word embedding toolkit released by  to obtain both the input unit embeddings and segment embeddings. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline</head><p>We compare our models with three baselines:</p><p>1. SPARSE-CRF: The CRF model using sparse handcrafted features.</p><p>2. NN-LABELER: The neural network sequence labeling model making classification on each input unit.</p><p>3. NN-CRF: The neural network CRF which models the conditional probability of a label sequence over the input sequence.</p><p>BIESO-tag schema is used in all the CRF and sequence labeling models. <ref type="bibr">4</ref> For SPARSE-CRF, we use the baseline feature templates in Guo et al. <ref type="bibr">[2014]</ref> for NER and Jiang et al. <ref type="bibr">[2013]</ref>'s feature templates for CWS. Both NN-LABELER and NN-CRF take the same input unit representation as our neural semi-CRF models but vary on the output structure and do not explicitly model segment-level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing Different Input Composition Functions</head><p>We first consider the problem of representing segments via composing input units and compare different input composition functions. Results on NER and CWS data are shown in <ref type="table" target="#tab_2">Table 2</ref>. From this table, the SRNN and SCONCATE achieve comparable results and perform better than the SCNN. Although CNN can model input sequence at any length, its invariance to the exact position can be a flaw in representing segments. The experimental results confirm that and show the importance of properly handling the input position. Considering SCNN's relatively poor performance, we only study SRNN and SCONCATE in the following experiments.</p><p>Comparing with NN-LABELER, structure prediction models (NN-CRF and neural semi-CRF) generally achieve better performance. The best structure prediction model outperforms NN-LABELER by 0.4% on NER and 1.11% averagely on CWS according to  The NER and CWS results of the baseline models and our neural semi-CRF models with different input composition functions. spd represents the inference speed and is evaluated by the number of tokens processed per millisecond.</p><p>• (SCONCATE) on NER while the both SRNN and SCON-CATE outperform NN-CRF on three CWS datasets. We address this to the fact either the NN-CRF or the neural semi-CRF merely takes input-level information and not sufficiently adopts segment-level information into the models. A further comparison on inference speed shows that SCONCATE runs 1.7 times faster than SRNN, but slower than the NN-LABELER and NN-CRF, which is resulted from the intrinsic difference in time complexity.</p><formula xml:id="formula_6">• • • • • • • • • • • • • • • • • • •</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparing Different Segment Embeddings</head><p>Next we study the effect of different segment embeddings. Using a segmentation model, we can obtain auto-segmented unlabeled data, then learn the segment embeddings. In this paper, we tried two segmentation models. One is the neural semi-CRF baseline which represents segment by composing input and another one is the CRF model using sparse handcrafted features. For convenience, we use SEMB-HOMO and SEMB-HETERO to note the segment embeddings learned from their auto-segmented data respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Pre-trained Segment Embeddings</head><p>We first incorporate randomly initialized segment embeddings into our model and tune the embeddings along with other parameters. However our preliminary experiments of adding these embeddings into SRNN witness a severe drop of F-score on the CoNLL03 development set (from 92.97% to 77.5%). A further investigation shows that the randomly initialized segment embeddings lead to severe overfitting. <ref type="figure" target="#fig_3">Fig- ure 5</ref> shows the learning curve in training the NER model.  <ref type="table">Table 3</ref>: Effect of fine-tuning (FT) segment embedding on development data. For CoNLL03 data, a named entity is "outof-vocabulary" when it is not included in the training data as a named entity.</p><p>From this figure, the model with randomly initialized segment embeddings converge to the training data at about 5 th iteration and the development performance stops increasing at the same time. However, by initializing with SEMB-HOMO, the development set performance increase to 93%, which shows the necessity of pre-trained segment embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Fine-tuning Segment Embeddings</head><p>We study the effect of fine-tuning the segment embeddings by imposing SEMB-HOMO into our model. <ref type="table">Table 3</ref> shows the experimental results on development data. We find that our models benefit from fixing the segment embeddings on CoNLL03. While on MSR, fine-tuning the embeddings helps. Further study on the out-of-vocabulary rate shows that the OOV rate of MSR is very low, thus fine-tuning on segment embeddings help to learn a better task-specified segment representation. However, on CoNLL03 data whose OOV rate is high, fine-tuning the segment embedding harms the generalization power of pre-trained segment embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Heterogeneous Segment Embeddings</head><p>In previous sections, our experiments are mainly carried on the segment embeddings obtained from homogeneous models. In this section, we use our SPARSE-CRF as the heterogeneous model to obtain SEMB-HETERO. We compare the models with SEMB-HETERO and SEMB-HOMO on the development data in <ref type="figure" target="#fig_4">Figure 6</ref>. These results show that SEMB-HETERO generally achieve better performance than the SEMB-HOMO. On the CoNLL03 and MSR dataset, the differences are significant. Meanwhile, we see that finetuning the segment embedding can narrow the gap between SEMB-HETERO and SEMB-HOMO.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Result</head><p>At last, we compare our neural semi-CRF model leveraging additional segment embeddings with those only represent segment by composing input. <ref type="table" target="#tab_7">Table 4</ref> shows the result on the NER and CWS test data. Style of segment embeddings (HOMO or HETERO) and whether fine-tune it is decided by the development data. From this result, we see that segmentlevel representation greatly boost up model's performance. On NER, an improvement of 0.7% is observed and that improvement on CWS is more than 2.0% on average. We compare our neural semi-CRF model leveraging multilevels segment representation with other state-of-the-art NER and CWS systems. <ref type="table" target="#tab_8">Table 5</ref> shows the NER comparison results. The first block shows the results of neural NER models and the second one shows the non-neural models. All these work employed hand-crafted features like capitalization. <ref type="bibr" target="#b1">Collobert et al. [2011]</ref>, <ref type="bibr" target="#b2">Guo et al. [2014]</ref>, and Passos et al. <ref type="bibr">[2014]</ref> also utilize lexicon as an additional knowledge resource. Without any hand-crafted features, our model can achieve comparable performance with the models utilizing domain-specific features. <ref type="table">Table 6</ref> shows the comparison with the state-of-the-art CWS systems. The first block of <ref type="table">Table 6</ref> shows the neural CWS models and second block shows the non-neural models. Our neural semi-CRF model with multi-level segment representation achieves the state-of-the-art performance on PKU and MSR data. On CTB6 data, our model's performance is also close to <ref type="bibr" target="#b4">Wang et al. [2011]</ref> which uses semi-supervised features extracted auto-segmented unlabeled data. Accord- <ref type="bibr">et al., 2011]</ref> 89.59 <ref type="bibr" target="#b2">[Huang et al., 2015]</ref> 90.10 non-NN [Ando and <ref type="bibr" target="#b0">Zhang, 2005]</ref> 89.31 <ref type="bibr" target="#b2">[Guo et al., 2014]</ref> 88.58 <ref type="bibr" target="#b4">[Passos et al., 2014]</ref> 90.90 our best 89.77  <ref type="table">Table 6</ref>: Comparison with the state-of-the-art CWS systems.</p><formula xml:id="formula_7">genre model CoNLL03 NN [Collobert</formula><p>ing to <ref type="bibr" target="#b4">Pei et al. [2014]</ref>, significant improvements can be achieved by replacing character embeddings with characterbigram embeddings. However we didn't employ this trick considering the unification of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Semi-CRF has been successfully used in many NLP tasks like information extraction <ref type="bibr" target="#b4">[Sarawagi and Cohen, 2004]</ref>, opinion extraction <ref type="bibr">[Yang and Cardie, 2012]</ref> and Chinese word segmentation <ref type="bibr" target="#b1">[Andrew, 2006;</ref><ref type="bibr">Sun et al., 2009]</ref>. Its combination with neural network is relatively less studied. To the best of our knowledge, our work is the first one that achieves stateof-the-art performance with neural semi-CRF model. Domain specific knowledge like capitalization has been proved effective in named entity recognition <ref type="bibr" target="#b4">[Ratinov and Roth, 2009]</ref>. Segment-level abstraction like whether the segment matches a lexicon entry also leads performance improvement <ref type="bibr" target="#b1">[Collobert et al., 2011]</ref>. To keep the simplicity of our model, we didn't employ such features in our NER experiments. But our model can easily take these features and it is hopeful the NER performance can be further improved.</p><p>Utilizing auto-segmented data to enhance Chinese word segmentation has been studied in <ref type="bibr" target="#b4">Wang et al. [2011]</ref>. However, only statistics features counted on the auto-segmented data was introduced to help to determine segment boundary and the entire segment was not considered in their work. Our model explicitly uses the entire segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we systematically study the problem of representing a segment in neural semi-CRF model. We propose a concatenation alternative for representing segment by composing input units which is equally accurate but runs faster than SRNN. We also propose an effective way of incorporating segment embeddings as segment-level representation and it significantly improves the performance. Experiments on named entity recognition and Chinese word segmentation show that the neural semi-CRF benefits from rich segment representation and achieves state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration for the semi-CRF, SRNN, SCNN and SCONCATE. In these figures, circles represent the inputs, blue rectangles represent factors in graphic model and yellow rectangles represent generic nodes in the neural network model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example for fine-tuning decreases the generalization power of pre-trained segment embedding. "1994 World Cup" does not occur in the training data and its similarity with "1998 World Cup" is broken because "1998 World Cup" is tuned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Negative log-likelihood (blue lines) and development F-score (red lines) by iterations. Solid lines show the model with randomly initialized segment embeddings. Dashed lines show that initialized with pre-trained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison between models with SEMB-HOMO and SEMB-HETERO on development data. The rows show different baseline neural semi-CRF models and the columns show whether fine-tuning (FT) the segment embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>But the difference between the neural structure prediction models is not significant. NN- CRF performs better than the best neural semi-CRF model</figDesc><table>NER 

CWS 
CoNLL03 
CTB6 
PKU 
MSR 
model 
dev 
test 
dev 
test 
dev 
test 
dev 
test 
spd 

baseline 
NN-LABELER 
93.03 88.62 93.70 93.06 93.57 92.99 93.22 93.79 3.30 
NN-CRF 
93.06 89.08 94.33 93.65 94.09 93.28 93.81 94.17 2.72 
SPARSE-CRF 
88.87 83.43 95.68 95.08 95.85 95.06 96.09 96.54 

neural semi-CRF 
SRNN 
92.97 88.63 94.56 94.06 94.86 93.91 94.38 95.21 0.62 
SCONCATE 
92.96 89.07 94.34 93.96 94.41 93.57 94.05 94.53 1.08 
SCNN 
91.53 87.68 87.82 87.51 79.64 80.75 85.04 85.79 1.46 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison between baselines and our neural semi-
CRF model with segment embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with the state-of-the-art NER systems. 

genre 
model CTB6 PKU MSR 

NN 

[Zheng et al., 2013] 
-
92.4 
93.3 
[Pei et al., 2014] 
94.0 
94.9 
[Pei et al., 2014] w/bigram 
-
95.2 
97.2 
[Kong et al., 2015] 
90.6 
90.7 

non-NN 

[Tseng, 2005] 
-
95.0 
96.4 
[Zhang and Clark, 2007] 
-
95.1 
97.2 
[Sun et al., 2009] 
-
95.2 
97.3 
[Wang et al., 2011] 
95.7 
-
-
our best 
95.48 95.67 97.58 

</table></figure>

			<note place="foot" n="2"> score script in 2 nd SIGHAN bakeoff is used. 3 https://github.com/wlin12/wang2vec 4 O tag which means OUTSIDE is not adopted in CWS experiments since CWS doesn&apos;t involve assigning tags to segments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61133012 and 61370164.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang ; Rie Kubota</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hybrid markov/semimarkov conditional random field for sequence segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew ; Galen Andrew ; Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-2006</title>
		<editor>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa</editor>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
	<note>Natural language processing (almost) from scratch</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Yating Yang, and Qun Liu. Discriminative learning with natural annotations: Word segmentation as a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dyer</surname></persName>
		</author>
		<idno>abs/1511.06018</idno>
	</analytic>
	<monogr>
		<title level="m">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<editor>Zhiheng Huang, Wei Xu, and Kai Yu</editor>
		<meeting><address><addrLine>Beijing, China; Doha, Qatar; Meng Sun, Yajuan Lü; Sofia, Bulgaria; San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="1997-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
	<note>ICML 01</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno>abs/1310.4546</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-2015</title>
		<meeting><address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
	<note>Two/too simple adaptations of word2vec for syntax problems. Okanohara et al., 2006] Daisuke Okanohara</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yiou Wang, Jun&apos;ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. Improving chinese word segmentation and pos tagging with semi-supervised methods using large autoanalyzed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsujii ; Passos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. [Tseng, 2005] Huihsin Tseng. A conditional random field word segmenter. In In Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<editor>AFNLP. [Yang and Cardie, 2012] Bishan Yang and Claire Cardie</editor>
		<meeting><address><addrLine>Sydney, Australia; Ann Arbor, Michigan; Baltimore, Maryland; Boulder; Cambridge, MA; Boulder, Colorado; Chiang Mai, Thailand; Jeju Island, Korea; Stephen Clark; Prague, Czech Republic; Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
	<note>EMNLP-2013</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
