<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T01:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parsing as Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do</forename><forename type="middle">Kook</forename><surname>Choe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University Providence</orgName>
								<orgName type="institution" key="instit2">Brown University Providence</orgName>
								<address>
									<region>RI, RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University Providence</orgName>
								<orgName type="institution" key="instit2">Brown University Providence</orgName>
								<address>
									<region>RI, RI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parsing as Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2331" to="2336"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing-93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stan-ford dependencies, UAS and LAS are 95.9% and 94.1%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., <ref type="bibr" target="#b3">Dyer et al. (2016)</ref> with 92.4 F 1 on Penn Treebank constituency parsing and <ref type="bibr" target="#b24">Vinyals et al. (2015)</ref> with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture.</p><p>In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5). There is a one-to-one mapping between a tree and its sequential form. (Part-of-speech tags are not used.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Language Modeling</head><p>Formally, a language model (LM) is a probability distribution over strings of a language:</p><formula xml:id="formula_0">P (x) = P (x 1 , · · · , x n ) = n t=1 P (x t |x 1 , · · · , x t−1 ),<label>(1)</label></formula><p>where x is a sentence and t indicates a word position. The efforts in language modeling go into computing P (x t |x 1 , · · · , x t−1 ), which as described next is useful for parsing as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Parsing as Language Modeling</head><p>A generative parsing model parses a sentence (x) into its phrasal structure (y) according to</p><formula xml:id="formula_1">argmax y ∈Y(x) P (x, y ),</formula><p>where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z) <ref type="bibr" target="#b24">(Vinyals et al., 2015</ref>) as illustrated in <ref type="figure">Figure 1</ref>, we can define a probability distribution over (x, y) as follows:</p><formula xml:id="formula_2">P (x, y) = P (z) = P (z 1 , · · · , z m ) = m t=1 P (z t |z 1 , · · · , z t−1 ),<label>(2)</label></formula><p>which is equivalent to Equation (1). We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , · · · , z t−1 ) for parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>We look here at three neural net (NN) models closest to our research along various dimensions. The first ( <ref type="bibr">Zaremba et al., 2014)</ref> gives the basic language modeling architecture that we have adopted, while the other two ( <ref type="bibr" target="#b24">Vinyals et al., 2015;</ref><ref type="bibr" target="#b3">Dyer et al., 2016)</ref> are parsing models that have the current best results in NN parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LSTM-LM</head><p>The LSTM-LM of <ref type="bibr">Zaremba et al. (2014)</ref> turns (x 1 , · · · , x t−1 ) into h t , a hidden state of an LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b4">Gers et al., 2003;</ref><ref type="bibr" target="#b5">Graves, 2013)</ref>, and uses h t to guess x t :</p><formula xml:id="formula_3">P (x t |x 1 , · · · , x t−1 ) = P (x t |h t ) = softmax(W h t )[x t ],</formula><p>where W is a parameter matrix and [i] indexes ith element of a vector. The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages <ref type="bibr" target="#b11">(Kim et al., 2016)</ref> and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7 ( <ref type="bibr" target="#b10">Jozefowicz et al., 2016)</ref>. In this paper, we build a parsing model based on the LSTM-LM of <ref type="bibr">Zaremba et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MTP</head><p>Vinyals et al. <ref type="bibr">(2015)</ref> observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a conditional probability:</p><formula xml:id="formula_4">P (y|x) = P (y 1 , · · · , y l |x) = l t=1 P (y t |x, y 1 , · · · , y t−1 ),</formula><p>where the conditioning event (x, y 1 , · · · , y t−1 ) is modeled by an LSTM encoder and an LSTM decoder. The encoder maps x into h e , a set of vectors that represents x, and the decoder obtains a summary vector (h t ) which is concatenation of the decoder's hidden state (h d t ) and weighted sum of word representations ( n i=1 α i h e i ) with an alignment vector (α). Finally the decoder predicts y t given h t . Inspired by MTP, our model processes sequential trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">RNNG</head><p>Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree ( <ref type="bibr" target="#b3">Dyer et al., 2016)</ref>:</p><formula xml:id="formula_5">P (x, y) = P (a) = m t=1 P (a t |a 1 , · · · , a t−1 ), (3)</formula><p>where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation <ref type="formula" target="#formula_2">(2)</ref>. RNNG and our model differ in how they compute the conditioning event (z 1 , · · · , z t−1 ): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM's hidden state as shown in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Our model, the model of <ref type="bibr">Zaremba et al. (2014)</ref> applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees:</p><formula xml:id="formula_6">P (x, y) = P (z) = m t=1 P (z t |z 1 , · · · , z t−1 ) = m t=1 P (z t |h t ) = m t=1 softmax(W h t )[z t ],</formula><p>where h t is a hidden state of an LSTM. Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y (x), whose size is polynomial, and use LSTM-LM to find y that satisfies</p><formula xml:id="formula_7">argmax y ∈Y (x) P (x, y ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hyper-parameters</head><p>The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50. We initialize starting states with previous minibatch's last hidden states <ref type="bibr" target="#b23">(Sutskever, 2013)</ref>. The forget gate bias is initialized to be one <ref type="table">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We describe datasets we use for evaluation, detail training and development processes. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing <ref type="formula" target="#formula_2">(23)</ref>    al., 2015) because in preliminary experiments Charniak parser <ref type="bibr" target="#b1">(Charniak, 2000</ref>) performed better when trained on all of 24 million trees than when trained on resampled two million trees. Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as <ref type="bibr" target="#b3">Dyer et al. (2016)</ref> do with their discriminative and generative models. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and Development</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Supervision</head><p>We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7. At the beginning of each epoch, we shuffle the order of trees in the training data. Both perplexity and F 1 of LSTM-LM (G) improve and then plateau <ref type="figure" target="#fig_2">(Figure 2)</ref>. Perplexity, the Base Final <ref type="bibr" target="#b24">Vinyals et al. (2015)</ref> 88.3 90.5 <ref type="bibr" target="#b3">Dyer et al. (2016)</ref> 89.8 92.4 LSTM-LM (G) 89.7 92.6 We also evaluate our model with varying n-best trees including optimal 51-best trees that contain gold trees (51 o ). As shown in Table 1, the LSTM-LM (G) is robust given sufficiently large n, i.e. 50, but does not exhibit its full capacity because of search errors in Charniak parser. We address this problem in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Semi-supervision</head><p>We unk words that appear at most once in the training (21,755 types). We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization. We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only. Training takes 26 epochs and 68 hours on a Titan X. LSTM-LM (GS) achieves 92.5 F 1 on the development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Supervision</head><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs ( <ref type="bibr" target="#b24">Vinyals et al., 2015</ref>) and RNNG ( <ref type="bibr" target="#b3">Dyer et al., 2016)</ref>, both of which are trained on the WSJ only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Semi-supervision</head><p>We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC) ( <ref type="bibr" target="#b24">Vinyals et al., 2015)</ref>; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs ( <ref type="bibr" target="#b14">Luong et al., 2016</ref>). We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers' parsing performance along with their training data is reported in <ref type="table">Table 3</ref>. LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Improved Semi-supervision</head><p>Due to search errors -good trees are missing in 50-best trees -in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y (x). To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y (x). As shown in <ref type="table">Table 3</ref>, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y (x). A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F 1 . When trees are converted to Stanford dependencies, 5 UAS and LAS are 95.9% and 94.1%, 6 more than 1% higher than those of the state of the art dependency parser ( <ref type="bibr" target="#b0">Andor et al., 2016)</ref>. Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered ( <ref type="bibr" target="#b12">Kong and Smith, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The generative parsing model we presented in this paper is very powerful. In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models <ref type="bibr" target="#b3">(Dyer et al., 2016)</ref>. We suspect building large models with character embeddings would lead to further improvement as in language modeling ( <ref type="bibr" target="#b11">Kim et al., 2016;</ref><ref type="bibr" target="#b10">Jozefowicz et al., 2016</ref>). We also wish to develop a complete parsing model using the LSTM-LM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Oracle Final</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Silver <ref type="bibr" target="#b8">Huang et al. (2010)</ref> --92.8 WSJ (40K) BLLIP (1.8M) <ref type="bibr" target="#b22">Shindo et al. (2012)</ref> --92.4 WSJ (40K) -Choe et al. <ref type="formula" target="#formula_0">(2015)</ref> --92.6 WSJ (40K) NYT (2M) <ref type="bibr" target="#b24">Vinyals et al. (2015)</ref> --92.8 HC (90K) HC (11M) <ref type="bibr" target="#b14">Luong et al. (2016)</ref> --93.0 HC (90K) HC (11M) Charniak (G) + LSTM-LM (G) 89.7 96.7 92.6 WSJ (40K) -Charniak (G) + LSTM-LM (GS) 89.7 96.7 93.1 WSJ (40K) NYT (0/10M) Charniak (GS) + LSTM-LM (G) 91.2 97.1 92.9 WSJ (40K) NYT (24M/0) Charniak (GS) + LSTM-LM (GS) 91.2 97.1 93.6 WSJ (40K) NYT (24M/10M) Charniak (GS) + E(LSTM-LMs (GS)) 91.2 97.1 93.8 WSJ (40K) NYT (24M/11.2M) <ref type="table">Table 3</ref>: Evaluation of models trained on the WSJ and additional resources. Note that the numbers of <ref type="bibr" target="#b24">Vinyals et al. (2015)</ref> and <ref type="bibr" target="#b14">Luong et al. (2016)</ref> are not directly comparable as their models are evaluated on OntoNotesstyle trees instead of PTB-style trees. E(LSTM-LMs (GS)) is an ensemble of eight LSTM-LMs (GS). X/Y in Silver column indicates the number of silver trees used to train Charniak parser and LSTM-LM. For the ensemble model, we report the maximum number of trees used to train one of LSTM-LMs (GS).</p><p>at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Figure 1: A tree (a) and its sequential form (b). There is a one-to-one mapping between a tree and its sequential form. (Part-of-speech tags are not used.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Jozefowicz et al., 2015) and the rest of model parameters are sampled from U(−0.05, 0.05). Dropout is applied to non-recurrent connections (Pham et al., 2014) and gradients are clipped when their norm is big- ger than 20 (Pascanu et al., 2013). The learning rate is 0.25 · 0.85 max(−15, 0) where is an epoch number. For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical soft- max (Morin and Bengio, 2005) or noise contrastive estimation (Gutmann and Hyvärinen, 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Perplexity and F 1 on the development set at each epoch during training.</figDesc><graphic url="image-1.png" coords="3,318.50,57.83,221.50,146.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : F 1 of models trained on WSJ. Base refers to performance of a single base parser and</head><label>2</label><figDesc></figDesc><table>Final that 
</table></figure>

			<note place="foot" n="3"> Dyer et al. (2016)&apos;s discriminative model performs comparably to Charniak (89.8 vs. 89.7).</note>

			<note place="foot" n="4"> The HC consists of 90,000 gold trees, from the WSJ, English Web Treebank and Question Treebank, and 11 million silver trees, whose sentence length distribution matches that of the WSJ, parsed and agreed on by Berkeley parser and ZPar.</note>

			<note place="foot" n="5"> Version 3.3.0. 6 We use the CoNLL evaluator available through the CoNLL website: ilk.uvt.nl/conll/software/eval.pl. Following the convention, we ignore punctuation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the NVIDIA corporation for its donation of a Titan X GPU, Tstaff of Computer Science</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum-entropy-inspired parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Meeting of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Syntactic parse fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Do Kook Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-training with products of latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An empirical comparison of parsing methods for stanford dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.4314</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ambiguity-aware ensemble training for semisupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL</title>
		<meeting>the Human Language Technology Conference of the NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">English gigaword fifth edition. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Vu Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian symbol-refined tree substitution grammars for syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and accurate shift-reduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
