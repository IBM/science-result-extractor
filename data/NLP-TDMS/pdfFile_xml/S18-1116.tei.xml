<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 5-6, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bernier-Colborne</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Barrì</surname></persName>
							<email>erecarolinebarriere@yahoo.ca</email>
						</author>
						<title level="a" type="main">CRIM at SemEval-2018 Task 9: A Hybrid Approach to Hypernym Discovery</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
						<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018) <address><addrLine>New Orleans, Louisiana</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="725" to="731"/>
							<date type="published">June 5-6, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This report describes the system developed by the CRIM team for the hypernym discovery task at SemEval 2018. This system exploits a combination of supervised projection learning and unsupervised pattern-based hypernym discovery. It was ranked first on the 3 sub-tasks for which we submitted results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the hypernym discovery task at SemEval 2018 is to predict the hypernyms of a query given a large vocabulary of candidate hypernyms. A query can be either a concept (e.g. cocktail or epistemology) or a named entity (e.g. Craig Anderson or City of Whitehorse). Two types of data were provided to train the systems: a large unlabeled text corpus and a small training set of examples comprising a query and its hypernyms. More details on this task may be found in the task description paper <ref type="bibr" target="#b3">(Camacho-Collados et al., 2018</ref>).</p><p>The system developed by the CRIM team for the task of hypernym discovery exploits a combination of two approaches: an unsupervised, pattern-based approach and a supervised, projection learning approach. These two approaches are described in Sections 2 and 3, then Section 4 describes our hybrid system and Section 5 presents our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pattern-Based Hypernym Discovery</head><p>Pattern-based approaches to relation extraction have been discussed in the literature for quite some time (see surveys by <ref type="bibr">Auger andBarrì ere (2008)</ref> and <ref type="bibr" target="#b11">Nastase et al. (2013)</ref>). They can be used to discover various relations, including domain-specific ones <ref type="bibr">(Halskov andBarrì ere, 2008</ref>) and more general ones, such as hypernymy. The patternbased approach to hypernym discovery was pioneered by <ref type="bibr" target="#b7">Hearst (1992)</ref>, who defined specific textual patterns (e.g. Y such as X) to mine hyponym/hypernym pairs from corpora.</p><p>This approach is known to suffer from low recall because it assumes that hyponym/hypernym pairs will occur together in one of these patterns, which is often not the case. For instance, using the training data of sub-task 1A, we found that the majority of training pairs never co-occur within the same paragraph in corpus 1A, let alone within a pattern that suggests hypernymy.</p><p>To increase recall, we extend the basic patternbased approach to hypernym discovery in two ways. First, we identify co-hyponyms for each query and add the hypernyms discovered for these terms to those found for the query. These cohyponyms are identified using patterns, and filtered based on distributional similarity using the embeddings described in Section 3.3. Furthermore, we discover additional hypernyms using a method based on the following assumptions: most multi-word expressions are compositional, and the prevailing head-modifier relation is hypernymy.</p><p>The co-hyponym patterns we use are limited to enumeration patterns (e.g. X 1 , X 2 and X 3 ). For hypernyms, we use an extended set of Hearst-like patterns which we selected empirically (e.g. Y such as X, Y other than X, not all Y are X, Y including X, Y especially X, Y like X, Y for example X, Y which includes X, X are also Y, X are all Y, not Y so much as X).</p><p>Our pattern-based hypernym discovery algorithm can be defined as follows: given a query q, 1. Create the empty set Q, which will contain an extended set of queries.</p><p>2. Search for the co-hyponym patterns in the corpus to discover co-hyponyms of q. Add these to Q and store their frequency (number of times a given co-hyponym was found using these patterns).</p><p>3. Score each co-hyponym q ∈ Q by multiplying the frequency of q by the cosine similarity of the embeddings of q and q . Rank the co-hyponyms in Q according to this score, keep the top n, 1 and discard the rest.</p><p>4. Add the original query q to Q.</p><p>5. Create the empty set of hypernyms H q .</p><p>6. For each query q ∈ Q, search for the hypernym patterns in the corpus to discover hypernyms of q . Add these to H q .</p><p>7. Add the head of each term in H q to this set, as well as the head of the original query q.</p><p>8. Score each candidate c ∈ H q by multiplying its normalized frequency 2 by the cosine similarity between the embeddings of c and q, and rank the candidates according to this score.</p><p>Although the pattern-based search for both cohyponyms and hypernyms can find terms not included in the provided vocabulary (which could also be useful), we discarded out-of-vocabulary terms because we had not learned embeddings for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Projections for Hypernym Discovery</head><p>Several supervised learning approaches based on word embeddings have recently been developed for the task of hypernym detection and the related task of hypernym discovery <ref type="bibr" target="#b2">(Camacho-Collados, 2017)</ref>. The general idea is to learn a function that takes as input the word embeddings of a query q and a candidate hypernym h and outputs the likelihood that there is a hypernymy relationship between q and h. To discover hypernyms for a given query q (rather than classify a given pair of words), we apply this decision function to all candidate hypernyms, and select the most likely candidates (or all those classified as hypernyms). This decision function can be learned in a supervised fashion using examples of pairs of words that are related by hypernymy and pairs that are not. The supervised model can take as input a combination of the embeddings of q and h, and different ways of combining the embeddings for this purpose have been used ( <ref type="bibr" target="#b1">Baroni et al., 2012;</ref><ref type="bibr" target="#b12">Roller et al., 2014;</ref><ref type="bibr" target="#b14">Weeds et al., 2014</ref>). In related work, models have been proposed that learn to project the embedding of q such that its projection is close to that of its hypernym h ( <ref type="bibr" target="#b5">Fu et al., 2014;</ref><ref type="bibr" target="#b15">Yamane et al., 2016;</ref><ref type="bibr" target="#b4">Espinosa-Anke et al., 2016)</ref>. This has been termed projection learning <ref type="bibr" target="#b13">(Ustalov et al., 2017</ref>). The decision function is then based on how close the projection of q is to a given candidate h. <ref type="bibr" target="#b5">Fu et al. (2014)</ref> introduced a model that learns multiple projection matrices representing different kinds of hypernymy relationships. In this model, each (q, h) pair is first assigned to a cluster based on their vector offsets, then projection matrices are learned for each cluster. Based on this work, Yamane et al. <ref type="formula">(2016)</ref> proposed a model that jointly learns the clusters and projection matrices.</p><p>We use a similar method to learn projections for hypernym discovery, but our approach differs from that of <ref type="bibr" target="#b15">Yamane et al. (2016)</ref> in several ways: our model performs a soft clustering of query-hypernym pairs rather than a hard clustering, and we modified the training algorithm in several ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Model</head><p>Given a query q and a candidate hypernym h, the model retrieves their embeddings e q , e h ∈ R d×1 using a lookup table. These embeddings were learned beforehand on a large unlabeled text corpus (i.e. the corpora provided for this task). The embedding e q is then multiplied by a 3-D tensor containing k square projection matrices φ i ∈ R d×d for i ∈ {1, . . . , k}, producing a matrix P ∈ R k×d containing the projections of e q :</p><formula xml:id="formula_0">P i = (φ i · e q ) T (1)</formula><p>The model then checks how close each of the k projections of e q are to e h by taking the dot product:</p><formula xml:id="formula_1">s = P · e h (2)</formula><p>The column vector s ∈ R k×1 is then fed to an affine transformation and a sigmoid activation function (in other words, a logistic regression classifier) to obtain an estimate of the likelihood that q and h are related by hypernymy:</p><formula xml:id="formula_2">y = σ(W · s + b)<label>(3)</label></formula><p>To discover the hypernyms of a given query, we compute the likelihood y for all candidates and select the top-ranked ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Training Algorithm</head><p>We train the model using negative sampling: for each positive example of a (query, hypernym) pair in the training data, we generate a fixed number m of negative examples by replacing the hypernym with a word randomly drawn from the vocabulary. <ref type="bibr">3</ref> We then train the model to output a likelihood (y) close to 1 for positive examples and close to 0 for negative examples. This is accomplished by minimizing the binary cross-entropy of the positive and negative training examples. For a particular example, this is computed as follows:</p><formula xml:id="formula_3">H(q, h, t) = t × log(y) + (1 − t) × log(1 − y)</formula><p>where q is a query, h is a candidate hypernym, t is the target (1 for positive examples, 0 for negative), and y is the likelihood predicted by the model. If we sum H for every example in the training set D (containing both the positive and negative examples), we obtain the cost function J = (q,h,t)∈D H(q, h, t). This function is minimized by gradient descent, using the Adam optimizer <ref type="bibr">4</ref> ( <ref type="bibr" target="#b8">Kingma and Ba, 2014)</ref>.</p><p>A few details of the setup we use for training are worth mentioning:</p><p>• We use a fixed number of projections (k) rather than the dynamic clustering algorithm of <ref type="bibr" target="#b15">Yamane et al. (2016)</ref>. For our official runs, we used k = 24.</p><p>• The word embeddings are normalized to unitlength before training.</p><p>• For the initialization of the projection matrices, we add random noise to an identity matrix, which means that at first, the projections of a query are simply k randomly corrupted copies of the query's embedding.</p><p>• We train the model on random mini-batches containing 32 positive examples and 32 × m negative examples (m being the number of negative examples).</p><p>• Dropout is applied to the embeddings e q and e h and the query projections P . For regularization, we also use gradient clipping, as well as early stopping.</p><p>3 Different ways of selecting the negative examples for this purpose have been proposed. See <ref type="bibr" target="#b13">Ustalov et al. (2017)</ref>. <ref type="bibr">4</ref> We use β1 = β2 = 0.9.</p><p>• We sample positive examples using a function based on the frequency of the hypernyms in the training data, such that we subsample (q, h) pairs where h occurs often in the training data. The probability of sampling (q, h) is given by:</p><formula xml:id="formula_4">P (q, h) = min h ∈D (freq(h )) freq(h)</formula><p>where freq(h) returns the frequency of h in the training data. 5</p><p>• The word embeddings are optimized (or "fine-tuned") during training.</p><p>• We use a multi-task learning setup whereby we train two separate logistic regression classifiers, each with their own parameters W and b, and use one for queries that are named entities, and the other for queries that are concepts. <ref type="bibr">6</ref> The rest of the parameters (i.e. the projection matrices φ) are shared.</p><p>The various hyperparameters mentioned above were tuned on the trial set (i.e. development set) provided for sub-task 1A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Word Embeddings</head><p>We learned term embeddings for all queries and candidates using the pre-tokenized corpora provided for sub-tasks 1A, 2A, and 2B. We preprocessed the corpora by converting all characters to lower case and replacing multi-word terms found in the vocabulary (candidates and lowercased queries) with a single token, starting with trigrams, then bigrams. <ref type="bibr">7</ref> We then used the skipgram algorithm with negative sampling ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>) to learn the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Augmentation</head><p>For one of our 2 runs, we experimented with a method to add synthetic examples to the positive examples in the training set provided (D). This was meant to provide additional training data and avoid overfitting the embeddings of the words in the training set. We use 2 heuristics to generate these synthetic examples:</p><p>1. Given a positive example (q, h) ∈ D, add (q , h) to the positive examples, where q is the nearest neighbour of q, based on the cosine similarity of the embeddings of all the words in the vocabulary. This was motivated by the observation that nearest neighbours were often co-hyponyms.</p><p>2. Given a query q and the set H q containing the hypernyms of q according to the training data, compute the α nearest neighbours of each hypernym in H q , and for each neighbour that is shared by at least 2 of the hypernyms in H q , add that neighbour to H q . 8</p><p>Negative examples are generated for each of the synthetic examples, as with the actual positive examples in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hybrid Hypernym Discovery</head><p>Our hybrid approach to hypernym discovery combines supervised projection learning and unsupervised pattern-based hypernym discovery (see Sections 2 and 3). To combine the outputs of the 2 systems, we take the top 100 candidates according to each, 9 normalize their scores and sum them, then rerank the candidates according to this new score. This reranking function favours candidates found by both systems, but also gives a chance to strong candidates found by a single system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>We submitted 2 runs on 3 of the 5 sub-tasks: 1A (general), 2A (medical), and 2B (music). The system outputs its top 15 predictions in all cases. The difference between the 2 runs is that for run 1, we used data augmentation (see Section 3.4) to train the supervised system -the same unsupervised output was used for both runs. We also submitted one run for cross-evaluation (training on 1A, but testing on 2A or 2B). First, we added the queries and candidates of 2A or 2B to those of 1A be-fore training embeddings on the corpus of 1A. <ref type="bibr">10</ref> These embeddings were used to train the supervised model on the 1A training data. We then combined the predictions of the supervised and unsupervised models on test set 2A/2B. <ref type="bibr">11</ref> A summary of our system's results is shown in <ref type="table">Table 1</ref>. This table shows the mean average precision (MAP), mean reciprocal rank (MRR) and precision at rank 1 (P@1) of our system and those of the 2 strongest baselines which were computed by the task organizers. The first is a supervised baseline <ref type="bibr">12</ref> and the second is based on the most frequent hypernyms in the training data. For more details, see <ref type="bibr" target="#b3">(Camacho-Collados et al., 2018</ref>).</p><p>Our hybrid system was ranked 1st on all three sub-tasks for which we submitted runs. As shown in <ref type="table">Table 1</ref>, the scores obtained using this system are much higher than the strongest baselines for this task. Furthermore, it is likely that we could improve our scores on 2A and 2B, since we only tuned the system on 1A.</p><p>If we compare runs 1 and 2 of our hybrid system, we see that data augmentation improved our scores slightly on 1A and 2B, and increased them by several points on 2A.</p><p>Our cross-evaluation results are better than the supervised baseline computed using the normal evaluation setup, so training our system on general-purpose data produced better results on a domain-specific test set than a strong, supervised baseline trained on the domain-specific data. <ref type="table">Table 1</ref> also shows the scores we would have obtained on the test set if we had used only the unsupervised (pattern-based) or supervised (projection learning) parts of our system. Note that the unsupervised system outperformed all other unsupervised systems evaluated on this task, and even outperformed the supervised baseline on 2A.</p><p>Combining the outputs of the 2 systems improves the best score of either system on all test sets, sometimes by as much as 10 points.</p><p>Notice also that the results obtained using only the supervised system indicate that data augmentation had a positive effect on our 2A scores only (compare runs 1 and 2), although our tests on the  <ref type="table">Table 1</ref>: Our system's results on test sets 1A, 2A, and 2B. The runs we submitted are the hybrid runs. The supervised and unsupervised runs were produced by using our 2 sub-systems separately. BaselineSUP is a strong, supervised baseline and BaselineMFH is the most-frequent-hypernym baseline. Cross-evaluation results were obtained by training the supervised system on 1A and evaluating on 2A or 2B.</p><p>trial set suggested it would also have a positive effect on our 1A scores. Given this observation, we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system. One possible explanation is that adding the synthetic examples makes the errors of the supervised system more different from those of the unsupervised system, and that this in turn makes the ensemble method more beneficial, but we haven't looked into this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Tests</head><p>To assess the influence of different aspects of the supervised system and its training algorithm, we carried out a few simple ablation tests on subtask 1A. The baseline for these tests is our supervised projection learning system -we did not apply pattern-based hypernym discovery for any of these tests. We used the setup of run 1 (with data augmentation) and used the trial set for early stopping. We conducted the following tests (one by one, without combining any of the ablations):</p><p>1. No subsampling: we sample positive examples uniformly from the training set.</p><p>2. No MTL: instead of multi-task learning (MTL), we use a single classifier for both named entities and concepts.</p><p>3. Random init: the weights of φ are initialized randomly, instead of adding random noise to an identity matrix. The results obtained on test set 1A are shown in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="bibr">13</ref> These results show that 2 of the techniques we used, namely subsampling and multitask learning, actually harmed our system's performance on test set 1A, although our experiments on the trial set suggested that they would be beneficial. This may be due to the small size of the trial set (i.e. 50 queries) or some difference in the underlying distributions of the trial and test sets.  On the other hand, fine-tuning the word embeddings during training seems to be one of the keys to the success of this approach, as are the use of multiple projection matrices, and the sampling of Query Predictions Suzy Favor Hamilton athlete, sportsperson, person, competitor, sport, olympic sport, . . . wicketkeeper cricketer, sportsperson, athlete, competitor, footballer, person, . . . aquamarine stone, crystal, precious stone, pebble, gem, rock, gemstone, . . . tenpence monetary unit, metal money, note of hand, person, silver coin, coin, . . . vegetarian dessert, dish, recipe, veggie, food product, organic food, meal, salad, . . . Local Group voluntary association, locale, coalition, club, country, mapmaking, . . . Swarthmore university, college, educational institution, school, student, . . . hypostasis figure of speech, intellection, philosophy, ordinary language, . . . multiple negative examples for each positive example. The way we initialize φ also seems to have helped quite a bit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAP MRR P@1</head><p>It is important to remember that a more thorough exploration of hyperparameter space would produce results very different from those of simple ablation tests such as these.</p><p>It is worth noting that our supervised model outperforms the supervised baseline provided for this task (see <ref type="table">Table 1</ref>) even when it exploits a single projection matrix, however the difference in scores between these 2 systems is only 2 or 3 points, depending on the evaluation metric.</p><p>We should also note that the supervised model is prone to overfitting, and we found early stopping to be particularly important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Analysis</head><p>We manually inspected the results of run 1 on some of the 1A test queries to get a better idea of the ability of the model to discover hypernyms, and to identify potential sources of errors. <ref type="table" target="#tab_3">Table 3</ref> shows a few of these test cases. Below we will outline a few of our observations, and will refer repeatedly to examples in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>The model is indeed able to discover valid hypernyms for both concepts and named entities. It seems that it can even handle very low-frequency queries in some cases (Suzy Favor Hamilton occurs only 5 times in the corpus), but we have not had the chance to investigate how sensitive the model is to term frequency.</p><p>Lexical memorization ( <ref type="bibr" target="#b9">Levy et al., 2015</ref>) can sometimes be observed. For example, person is the most frequent hypernym in the 1A training data, and the model often predicts this candidate incorrectly, even when its other top predictions are completely unrelated (e.g. tenpence).</p><p>The model can discover hypernyms of different senses of the same query (e.g. aquamarine, for which the top 15 predictions contain the valid hypernyms spectral color and primary color), and it sometimes discovers hypernyms for senses that are not represented in the gold standard (e.g. there is a college named Swarthmore, and hypostasis has senses related to linguistics and philosophy). It is likely that the senses that dominate the model's top predictions for a given query are its most frequent senses in the corpus. The case of vegetarian suggests that syntactic ambiguity is a source of errors: the predicted hypernyms include some that might be considered valid for the query vegetarian food, where vegetarian is an adjective, but not for the noun vegetarian.</p><p>Lastly, the model sometimes confuses concepts and named entities (e.g. Local Group, which refers to a group of galaxies). Preserving the true case of the characters in the corpus would mitigate this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>Our approach to hypernym discovery combines a novel supervised projection learning algorithm and an unsupervised pattern-based algorithm which exploits co-hyponyms in its search for hypernyms. This hybrid approach produced very good results on the hypernym discovery task, and was ranked first on all 3 sub-tasks for which we submitted results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 .</head><label>4</label><figDesc>Single projection: k = 1 instead of 24. 5. Single neg. example: m = 1 instead of 10. 6. Frozen embeddings: the word embeddings are not fine-tuned during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of ablation tests on test set 1A. The 
baseline is our supervised system (run 1). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of predictions made by our system (run 1) on the test queries of 1A. Correct predictions are in 
bold. Midline separates high-accuracy examples from low-accuracy examples. 

</table></figure>

			<note place="foot" n="1"> We set n = 5 empirically. 2 Frequencies were normalized in the range [0.05, 1.0].</note>

			<note place="foot" n="5"> On the training data of subtask 1A, this produces a sampling probability of 0.06 for the most frequent hypernym (person), while the least frequent hypernyms have a sampling probability of 1. 6 Multi-task learning was not used on subtask 2A, because all queries were concepts. 7 It is worth noting that a small number of candidates (e.g. less than 0.1% of candidates on corpus 1A) had a frequency of 0 in this preprocessed corpus, so we could not train an embedding for these candidates. These appear to be terms that always occur within a longer candidate.</note>

			<note place="foot" n="8"> We use α = 2. 9 Analyzing the individual and combined recall of the two systems at various ranks indicated that using more than 100 candidates would not increase recall.</note>

			<note place="foot" n="10"> Several of the domain-specific queries that were added to the vocab were not found in corpus 1A. We decided to only use the output of the unsupervised system in these cases. 11 Note that the unsupervised system used the corpora of 2A/2B, but no supervised learning was carried out on the training data of 2A/2B. 12 This is a &quot;vanilla&quot; version of TaxoEmbed (EspinosaAnke et al., 2016).</note>

			<note place="foot" n="13"> Note that the baseline results are slightly different than those shown in Table 1 for run 1 (supervised), because we retrained the model to get the results on the trial set, and a random number generator used during training was not seeded with a fixed value.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was conducted while both authors were affiliated with the Computer Research Institute of Montréal (CRIM), and was supported by the Natural Sciences and Engineering Research Council of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Patternbased approaches to semantic relation extraction: A state-of-the-art. Terminology, Special Issue on Pattern-based Approaches to Semantic Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Auger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolinebarrì</forename><surname>Ere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Why we have switched from building full-fledged taxonomies to simply detecting hypernymy relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno>abs/1703.04178</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vered Shwartz, Roberto Navigli, and Horacio Saggion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018)</title>
		<meeting>the 12th International Workshop on Semantic Evaluation (SemEval-2018)<address><addrLine>New Orleans, LA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>SemEval-2018 Task 9: Hypernym Discovery</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised distributional hypernym discovery via domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Webbased extraction of semantic relation instances for terminology work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Halskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolinebarrì</forename><surname>Ere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Terminology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="44" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics</title>
		<meeting>the 14th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Diarmuid´ODiarmuid´ Diarmuid´O Séaghdha, and Stan Szpakowicz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Morgan &amp; Claypool, Toronto</pubPlace>
		</imprint>
	</monogr>
	<note>Semantic Relations Between Nominals</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Negative sampling improves hypernymy extraction based on projection learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Arefyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="543" to="550" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COL-ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributional hypernym generation by jointly learning clusters and projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josuke</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Takatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1871" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
