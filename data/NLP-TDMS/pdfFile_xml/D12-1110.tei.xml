<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2012-07">July 2012. 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
							<email>richard@socher.org</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
						<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning <address><addrLine>Jeju Island, Korea</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="12" to="14"/>
							<date type="published" when="2012-07">July 2012. 2012</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the com-positional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions ( <ref type="bibr" target="#b17">Jones et al., 2006</ref>), fact extraction for information retrieval <ref type="bibr" target="#b24">(Pas¸caPas¸ca et al., 2006</ref>) and automatic annotation of text with disambiguated Wikipedia links <ref type="bibr" target="#b30">(Ratinov et al., 2011</ref>), among many others ( <ref type="bibr" target="#b38">Turney and Pantel, 2010)</ref>. In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Such vectors have been shown to correlate well with human judgments of word similarity ( <ref type="bibr" target="#b14">Griffiths et al., 2007)</ref>. Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them <ref type="bibr">(Frege, 1892)</ref>. This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., <ref type="bibr" target="#b21">(Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b3">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b41">Zanzotto et al., 2010;</ref><ref type="bibr" target="#b40">Yessenalina and Cardie, 2011;</ref><ref type="bibr" target="#b37">Socher et al., 2011c</ref>) (see related work). We extend these approaches with a more general and powerful model of semantic composition.</p><p>We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) representation. The vector captures the meaning of that constituent. The matrix captures how it modifies the meaning of the other word that it combines with. A representation for a longer phrase is computed bottom-up by recursively combining the words according to the syntactic structure of a parse tree. Since the model uses the MV representation with a neural network as the final merging function, we call our model a matrix-vector recursive neural network (MV-RNN).</p><p>We show that the ability to capture semantic compositionality in a syntactically plausible way translates into state of the art performance on various tasks. The first experiment demonstrates that our model can learn fine-grained semantic compositionality. The task is to predict a sentiment distribution over movie reviews of adverb-adjective pairs such as unbelievably sad or really awesome. The MV-RNN is the only model that is able to properly negate sentiment when adjectives are combined with not. The MV-RNN outperforms previous state of the art models on full sentence sentiment prediction of movie reviews. The last experiment shows that the MV-RNN can also be used to find relationships between words using the learned phrase vectors. The relationship between words is recursively constructed and composed by words of arbitrary type in the variable length syntactic path between them. On the associated task of classifying relationships between nouns in arbitrary positions of a sentence the model outperforms all previous approaches on the <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 competition (Hendrickx et al., 2010</ref>. It outperforms all but one of the previous approaches without using any hand-designed semantic resources such as WordNet or FrameNet. By adding WordNet hypernyms, POS and NER tags our model outperforms the state of the art that uses significantly more resources. The code for our model is available at www.socher.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MV-RNN: A Recursive Matrix-Vector Model</head><p>The dominant approach for building representations of multi-word units from single word vector representations has been to form a linear combination of the single word representations, such as a sum or weighted average. This happens in information retrieval and in various text similarity functions based on lexical similarity. These approaches can work well when the meaning of a text is literally "the sum of its parts", but fails when words function as operators that modify the meaning of another word: the meaning of "extremely strong" cannot be captured as the sum of word representations for "extremely" and "strong."</p><p>The model of <ref type="bibr" target="#b37">Socher et al. (2011c)</ref> provided a new possibility for moving beyond a linear combination, through use of a matrix W that multiplied the word vectors (a, b), and a nonlinearity function g (such as a sigmoid or tanh). They compute the parent vector p that describes both words as</p><formula xml:id="formula_0">p = g W a b<label>(1)</label></formula><p>and apply this function recursively inside a binarized parse tree so that it can compute vectors for multiword sequences. Even though the nonlinearity allows to express a wider range of functions, it is almost certainly too much to expect a single fixed W matrix to be able to capture the meaning combination effects of all natural language operators. After all, inside the function g, we have the same linear transformation for all possible pairs of word vectors. Recent work has started to capture the behavior of natural language operators inside semantic vector spaces by modeling them as matrices, which would allow a matrix for "extremely" to appropriately modify vectors for "smelly" or "strong" <ref type="bibr">(Ba- roni and Zamparelli, 2010;</ref><ref type="bibr" target="#b41">Zanzotto et al., 2010)</ref>. These approaches are along the right lines but so far have been restricted to capture linear functions of pairs of words whereas we would like nonlinear functions to compute compositional meaning representations for multi-word phrases or full sentences.</p><p>The MV-RNN combines the strengths of both of these ideas by (i) assigning a vector and a matrix to every word and (ii) learning an input-specific, nonlinear, compositional function for computing vector and matrix representations for multi-word sequences of any syntactic type. Assigning vector-matrix representations to all words instead of only to words of one part of speech category allows for greater flexibility which benefits performance. If a word lacks operator semantics, its matrix can be an identity matrix. However, if a word acts mainly as an operator, such as "extremely", its vector can become close to zero, while its matrix gains a clear operator meaning, here magnifying the meaning of the modified word in both positive and negative directions.</p><p>In this section we describe the initial word representations, the details of combining two words as well as the multi-word extensions. This is followed by an explanation of our training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Matrix-Vector Neural Word Representation</head><p>We represent a word as both a continuous vector and a matrix of parameters. We initialize all word vectors x ∈ R n with pre-trained 50-dimensional word vectors from the unsupervised model of <ref type="bibr">Col- lobert and Weston (2008)</ref>. Using Wikipedia text, their model learns word vectors by predicting how likely it is for each word to occur in its context. Similar to other local co-occurrence based vector space models, the resulting word vectors capture syntactic and semantic information. Every word is also associated with a matrix X. In all experiments, we initialize matrices as X = I + , i.e., the identity plus a small amount of Gaussian noise. If the vectors have dimensionality n, then each word's matrix has dimensionality X ∈ R n×n . While the initialization is random, the vectors and matrices will subsequently be modified to enable a sequence of words to compose a vector that can predict a distribution over semantic labels. Henceforth, we represent any phrase or sentence of length m as an ordered list of vectormatrix pairs ((a, A), . . . , (m, M )), where each pair is retrieved based on the word at that position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Composition Models for Two Words</head><p>We first review composition functions for two words. In order to compute a parent vector p from two consecutive words and their respective vectors a and b, <ref type="bibr" target="#b21">Mitchell and Lapata (2010)</ref> give as their most general function: p = f (a, b, R, K),where R is the a-priori known syntactic relation and K is background knowledge.</p><p>There are many possible functions f . For our models, there is a constraint on p which is that it has the same dimensionality as each of the input vectors. This way, we can compare p easily with its children and p can be the input to a composition with another word. The latter is a requirement that will become clear in the next section. This excludes tensor products which were outperformed by simpler weighted addition and multiplication methods in <ref type="bibr" target="#b21">(Mitchell and Lapata, 2010</ref>).</p><p>We will explore methods that do not require any manually designed semantic resources as background knowledge K. No explicit knowledge about the type of relation R is used. Instead we want the model to capture this implicitly via the learned matrices. We propose the following combination function which is input dependent:</p><formula xml:id="formula_1">p = f A,B (a, b) = f (Ba, Ab) = g W Ba Ab ,</formula><p>(2) where A, B are matrices for single words, the global W ∈ R n×2n is a matrix that maps both transformed words back into the same n-dimensional space. The element-wise function g could be simply the identity function but we use instead a nonlinearity such as the sigmoid or hyperbolic tangent tanh. Such a nonlinearity will allow us to approximate a wider range of functions beyond purely linear functions. We can also add a bias term before applying g but omit this for clarity. Rewriting the two transformed vectors as one vector z, we get p = g(W z) which is a single layer neural network. In this model, the word matrices can capture compositional effects specific to each word, whereas W captures a general composition function.</p><p>This function builds upon and generalizes several recent models in the literature. The most related work is that of <ref type="bibr" target="#b21">(Mitchell and Lapata, 2010;</ref><ref type="bibr">Zan- zotto et al., 2010</ref>) who introduced and explored the composition function p = Ba + Ab for word pairs. This model is a special case of Eq. 2 when we set W = [II] (i.e. two concatenated identity matrices) and g(x) = x (the identity function). <ref type="bibr" target="#b3">Baroni and Zamparelli (2010)</ref> computed the parent vector of adjective-noun pairs by p = Ab, where A is an adjective matrix and b is a vector for a noun. This cannot capture nouns modifying other nouns, e.g., disk drive. This model too is a special case of the above model with B = 0 n×n . Lastly, the models of ( <ref type="bibr" target="#b36">Socher et al., 2011b;</ref><ref type="bibr" target="#b37">Socher et al., 2011c;</ref><ref type="bibr" target="#b35">Socher et al., 2011a</ref>) as described above are also special cases with both A and B set to the identity matrix. We will compare to these special cases in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>… very good movie … (a , A) (b , B) (c , C)</head><p>Matrix-Vector Recursive Neural Network </p><formula xml:id="formula_2">(p1 , P1) ( p2, P2 ) p2 = g(W ) P2 = WM Cp1 P1c [ ] P1 C [ ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Recursive Compositions of Multiple Words and Phrases</head><p>This section describes how we extend a word-pair matrix-vector-based compositional model to learn vectors and matrices for longer sequences of words.</p><p>The main idea is to apply the same function f to pairs of constituents in a parse tree. For this to work, we need to take as input a binary parse tree of a phrase or sentence and also compute matrices at each nonterminal parent node. The function f can be readily used for phrase vectors since it is recursively compatible (p has the same dimensionality as its children). For computing nonterminal phrase matrices, we define the function</p><formula xml:id="formula_3">P = f M (A, B) = W M A B ,<label>(3)</label></formula><p>where W M ∈ R n×2n , so P ∈ R n×n just like each input matrix. After two words form a constituent in the parse tree, this constituent can now be merged with another one by applying the same functions f and f M . For instance, to compute the vectors and matrices depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>, we first merge words a and b and their matrices:</p><formula xml:id="formula_4">p 1 = f (Ba, Ab), P 1 = f M (A, B).</formula><p>The resulting vector-matrix pair (p 1 , P 1 ) can now be used to compute the full phrase when combining it with word c and computing p 2 = f (Cp 1 , P 1 c), P 2 = f M (P 1 , C). The model computes vectors and matrices in a bottom-up fashion, applying the functions f, f M to its own previous output (i.e. recursively) until it reaches the top node of the tree which represents the entire sentence.</p><p>For experiments with longer sequences we will compare to standard RNNs and the special case of the MV-RNN that computes the parent by p = Ab + Ba, which we name the linear Matrix-Vector Recursion model (linear MVR). Previously, this model had not been trained for multi-word sequences. Sec. 6 talks about alternatives for compositionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Objective Functions for Training</head><p>One of the advantages of RNN-based models is that each node of a tree has associated with it a distributed vector representation (the parent vector p) which can also be seen as features describing that phrase. We train these representations by adding on top of each parent node a simple softmax classifier to predict a class distribution over, e.g., sentiment or relationship classes:</p><formula xml:id="formula_5">d(p) = softmax(W label p). If there are K labels, then d ∈ R K is a K-dimensional multinomial distribution.</formula><p>For the applications below (excluding logic), the corresponding error function E(s, t, θ) that we minimize for a sentence s and its tree t is the sum of cross-entropy errors at all nodes.</p><p>The only other methods that use this type of objective function are <ref type="bibr" target="#b36">(Socher et al., 2011b;</ref><ref type="bibr" target="#b37">Socher et al., 2011c</ref>), who also combine it with either a score or reconstruction error. Hence, for comparisons to other related work, we need to merge variations of computing the parent vector p with this classifier. The main difference is that the MV-RNN has more flexibility since it has an input specific recursive function f A,B to compute each parent. In the following applications, we will use the softmax classifier to predict both sentiment distributions and noun-noun relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Learning</head><formula xml:id="formula_6">Let θ = (W, W M , W label , L, L M )</formula><p>be our model parameters and λ a vector with regularization hyperparameters for all model parameters. L and L M are the sets of all word vectors and word matrices. The gradient of the overall objective function J becomes:</p><formula xml:id="formula_7">∂J ∂θ = 1 N (x,t) ∂E(x, t; θ) ∂θ + λθ.<label>(4)</label></formula><p>To compute this gradient, we first compute all tree nodes (p i , P i ) from the bottom-up and then take derivatives of the softmax classifiers at each node in the tree from the top down. Derivatives are computed efficiently via backpropagation through structure ( <ref type="bibr" target="#b12">Goller and Küchler, 1996)</ref>. Even though the objective is not convex, we found that L-BFGS run over the complete training data (batch mode) minimizes the objective well in practice and convergence is smooth. For more information see <ref type="bibr" target="#b34">(Socher et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Low-Rank Matrix Approximations</head><p>If every word is represented by an n-dimensional vector and additionally by an n × n matrix, the dimensionality of the whole model may become too large with commonly used vector sizes of n = 100. In order to reduce the number of parameters, we represent word matrices by the following low-rank plus diagonal approximation:</p><formula xml:id="formula_8">A = U V + diag(a),<label>(5)</label></formula><p>where U ∈ R n×r , V ∈ R r×n , a ∈ R n and we set the rank for all experiments to r = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Discussion: Evaluation and Generality</head><p>Evaluation of compositional vector spaces is a complex task. Most related work compares similarity judgments of unsupervised models to those of human judgments and aims at high correlation. These evaluations can give important insights. However, even with good correlation the question remains how these models would perform on downstream NLP tasks such as sentiment detection. We experimented with unsupervised learning of general vector-matrix representations by having the MV-RNN predict words in their correct context. Initializing the models with these general representations, did not improve the performance on the tasks we consider. For sentiment analysis, this is not surprising since antonyms often get similar vectors during unsupervised learning from co-occurrences due to high similarity of local syntactic contexts. In our experiments, the high prediction performance came from supervised learning of meaning representations using labeled data. While these representations are task-specific, they could be used across tasks in a multi-task learning setup. However, in order to fairly compare to related work, we use only the supervised data of each task. Before we describe our fullscale experiments, we analyze the model's expressive powers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Analysis</head><p>This section analyzes the model with two proof-ofconcept studies. First, we examine its ability to learn operator semantics for adverb-adjective pairs. If a model cannot correctly capture how an adverb operates on the meaning of adjectives, then there's little chance it can learn operators for more complex relationships. The second study analyzes whether the MV-RNN can learn simple boolean operators of propositional logic such as conjunctives or negation from truth values. Again, if a model did not have this ability, then there's little chance it could learn these frequently occurring phenomena from the noisy language of real texts such as movie reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Predicting Sentiment Distributions of Adverb-Adjective Pairs</head><p>The first study considers the prediction of finegrained sentiment distributions of adverb-adjective pairs and analyzes different possibilities for computing the parent vector p. The results show that the MV-RNN operators are powerful enough to capture the operational meanings of various types of adverbs. For example, very is an intensifier, pretty is an attenuator, and not can negate or strongly attenuate the positivity of an adjective. For instance not great is still pretty good and not terrible; see Potts (2010) for details. We use a publicly available IMDB dataset of extracted adverb-adjective pairs from movie reviews. <ref type="bibr">1</ref> The dataset provides the distribution over star ratings: Each consecutive word pair appears a certain number of times in reviews that have also associated with them an overall rating of the movie. After normalizing by the total number of occurrences, one gets a multinomial distribution over ratings. Only word pairs that appear at least 50 times are kept. Of the remaining pairs, we use 4211 randomly sampled ones for training and a separate set of 1804 for testing. We never give the algorithm sentiment distributions for single words, and, while single words overlap between training and testing, the test set consists of never before seen word pairs.</p><p>The softmax classifier is trained to minimize the cross entropy error. Hence, an evaluation in terms of KL-divergence is the most reasonable choice. It is  defined as KL(g||p) = i g i log(g i /p i ), where g is the gold distribution and p is the predicted one.</p><formula xml:id="formula_9">Method Avg KL Uniform 0.327 Mean train 0.193 p = 1 2 (a + b) 0.103 p = a ⊗ b 0.103 p = [a; b] 0.101 p = Ab 0</formula><p>We compare to several baselines and ablations of the MV-RNN model. An (adverb,adjective) pair is described by its vectors (a, b) and matrices (A, B). We cross-validated all models over regularization parameters for word vectors, the softmax classifier, the RNN parameter W and the word operators (10 −4 , 10 −3 ) and word vector sizes <ref type="bibr">(n = 6, 8, 10, 12, 15, 20)</ref>. All models performed best at vector sizes of below 12. Hence, it is the model's power and not the number of parameters that determines the performance. The table in <ref type="figure" target="#fig_2">Fig. 3</ref> shows the average KL-divergence on the test set. It shows that the idea of matrix-vector representations for all words and having a nonlinearity are both important. The MV-RNN which combines these two ideas is best able to learn the various compositional effects. The main difference in KL divergence comes from the few negation cases in the test set. <ref type="figure" target="#fig_2">Fig. 3</ref> shows examples of predicted distributions. Many of the predictions are accurate and similar between the top models. However, only the MV-RNN has enough expressive power to allow negation to completely shift the sentiment with respect to an adjective. A negated adjective carrying negative sentiment becomes slightly positive, whereas not awesome is correctly attenuated. All three top models correctly capture the U-shape of unbelievably sad. This pair peaks at both the negative and positive spectrum because it is ambiguous. When referring to the performance of actors, it is very negative, but, when talking about the plot, many people enjoy sad and thought-provoking movies. The p = Ab model does not perform well because it cannot model the fact that for an adjective like "sad," the operator of "unbelievably" behaves differently.   <ref type="figure">Figure 4</ref>: Training trees for the MV-RNN to learn propositional operators. The model learns vectors and operators for ∧ (and) and ¬ (negation). The model outputs the exact representations of false and true respectively at the top node. Hence, the operators can be combined recursively an arbitrary number of times for more complex logical functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Logic-and Vector-based Compositionality</head><p>Another natural question is whether the MV-RNN can, in general, capture some of the simple boolean logic that is sometimes found in language. In other words, can it learn some of the propositional logic operators such as and, or, not in terms of vectors and matrices from a few examples. Answering this question can also be seen as a first step towards bridging the gap between logic-based, formal semantics <ref type="bibr" target="#b22">(Montague, 1974)</ref> and vector space models.</p><p>The logic-based view of language accounts nicely for compositionality by directly mapping syntactic constituents to lambda calculus expressions. At the word level, the focus is on function words, and nouns and adjectives are often defined only in terms of the sets of entities they denote in the world. Most words are treated as atomic symbols with no relation to each other. There have been many attempts at automatically parsing natural language to a logical form using recursive compositional rules.</p><p>Conversely, vector space models have the attractive property that they can automatically extract knowledge from large corpora without supervision. Unlike logic-based approaches, these models allow us to make fine-grained statements about the semantic similarity of words which correlate well with human judgments ( <ref type="bibr" target="#b14">Griffiths et al., 2007)</ref>. Logic-based approaches are often seen as orthogonal to distributional vector-based approaches. However, One open question is whether vector-based models can learn some of the simple logic encountered in language such as negation or conjunctives. To this end, we illustrate in a simple example that our MV-RNN model and its learned word matrices (operators) have the ability to learn propositional logic operators such as ∧, ∨, ¬ (and, or, not). This is a necessary (though not sufficient) condition for the ability to pick up these phenomena in real datasets and tasks such as sentiment detection which we focus on in the subsequent sections.</p><p>Our setup is as follows. We train on 6 strictly right-branching trees as in <ref type="figure">Fig. 4</ref>. We consider the 1-dimensional case and fix the representation for true to (t = 1, T = 1) and false to (f = 0, F = 1). Fixing the operators to the 1 × 1 identity matrix 1 is essentially ignoring them. The objective is then to create a perfect reconstruction of (t, T ) or (f, F ) (depending on the formula), which we achieve by the least squares error between the top vector's representation and the corresponding truth value, e.g. for ¬f alse:</p><formula xml:id="formula_10">min ||p top − t|| 2 + ||P top − T || 2 .</formula><p>As our function g (see Eq. 2), we use a linear threshold unit: g(x) = max(min(x, 1), 0). Giving the derivatives computed for the objective function for the examples in <ref type="figure">Fig. 4</ref> to a standard L-BFGS optimizer quickly yields a training error of 0. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Predicting Movie Review Ratings</head><p>In this section, we analyze the model's performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and <ref type="bibr" target="#b26">Lee, 2005;</ref><ref type="bibr">Nak- agawa et al., 2010;</ref><ref type="bibr" target="#b37">Socher et al., 2011c</ref>). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser ( <ref type="bibr" target="#b18">Klein and Manning, 2003)</ref>. We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of <ref type="bibr" target="#b37">Socher et al. (2011c)</ref>. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Tree-CRF ( <ref type="bibr" target="#b23">Nakagawa et al., 2010)</ref> 77.3 RAE ( <ref type="bibr" target="#b37">Socher et al., 2011c)</ref> 77.7 Linear MVR 77.1 MV-RNN 79.0   <ref type="formula" target="#formula_0">(1)</ref> and negative (0) sentiment (S.) that of all methods only the MV-RNN predicted correctly (C: √ ) or could not classify as correct either (C: x). <ref type="table" target="#tab_2">Table 1</ref> shows comparisons to the system of <ref type="bibr">(Nak- agawa et al., 2010</ref>), a dependency tree based classification method that uses CRFs with hidden variables. The state of the art recursive autoencoder model of <ref type="bibr" target="#b37">Socher et al. (2011c)</ref> obtained 77.7% accuracy. Our new MV-RNN gives the highest performance, outperforming also the linear MVR (Sec. 2.2). <ref type="table" target="#tab_3">Table 2</ref> shows several hard examples that only the MV-RNN was able to classify correctly. None of the methods correctly classified the last two examples which require more world knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Classification of Semantic Relationships</head><p>The previous task considered global classification of an entire phrase or sentence. In our last experiment we show that the MV-RNN can also learn how a syntactic context composes an aggregate meaning of the semantic relationships between words. In particular, the task is finding semantic relationships between pairs of nominals. For instance, in the sentence "My <ref type="bibr">[apartment]</ref> e1 has a pretty large <ref type="bibr">[kitchen]</ref> e2 .", we want to predict that the kitchen and apartment are in a component-whole relationship. Predicting such  semantic relations is useful for information extraction and thesaurus construction applications. Many approaches use features for all words on the path between the two words of interest. We show that by building a single compositional semantics for the minimal constituent including both terms one can achieve a higher performance. This task requires the ability to deal with sequences of words of arbitrary type and length in between the two nouns in question. <ref type="figure" target="#fig_6">Fig. 5</ref> explains our method for classifying nominal relationships. We first find the path in the parse tree between the two words whose relation we want to classify. We then select the highest node of the path and classify the relationship using that node's vector as features. We apply the same type of MV-RNN model as in sentiment to the subtree spanned by the two words.</p><p>We use the dataset and evaluation framework of <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2010</ref>). There are 9 ordered relationships (with two directions) and an undirected other class, resulting in 19 classes. Among the relationships are: messagetopic, cause-effect, instrument-agency (etc. see <ref type="table" target="#tab_5">Ta- ble 3</ref> for list). A pair is counted as correct if the order of the words in the relationship is correct. <ref type="table" target="#tab_6">Table 4</ref> lists results for several competing methods together with the resources and features used by each method. We compare to the systems of the competition which are described in <ref type="bibr" target="#b15">Hendrickx et al. (2010)</ref> as well as the RNN and linear MVR. Most systems used a considerable amount of handdesigned semantic resources. In contrast to these methods, the MV-RNN only needs a parser for the tree structure and learns all semantics from unlabeled corpora and the training data. Only the SemEval training dataset is specific to this task, the re-  maining inputs and the training setup are the same as in previous sentiment experiments.</p><p>The best method on this dataset (Rink and Harabagiu, 2010) obtains 82.2% F1. In order to see whether our system can improve over this system, we added three features to the MV-RNN vector and trained another softmax classifier. The features and their performance increases were POS tags (+0.9); WordNet hypernyms (+1.3) and named entity tags (NER) of the two words (+0.6). Features were computed using the code of Ciaramita and Altun (2006). 3 With these features, the performance improved over the state of the art system. <ref type="table" target="#tab_5">Table 3</ref> shows random correct classification examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Distributional approaches have become omnipresent for the recognition of semantic similarity between words and the treatment of compositionality has seen much progress in recent years. Hence, we cannot do justice to the large amount of literature. Commonly, single words are represented as vectors of distributional characteristics -e.g., their frequencies in specific syntactic relations or their co-occurrences with given context words ( <ref type="bibr" target="#b25">Pado and Lapata, 2007;</ref><ref type="bibr">Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b38">Turney and Pantel, 2010)</ref>. These representations have proven very effective in sense discrimination and disambiguation <ref type="bibr" target="#b33">(Schütze, 1998)</ref>, automatic thesaurus extraction <ref type="bibr" target="#b19">(Lin, 1998;</ref><ref type="bibr" target="#b8">Curran, 2004</ref>) and selectional preferences.</p><p>There are several sophisticated ideas for compositionality in vector spaces. <ref type="bibr">Mitchell and Lap- ata (2010)</ref> present an overview of the most important compositional models, from simple vector addition and component-wise multiplication to tensor products, and convolution <ref type="bibr" target="#b20">(Metcalfe, 1990)</ref>. They measured the similarity between word pairs such as compound nouns or verb-object pairs and compared these with human similarity judgments. Simple vector averaging or multiplication performed best, hence our focus on related baselines above.</p><p>Other important models are tensor products <ref type="bibr" target="#b6">(Clark and Pulman, 2007)</ref>, quantum logic <ref type="bibr" target="#b39">(Widdows, 2008)</ref>, holographic reduced representations <ref type="bibr" target="#b27">(Plate, 1995)</ref> and the Compositional Matrix Space model <ref type="bibr" target="#b32">(Rudolph and Giesbrecht, 2010)</ref>. RNNs are related to autoencoder models such as the recursive autoassociative memory (RAAM) <ref type="bibr" target="#b28">(Pollack, 1990)</ref> or recurrent neural networks <ref type="bibr" target="#b9">(Elman, 1991)</ref>. <ref type="bibr" target="#b4">Bottou (2011)</ref> and <ref type="bibr" target="#b16">Hinton (1990)</ref> discussed related models such as recursive autoencoders for text understanding.</p><p>Our model builds upon and generalizes the models of <ref type="bibr" target="#b21">(Mitchell and Lapata, 2010;</ref><ref type="bibr">Baroni and Zam- parelli, 2010;</ref><ref type="bibr" target="#b41">Zanzotto et al., 2010;</ref><ref type="bibr" target="#b37">Socher et al., 2011c</ref>) (see Sec. 2.2). We compare to them in our experiments. <ref type="bibr" target="#b40">Yessenalina and Cardie (2011)</ref> introduce a sentiment analysis model that describes words as matrices and composition as matrix multiplication. Since matrix multiplication is associative, this cannot capture different scopes of negation or syntactic differences. Their model, is a special case of our encoding model (when you ignore vectors, fix the tree to be strictly branching in one direction and use as the matrix composition function P = AB). Since our classifiers are trained on the vectors, we cannot compare to this approach directly. Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. The trained matrices improve correlation with human judgments on the task of identifying relatedness of subjectverb-object triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduced a new model towards a complete treatment of compositionality in word vector spaces. Our model builds on a syntactically plausible parse tree and can handle compositional phenomena. The main novelty of our model is the combination of matrix-vector representations with a recursive neural network. It can learn both the meaning vectors of a word and how that word modifies its neighbors (via its matrix). The MV-RNN combines attractive theoretical properties with good performance on large, noisy datasets. It generalizes several models in the literature, can learn propositional logic, accurately predicts sentiment and can be used to classify semantic relationships between nouns in a sentence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. Each word and phrase is represented by a vector and a matrix, e.g., very = (a, A). The matrix is applied to neighboring vectors. The same function is repeated to combine the phrase very good with movie.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of how the MV-RNN merges a phrase with another word at a nonterminal node of a parse tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: Average KL-divergence for predicting sentiment distributions of unseen adverb-adjective pairs of the test set. See text for p descriptions. Lower is better. The main difference in the KL divergence comes from the few negation pairs in the test set. Right: Predicting sentiment distributions (over 1-10 stars on the x-axis) of adverbadjective pairs. Each row has the same adverb and each column the same adjective. Many predictions are similar between the two models. The RNN and linear MVR are not able to modify the sentiment correctly: not awesome is more positive than fairly awesome and not annoying has a similar shape as unbelievably annoying. Predictions of the linear MVR model are almost identical to the standard RNN for these examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>p = a ⊗ b, element-wise vector multiplication 3. p = [a; b], vector concatenation 4. p = Ab, similar to (Baroni and Lenci, 2010) 5. p = g(W [a; b]), RNN, similar to Socher et al. 6. p = Ab + Ba, Linear MVR, similar to (Mitchell and Lapata, 2010; Zanzotto et al., 2010) 7. p = g(W [Ba; Ab]), MV-RNN The final distribution is always predicted by a softmax classifier whose inputs p vary for each of the models. This objective function (see Sec. 2.4) is different to all previously published work except that of (Socher et al., 2011c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>false</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Garrette et al. (2011) recently introduced a combination of a vector space model inside a Markov Logic Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The MV-RNN learns vectors in the path connecting two words (dotted lines) to determine their semantic relationship. It takes into consideration a variable length sequence of various word types in that path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>false ∧ false false true ∧ false false false ∧ true true true ∧ true true ¬ false false ¬ true</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Accuracy of classification on full length movie 
review polarity (MR). 

S. C. Review sentence 
1 
√ 
The film is bright and flashy in all the right ways. 
0 
√ 
Not always too whimsical for its own good this 
strange hybrid of crime thriller, quirky character 
study, third-rate romance and female empowerment 
fantasy never really finds the tonal or thematic glue 
it needs. 
0 
√ 
Doesn't come close to justifying the hype that sur-
rounded its debut at the Sundance film festival two 
years ago. 
0 x 
Director Hoffman, his writer and Kline's agent 
should serve detention. 
1 x 
A bodice-ripper for intellectuals. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Hard movie review examples of positive</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Examples of correct classifications of ordered, semantic relations between nouns by the MV-RNN. Note that the final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketed words. The paths vary in length and the words vary in type.</figDesc><table>Classifier 
Feature Sets 
F1 
SVM 
POS, stemming, syntactic patterns 
60.1 
SVM 
word pair, words in between 
72.5 
SVM 
POS, WordNet, stemming, syntactic 
patterns 

74.8 

SVM 
POS, WordNet, morphological fea-
tures, thesauri, Google n-grams 

77.6 

MaxEnt 
POS, WordNet, morphological fea-
tures, noun compound system, the-
sauri, Google n-grams 

77.6 

SVM 
POS, WordNet, prefixes and other 
morphological features, POS, depen-
dency parse features, Levin classes, 
PropBank, FrameNet, NomLex-Plus, 
Google n-grams, paraphrases, Tex-
tRunner 

82.2 

RNN 
-
74.8 
Lin.MVR -
73.0 
MV-RNN -
79.1 
RNN 
POS,WordNet,NER 
77.6 
Lin.MVR POS,WordNet,NER 
78.7 
MV-RNN POS,WordNet,NER 
82.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Learning methods, their feature sets and F1 
results for predicting semantic relations between nouns. 
The MV-RNN outperforms all but one method without 
any additional feature sets. By adding three such features, 
it obtains state of the art performance. 

</table></figure>

			<note place="foot" n="1"> http://compprag.christopherpotts.net/reviews.html</note>

			<note place="foot" n="2"> www.socher.org</note>

			<note place="foot" n="3"> sourceforge.net/projects/supersensetag/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank for great discussions about the paper: John Platt, Chris Potts, Josh Tenenbaum, Mihai Surdeanu, Quoc Le and Kevin Miller. The authors gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181, and the DARPA Deep Learning program under contract number FA8650-10-C-7020. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entity-Origin(e1,e2) The [mother]e1 left her native [land]e2 about the same time and they were married in that city</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Avian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Message-Topic</title>
		<imprint/>
	</monogr>
	<note>influenza]e1 is an infectious disease caused by type a strains of the influenza [virus]e2. e2,e1) Roadside [attractions]e1 are frequently advertised with [billboards]e2 to attract tourists. Product-Producer(e1,e2) A child is told a [lie]e1 for several years by their [parents]e2 before he/she realizes that ..</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Entity-Destination(e1,e2) The accident has spread</title>
		<imprint/>
	</monogr>
	<note>oil]e1 into the [ocean]e2</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instrument-Agency(e2,e1) The core of the [analyzer]e1 identifies the paths using the constraint propagation [method]e2. Component-Whole(e2,e1) The size of a [tree]e1 [crown]e2 is strongly correlated with the growth of the tree. Content-Container(e1,e2) The hidden [camera]e1, found by a security guard, was hidden in a business card-sized [leaflet box]e2 placed at an unmanned ATM in Tokyo&apos;s Minato</title>
	</analytic>
	<monogr>
		<title level="m">Member-Collection(e2,e1) The siege started</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="673" to="721" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">From machine learning to machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>abs/1102.1808</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining symbolic and distributional models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Spring Symposium on Quantum Interaction</title>
		<meeting>the AAAI Spring Symposium on Quantum Interaction</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">From Distributional to Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distributed representations, simple recurrent networks, and grammatical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Machine Learning</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">1892. ¨ Uber Sinn und Bedeutung</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Frege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zeitschrift für Philosophie und philosophische Kritik</title>
		<imprint>
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integrating Logical Representations with Probabilistic Information using Markov Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Semantics</title>
		<meeting>the International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning taskdependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Küchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Networks (ICNN-96)</title>
		<meeting>the International Conference on Neural Networks (ICNN-96)</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Topics in semantic representation. Psychological Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mapping part-whole hierarchies into connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating query substitutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A compositive holographic associative recall model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Metcalfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="627" to="661" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">English as a formal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguaggi nella Societa e nella Tecnica</title>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="page" from="189" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dependency tree-based sentiment classification using CRFs with hidden variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kurohashi</surname></persName>
		</author>
		<editor>NAACL, HLT</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Names and similarities on the web: fact extraction in the fast lane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lifchits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="1990-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the negativity of negation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semantics and Linguistic Theory 20</title>
		<editor>David Lutz and Nan Li</editor>
		<meeting>Semantics and Linguistic Theory 20<address><addrLine>Ithaca, NY</addrLine></address></meeting>
		<imprint>
			<publisher>CLC Publications</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">UTD: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Giesbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="97" to="124" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop</title>
		<meeting>the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic vector products: Some initial investigations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second AAAI Symposium on Quantum Interaction</title>
		<meeting>the Second AAAI Symposium on Quantum Interaction</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Estimating linear models for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Korkontzelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fallucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>COLING</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
