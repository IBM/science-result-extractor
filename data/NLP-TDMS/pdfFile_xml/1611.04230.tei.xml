<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
							<email>nallapati@us.ibm.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1011 Kitchawan Road</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>1011 Kitchawan Road</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
							<email>zhou@us.ibm.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1011 Kitchawan Road</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level ex-tractive labels.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Document summarization is an important problem that has many applications in information retrieval and natural language understanding. Summarization techniques are mainly classified into two categories: extractive and abstractive. Extractive methods aim to select salient snippets, sentences or passages from documents, while abstractive summarization techniques aim to concisely paraphrase the information content in the documents.</p><p>A vast majority of the literature on document summarization is devoted to extractive summarization. Traditional methods for extractive summarization can be broadly classified into greedy approaches (e.g., <ref type="bibr" target="#b1">(Carbonell and Goldstein 1998)</ref>), graph based approaches (e.g., <ref type="bibr" target="#b2">(Radev and Erkan 2004)</ref>) and constraint optimization based approaches (e.g., <ref type="bibr" target="#b1">(McDonald 2007)</ref>).</p><p>Recently, neural network based approaches have become popular for extractive summarization. For example, ( <ref type="bibr" target="#b1">Kageback et al. 2014</ref>) employed the recursive autoencoder ( <ref type="bibr" target="#b3">Socher et al. 2011</ref>) to summarize documents, producing best performance on the Opinosis dataset <ref type="bibr" target="#b1">(Ganesan, Zhai, and Han 2010)</ref>. <ref type="bibr" target="#b4">(Yin and Pei 2015)</ref> applied Convolutional Neural Networks (CNN) to project sentences to continuous vector space and then select sentences by minimizing the cost based on their 'prestige' and 'diverseness', on the task of multi-document extractive summarization. Another related work is that of ( <ref type="bibr" target="#b1">Cao et al. 2016</ref>), who address the problem of query-focused multi-document summarization using CNNs, where they use weighted-sum pooling over sentence * Work was done while the author was an employee at IBM. representations to represent documents. The weights are learned from attention over sentence representations based on the query.</p><p>Recently, with the emergence of strong generative neural models for text <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref>, abstractive techniques are also becoming increasingly popular. For example, <ref type="bibr" target="#b3">(Rush, Chopra, and Weston 2015)</ref> proposed an attentional feed-forward network for abstractive summarization of sentences into short headlines. Further developing on their work, <ref type="bibr" target="#b2">(Nallapati, Zhou, and Xiang 2016)</ref> propose a set of recurrent neural network based encoder-decoder models that focus on various aspects of summarization like handling out-of-vocabulary words and modeling syntactic features of words in the sentence. In a follow-up work ( <ref type="bibr" target="#b2">Nallapati et al. 2016)</ref>, they also propose abstractive techniques for summarization of large documents into multi-sentence summaries, using the CNN/DailyMail corpus <ref type="bibr">1</ref> . Despite the emergence of abstractive techniques, extractive techniques are still attractive as they are less complex, less expensive, and generate grammatically and semantically correct summaries most of the time. In a very recent work, <ref type="bibr" target="#b1">Cheng and Lapata (2016)</ref> proposed an attentional encoder-decoder for extractive single-document summarization and applied to the CNN/Daily Mail corpus.</p><p>Like <ref type="bibr" target="#b1">(Cheng and Lapata 2016)</ref>, our work also focuses only on sentential extractive summarization of single documents using neural networks. We use the same corpus used by <ref type="bibr" target="#b2">(Nallapati et al. 2016</ref>) and (Cheng and Lapata 2016) for our experiments, since its large size makes it attractive for training deep neural networks such as ours, with several thousands of parameters.</p><p>Our main contributions are as follows: (a) we propose SummaRuNNer, a simple recurrent network based sequence classifier that outperforms or matches state-of-the-art models for extractive summarization; (b) the simple formulation of our model facilitates interpretable visualization of its decisions; and (c) we present a novel training mechanism that allows our extractive model to be trained end-to-end using abstractive summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SummaRuNNer</head><p>In this work, we treat extractive summarization as a sequence classification problem wherein, each sentence is visited sequentially in the original document order and a binary decision is made (taking into account previous decisions made) in terms of whether or not it should be included in the summary. We use a GRU based Recurrent Neural Network ( <ref type="bibr" target="#b1">Chung et al. 2014</ref>) as the basic building block of our sequence classifier. A GRU-RNN is a recurrent network with two gates, u called the update gate and r , the reset gate, and can be described by the following equations:</p><formula xml:id="formula_0">u j = σ(W ux x j + W uh h j−1 + b u ) (1) r j = σ(W rx x j + W rh h j−1 + b r ) (2) h j = tanh(W hx x j + W hh (r j h j−1 ) + b h ) (3) h j = (1 − u j ) h j + u j h j−1<label>(4)</label></formula><p>where the W's and b's are the parameters of the GRU-RNN and h j is the real-valued hidden-state vector at timestep j and x j is the corresponding input vector, and represents the Hadamard product.</p><p>Our model consists of a two-layer bi-directional GRU-RNN, whose graphical representation is presented in <ref type="figure">Figure  1</ref>. The first layer of the RNN runs at the word level, and computes hidden state representations at each word position sequentially, based on the current word embeddings and the previous hidden state. We also use another RNN at the word level that runs backwards from the last word to the first, and we refer to the pair of forward and backward RNNs as a bidirectional RNN. The model also consists of a second layer of bi-directional RNN that runs at the sentence-level and accepts the average-pooled, concatenated hidden states of the bi-directional word-level RNNs as input. The hidden states of the second layer RNN encode the representations of the sentences in the document. The representation of the entire document is then modeled as a non-linear transformation of the average pooling of the concatenated hidden states of the bi-directional sentence-level RNN, as shown below.</p><formula xml:id="formula_1">d = tanh(W d 1 N d N d j=1 [h f j , h b j ] + b),<label>(5)</label></formula><p>where h f j and h b j are the hidden states corresponding to the j th sentence of the forward and backward sentence-level RNNs respectively, N d is the number of sentences in the document and '[]' represents vector concatenation.</p><p>For classification, each sentence is revisited sequentially in a second pass, where a logistic layer makes a binary decision as to whether that sentence belongs to the summary, as shown below.</p><formula xml:id="formula_2">P (y j = 1|h j , s j , d) = σ(W c h j #(content) +h T j W s d #(salience) −h T j W r tanh(s j ) #(novelty) +W ap p a j #(abs. pos. imp.) +W rp p r j #(rel. pos. imp.) +b), #(bias term)<label>(6)</label></formula><p>where y j is a binary variable indicating whether the j th sentence is part of the summary, h j , the representation of the sentence is given by a non-linear transformation of the concatenated hidden states at the j th time step of the bidirectional sentence-level RNN, and s j is the dynamic representation of the summary at the j th sentence position, given by:</p><formula xml:id="formula_3">s j = j−1 i=1 h i P (y i = 1|h i , s i , d).<label>(7)</label></formula><p>In other words, the summary representation is simply a running weighted summation of all the sentence-level hidden states visited till sentence j, where the weights are given by their respective probabilities of summary membership. Figure 1: SummaRuNNer: A two-layer RNN based sequence classifier: the bottom layer operates at word level within each sentence, while the top layer runs over sentences. Double-pointed arrows indicate a bi-directional RNN. The top layer with 1's and 0's is the sigmoid activation based classification layer that decides whether or not each sentence belongs to the summary. The decision at each sentence depends on the content richness of the sentence, its salience with respect to the document, its novelty with respect to the accumulated summary representation and other positional features.</p><p>In Eqn. <ref type="formula" target="#formula_2">(6)</ref>, the term W c h j represents the information content of the j th sentence, h T j W s d denotes the salience of the sentence with respect to the document, h T j W r tanh(s j ) captures the redundancy of the sentence with respect to the current state of the summary 2 , while the next two terms model the notion of the importance of the absolute and relative position of the sentence with respect to the document. <ref type="bibr">3</ref> We consider p a and p r , the absolute and relative positional embeddings respectively, as model parameters as well.</p><p>We minimize the negative log-likelihood of the observed labels at training time.</p><formula xml:id="formula_4">l(W, b) = − N d=1 N d j=1 (y d j log P (y d j = 1|h d j , s d j , d d ) + (1 − y d j ) log(1 − P (y d j = 1|h d j , s d j , d d )) (8)</formula><p>where x is the document representation and y is the vector of its binary summary labels. At test time, the model emits probability of summary membership P (y j ) at each sentence sequentially, which is used as the model's soft prediction of the extractive summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extractive Training</head><p>In order to train our extractive model, we need ground truth in the form of sentence-level binary labels for each document, representing their membership in the summary. However, most summarization corpora only contain human written abstractive summaries as ground truth. To solve this problem, we use an unsupervised approach to convert the abstractive summaries to extractive labels. Our approach is based on the idea that the selected sentences from the document should be the ones that maximize the Rouge score with respect to gold summaries. Since it is computationally expensive to find a globally optimal subset of sentences that maximizes the Rouge score, we employ a greedy approach, where we add one sentence at a time incrementally to the summary, such that the Rouge score of the current set of selected sentences is maximized with respect to the entire gold summary . We stop when none of the remaining candidate sentences improves the Rouge score upon addition to the current summary set. We return this subset of sentences as the extractive ground-truth, which is used to train our RNN based sequence classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstractive Training</head><p>In this section, we propose a novel training technique to train SummaRuNNer abstractively, thus eliminating the need to generate approximate extractive labels. To train SummaRuNNer using reference summaries, we couple it with an RNN decoder that models the generation of abstractive summaries at training time only. The RNN decoder uses the summary representation at the last time-step of SummaRuNNer as context, which modifies Eqs. 1 through 3 as follows:</p><formula xml:id="formula_5">u k = σ(W ux x k + W uh h k−1 + W uc s −1 + b u ) r k = σ(W rx x k + W rh h k−1 + W rc s −1 + b r ) h k = tanh(W hx x k + W hh (r k h k−1 ) + W hc s −1 + b h )</formula><p>where s −1 is the summary representation as computed at the last sentence of the sentence-level bidirectional RNN of SummaRuNNer as shown in Eq. 7. The parameters of the decoder are distinguished from those of SummaRuNNer using the 'prime' notation, and the time-steps of the decoder use index k to distinguish word positions in the summary from sentence indices j in the original document. For each time-step of the decoder, the embedding of the word from the previous time-step is treated as its input x k .</p><p>Further, the decoder is equipped with a soft-max layer to emit a word at each time-step. The emission at each timestep is determined by a feed-forward layer f followed by a softmax layer that assigns p k , probabilities over the entire vocabulary at each time-step, as shown below.</p><formula xml:id="formula_6">f k = tanh(W f h h k + W f x x k + W f c s −1 + b f ) P v (w) k = softmax(W v f k + b v )</formula><p>Instead of optimizing the log-likelihood of the extractive ground truth as shown in Eq. 8, we minimize the negative log-likelihood of the words in the reference summary as follows.</p><formula xml:id="formula_7">l(W, b, W , b ) = − Ns k=1 log(P v (w k ))<label>(9)</label></formula><p>where N s is the number of words in the reference summary. At test time, we uncouple the decoder from SummaRuNNer and emit only the sentence-level extractive probabilities p(y j ) of Eq. 6. Intuitively, since the summary representation s −1 acts as the only information channel between the SummaRuNNer model and the decoder, maximizing the probability of abstractive summary words as computed by the decoder will require the model to learn a good summary representation which in turn depends on accurate estimates of extractive probabilities p(y j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Treating document summarization as a sequence classification model has been considered by earlier researchers. For example, ( <ref type="bibr" target="#b3">Shen et al. 2007</ref>) used Conditional Random Fields to binary-classify sentences sequentially. Our approach is different from theirs in the sense that we use RNNs in our model that do not require any handcrafted features for representing sentences and documents.</p><p>Since the sequence classifier requires sentence-level summary membership labels to train on, we used a simple greedy approach to convert the abstractive summaries to extractive labels. Similar approaches have been employed by other researchers such as <ref type="bibr" target="#b4">(Svore, Vanderwende, and Burges 2007)</ref>. Further, recently <ref type="bibr" target="#b1">(Cao et al. 2015</ref>) propose an ILP based approach to solve this problem optimally.</p><p>Most single-document summarization datasets available for research such as DUC corpora are not large enough to train deep learning models. Two recent papers ( <ref type="bibr">(Nalla- pati et al. 2016</ref>) and (Cheng and Lapata 2016)) solve this problem by proposing a new corpus based on news stories from CNN and Daily Mail that consist of around 280,000 documents and human generated summaries. Of these, the work of (Cheng and Lapata 2016) is the closest to our work since they also employ an extractive approach for summarization. Their model is based on an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence based on encoder's representations using an attention mechanism. Our model, when extractively trained, employs a single sequence model with no decoder, and therefore may have fewer parameters. Our abstractively trained model has a decoder too, but it is different from that of (Cheng and Lapata 2016) since our decoder is used to model the likelihood of abstractive gold summaries at training time, so as to eliminate the need for extractive labels. Their model, on the other hand, requires extractive labels even with the decoder. In fact, unlike our unsupervised greedy approach to convert abstractive summaries to extractive labels, (Cheng and Lapata 2016) chose to train a separate supervised classifier using manually created labels on a subset of the data. This may yield more accurate gold extractive labels, but incurs additional annotation costs.</p><p>The work of ( <ref type="bibr" target="#b2">Nallapati et al. 2016</ref>) also uses an encoderdecoder approach, but is fully abstractive in the sense that it generates its own summaries at test time. Our abstractive trainer comes close to their work, but only generates sentence-extraction probabilities at test time. We include comparison numbers with this work too, in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpora</head><p>For our experiments, we used the CNN/DailyMail corpus originally constructed by ( <ref type="bibr" target="#b1">Hermann et al. 2015</ref>) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in (Cheng and Lapata 2016) for extractive summarization and ( <ref type="bibr">Nal- lapati et al. 2016</ref>) for abstractive summarization. In order to make a fair comparison with the former, we left out the CNN subset of the corpus, as done by them. We also used the DUC 2002 single-document summarization dataset 4 consisting of 567 documents as an additional out-of-domain test set to evaluate our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>In our experiments below, we evaluate the performance of SummaRuNNer using different variants of the Rouge metric <ref type="bibr">5</ref>  Rouge recall metric at 75 words. We report the scores from Rouge-1, Rouge-2 and Rouge-L, which are computed using the matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by (Cheng and Lapata 2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend and Lapata 2010), and graph based approaches such as TGRAPH (Parveen, Ramsl, and Strube 2015) and URANK (Wan 2010) which achieve very high performance on this corpus. In addition, we also compare with the state-of-the art deep learning models from (Cheng and Lapata 2016) and ( <ref type="bibr" target="#b2">Nallapati et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SummaRuNNer Settings</head><p>We used 100-dimensional word2vec ( <ref type="bibr">Mikolov et al. 2013</ref>) embeddings trained on the CNN/Daily Mail corpus as our embedding initialization. We limited the vocabulary size to 150K and the maximum number of sentences per document to 100, and the maximum sentence length to 50 words, to speed up computation. We fixed the model hidden state size at 200. We used a batch size of 64 at training time, and adadelta <ref type="bibr" target="#b5">(Zeiler 2012</ref>) to train our model. We employed gradient clipping to regularize our model and an early stopping criterion based on validation cost. We trained SummaRuNNer both extractively as well as abstractively. When the model is abstractively trained, we denote it as SummaRuNNer-abs in the results.</p><p>At test time, picking all sentences with P (y = 1) ≥ 0.5 may not be an optimal strategy since the training data is very imbalanced in terms of summary-membership of sentences. Instead, we pick sentences sorted by the predicted probabilites until we exceed the length limit when limited-length Rouge is used for evaluation. When full-length F1 is used as the metric, we fixed the number of top sentences to be selected based on the validation set. <ref type="table">Table 1</ref> shows the performance comparison of SummaRuNNer with state-of-the-art model of (Cheng and Lapata 2016) and other baselines on the DailyMail corpus using Rouge recall with summary length restricted to 75 bytes. While the abstractively trained SummaRuNNer performs on par with the state-of-the-art model, the extractively trained model significantly improves over their model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Daily Mail corpus</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we report the performance of our model with respect to Rouge recall at 275 bytes of summary length. In this case, our abstractively trained model underperforms the extractive model of <ref type="bibr" target="#b1">(Cheng and Lapata 2016)</ref> while the extractively trained model is statistically indistinguishable from their model. This shows that the SummaRuNNer is better at picking the best sentence for summarization than the subsequent ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Summary:</head><p>Redpath has ended his eight-year association with Sale Sharks. Redpath spent five years as a player and three as a coach at sale. He has thanked the owners, coaches and players for their support.</p><p>Bryan Redpath has left his coaching role at Sale Sharks with immediate effect. 0.1 0.1 0.9 0.1 0.3</p><p>The 43 -year -old Scot ends an eight-year association with the Aviva Premiership side, having spent five years with them as a player and three as a coach.</p><p>0.9 0.6 0.9 0.9 0.7</p><p>Redpath returned to Sale in June 2012 as director of rugby after starting a coaching career at Gloucester and progressing to the top job at Kingsholm .</p><p>0.8 0.5 0.5 0.9 0.6</p><p>Redpath spent five years with Sale Sharks as a player and a further three as a coach but with Sale Sharks struggling four months into Redpath's tenure, he was removed from the director of rugby role at the Salford-based side and has since been operating as head coach . Figure 2: Visualization of SummaRuNNer output on a representative document. Each row is a sentence in the document, while the shadingcolor intensity is proportional to its probability of being in the summary, as estimated by the RNN-based sequence classifier. In the columns are the normalized scores from each of the abstract features in Eqn. <ref type="formula" target="#formula_2">(6)</ref> as well as the final prediction probability (last column). Sentence 2 is estimated to be the most salient, while the longest one, sentence 4, is considered the most content-rich, and not surprisingly, the first sentence the most novel. The third sentence gets the best position based score.</p><p>Rouge-1 Rouge-2 Rouge-L Lead-3 21.  One potential reason SummaRuNNer does not consistently outperform the extractive model of (Cheng and Lapata 2016) is the additional supervised training they used to create sentence-level extractive labels to train their model. Our model instead uses an unsupervised greedy approximation to create extractive labels from abstractive summaries, and as a result, may be more noisy than their ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on CNN/Daily Mail corpus</head><p>We also report the performance of SummaRuNNer on the joint CNN/Daily Mail corpus. The only other work that reports performance on this dataset is the abstractive encoderdecoder based model of ( <ref type="bibr" target="#b2">Nallapati et al. 2016)</ref>, in which they use full-length F1 as the metric since neural abstractive approaches can learn when to stop generating words in the summary. In order to do a fair comparison with their work, we use the same metric as them. On this dataset, SummaRuNNer significantly outperforms their model as shown in <ref type="table" target="#tab_5">Table 3</ref>. The superior performance of our model is not entirely surprising since abstractive summarization is a much harder problem, but the table serves to quantify the current performance gap between extractive and abstractive approaches to summarization. The results also demonstrate the difficulty of using the F1 metric for extractive summarization since SummaRuNNer, with its top three sentences with highest prediction probability as the summary, errs on the side of high recall at the expense of precision. Dynamically adjusting the summary length based on predicted probability distribution may help balance precision and recall and may further boost F1 performance, but we have not experimented with it in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rouge-1</head><p>Rouge  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on the Out-of-Domain DUC 2002 corpus</head><p>We also evaluated the models trained on the DailyMail corpus on the out-of-domain DUC 2002 set as shown in <ref type="table" target="#tab_7">Table 4</ref>. SummaRuNNer is again statistically on par with the model of (Cheng and Lapata 2016). However, both models perform worse than graph-based TGRAPH <ref type="bibr" target="#b2">(Parveen, Ramsl, and Strube 2015)</ref> and URANK (Wan 2010) algorithms, which are the state-of-the-art models on this corpus. Deep learning based supervised models such as SummaRuNNer and that of (Cheng and Lapata 2016) perform very well on the domain they are trained on, but may suffer from domain adaptation issues when tested on a different corpus such as DUC 2002. Graph based unsupervised approaches, on the other hand, may be more robust to domain variations.</p><p>Rouge  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>In addition to being a state-of-the-art performer, SummaRuNNer has the additional advantage of being very interpretable. The clearly separated terms in the classification layer (see Eqn. 6) allow us to tease out various factors responsible for the classification of each sentence. This is illustrated in <ref type="figure">Figure 2</ref>, where we display a representative document from our validation set along with normalized scores from each abstract feature responsible for its final classification. Such visualization is especially useful in explaining to the end-user the decisions made by the system. We also display a couple of example documents from the Daily Mail and DUC corpora highlighting the sentences chosen by SummaRuNNer and comparing them with the gold summary in <ref type="table" target="#tab_8">Table 5</ref>. The examples demonstrate qualitatively that SummaRuNNer performs a reasonably good job in identifying the key points of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose a very interpretable neural sequence model for extractive document summarization that allows intuitive visualization, and show that it is better performing than or is comparable to the state-of-the-art deep learning models.</p><p>We also propose a novel abstractive training mechanism to eliminate the need for extractive labels at training time, Document: @entity0 have an interest in @entity3 defender @en-tity2 but are unlikely to make a move until january . the 00 -year -old @entity6 captain has yet to open talks over a new contract at @entity3 and his current deal runs out in 0000 . @entity3 defender @entity2 could be targeted by @entity0 in the january transfer window @entity0 like @entity2 but do n't expect @entity3 to sell yet they know he will be free to talk to foreign clubs from january . @entity12 will make a 0million offer for @entity3 goalkeeper @entity14 this summer . the 00 -year -old is poised to leave @entity16 and wants to play for a @entity18 contender . @entity12 are set to make a 0million bid for @entity2 's @en-tity3 team -mate @entity14 in the summer Gold Summary: @entity2 's contract at @entity3 expires at the end of next season . 00 -year -old has yet to open talks over a new deal at @entity16 . @entity14 is poised to leave @entity3 at the end of the season Document: today , the foreign ministry said that control operations carried out by the corvette spiro against a korean-flagged as received ship fishing illegally in argentine waters were carried out " in accordance with international law and in coordination with the foreign ministry " . the foreign ministry thus approved the intervention by the argentine corvette when it discovered the korean ship chin yuan hsing violating argentine jurisdictional waters on 00 may . ... the korean ship , which had been fishing illegally in argentine waters , was sunk by its own crew after failing to answer to the argentine ship 's warnings . the crew was transferred to the chin chuan hsing , which was sailing nearby and approached to rescue the crew of the sinking ship ..... Gold Summary: the korean-flagged fishing vessel chin yuan hsing was scuttled in waters off argentina on 00 may 0000 . adverse weather conditions prevailed when the argentine corvette spiro spotted the korean ship fishing illegally in restricted argentine waters . the korean vessel did not respond to the corvette 's warning . instead , the korean crew sank their ship , and transferred to another korean ship sailing nearby . in accordance with a uk-argentine agreement , the argentine navy turned the surveillance of the second korean vessel over to the british when it approached within 00 nautical miles of the malvinas ( falkland ) islands . but this approach is still a couple of Rouge points below our extractive training on most datasets. We plan to further explore combining extractive and abstractive approaches as part of our future work. One simple approach could be to pre-train the extractive model using abstractive training. Further, we plan to construct a joint extractive-abstractive model where the predictions of our extractive component form stochastic intermediate units to be consumed by the abstractive component.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>'</head><label></label><figDesc>I would like to thank the owners, coaches, players and staff for all their help and support since I returned to the club in 2012.Also to the supporters who have been great with me both as a player and as a coach,' Redpath said.Also to the supporters who have been great with me both as a player and as a coach,' Redpath said.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>To compare with the latter, we used the joint CNN/Daily Mail corpora. Overall, we have 196,557 training documents, 12,147 vali- dation documents and 10,396 test documents from the Daily Mail corpus. If we also include the CNN subset, we have 286,722 training documents, 13,362 validation documents and 11,480 test documents. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>computed with respect to the gold summaries. To compare with (Cheng and Lapata 2016) on the Daily Mail corpus, we use limited length Rouge recall and 75 bytes and 275 bytes as reported by them. To compare with (Nallapati et al. 2016) on the CNN/Daily Mail corpus, we use the same full-length Rouge F1 metric used by the authors. On DUC 2002 corpus, following the official guidelines, we use the limited length</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of various models on the entire Daily Mail 

test set using the limited length recall variants of Rouge at 275 
bytes. SummaRuNNer is statistically indistinguishable from the 
model of Cheng et al, '16 at 95% C.I. on Rouge-1 and Rouge-2. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance comparison of abstractive and extractive 

models on the entire CNN Daily Mail test set using full-length F1 
variants of Rouge. SummaRuNNer is able to significantly outper-
form the abstractive state-of-the-art as well as the Lead-3 baseline 
(on Rouge-1 and Rouge-2). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of various models on the DUC 2002 set us-

ing the limited length recall variants of Rouge at 75 words. Sum-
maRuNNer is statistically within the margin of error at 95% C.I. 
with respect to Cheng et al '16, but both are lower than state-of-
the-art results. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example documents and gold summaries from Daily Mail 

(top) and DUC 2002 (bottom) corpora. The sentences chosen by 
SummaRuNNer for extractive summarization are highlighted in 
bold. 

</table></figure>

			<note place="foot" n="1"> https://github.com/deepmind/rc-data arXiv:1611.04230v1 [cs.CL] 14 Nov 2016</note>

			<note place="foot" n="2"> We squash the summary representation using the tanh operation so that the magnitude of summary remains the same for all time-steps. 3 The absolute position denotes the actual sentence number, whereas the relative position refers to a quantized representation that divides each document into a fixed number of segments and computes the segment ID of a given sentence.</note>

			<note place="foot" n="4"> http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html 5 http://www.berouge.com/Pages/default.aspx</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00125</idno>
		<idno>CoRR abs/1506.03340. [Kageback et al. 2014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond. The SIGNLL Conference on Computational Natural Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Nallapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="457" to="479" />
		</imprint>
	</monogr>
	<note>Lexrank: Graph-based lexical centrality as salience in text summarization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chopra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston ;</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
	</analytic>
	<monogr>
		<title level="m">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Proceedings of IJCAI</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards a unified approach to simultaneous single-document and multidocument summarizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanderwende</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">565574</biblScope>
			<biblScope unit="page" from="1383" to="1389" />
		</imprint>
	</monogr>
	<note>Proceedings of the 24th International Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
