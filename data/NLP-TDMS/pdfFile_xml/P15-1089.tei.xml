<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T07:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tweet Normalization with Syllables</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software Eng. Beijing U. of Posts &amp; Telecom</orgName>
								<orgName type="department" key="dep2">School of Electr. &amp; Comp. Eng. Georgia Institute of Technology Atlanta</orgName>
								<orgName type="institution">STCA Microsoft</orgName>
								<address>
									<postCode>100876, 100084, 30332-0250</postCode>
									<settlement>Beijing, Beijing</settlement>
									<region>GA</region>
									<country>China, China, USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
							<email>yxia@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software Eng. Beijing U. of Posts &amp; Telecom</orgName>
								<orgName type="department" key="dep2">School of Electr. &amp; Comp. Eng. Georgia Institute of Technology Atlanta</orgName>
								<orgName type="institution">STCA Microsoft</orgName>
								<address>
									<postCode>100876, 100084, 30332-0250</postCode>
									<settlement>Beijing, Beijing</settlement>
									<region>GA</region>
									<country>China, China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software Eng. Beijing U. of Posts &amp; Telecom</orgName>
								<orgName type="department" key="dep2">School of Electr. &amp; Comp. Eng. Georgia Institute of Technology Atlanta</orgName>
								<orgName type="institution">STCA Microsoft</orgName>
								<address>
									<postCode>100876, 100084, 30332-0250</postCode>
									<settlement>Beijing, Beijing</settlement>
									<region>GA</region>
									<country>China, China, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tweet Normalization with Syllables</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="920" to="928"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a syllable-based method for tweet normalization to study the cognitive process of non-standard word creation in social media. Assuming that syllable plays a fundamental role in forming the non-standard tweet words, we choose syllable as the basic unit and extend the conventional noisy channel model by incorporating the syllables to represent the word-to-word transitions at both word and syllable levels. The syllables are used in our method not only to suggest more candidates, but also to measure similarity between words. Novelty of this work is threefold: First, to the best of our knowledge, this is an early attempt to explore syllables in tweet normalization. Second, our proposed normalization method relies on unlabeled samples, making it much easier to adapt our method to handle non-standard words in any period of history. And third, we conduct a series of experiments and prove that the proposed method is advantageous over the state-of-art solutions for tweet normalization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to the casual nature of social media, there exists a large number of non-standard words in text expressions which make it substantially different from formal written text. It is reported in ( ) that more than 4 million distinct out-of-vocabulary (OOV) tokens are found in the Edinburgh Twitter corpus ( <ref type="bibr" target="#b18">Petrovic et al., 2010)</ref>. This variation poses challenges when performing natural language processing (NLP) tasks <ref type="bibr" target="#b20">(Sproat et al., 2001</ref>) based on such texts. Tweet normalization, aiming at converting these OOV non-standard words into their in-vocabulary (IV) formal forms, is therefore viewed as a very important pre-processing task.</p><p>Researchers focus their studies in tweet normalization at different levels. A character-level tagging system is used in <ref type="bibr" target="#b16">(Pennell and Liu, 2010)</ref> to solve deletion-based abbreviation. It was further extended in ( ) using more characters instead of Y or N as labels. The character-level machine translation (MT) approach (Pennell and Liu, 2011) was modified in (Li and Liu, 2012a) into character-block. While a string edit distance method was introduced in ( <ref type="bibr" target="#b2">Contractor et al., 2010)</ref> to represent word-level similarity, and this orthographical feature has been adopted in <ref type="bibr" target="#b5">(Han and Baldwin, 2011)</ref>, and <ref type="bibr" target="#b25">(Yang and Eisenstein, 2013)</ref>.</p><p>Challenges are encountered in these different levels of tweet normalization. In the characterlevel sequential labeling systems, features are required for every character and their combinations, leading to much more noise into the later reverse table look-up process ( ). In the character-block level MT systems equal number of blocks and their corresponding phonetic symbols are required for alignment ( <ref type="bibr" target="#b10">Li and Liu, 2012b)</ref>. This strict restriction can result in a great difficulty in training set construction and a loss of useful information. Finally, word-level normalization methods cannot properly model how non-standard words are formed, and some patterns or consistencies within words can be omitted and altered.</p><p>We observe the cognitive process that, given non-standard words like tmr, people tend to first segment them into syllables like t-m-r. Then they will find the corresponding standard word with syllables like to-mor-row. Inspired by this cognitive observation, we propose a syllable based tweet normalization method, in which nonstandard words are first segmented into syllables. Since we cannot predict the writers deterministic intention in using tmr as a segmentation of tm-r (representing tim-er) or t-m-r (representing to-mor-row), every possible segmentation form is considered. Then we represent similarity of standard syllables and non-standard syllables using an exponential potential function. After every transition probabilities of standard syllable and non-standard syllable are assigned, we then use noisy channel model and Viterbi decoder to search for the most possible standard candidate in each tweet sentence.</p><p>Our empirical study reveals that syllable is a proper level for tweet normalization. The syllable is similar to character-block but it represents phonetic features naturally because every word is pronounced with syllables. Our syllable-based tweet normalization method utilizes effective features of both character-and word-level: (1) Like characterlevel, it can capture more detailed information about how non-standard words are generated; (2) Similar to word-level, it reduces a large amount of noisy candidates. Instead of using domain-specific resources, our method makes good use of standard words to extract linguistic features. This makes our method extendable to new normalization tasks or domains.</p><p>The rest of this paper is organized as follows: previous work in tweet normalization are reviewed and discussed in Section 2. Our approach is presented in Section 3. In Section 4 and Section 5, we provide implementation details and results. Then we make some analysis of the results in Section 6. This work is finally concluded in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Non-standard words exhibit different forms and change rapidly, but people can still figure out their original standard words. To properly model this human ability, researchers are studying what remain unchanged under this dynamic characteristic. Human normalization of an non-standard word can be as follows: After realizing the word is non-standard, people usually first figure out standard candidate words in various manners. Then they replace the non-standard words with the standard candidates in the sentence to check whether the sentence can carry a meaning. If not, they switch to a different candidate until a good one is found. Most normalization methods in existence follow the same procedure: candidates are first generated, and then put into the sentence to check whether a reasonable sentence can be formed. Differences lie in how the candidates are generated and weighted. Related work can be classified into three groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Orthographical similarity</head><p>Orthographical similarity is built upon the assumption that the non-standard words look like its standard counterparts, leading to a high Longest Common Sequence (LCS) and low Edit Distance (ED). This method is widely used in spell checker, in which the LCS and ED scores are calculated for weighting possible candidates. However, problems are that the correct word cannot always be the most looked like one. Taking the nonstandard word nite for example, note looks more likely than the correct form night. To overcome this problem, an exception dictionary of strongly-associated word pairs are constructed in <ref type="bibr" target="#b4">(Gouws et al., 2011</ref>). Further, these pairs are added into a unified log-linear model in <ref type="bibr" target="#b25">(Yang and Eisenstein, 2013)</ref> and Monte Carlo sampling techniques are used to estimate parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Phonetic similarity</head><p>The assumption underlying the phonetic similarity is that during transition, non-standard words sound like the standard counterparts, thus the pronunciation of non-standard words can be traced back to a standard dictionary. The challenge is the algorithm to annotate pronunciation of the nonstandard words. Double Metaphone algorithm <ref type="bibr" target="#b19">(Philips, 2000</ref>) is used to decode pronunciation and then to represent phonetic similarity by edit distance of these transcripts <ref type="bibr" target="#b5">(Han and Baldwin, 2011)</ref>. IPA symbols are utilized in ( <ref type="bibr" target="#b10">Li and Liu, 2012b</ref>) to represent sound of words and then word alignment-based machine translation is applied to generate possible pronunciation of non-standard words. And also, phoneme is used in (  as one kind of features to train their CRF model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contextual similarity</head><p>It is accepted that after standard words are transformed into non-standard words, the meaning of a sentence remains unchanged. So the normalized standard word must carry a meaning. Most researchers use n-gram language model to normalize a sentence, and several researches use more contextual information. For example, training pairs are generated in ( ) by a cosine contextual similarity formula whose items are defined by TF-IDF scheme. A bipartite graph is constructed in <ref type="bibr" target="#b6">(Hassan and Menezes, 2013)</ref> to represent tokens (both non-standard and standard words) and their context. Thus, random walks on the graph can represent contextual-similarity between non-standard and standard words. Very recently, word-embedding ( <ref type="bibr" target="#b14">Mikolov et al., 2010;</ref><ref type="bibr">Mikolov et al., 2013</ref>) is utilized in ( <ref type="bibr" target="#b11">Li and Liu, 2014</ref>) to represent more complex contextual relationship.</p><p>In word-to-word candidate selection, most researches use orthographical similarity and phonetic similarity separately. In the log-linear model <ref type="bibr" target="#b25">(Yang and Eisenstein, 2013)</ref>, edit distance is modeled as major feature. In the character-and phonebased approaches (Li and Liu, 2012b), orthographical information and phonetic information were treated separately to generate candidates.</p><p>In (Han and Baldwin, 2011), candidates from lexical edit distance and phonemic edit distance are merged together. Then an up to 16% increasing recall was reported when adding candidates from phonetic measure. But improper processing level makes it difficult to model the two types of information simultaneously: (1) Single character can hardly reflect orthographical features of one word. (2) As fine-grained reasonable restrictions are lacked, as showed in (Han and Baldwin, 2011), several times of candidates are included when adding phonetic candidates and this will bring much more noise. To combine orthographical and phonetic measure in a fine-grained level, we proposed the syllable-level approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>The framework of the proposed tweet normalization method is presented in <ref type="figure" target="#fig_2">Figure 1</ref>. The proposed method extends the basic HMM channel model <ref type="bibr" target="#b1">(Choudhury et al., 2007;</ref><ref type="bibr">Cook and Steven- son, 2009</ref>) into syllable level. And the following four characteristics are very intersting.</p><p>(1) Combination: When reading a sentence, fast subvocalization will occur in our mind.</p><p>In the process, some non-standard words generated by phonetic substitution are correctly pronounced and then normalized. And also, because subvocalization is fast, people tend to ignore some minor flaws in spelling intentionally or unintentionally. As this often occurs in people's real-life interacting with these social media language, we believe the combination of phonetic and orthographical information is of great significance.</p><p>(2) Syllable level: Inspired by Chinese normalization ( <ref type="bibr" target="#b24">Xia et al., 2006</ref>) using pinyin (phonetic transcripts of Chinese), syllable can be seen as basic unit when processing pronunciation. Different from mono-syllable Chinese words, English words can be multi-syllable; this will bring changes in our method that extra layers of syllables must be put into consideration. Thus, apart from word-based noisy-channel model, we extend it into a syllable-level framework.</p><p>(3) Priori knowledge: Priori knowledge is acquired from standard words, meaning that both standard syllabification and pronunciation can shed some lights to non-standard words. This assumption makes it possible to obtain non-standard syllables by standard syllabification and gain pronunciation of syllables by standard words and rules generated with them.</p><p>(4) General patterns: Social media language changes rapidly while labeled data is expensive thus limited. To effectively solve the problem, linguistic features instead of statistical features should be emphasized. We exploit standard words of their syllables, pronunciation and possible transition patterns and proposed the four-layer HMM-based model (see <ref type="figure" target="#fig_2">Figure 1</ref>).</p><p>In our method, non-standard words c i are first segmented into syllables sc  i . . . sw (k) i make up one standard candidates. Since candidates are generated and weighted, we can use Viterbi decoder to perform sentence normalization. Table 1 shows some possible candidates for the nonstandard word tmr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method</head><p>We extend the noisy channel model to syllablelevel as follows: </p><formula xml:id="formula_0">= argmax p(c|w) × p(w) = argmax p( sc| sw) × p( sw),<label>(1)</label></formula><p>where w indicates the standard word and c the non-standard word, and sw and sc represent their syllabic form, respectively. To simplify the problem, we restrict the number of standard syllables equals to the number of non-standard syllables in our method.</p><p>Assuming that syllables are independent of each other in transforming, we obtain:</p><formula xml:id="formula_1">p( sc| sw) = k j=1 p(sc j |sw j ).<label>(2)</label></formula><p>For syllable similarity, we use an exponential potential function to combine orthographical distance and phonetic distance. Because pronunciation can be represented using letter-to-phone transcripts, we can treat string similarity of these tmr t-mr tm-r t-m-r tamer ta-mer tim-er to-mor-row ti-mor tim-ber tri-mes-ter ti-more ton-er tor-men-tor tu-mor tem-per ta-ma-ra . . .</p><p>. . . . . . transcripts as phonetic similarity. Thus the syllable similarity can be calculated as follows.</p><formula xml:id="formula_2">p(sc j |sw j , λ) = Φ(sc j , sw j ) Z(sw j )<label>(3)</label></formula><formula xml:id="formula_3">Z(sw j ) = sc j Φ(sc j , sw j )<label>(4)</label></formula><p>Φ(sc, sw) = exp(λ(LCS(sc, sw) − ED(sc, sw))</p><formula xml:id="formula_4">+(1 − λ)(P LCS(sc, sw) − P ED(sc, sw)))<label>(5)</label></formula><p>Exponential function grows tremendously as its argument increases, so much more weight can be assigned if syllables are more similar. The parameter λ here is used to empirically adjust relative contribution of letters and sounds. Longest common sequence (LCS) and edit distance (ED) are used to measure orthographical similarity, while phonetic longest common sequence (PLCS) and phonetic edit distant (PED) are used to measure phonetic similarity but based on letter-to-sound transcripts. The PLCS are defined as basic LCS but PED here is slightly different.</p><p>When performing phonetic similarity calculation based on syllables, we follow ( <ref type="bibr" target="#b24">Xia et al., 2006</ref>) in treating consonant and vowels separately because transition of consonants can make a totally different pronunciation. So if consonants of sc j and sw j are exactly the same or fit rules listed in <ref type="table">Table 2</ref>, P ED(sc j , sw j ) equals to edit</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rules</head><p>Examples 1. -ng as suffix: g-dropping -n/-ng do-in/do-ing, go-in/go-ing, talk-in/talk-ing, mak-in/mak-ing 2. -ng as suffix: n-dropping -g/-ng tak-ig/tak-ing, likig/lik-ing 3. suffix: z/s equaling -z/-s, -s/-z jamz/james, plz/please 4. suffix: n/m equaling -m/-n, -n/-m in-portant/im-portant, get-tim/get-ting 5. suffix: t/d equaling -t/-d, -d/-t shid/shit, shult/should 6. suffix: t-dropping -/-t jus/just, wha/what, mus/must, ain/ain't 7. suffix: r-dropping -/-r holla/holler, t-m-r/tomorrow 8. prefix: th-/d-equaling d-/th-, th-/d-de/the, dat/that, dats/that's, dey/they <ref type="table">Table 2</ref>: The consonant rules.</p><p>distance of letter-to-phone transcripts, or it will be assigned infinity to indicate that their pronunciation are so different that this transition can seldom happen. For example, as consonantal transition between suffix z and s can always happen, PED(plz,please) equals string edit distance of their transcripts. But as consonatal transition of f and d is rare, phonetic distance of fly and sky is assigned infinity. Note the consonant rules in <ref type="table">Table 2</ref> are manually defined in our empirical study, which represent the most commonly used ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter</head><p>Parameter in the proposed method is only the λ in Equation (5), which represents the relative contribution of orthographical similarity and phonetic similarity. Because the limited number of annotated corpus, we have to enumerate the parameter in {0, 0.1, 0.2, ..., 1} in the experiment to find the optimal setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>The method described in the previous section are implemented with the following details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing</head><p>Before performing normalization, we need to process several types of non-standard words:</p><p>• Words containing numbers: People usually substitute some kind of sounds with numbers like 4/four, 2/two and 8/eight or numbers can be replacement of some letters like 1/i, 4/a. So we replace numbers with its words or characters and then use them to generate possible candidates.</p><p>• Words with repeating letters: As our method is syllable-based, repeating letters for sentiment expressing (like cooool, (Brody and Diakopoulos, 2011)) can cause syllabifying failure. For repeating letters, we reduce it to both two and one to generate candidate separately. Then the two lists are merged together to form the whole candidate list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Letter-to-sound conversion</head><p>Syllable in this work refers to orthographic syllables. For example, we convert word tomorrow into to-mor-row. However, when comparing the syllable of a standard word and that of a nonstandard word, sound (i.e., phones) of the syllables are considered. Thus letter-to-sound conversion tools are required. Several TTS system can perform the task according to some linguistic rules, even for nonstandard words. The Double Metaphone algorithm used in ( <ref type="bibr" target="#b5">Han and Baldwin, 2011</ref>) is one of them. But it uses consonants to encode a word, which gives less information than we need. In our method, we use freeTTS ( <ref type="bibr" target="#b22">Walker et al., 2002</ref>) with CMU lexicon 1 to transform words into APRAbet 2 symbols. For example, word tomorrow is transcribed to {T-UW M-AA R-OW} and tmr to {T M R}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dictionary preparation</head><p>• Dictionary #1: In-vocabulary (IV) words Following ( <ref type="bibr" target="#b25">Yang and Eisenstein, 2013)</ref>, our set of IV words is also based on the GNU aspell dictionary (v0.60.6). Differently, we use a collection of 100 million tweets (roughly the same size of Edinburgh Twitter corpus) because the Edinburgh Twitter corpus is no longer available due to Twitter policies. The final IV dictionary contains 51,948 standard words.</p><p>• Dictionary #2: Syllables for the standard words</p><p>Following (Pennell and Liu, 2010), we use the online dictionary 3 to extract syllables for each standard words. We encountered same problem when accessing words with prefixes or suffixes, which are not syllabified in the same format as the base words on the website. To address the issue, we simply regard these prefixes and suffixes as syllables.</p><p>• Dictionary #3: Pronunciation of the syllables</p><p>Using the CMU pronouncing dictionary (Weide, 1998) and dictionary 2, and knowing all possible APRAbet symbol for all consonant characters, we can program to capture every possible pronunciation of all syllables in the standard dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Automatic syllabification of non-standard words</head><p>Automatic syllabification of non-standard words is a supervised problem. A straightforward idea is to train a CRF model on manually labeled syllables of non-standard words. Unfortunately, such a corpus is not available and very expensive to produce. We assume that both standard and non-standard forms follow the same syllable rules (i.e., the cognitive process). Thus we propose to train the CRF model on the corpus of syllables of standard words (which is easy to obtain) to construct an automatic annotation system based on CRF++ ( <ref type="bibr" target="#b8">Kudo, 2005)</ref>. In this work, we extract syllables of standard words from Dictionary #2 as training set. Annotations follow ( <ref type="bibr" target="#b16">Pennell and Liu, 2010)</ref> to identify boundaries of syllables and in our work, CRF++ can suggest several candidate solutions, rather than an optimal segmentation solution for syllable segmentation of the non-standard words.</p><p>In the HMM channel model, the candidate solutions are included as part of the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Language model</head><p>Using Tweets from our corpus that contain no OOV words besides hashtags and username mentions (following (Han and Baldwin, 2011)), the Kneser-Ney smoothed tri-gram language model is estimated using SRILM toolkit <ref type="bibr" target="#b21">(Stolcke, 2002)</ref>. Note that punctuations, hashtags, and username mentions have some syntactic value ( <ref type="bibr" target="#b7">Kaufmann and Kalita, 2010)</ref> to some extent, we replace them with '&lt;PUNCT&gt;', '&lt;TOPIC&gt;' and '&lt;USER&gt;'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use two labeled twitter datasets in existence to evaluate our tweet normalization method.</p><p>• LexNorm1.1 contains 549 complete tweets with 1184 non-standard tokens (558 unique word type) (Han and Baldwin, 2011).</p><p>• LexNorm1.2 is a revised version of LexNorm1.1 <ref type="bibr" target="#b25">(Yang and Eisenstein, 2013)</ref>. Some inconsistencies and errors in LexNorm1.1 are corrected and some more non-standard words are properly recovered.</p><p>In both datasets, to-be-normalized non-standard words are detected manually as well as the corresponding standard words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation criteria</head><p>Here we use precision, recall and F-score to evaluate our method. As normalization methods on these datasets focused on the labeled nonstandard words <ref type="bibr" target="#b25">(Yang and Eisenstein, 2013)</ref>, recall is the proportion of words requiring normalization which are normalized correctly; precision is the proportion of normalizations which are correct. When we perform the tweet normalization methods, every error is both a false positive and false negative, so in the task, precision equals to recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sentence level normalization</head><p>We choose the following prior normalization methods:</p><p>• (Liu et al., 2012): the extended characterlevel CRF tagging system;</p><p>• (Yang and Eisenstein, 2013): log-linear model using string edit distance and longest common sequence measures as major features;</p><p>• (Hassan and Menezes, 2013): bipartite graph major exploit contextual similarity;  <ref type="table">Table 3</ref>: Experiment results of the tweet normalization methods.</p><p>• (Han and Baldwin, 2011): the orthographyphone combined system using lexical edit distance and phonemic edit distance.</p><p>In our method, we set λ=0.7 because it is found best in our experiments (see <ref type="figure" target="#fig_3">Figure 2)</ref>. The experimental results are presented in <ref type="table">Table 3</ref>, which indicate that our method outperforms the state-of-the-art methods. Details on how to adjust parameter is given in Section 5.4.</p><p>Recall we argue that combination of three similarity is necessary when performing sentence-level normalization. Apart from contextual similarity like language model or graphic model, methods in ( <ref type="bibr" target="#b25">Yang and Eisenstein, 2013)</ref> or (Hassan and Menezes, 2013) do not include phonetic measure, causing loss of important phonetic information. Though using phoneme, morpheme boundary and syllable boundary as features ( , the character-level reversed approach will bring much more noise into the later reversed look-up table, and also, features of whole word are omitted.</p><p>Like (Han and Baldwin, 2011), we also use lexical measure and phonetic measure. Great difference between the two approaches is the processing level: word level and syllable level. In their work, average candidates number suffers times of increase when adding phonetic measure. This is because when introducing phonemic edit distance, important pronunciations can be altered (phonemic edit distance of night-need and night-kite is equal). Syllable level allows us to reflect consistencies during transition in a finergrained level. Thus the phonetic similarity can be more precisely modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Contributions of phone and orthography</head><p>In our method, the parameter λ in Equation 5 is used to represent the relatively contributions of both phonetic and orthographical information. But as the lack of prior knowledge, we cannot judge an optimal λ. We choose to conduct experiments varying λ = {0, 0.1, ..., 1} to find out how this adjustment can affect performance. The experimental results are presented in <ref type="figure" target="#fig_3">Figure 2</ref>. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, when λ is set 0 or 1 (indicating no contribution of either orthographical or phonetic in assigning weight to candidates), our method performs much worse. In our experiment, when λ = 0.7, the models performs best, showing that orthographical measure makes relatively more contribution over phonetic measure, but the latter is indispensable. This justifies the effectiveness of combining orthographical and phonetic measure, indicating that human normalization process is properly modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Our exceptions</head><p>Deeper observation of our normalization results shows that there are several types of exceptions beyond our consonant-based rules. For example, thanks fails to be selected as a candidate for the non-standard word thx because the pronunciation of thanks contains an N but thx does not. The same situation happens when we process stong/strong because of the lacking R. We believe some more consonant should be exploited and more precisely described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Non-standard words involving multiple syllables</head><p>There are one type of transition that we cannot solve like acc/accelerate and bio/biology because the mapping is between single-syllable word and multi-syllable word. We add possible standard syllable sw (i) 0 and sw (i) k+1 to the head and tail of origin syllables, but this extended form failed to be assigned high probability because the string edit distances are too large. We leave this problem for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Annotation issue</head><p>Though similar, our results of LexNorm1.2 is better than LexNorm1.1. After scrutinizing, we notice that several issues in LexNorm1.1 are fixed in LexNorm1.2. So our results like meh/me (meaning the non-standard word meh are corrected to me) in LexNorm1.1 is wrong but in LexNorm1.2 is right. Even in LexNorm1.2, there exist some inconsistencies and errors. For example, our result buyed/bought is wrong for both datasets, which is actually correct. For another example, til is normalized to until in some cases but to till in other cases. We show that the LexNorm test corpus is still imperfect. We appeal for systematic efforts to produce a standard dataset under a widely-accepted guideline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Conventions</head><p>Social media language often contains words that are culture-specific and widely used in daily life. Some word like congrats, tv and pic are included into several dictionaries. We also observed several transitions like atl/atlanta or wx/weather in the datasets. These kinds of conventional abbreviations pose great difficulty to us. Normalization of those conventional nonstandard words still needs further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, a syllable-based tweet normalization method is proposed for social media text normalization. Results on publicly available standard datasets justify our assumption that syllable plays a fundamental role in social media non-standard words. Advantage of our proposed method lies in that syllable is viewed as the basic processing unit and syllable-level similarity. This accords to the human cognition in creating and understanding the social non-standard words. Our method is domain independent. It is robust on non-standard words in any period of history. Furthermore, give the syllable transcription tool, our method can be easily adapted to a new language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i</head><label></label><figDesc>, we calculate their similarity by combining the orthographical and phonetic mea- sures. Standard syllables sw (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Framework of the propose tweet normalization method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Contribution of phone and orthography.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Standard candidates of tmr in syllable lev-
el. The first row gives the different segmentations 
and the second row presents the candidates. 

</table></figure>

			<note place="foot" n="1"> http://www.speech.cs.cmu.edu/cgi-bin/cmudict 2 http://en.wikipedia.org/wiki/Arpabet</note>

			<note place="foot" n="3"> http://www.dictionary.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research work was carried out when the authors worked at Tsinghua University. We acknowledge the financial support from Natural Science Foundation of China (NSFC: 61272233, 61373056, 61433018). We thank the anonymous reviewers for the insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Diakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Investigation and modeling of the structure of texting language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="157" to="174" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised cleansing of noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Faruquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<editor>Chu-Ren Huang and Dan Jurafsky</editor>
		<imprint>
			<publisher>Chinese Information Processing Society of China</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An unsupervised model for text message normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CALC &apos;09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised mining of lexical variants from noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First workshop on Unsupervised Learning in NLP</title>
		<meeting>the First workshop on Unsupervised Learning in NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a #twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Dekang Lin, Yuji Matsumoto, and Rada Mihalcea</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="368" to="378" />
		</imprint>
		<respStmt>
			<orgName>The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social text normalization using contextual graph random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
	<note>ACL (1)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Syntactic normalization of Twitter messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jugal</forename><surname>Kalita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on natural language processing</title>
		<meeting><address><addrLine>Kharagpur, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Crf++: Yet another crf toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<ptr target="http://crfpp.sourceforge.net" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving text normalization using character-blocks based models and system combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-08-15" />
			<biblScope unit="page" from="1587" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Normalization of text messages using character-and phone-based machine translation approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<editor>INTERSPEECH. ISCA</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving text normalization via unsupervised model and discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA, Student Research Workshop</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-22" />
			<biblScope unit="page" from="86" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Insertion, deletion, or substitution?: normalizing text messages without precategorization nor supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A broad-coverage normalization system for social media language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Long Papers</title>
		<meeting>ACL: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Normalization of text messages for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4842" to="4845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A character-level machine translation approach for normalization of sms abbreviations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="974" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The edinburgh twitter corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT Workshop on Computational Linguistics in a World of Social Media</title>
		<meeting>the NAACL HLT Workshop on Computational Linguistics in a World of Social Media</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The double metaphone search algorithm. C/C++ Users Journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Philips</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-06" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalization of non-standard words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="333" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Spoken Language Processing</title>
		<meeting>International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-11" />
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Freetts: a performance case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The cmu pronouncing dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert L Weide</surname></persName>
		</author>
		<ptr target="http://www.speech.cs.cmu.edu/cgibin/cmudict" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A phonetic-based approach to chinese chat text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<editor>Nicoletta Calzolari, Claire Cardie, and Pierre Isabelle</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A log-linear model for unsupervised text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
