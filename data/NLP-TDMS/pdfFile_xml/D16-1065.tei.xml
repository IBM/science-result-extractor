<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMR Parsing with an Incremental Joint Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Language Information Processing and Social Computing Lab</orgName>
								<orgName type="institution">Nanjing Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Technology Lab, DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
							<email>uszkoreit@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Technology Lab, DFKI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Language Information Processing and Social Computing Lab</orgName>
								<orgName type="institution">Nanjing Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Language Information Processing and Social Computing Lab</orgName>
								<orgName type="institution">Nanjing Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhui</forename><surname>Gu</surname></persName>
							<email>gu@njnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Language Information Processing and Social Computing Lab</orgName>
								<orgName type="institution">Nanjing Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AMR Parsing with an Incremental Joint Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="680" to="689"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To alleviate the error propagation in the traditional pipelined models for Abstract Meaning Representation (AMR) parsing, we formulate AMR parsing as a joint task that performs the two subtasks: concept identification and relation identification simultaneously. To this end, we first develop a novel component-wise beam search algorithm for relation identification in an incremental fashion, and then incorporate the decoder into a unified framework based on multiple-beam search, which allows for the bi-directional information flow between the two subtasks in a single incre-mental model. Experiments on the public datasets demonstrate that our joint model significantly outperforms the previous pipelined counterparts, and also achieves better or comparable performance than other approaches to AMR parsing, without utilizing external semantic resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Producing semantic representations of text is motivated not only by theoretical considerations but also by the hypothesis that semantics can be used to improve many natural language tasks such as question answering, textual entailment and machine translation. <ref type="bibr" target="#b1">Banarescu et al. (2013)</ref> described a semantics bank of English sentences paired with their logical meanings, written in Abstract Meaning Representation (AMR), which is rapidly emerging as an important practical form of structured sentence semantics. Recently, some literatures reported some promising applications of AMR. <ref type="bibr" target="#b17">Pan et al. (2015)</ref> presented an unsupervised entity linking system with AMR, achieving the performance comparable to the supervised state-of-the-art. <ref type="bibr" target="#b14">Liu et al. (2015)</ref> demonstrated a novel abstractive summarization framework driven by the AMR graph that shows promising results. <ref type="bibr" target="#b8">Garg et al. (2016)</ref> showed that AMR can significantly improve the accuracy of a biomolecular interaction extraction system compared to only using surface-and syntax-based features. <ref type="bibr" target="#b15">Mitra and Baral (2016)</ref> presented a question-answering system by exploiting the AMR representation, obtaining good performance.</p><p>Automatic AMR parsing is still in a nascent stage. <ref type="bibr" target="#b7">Flanigan et al. (2014)</ref> built the first AMR parser, JAMR, based on a pipelined approach, which breaks down the whole task into two separate subtasks: concept identification and relation identification. Considering that node generation is an important limiting factor in AMR parsing, <ref type="bibr" target="#b21">Werling et al. (2015)</ref> proposed an improved approach to the concept identification subtask by using a simple classifier over actions which generate these subgraphs. However, the overall architecture is still based on the pipelined model.</p><p>As a common drawback of the staged architecture, errors in upstream component are often compounded and propagated to the downstream prediction. The downstream components, however, cannot impact earlier decision. For example, for the verb "affect" in the example shown in <ref type="figure" target="#fig_0">Figure 1</ref>, there exist two possible concepts: "affect-01" and "affect-02". Comparatively, the first concept has more common use cases than the second one. But, when the verb "affect" is followed by the noun "ac-cent", it should evoke the concept "affect-02". Obviously, the correct concept choice for the verb "affect" should exploit a larger context, and even the whole semantic structure of the sentence, which is more probable to be unfolded at the downstream relation identification stage. This example indicates that it is necessary to allow for the interaction of information between the two stages. To address this problem, in this paper we reformulate this task as a joint parsing problem by exploiting an incremental parsing model. The underlying learning algorithm has shown the effectiveness on some other Natural Language Processing (NLP) tasks, such as dependency parsing and extraction of entity mentions and relations ( <ref type="bibr" target="#b3">Collins and Roark, 2004;</ref><ref type="bibr" target="#b10">Hatori et al., 2012;</ref><ref type="bibr" target="#b13">Li and Ji, 2014)</ref>. However, compared to these NLP tasks, the AMR parsing is more challenging in that the AMR graph is more complicated. In addition, the nodes in the graph are latent.</p><p>One main challenge to search for concept fragments and relations incrementally is how to combine the two subtasks in a unified framework. To this end, we first develop a novel Component-Wise Beam Search (CWBS) algorithm for incremental relation identification to examine the accuracy loss in a fully incremental fashion compared to the global fashion in which a sequence of concept fragments derived from the whole sentence are required as input, as the MSCG algorithm in JAMR. Secondly, we adopt a segment-based decoder similar to the multiple-beam algorithm <ref type="bibr" target="#b23">(Zhang and Clark, 2008b</ref>) for concept identification, and then incorporate the CWBS algorithm for relation identification into this framework, combining the two subtasks in a single incremental model. For parameter estimation, "violation-fixing" perceptron is adopted since it is designed specifically for inexact search in structured learning ( <ref type="bibr" target="#b11">Huang et al., 2012)</ref>.</p><p>Experimental results show that the proposed joint framework significantly outperforms the pipelined counterparts, and also achieves better or comparable performance than other AMR parsers, even without employing external semantic resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AMR Parsing Task</head><p>Nodes of an AMR graph are labeled with concepts, and edges are labeled with relations. Concepts can be English words ("He"), PropBank event predicates ("try-01", "affect-02"), or special keywords ("British"). For example, "affect-02" represents a PropBank roleset that corresponds to the first sense of "affect". According to ( <ref type="bibr" target="#b1">Banarescu et al., 2013)</ref>, AMR uses approximately 100 relations. The rolesets and core semantic relations (e.g., ARG0 to ARG5) are adopted from the PropBank annotations in OntoNotes. Other semantic relations include "mode", "name", "time", "topic" and so on. The AMR guidelines provide more detailed descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Pipelined Models for AMR Parsing</head><p>The AMR parser JAMR is a two-stage algorithm that first identifies concepts and then identifies the relations that obtain between these.</p><p>The concept identification stage maps spans of words in the input sentence to a sequence of concept graph fragments. Note that these graph fragments, in some cases, are subgraphs with multiple nodes and edges, not just one labeled concept node. The relation identification stage adds edges among the concept subgraph fragments identified in the first stage. JAMR requires the output subgraph G =&lt; V G , E G &gt; should respect the following constraints:</p><p>(1) Simple: For any two vertices u and v ∈ V G , E G includes at most one edge between u and v.</p><p>(2) Connected: G must be weakly connected (every vertex reachable from every other vertex, ignoring the direction of edges).</p><p>(3) Deterministic: For each node u ∈ V G , and for each label l ∈ {ARG0, . . . , ARG5} , there is at most one outgoing edge in E G from u with label l.</p><p>To find a maximum spanning AMR graph, JAMR proposed a two-step approach <ref type="bibr">1</ref> . First, a graph that ignores constraint (3) but respects the others was created, by searching for the maximum spanning connected subgraph from an edge-labeled, directed graph representing all possible relations between the identified concepts; Second, a Lagrangian relaxation was adopted to iteratively adjust the edge scores so as to enforce constraint (3).</p><p>In order to train the parser, JAMR built an automatic aligner that uses a set of rules to greedily align concepts to spans of words in the training data to generate an alignment table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithms</head><p>Based on the hypothesis that concept identification and relation identification are interrelated, we propose to jointly perform the two subtasks in a single model. To this end, we present an incremental model for AMR parsing. Evidence from psycholinguistic research also suggests that human language comprehension is incremental. Comprehenders do not wait until the end of the sentence before they build a syntactic or semantic representation for the sentence.</p><p>However, the challenges of successfully applying the incremental joint model to this problem formulation are: 1) how can we design an effective decoding algorithm for identifying the relations between the nodes in an incremental fashion, given a partial sequence of spans, i.e., a partial sequence of goldstandard concept fragments; 2) further, if given a sentence, how can we design an incremental framework to perform concept identification and relation identification simultaneously. In the following subsections we introduce our solutions to these challenges in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Incremental Decoding Algorithm for Relation Identification</head><p>We define the relation identification problem as finding the highest scoring graph y from all possible out-puts given a sequence of concept fragments c:</p><formula xml:id="formula_0">F (c) = arg max Gen(c) Score(y)<label>(1)</label></formula><p>where Gen(c) denotes the set of possible AMR graph for the input c. The score of an output parse y is defined to be decomposed by edges, and with a linear model:</p><formula xml:id="formula_1">Score(y) = e∈Ey w T · ϕ(e)<label>(2)</label></formula><p>where ϕ(e) is the feature vector over the edge e, and w is weight vector of the model. The AMR graph is a directed graph that respects three constraints (see section 2.2) and has a node marked as the focus node. Obviously, finding such a maximum spanning graph in AMR parsing in fact carries more complexities than that of maximum spanning tree (MST) decoding for syntactic parsing. Especially, performing the task incrementally is substantially harder than doing it non-incrementally. In both cases, parsing is in general intractable and we provide an approximate inference algorithm to make these cases tractable.</p><p>Inspired by the graph-based dependency parser under the framework of beam-search, which yields a competitive performance compared to the exactsearch-based counterpart <ref type="bibr" target="#b22">(Zhang and Clark, 2008a</ref>), we develop a CWBS algorithm for the relation identification task.</p><p>Basically, the decoder works incrementally, building a state item (i.e. a partial AMR graph) fragment by fragment. When each concept fragment is processed, edges are added between the current concept fragment and its predecessors. However, how to treat its predecessors is a difficult problem. In our experiments, we found that if we consider every preceding concept fragment to the left of the current fragment in a right-to-left order in the search process, the decoder suffers from low efficiency and poor performance. Unlike the beam-search for dependency parsing, which can greatly reduce the search space by exploiting the projectivity property of the dependency tree <ref type="bibr" target="#b5">(Covington, 2001;</ref><ref type="bibr" target="#b22">Zhang and Clark, 2008a)</ref>, this naive search process in this context inevitably leads to huge search space, and furthermore is difficult to guarantee the connectivity of output graph. Instead, we propose a componentwise beam search scheme, which can not only alleviate much noisy partial candidate, but also ensure that the final output graph is connected.</p><p>Algorithm 1 shows the pseudocode for the complete procedure of the decoder. In a nutshell, the algorithm builds the AMR graph in one left-to-right pass over the sequence of concept fragments. Beam search is applied by keeping the B-best 2 items in the agenda at each processing stage, according to the scores of partial graph up to the current concept fragment. Lets take an illustrative diagram to demonstrate the procedure (see <ref type="figure" target="#fig_2">Figure 2)</ref>. When appending the current concept fragment to the left partial graph to extend it, we just need to consider the relations between current concept and each preceding connected component. However, even at this single step, picking B-best extended partial graphs is still a difficult task due to the large combination space. Here, we adopt an effective nested beam search strategy at this step. In other words, edges are added between the current concept fragment and its preceding connected components by iterating through these components in a right-to-left order 3 using an inner beamsearch. When examining the edges between the current concept fragment and some preceding component, four elementary actions are used: The first three actions are similar in form to those in the Arc-Standard algorithm for transition-based <ref type="bibr">2</ref> The constant B denotes the beam size. <ref type="bibr">3</ref> The right-to-left order reflects the principle of local priority. dependency parsing <ref type="bibr" target="#b16">(Nivre, 2008;</ref><ref type="bibr" target="#b22">Zhang and Clark, 2008a</ref>). The last one is defined to cope with the cases where there may be multiple parents for some nodes in an AMR graph. Note that the "SHIFT" action does not add any edges. This operation is particularly necessary because the partial graphs are not always connected during the search process. In our experiments, we also found that the number of connected components during search process is relatively small, which is generally less than 6. It is important to note that, in order to guarantee the output graph connected, when the last concept fragment is encountered, the "SHIFT" action is skipped (see line 10 in Algorithm 1), and the other three 'arc' actions will add edges to connect the last concept fragment with all preceding connected components to yield a connected graph.</p><p>For purpose of brevity, we introduce some functional symbols in Algorithm 1. Function CalEdgeScores(state, c i ) calculates the scores of all candidate edges between the nodes in current concept fragment c i and the nodes in the partial graph in state covering (c 1 , c 2 , . . . , c i−1 ). For computing the scores of edges, we use the same features as JAMR (refer to <ref type="bibr" target="#b7">Flanigan et al. (2014)</ref> for more details). Function FindComponents(state) returns all connected components (p 1 , p 2 , . . . , p m ) in the partial graph in state, sorted by the maximum end position of spans including in every component. The AddItem function adds the current concept fragment and left/right arc to the partial graph. Function AppendItem(buf, item) inserts the partial graph item into buf by its score.</p><p>Functions GetMaxLeftEdge(c i , p j ) and Algorithm 1 The incremental decoding algorithm for relation identification. Input: A sequence of concept fragments (c 1 , c 2 , . . . , c n ) Output: Best AMR graph including (c 1 , c 2 , . . . , c n ) 1: agenda ← {Empty-graph} 2: for i ← 1 . . . n do buf ← N U LL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>for item in innerAgenda do 10:</p><p>if i &lt; n then Finally, the function CalRootFeatures(g) first computes the scores for all nodes in the output graph g by treating them as the candidate root respectively, and then pick the node with the highest score as the focus node of the graph. When computing the score for each candidate node, similar to JAMR, two types of features were used: the concept of the node, and the shortest dependency path from a word in the span to the root of the dependency tree.</p><p>The time complexity of the above algorithm is O(M B 2 n), where M is the maximum number of connected components during search, B is beam size and n is the number of concept fragments. It is linear in the length of sequence of concept fragments. However, the constant in the O is relatively large. In practice, the search space contains a large number of invalid partial candidates. Therefore, we introduce three partial output pruning schemes which are helpful in reducing search space as well as making the input for parameter update less noisy.</p><p>Firstly, we limit the number of children and parents of every node. By observing the training data, we set the maximum numbers of children and parents of every node as 7 and 4, respectively. Secondly, due to the fact that all frame arguments ARG0-ARG5 are derived from the verb framesets, the edges with label l ∈ {ARG0, . . . , ARG5} that do not outgo from a verb node will be skipped.</p><p>Finally, consider the determinism constraint (as illustrated in section 2.2) that should be satisfied by an AMR representation. When one edge has the same label l ∈ {ARG0, . . . , ARG5} as one of edges outgoing from the same parent node, this edge will also be skipped. Obviously, this type of pruning can enforce the determinism constraint for every decoding output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Decoding for Concept Identifica-tion and Relation Identification</head><p>In this section, we further consider the joint decoding problem for a given sentence x, which maps the sentence x to an output AMR graph y. The objective function for the joint decoding is as follows:</p><formula xml:id="formula_2">ˆ y = arg max y ∈Gen(x) (w T · φ(x, y ) + w T · f(y )) (3)</formula><p>where the first term is to calculate the score over all concept fragments derived from the words in the sentence x, and the second one is to calculate the score over all edges linking the concept fragments. Maximizing Equation (3) amounts to concurrently maximizing the score over the concept fragments and the score over the edges. Admittedly, the joint decoding problem is more intricate and in general intractable. Therefore, we use a beam-search-based incremental decoder for approximate joint inference during training and testing. In order to combine the two subtasks in a unified framework, we first relax the exact-search for concept identification in JAMR by beam search, resulting in a segment-based decoder similar to the multiple-beam algorithm in ( <ref type="bibr" target="#b23">Zhang and Clark, 2008b;</ref><ref type="bibr" target="#b13">Li and Ji, 2014)</ref>, and then incorporate the CWBS algorithm for relation identification (as depicted in section 3.1) into this framework, which provides a natural formulation for combining the two subtasks in a single incremental model. Algorithm 2 shows the joint decoding algorithm. In short, during performing joint decoding incrementally for the input sentence, for each word index i in the input sentence, it maintains a beam for the partial graphs whose last segments end at the i-th word, which is denoted as agendas <ref type="bibr">[i]</ref> in the algorithm. When the i-th word is processed, it either triggers concepts starting from this word by looking up the alignment table generated from the training data, or evokes no concept (we refer to this type of words as function words). If the current word triggers multiple concepts, we first append each candidate concept to the partial graphs in the beam agendas <ref type="bibr">[i−1]</ref>, by using a component-wise beam search way (see section 3.1), and then pick B-best extended partial graphs by exploiting the features from both the concept level and relation level to compute the overall scores.</p><p>In particular, judging whether a word is a function word is an important and difficult task. For example, the word "make" corresponds to multiple candidate concepts in the alignment table, such as "make-01" and "make-02". However, it can also act as a functional word in some cases. To resolve the judgement problem, we view each word as a function word and a non-function word at the same time to allow them to compete against each other by their scores. For instance, for the i-th word, this is done by combining all partial graphs in the beam agendas <ref type="bibr">[i − 1]</ref> with those in the beam agendas <ref type="bibr">[i]</ref> to select B-best items and then record them in agendas <ref type="bibr">[i]</ref>, which is represented as the Union function in Algorithm 2.</p><p>After all words are processed, the highest-scoring graph in the beam corresponding to the terminal poAlgorithm 2 The joint decoding algorithm.</p><p>Input: Input sentence x = (w 1 , w 2 , . . . , w n ) Output: Best AMR graph derived from</p><formula xml:id="formula_3">x 1: agendas[0] ← ∅ 2: last ← Scan(x) 3: for i ← 1 . . . n do 4: list ← Lookup(x, i) 5:</formula><p>if list.size &gt; 0 then <ref type="bibr">6:</ref> preAgenda ← agendas[i − 1]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for cf ∈ list do 8:</p><formula xml:id="formula_4">end ← i + cf .size − 1 9:</formula><p>if preAgenda.size = 0 then 10:</p><p>g ← Graph.empty 11:</p><p>CalConceptFeatures(g, cf )</p><p>12:</p><p>AppConcept(agendas, end, g, cf, last)</p><p>13:</p><formula xml:id="formula_5">else 14:</formula><p>for item ∈ preAgenda do 15:</p><formula xml:id="formula_6">g ← item 16:</formula><p>CalConceptFeatures(g, cf )</p><p>17:</p><p>AppConcept(agendas, end, g, cf, last)</p><p>18:</p><p>Union(agendas, i, i − 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>else 20:</p><formula xml:id="formula_7">agendas[i] ← agendas[i − 1] 21: bestGraph ← agendas[last][0] 22: return bestGraph</formula><p>sition of the sentence is selected as the output.</p><p>In algorithm 2, function Scan(x) is used to search the terminal position corresponding to the last concept fragment in the sentence x, which will be passed as a parameter to the function AppConcept. The Scan function can be efficiently implemented by calling the function Lookup in a right-to-left order. Function Lookup(x, i) maps a sequence of words starting from the index i in sentence x to a set of candidate concept fragments, by looking up the alignment table that was generated from the training data. The alignments are accomplished using an aligner from JAMR. Motivated by <ref type="bibr" target="#b21">Werling et al. (2015)</ref>, we also adopt two additional actions to generate the candidate concept fragments: LEMMA and VERB. The action LEMMA is executed by using the lemma of the source token as the generated node title, and the action VERB is to find the most similar verb in PropBank based on Jaro-Winkler distance, and adopt its most frequent sense.</p><p>Function CalConceptFeatures(g, cf ) calculates the feature vector for the candidate concept fragment cf and the partial graph g, using the features defined in <ref type="table" target="#tab_1">Table 1</ref>. Among them, features 1-4 are from JAMR. Additional features 5-16 aim to capture the association between the current concept and the context in which it appears. Function AppConcept(agendas, end, g, cf, last) appends the current concept cf to the partial graph g, and then inserts the extended partial graph into agendas <ref type="bibr">[end]</ref>. Note that when the parameter end equals to the parameter last, this function will call the function CalRootFeatures to select the focus node, as illustrated in Algorithm 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Violation-Fixing Perceptron for Training</head><p>Online learning is an attractive method for the structured learning since it quickly converges within a few iterations <ref type="bibr" target="#b4">(Collins, 2002</ref>). Particularly, Huang et al. (2012) establish a theoretical framework called "violation-fixing perceptron" which is tailored for structured learning with inexact search and has provable convergence properties. Since our incremental decoding for AMR parsing is an approximate inference, it is very natural to employ violation-fixing perceptron here for AMR parsing training.</p><p>Specifically, we use an improved update method "max-violation" which updates at the worst mistake, and converges much faster than early update with similar or better accuracy. We adopt this idea here as follows: decode the whole sentence, and find the word index i * where the difference between the candidate partial graph and gold-standard one is the biggest. Only part of the graph ending at the word index i * is used to calculate the weight update, in order to account for search errors.</p><p>To reduce overfitting, we used averaged parameters after training to decode test instances in our experiments. The resulting model is called averaged perceptron <ref type="bibr" target="#b4">(Collins, 2002</ref>).</p><p>Additionally, in our training algorithms, the implementation of the oracle function is rela-tively straightforward. Specifically, when the i-th span is processed in the incremental parsing process, the partial gold-standard AMR graph up to the i-th span consists of the edges and nodes that appear before the end position of the i-th span, over which the gold-standard feature vectors are calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metric</head><p>Following previous studies on AMR parsing, our experiments were performed on the newswire sections of LDC2013E117 and LDC2014T12, and we also follow the official split for training, development and evaluation. Finally, we also show our parsers performance on the full LDC2014T12 dataset. We evaluate the performance of our parser using Smatch v2.0 , which counts the precision, recall and F1 of the concepts and relations together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Development Results</head><p>Generally, larger beam size will increase the computational cost while smaller beam size may reduce the performance. As a tradeoff, we set the beam size as 4 throughout our experiments. <ref type="figure">Figure 3</ref> shows the training curves of the averaged violation-fixing perceptron with respect to the performance on the both development sets. As we can see the curves converge very quickly, at around iteration 3.  <ref type="formula" target="#formula_0">0 1 2 3 4 5 6 7 8 9</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Incremental Relation Identification Performance</head><p>Before performing joint decoding, we should first verify the effectiveness of our incremental algorithm CWBS. The first question about CWBS is whether the component-wise search is a valid scheme for deriving the gold-standard AMR graph given the sequence of gold-standard concepts. Therefore, we first implement an oracle function by performing the incremental component-wise search for each fragment sequence c to get a "pseudo-gold" graph G c ; Then we compare with gold-standard AMR graph G c . On the training data of LDC2013E117 and LDC2014T12, we respectively got an overall 99.6% and 99.7% F-scores for all &lt; G c , G c &gt; pairs, which indicates that our component-wise search is an effective incremental search scheme.</p><p>Further, we train a perceptron model using the max-violation update to approximate the oracle search procedure. As shown in <ref type="table" target="#tab_3">Table 2</ref>, our incremental algorithm CWBS achieves almost the same performance as the non-incremental algorithm MSCG in JAMR, using the same features as MSCG.</p><p>The results indicate that CWBS is a competitive alternative to MSCG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Joint Model vs. Pipelined Model</head><p>In this section, we compare the overall performance of our joint model to the pipelined model, JAMR <ref type="bibr">4</ref> . To give a fair comparison, we first implemented system 1 only using the same features (i.e., features 1-4 in <ref type="table" target="#tab_1">Table 1</ref>) as JAMR for concept fragments. <ref type="table" target="#tab_0">Ta- ble 3</ref> gives the results on the two datasets. In terms of F-measure, we gain a 6% absolute improvement, and a 5% absolute improvement over the results of JAMR on the two different experimental setups respectively.</p><p>Next, we implemented system 2 by using more lexical features to capture the association between concept and the context (i.e., features 5-16 in <ref type="table" target="#tab_1">Table  1</ref>). Intuitively, these lexical contextual features should be helpful in identifying concepts in parsing process. As expected, the results in <ref type="table" target="#tab_0">Table 3</ref> show that we gain 3% improvement over the two different datasets respectively, by adding only some additional lexical features.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with State-of-the-art</head><p>We give a comparison between our approach and other state-of-the-art AMR parsers, including CCGbased parser ( <ref type="bibr" target="#b0">Artzi et al., 2015</ref>) and dependencybased parser ( <ref type="bibr" target="#b20">Wang et al., 2015b</ref>). For comparison purposes, we give two results from two different versions of dependency-based AMR parser <ref type="bibr">5</ref> : CAMR* and CAMR. Compared to the latter, the former denotes the system that does not use the extended features generated from the semantic role labeling system, word sense disambiguation system and so on, which is directly comparable to our system.</p><p>From <ref type="table" target="#tab_6">Table 4</ref> we can see that our parser achieves better performance than other approaches, even without utilizing any external semantic resources.</p><p>We also evaluate our parser on the full LDC2014T12 dataset.</p><p>We use the training/development/test split recommended in the release: 10,312 sentences for training, 1,368 sentences for development and 1,371 sentences for testing. For comparison, we include the results of JAMR, CAMR*, CAMR and SMBT-based parser <ref type="bibr" target="#b18">(Pust et al., 2015)</ref>, which are also trained on the same dataset. The results in <ref type="table" target="#tab_8">Table 5</ref> show that our approach outperforms CAMR*, and obtains comparable performance with CAMR. However, our approach achieves slightly lower performance, compared to the SMBT-based parser, which adds data and features drawn from various external semantic resources.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work is motivated by JAMR ( <ref type="bibr" target="#b7">Flanigan et al., 2014</ref>), which is based on a pipelined model, resulting in a large drop in overall performance when moving from gold concepts to system concepts. <ref type="bibr" target="#b19">Wang et al. (2015a)</ref> uses a two-stage approach; dependency parses are modified by executing a sequence of actions to resolve dis-crepancies between dependency tree and AMR structure. <ref type="bibr" target="#b9">Goodman et al. (2016)</ref> improves the transition-based parser with the imitation learning algorithms, achieving almost the same performance as that of <ref type="bibr" target="#b20">Wang et al. (2015b)</ref>, which exploits the extended features from additional trained analysers, including co-reference and semantic role labelers. <ref type="bibr" target="#b0">Artzi et al. (2015)</ref> introduces a new CCG grammar induction algorithm for AMR parsing, combined with a factor graph to model non-compositional phenomena. <ref type="bibr" target="#b18">Pust et al. (2015)</ref> adapts the SBMT parsing framework to AMR parsing by designing an AMR transformation, and adding external semantic resources. More recently, <ref type="bibr" target="#b6">Damonte et al. (2016)</ref> also presents an incremental AMR parser based on a simple transition system for dependency parsing. However, compared to our parser, their parser cannot parse non-projective graphs, resulting in a limited coverage.</p><p>Our work is also inspired by a new computational task of incremental semantic role labeling, in which semantic roles are assigned to incomplete input ( <ref type="bibr" target="#b12">Konstas et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we present a new approach to AMR parsing by using an incremental model for performing the concept identification and relation identification jointly, which alleviates the error propagation in the pipelined model.</p><p>In future work, we plan to improve the parsing performance by exploring more features from the coreference resolution, word sense disambiguation system and other external semantic resources. In addition, we are interested in further incorporating the incremental semantic role labeling into our incremental framework to allow bi-directional information flow between the two closely related tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The AMR graph for the sentence "He tries to affect a British accent."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>SHIFT (lines 12-14): Add only current concept to the partial graph. (2) LEFT-ARC (lines 16-19): Add current concept and a highest-scoring edge from a node in the current concept to a node in some preceding connected component to the partial graph. (3) RIGHT-ARC (lines 21-24): Add current con- cept and a highest-scoring edge from a node in some preceding connected component to a node in current concept to the partial graph. (4) LEFT &amp; RIGHT-ARCS (lines 26-27): Add current concept and highest-scoring left arc and right arc to the partial graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustrative diagram for CWBS algorithm. Each dotted box corresponds to a connected component in the partial graph, each of which consists one or multiple concept fragments. The rightmost subgraph corresponds to the current concept fragment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>11: //Add only c i to the item 12: newitem ← item 13: AddItem(newitem, c i ) 14: AppendAgenda(buf, newitem, i, n) 15: // Add a left arc from c i to p j to the item 16: newitem ← item 17: le ← GetMaxLeftEdge(c i , p j ) 18: AddItem(newitem, c i , le) 19: AppendAgenda(buf, newitem, i, n) 20: //Add a right arc from p j to c i the item 21: newitem ← item 22: re ← GetMaxRightEdge(p j , c i ) 23: AddItem(newitem, c i , le) 24: AppendAgenda(buf, newitem, i, n) 25: //Add both left and right arc to the item 26: AddItem(item, c i , le, re) 27: AppendAgenda(buf, item, i, n) 28: innerAgenda ← B-best(buf ) 29: agenda ← innerAgenda 30: return agenda[0] 31: function AppendAgenda(buf, item, i, n) 32: //parameter n represents the terminal position 33: if i = n then 34: CalRootFeatures(item) 35: AppendItem(buf, item) GetMaxRightEdge(p j , c i ) pick the highest-scoring left-arc and right-arc linking current fragment c i and the connected component p j by the scores returned from the CalEdgeScores function, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>c</head><label></label><figDesc>represents the current con- cept label, w represents the cur- rent words, lem represents the current lemmas, pos represents the current POS tags. w −1 de- notes the first word to the left of current word, w +1 denotes the first word to the right of current word, and so on.+ w +1 11 c + pos −1 12 c + pos +1 13 c + w −2 14 c + w +2 15 c + pos −2 16 c + pos +2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>3 :</head><label>3</label><figDesc></figDesc><table>for state in agenda do 

4: 

CalEdgeScores(state, c i ) 

5: 

(p 1 , p 2 , . . . , p m ) ← FindComponents(state) 

6: 

innerAgenda ← state 

7: 

for j ← m . . . 1 do 

8: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Features associated with the concept fragments.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>10 F-measure</head><label></label><figDesc></figDesc><table>Number of training iterations 

LDC2014T112 
LDC2103E117 

Figure 3: Learning curves on development sets. 

Dataset 
System 
P 
R 
F1 

LDC2013E117 
MSCG .85 .77 .81 
CWBS .85 .78 .81 

LDC2014T12 
MSCG .84 .77 .80 
CWBS .84 .77 .80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of two different relation identification algo-

rithms. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison between our joint approaches and the 

pipelined counterparts. 

Dataset 
System 
P 
R 
F1 

LDC2013E117 
CAMR* 
.69 .67 .68 
CAMR 
.71 .69 .70 
Our approach .73 .69 .71 

LDC2014T12 
CAMR* 
.70 .66 .68 
CAMR 
.72 .67 .70 
CCG-based 
.67 .66 .66 
Our approach .73 .68 .71 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Final results of various methods.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 : Final results on the full LDC2014T12 dataset.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> In this paper, we refer to this two-step approach for relation identification as MSCG algorithm.</note>

			<note place="foot" n="4"> We use the latest, fixed version of JAMR, available at https://tiny.cc/jamr.</note>

			<note place="foot" n="5"> The code is available at https://github.com/ Juicechuan/AMRParsing</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by projects 61472191, 61272221 under the National Natural Science Foundation of China, projects 14KJB520022, 15KJA420001 under the Natural Science Research of Jiangsu Higher Education Institutions of China, and partially supported by the German Federal Ministry of Education and Research (BMBF) through the project ALL SIDES (01IW14002) and BBDC (contract 01IS14013E). We would also like to thank the insightful comments from the three anonymous reviewers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad-coverage CCG Semantic Parsing with AMR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>of the Linguistic Annotation Workshop and Interoperability with Discourse</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smatch: an Evaluation Metric for Semantic Feature Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental Parsing with the Perceptron Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Fundamental Algorithm for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Covington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Southeast Conference</title>
		<meeting>of ACM Southeast Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Incremental Parser for Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06111</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint at</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Discriminative Graph-Based Parser for the Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahil</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Noise Reduction and Targeted Exploration in Imitation Learning for Abstract Meaning Representation Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hatori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1045" to="1053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured Perceptron with Inexact Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental Semantic Role Labeling with Tree Adjoining Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incremental Joint Extraction of Entity Mentions and Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward Abstractive Summarization Using Semantic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1086" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Addressing a Question Answering Challenge by Combining Statistical Methods with Inductive Rule Learning and Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithms for Deterministic Incremental Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Entity Linking with Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parsing English into Abstract Meaning Representation Using SyntaxBased Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1143" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Transition-based Algorithm for AMR Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="366" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting Transition-based AMR Parsing with Re-fined Actions and Auxiliary Analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust Subgraph Generation Improves Abstract Meaning Representation Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Tale of Two Parsers: Investigating and Combining GraphBased And transition-Based Dependency Parsing Using Beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint Word Segmentation and POS Tagging Using a Single Perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="888" to="896" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
