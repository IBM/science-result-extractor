<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Recurrent Convolutional Neural Network for Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
							<email>cairui@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">Collaborative Innovation Center for Lanuage Ability</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871, 221009</postCode>
									<settlement>Beijing, Xuzhou</settlement>
									<region>Jiangsu</region>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">Collaborative Innovation Center for Lanuage Ability</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871, 221009</postCode>
									<settlement>Beijing, Xuzhou</settlement>
									<region>Jiangsu</region>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<email>wanghf@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">Collaborative Innovation Center for Lanuage Ability</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871, 221009</postCode>
									<settlement>Beijing, Xuzhou</settlement>
									<region>Jiangsu</region>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Recurrent Convolutional Neural Network for Relation Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="756" to="765"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Relation classification is an important semantic processing task in the field of natural language processing (NLP). In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence. Some state-of-the-art systems concentrate on modeling the shortest dependency path (SDP) between two entities leveraging convolutional or recurrent neu-ral networks. We further explore how to make full use of the dependency relations information in the SDP, by combining convolutional neural networks and two-channel recurrent neural networks with long short term memory (LSTM) units. We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time, which benefits classifying the direction of relations. Experimental results show that our method outperforms the state-of-the-art approaches on the SemEval-2010 Task 8 dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation classification aims to classify the semantic relations between two entities in a sentence. For instance, in the sentence "The <ref type="bibr">[burst]</ref> e 1 has been caused by water hammer <ref type="bibr">[pressure]</ref> e 2 ", entities burst and pressure are of relation CauseEffect(e 2 , e 1 ). Relation classification plays a key role in robust knowledge extraction, and has become a hot research topic in recent years.</p><p>Nowadays, deep learning techniques have made significant improvement in relation classification, * Corresponding author compared with traditional relation classification approaches focusing on designing effective features ( <ref type="bibr" target="#b9">Rink and Harabagiu, 2010)</ref> or kernels <ref type="bibr">(Ze- lenko et al., 2003;</ref><ref type="bibr" target="#b0">Bunescu and Mooney, 2005</ref>) Although traditional approaches are able to exploit the symbolic structures in sentences, they still suffer from the difficulty to generalize over the unseen words. Some recent works learn features automatically based on neural networks (NN), employing continuous representations of words (word embeddings). The NN research for relation classification has centered around two main network architectures: convolutional neural networks and recursive/recurrent neural networks. Convolutional neural network aims to generalize the local and consecutive context of the relation mentions, while recurrent neural networks adaptively accumulate the context information in the whole sentence via memory units, thereby encoding the global and possibly unconsecutive patterns for relation classification. <ref type="bibr" target="#b11">Socher et al. (2012)</ref> learned compositional vector representations of sentences with a recursive neural network. <ref type="bibr" target="#b2">Kazuma et al. (2013)</ref> proposed a simple customizaition of recursive neural networks. <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> proposed a convolutional neural network with position embeddings.</p><p>Recently, more attentions have been paid to modeling the shortest dependency path (SDP) of sentences. <ref type="bibr" target="#b6">Liu et al. (2015)</ref> developed a dependency-based neural network, in which a convolutional neural network has been used to capture features on the shortest path and a recursive neural network is designed to model subtrees. <ref type="bibr" target="#b14">Xu et al. (2015b)</ref> applied long short term memory (LSTM) based recurrent neural networks (RNNs) along the shortest dependency path. However, SDP is a special structure in which every two neighbor words are separated by a dependency relations. Previous works treated dependency relations in the same way as words or some syntactic features like partof-speech (POS) tags, because of the limitations of convolutional neural networks and recurrent neural networks. Our first contribution is that we propose a recurrent convolutional neural network (RCNN) to encode the global pattern in SDP utilizing a two-channel LSTM based recurrent neural network and capture local features of every two neighbor words linked by a dependency relation utilizing a convolution layer.</p><p>We further observe that the relationship between two entities are directed. For instance, <ref type="figure" target="#fig_0">Fig- ure 1</ref> shows that the shortest path of the sentence "The <ref type="bibr">[burst]</ref> e 1 has been caused by water hammer <ref type="bibr">[pressure]</ref> e 2 ." corresponds to relation CauseEffect(e 2 , e 1 ). The SDP of the sentence also corresponds to relation Cause-Effect(e 2 , e 1 ), where e 1 refers to the entity at front end of SDP and e 2 refers to the entity at back end of SDP, and the inverse SDP corresponds to relation Cause-Effect(e 1 , e 2 ). Previous work ( <ref type="bibr" target="#b14">Xu et al., 2015b</ref>) simply transforms a (K+1)-relation task into a (2K + 1) classification task, where 1 is the Other relation and K is the number of directed relations. Besides, the recurrent neural network is a biased model, where later inputs are more dominant than earlier inputs. It could reduce the effectiveness when it is used to capture the semantics of a whole shortest dependency path, because key components could appear anywhere in a SDP rather than the end.</p><p>Our second contribution is that we propose a bidirectional recurrent convolutional neural networks (BRCNN) to learn representations with bidirectional information along the SDP forwards and backwards at the same time, which also strengthen the ability to classifying directions of relationships between entities. Experimental results show that the bidirectional mechanism significantly improves the performance.</p><p>We evaluate our method on the SemEval-2010 relation classification task, and achieve a state-ofthe-art F 1 -score of 86.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Method</head><p>In this section, we describe our method in detail. Subsection 2.1 provides an overall picture of our BCRNN model. Subsection 2.2 presents the rationale of using SDPs and some characteristics of SDP. Subsection 2.3 describes the two-channel recurrent neural network, and bidirectional recurrent convolutional neural network is introduced in Subsection 2.4. Finally, we present our training objective in Subsection 2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Framework</head><p>Our BCRNN model is used to learn representations with bidirectional information along the SDP forwards and backwards at the same time. <ref type="figure" target="#fig_2">Figure  2</ref> depicts the overall architecture of the BRCNN model.</p><p>Given a sentence and its dependency tree, we build our neural network on its SDP extracted from the tree. Along the SDP, two recurrent neural networks with long short term memory units are applied to learn hidden representations of words and dependency relations respectively. A convolution layer is applied to capture local features from hidden representations of every two neighbor words and the dependency relations between them. A max pooling layer thereafter gathers information from local features of the SDP or the inverse SDP. We have a so f tmax output layer after pooling layer for classification in the unidirectional model RCNN.</p><p>On the basis of RCNN model, we build a bidirectional architecture BRCNN taking the SDP and the inverse SDP of a sentence as input. During the training stage of a (K+1)-relation task, two fine-grained so f tmax classifiers of RCNNs do a (2K + 1)-class classification respectively. The pooling layers of two RCNNs are concatenated and a coarse-grained so f tmax output layer is followed to do a (K + 1)-class classification. The final (2K+1)-class distribution is the combination of two (2K+1)-class distributions provided by finegrained classifiers respectively during the testing stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Shortest Dependency Path</head><p>If e 1 and e 2 are two entities mentioned in the same sentence such that they are observed to be in a relationship R, the shortest path between e 1 and e 2 condenses most illuminating information for the relationship R(e 1 , e 2 ). It is because (1) if entities e 1 and e 2 are arguments of the same predicate, the shortest path between them will pass through the predicate; (2) if e 1 and e 2 belong to different predicate-argument structures that share a common argument, the shortest path will pass through this argument. <ref type="bibr" target="#b0">Bunescu and Mooney (2005)</ref> first used shortest dependency paths between two entities to capture the predicate-argument sequences, which provided strong evidence for relation classification. <ref type="bibr" target="#b14">Xu et al. (2015b)</ref> captured information from the sub-paths separated by the common ancestor node of two entities in the shortest paths. However, the shortest dependency path between two entities is usually short (∼4 on average) , and the common ancestor of some SDPs is e 1 or e 2 , which leads to imbalance of two sub-paths.</p><p>We observe that, in the shortest dependency path, each two neighbor words w a and w b are linked by a dependency relation r ab . The dependency relations between a governing word and its children make a difference in meaning. Besides, if we inverse the shortest dependency path, it corresponds to the same relationship with an opposite direction. For example , in <ref type="figure" target="#fig_0">Figure 1</ref>, the shortest path is composed of some sub-structure like "burst nsub jpass − −−−−−−− → caused". Following the above intuition, we design a bidirectional recurrent convolutional neural network, which can capture features from the local substructures and inversely at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Two-Channel Recurrent Neural Network with Long Short Term Memory Units</head><p>The recurrent neural network is suitable for modeling sequential data, as it keeps hidden state vector h, which changes with input data at each step accordingly. We make use of words and dependency relations along the SDP for relations classification ( <ref type="figure" target="#fig_2">Figure 2</ref>). We call them channels as these information sources do not interact during recurrent propagation. Each word and dependency relation in a given sentence is mapped to a real-valued vector by looking up in a embedding table. The embeddings of words are trained on a large corpus unsupervisedly and are thought to be able to capture their syntactic and semantic information, and the embeddings of dependency relations are initialized randomly. The hidden state h t , for the t-th input is a function of its previous state h t−1 and the embedding x t of current input. Traditional recurrent networks have a basic interaction, that is, the input is linearly transformed by a weight matrix and nonlinearly squashed by an activation function. Formally, we have</p><formula xml:id="formula_0">h t = f (W in · x t + W rec · h t−1 + b h ) (1)</formula><p>where W in and W rec are weight matrices for the input and recurrent connections, respectively. b h is a bias term for the hidden state vector, and f a non-linear activation function. It was difficult to train RNNs to capture longterm dependencies because the gradients tend to either vanish or explode. Therefore, some more sophisticated activation function with gating units were designed. Long short term memory units are proposed in <ref type="bibr" target="#b4">Hochreiter and Schmidhuber (1997)</ref> to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed. We adopt in our method a variant introduced by <ref type="bibr" target="#b16">Zaremba and Sutskever (2014)</ref>. Concretely, the LSTM-based recurrent neural network comprises four components: an input gate i t , a forget gate f t , an output gate o t , and a memory cell c t .</p><p>First, we compute the values for i t , the input gate, and g t the candidate value for the states of   the memory cells at time t:</p><formula xml:id="formula_1">i t = σ(W i · x t + U i · h t−1 + b i ) (2) g t = tanh(W c · x t + U c · h t−1 + b c )<label>(3)</label></formula><p>Second, we compute the value for f t , the activations of the memory cells' forget gates at time t:</p><formula xml:id="formula_2">f t = σ(W f · x t + U f · h t−1 + b f )<label>(4)</label></formula><p>Given the value of the input gate activations i t , the forget gate activation f t and the candidate state value g t , we can compute c t the memory cells' new state at time t:</p><formula xml:id="formula_3">c t = i t ⊗ g t + f t ⊗ c t−1<label>(5)</label></formula><p>With the new state of the memory cells, we can compute the value of their output gates and, subsequently, their outputs:</p><formula xml:id="formula_4">o t = σ(W o · x t + U o · h t−1 + b o )<label>(6)</label></formula><formula xml:id="formula_5">h t = o t ⊗ tanh(c t )<label>(7)</label></formula><p>In the above equations, σ denotes a sigmoid function; ⊗ denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Bidirectional Recurrent Convolutional Neural Network</head><p>We observe that a governing word w a and its children w b are linked by a dependency relation r ab , which makes a difference in meaning. </p><formula xml:id="formula_6">L ab = f (W con · [h a ⊕ h ab ⊕ h b ] + b con ) (8)</formula><p>where W con is the weight matrix for the convolution layer and b con is a bias term for the hidden state vector. f is a non-linear activation function(tanh is used in our model). A pooling layer thereafter gather global information G from local features of dependency units, which is de-</p><formula xml:id="formula_7">fined as G = D max d=1 L d<label>(9)</label></formula><p>where the max function is an element-wise function, and D is the number of dependency units in the SDP. The advantage of two-channel recurrent neural network is the ability to better capture the contextual information, adaptively accumulating the context information the whole path via memory units. However, the recurrent neural network is a biased model, where later inputs are more dominant than earlier inputs. It could reduce the effectiveness when it is used to capture features for relation classification, for the entities are located at both ends of SDP and key components could appear anywhere in a SDP rather than at the end. We tackle the problem with Bidirectional Convolutional Recurrent Neural Network.</p><p>On the basis of observation, we make a hypothesis that SDP is a symmetrical structure. </p><formula xml:id="formula_8">y = so f tmax(W c · [ ← − G ⊕ − → G] + b c )<label>(10)</label></formula><p>Where W c is the transformation matrix and b c is the bias vector. Coarse-grained classifier makes use of representation with bidirectional information ignoring the direction of relations, which learns the inherent correlation between the same directed relations with opposite directions, such as R x (e 1 , e 2 ) and R x (e 2 , e 1 ). Two fine-grained so f tmax classifiers are applied to − → G and ← − G with linear transformation to give the (2K+1)-class distribution − → y and ← − y respectively. Formally,</p><formula xml:id="formula_9">− → y = so f tmax(W f · − → G + b f )<label>(11)</label></formula><formula xml:id="formula_10">← − y = so f tmax(W f · ← − G + b f )<label>(12)</label></formula><p>where W f is the transformation matrix and b f is the bias vector. Classifying − → S and ← − S respecitvely at the same time can strengthen the model ability to judge the direction of relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Training Objective</head><p>The (K + 1)-class so f tmax classifier is used to estimate probability that − → S and ← − S are of relation R . The two (2K + 1)-class so f tmax classifiers are used to estimate the probability that − → S and ← − S are of relation − → R and ← − R respectively. For a single data sample, the training objective is the penalized cross-entropy of three classifiers, given by</p><formula xml:id="formula_11">J = 2K+1 i=1 − → t i log − → y i + 2K+1 i=1 ← − t i log ← − y i + K i=1 t i log y i + λ · ||θ|| 2<label>(13)</label></formula><p>where t ∈ R K+1 , − → t and ← − t ∈ R 2K+1 , indicating the one-hot represented ground truth. y, − → y and ← − y are the estimated probabilities for each class described in section 2.4. θ is the set of model parameters to be learned, and λ is a regularization coefficient. </p><formula xml:id="formula_12">y test = α · − → y + (1 − α) · z( ← − y )<label>(14)</label></formula><p>where α is the fraction of the composition of distributions, which is set to the value 0.65 according to the performance on validation dataset. During the implementation of BRCNN, elements in two class distributions at the same position are not corresponding, e.g. Cause-Effect(e 1 , e 2 ) in − → y should correspond to Cause-Effect(e 2 , e 1 ) in ← − y . We apply a function z to transform ← − y to a corresponding forward distribution like − → y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluated our BRCNN model on the SemEval-2010 Task 8 dataset, which is an established benchmark for relation classification <ref type="bibr" target="#b3">(Hendrickx et al., 2010</ref> The dataset has (K+1)=10 distinguished relations, as follows.</p><p>• Cause-Effect</p><formula xml:id="formula_13">• Component-Whole • Content-Container • Entity-Destination • Entity-Origin • Message-Topic • Member-Collection • Instrument-Agency • Product-Agency • Other</formula><p>The former K=9 relations are directed, whereas the Other class is undirected, we have (2K+1)=19 different classes for 10 relations. All baseline systems and our model use the official macroaveraged F 1 -score to evaluate model performance. This official measurement excludes the Other relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperparameter Settings</head><p>In our experiment, word embeddings were 200-dimensional as used in ( <ref type="bibr" target="#b15">Yu et al., 2014</ref>), trained on Gigaword with word2vec ( <ref type="bibr" target="#b7">Mikolov et al., 2013)</ref>. Embeddings of relation are 50-dimensional and initialized randomly. The hidden layers in each channel had the same number of units as their embeddings (200 or 50). The convolution layer was 200-dimensional. The above values were chosen according to the performance on the validation dataset.</p><p>As we can see in <ref type="figure" target="#fig_0">Figure 1</ref>, dependency relation r "</p><formula xml:id="formula_14">prep − −− →" in − → S becomes r −1 " prep ← −− −" in ← − S .</formula><p>Experiment results show that, the performance of BR-CNN is improved if r and r −1 correspond to different relations embeddings rather than a same embedding. We notice that dependency relations contain much fewer symbols than the words contained in the vocabulary, and we initialize the embeddings of dependency relations randomly for they can be adequately tuned during supervised training.</p><p>We add l 2 penalty for weights with coefficient 10 −5 , and dropout of embeddings with rate 0.5. We applied AdaDelta for optimization <ref type="bibr" target="#b17">(Zeiler, 2012)</ref>, where gradients are computed with an adaptive learning rate. <ref type="table">Table 1 compares our BRCNN model with other  state-of-the-art methods. The first entry in the  table presents the highest performance achieved</ref> by traditional feature-based methods. <ref type="bibr" target="#b9">Rink and Harabagiu. (2010)</ref> fed a variety of handcrafted features to the SVM classifier and achieve an F 1 -score of 82.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Recent performance improvements on this dataset are mostly achieved with the help of neural networks. <ref type="bibr" target="#b11">Socher et al. (2012)</ref> built a recursive neural network on the constituency tree and achieved a comparable performance with <ref type="bibr" target="#b9">Rink and Harabagiu. (2010)</ref>. Further, they extended their recursive network with matrix-vector interaction and elevated the F 1 to 82.4%. <ref type="bibr" target="#b14">Xu et al. (2015b)</ref> first introduced a type of gated recurrent neural network (LSTM) into this task and raised the F 1 -score to 83.7%.</p><p>From the perspective of convolution, <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> constructed a CNN on the word sequence; they also integrated word position embeddings, which helped a lot on the CNN architecture. dos <ref type="bibr" target="#b1">Santos et al. (2015)</ref> proposed a similar CNN model, named CR-CNN, by replacing the common so f tmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they have achieved an F 1 -score of 84.1%. Along the line of CNNs, Xu et al. (2015a) designed a simple negative sampling method, which introduced additional samples from other corpora like the NYT dataset. Doing so greatly improved the performance to a high F 1 -score of 85.6%. <ref type="bibr" target="#b6">Liu et al. (2015)</ref> proposed a convolutional neural network with a recursive neural network designed to model the subtrees, and achieve an F 1 -score of 83.6%.</p><p>Without the use of neural networks, <ref type="bibr" target="#b15">Yu et al. (2014)</ref> proposed a Feature-based Compositional Embedding Model (FCM), which combined unlexicalized linguistic contexts and word embeddings. They achieved an F 1 -score of 83.0%.</p><p>We make use of three types of information to improve the performance of BRCNN: POS tags, NER features and WordNet hypernyms. Our proposed BRCNN model yields an F 1 -score of 86.3%, outperforming existing competing approaches. Without using any human-designed features, our model still achieve an F 1 -score of 85.4%, while the best performance of state-of-theart methods is 84.1% (dos <ref type="bibr" target="#b1">Santos et al., 2015</ref>).  For a fair comparison, hyperparameters are set according to the performance on validation dataset as BRCNN . CNN with embeddings of words, positions and dependency relations as input achieves an F 1 -score of 81.8%. LSTM with word embeddings as input only achieves an F 1 -score of 76.6%, which proves that dependency relations in SDPs play an important role in relation classification. Two-channel LSTM concatenates the pooling layers of words and dependency relations along the shortest dependency path, achieves an F 1 -score of 81.5% which is still lower than CNN. RCNN captures features from dependency units by combining the advantages of CNN and RNN, and achieves an F 1 -score of 82.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Input   <ref type="table" target="#tab_4">Table 3</ref>, if we inverted the SDP of all relations as input, we observe a performance degradation of 1.2% compared with RCNN. As mentioned in section 3.1, the SemEval-2010 task 8 dataset contains an undirected class Other in addition to 9 directed relations(18 classes). For bidirectional model, it is natural that the inversed Other relation is also in the Other class itself. However, the class Other is used to indicate that relation between two nominals dose not belong to any of the 9 directed classes. Therefore, the class Other is very noisy since it groups many different types of relations with different directions.</p><p>On the basis of the analysis above, we only inverse the SDP of directed relations. A significant improvement is observed and Bi-RCNN achieves an F 1 -score of 84.9%. This proves bidirectional representations provide more useful information to classify directed relations. We can see that our model still benefits from the coarse-grained classification, which can help our model learn inherent correlation between directed relations with opposite directions. Compared with Bi-RCNN classifying − → S and ← − S into 19 classes separately, BRCNN also conducts a 10 classes (9 directed relations and Other) classification and improves 0.5% in F 1 -score. Beyond the relation classification task, we believe that our bidirectional method is general technique, which is not restricted in a specific dataset and has the potential to benefit other NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Relation classification is an important topic in NLP. Traditional Methods for relation classification mainly fall into three classes: feature-based, kernel-based and neural network-based.</p><p>In feature-based approaches, different types of features are extracted and fed into a classifier. Generally, three types of features are often used. Lexical features concentrate on the entities of interest, e.g., POS. Syntactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class. <ref type="bibr" target="#b5">Kambhatla (2004)</ref> used a maximum entropy model for feature combination. <ref type="bibr" target="#b9">Rink and Harabagiu (2010)</ref> collected various features, including lexical, syntactic as well as semantic features.</p><p>In kernel based methods, similarity between two data samples is measured without explicit feature representation. <ref type="bibr" target="#b0">Bunescu and Mooney (2005)</ref> designed a kernel along the shortest dependency path between two entities by observing that the relation strongly relies on SDPs. <ref type="bibr" target="#b12">Wang (2008)</ref> provided a systematic analysis of several kernels and showed that relation extraction can benefit from combining convolution kernel and syntactic features. <ref type="bibr" target="#b8">Plank and Moschitti (2013)</ref> combined structural information and semantic information in a tree kernel. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function, and thus designing an effective kernel becomes crucial.</p><p>Recently, deep neural networks are playing an important role in this task. <ref type="bibr" target="#b11">Socher et al. (2012)</ref> introduced a recursive neural network model that assigns a matrix-vector representation to every node in a parse tree, in order to learn compositional vector representations for sentences of arbitrary syntactic type and length.</p><p>Convolutional neural works are widely used in relation classification. <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> proposed an approach for relation classification where sentence-level features are learned through a CNN, which has word embedding and position features as its input. In parallel, lexical features were extracted according to given nouns. dos Santos et al. (2015) tackled the relation classification task using a convolutional neural network and proposed a new pairwise ranking loss function, which achieved the state-of-the-art result in SemEval-2010 Task 8. <ref type="bibr" target="#b15">Yu et al. (2014)</ref> proposed a Factor-based Compositional Embedding Model (FCM) by deriving sentence-level and substructure embeddings from word embeddings, utilizing dependency trees and named entities. It achieved slightly higher accuracy on the same dataset than <ref type="bibr" target="#b19">Zeng et al. (2014)</ref>, but only when syntactic information is used.</p><p>Nowadays, many works concentrate on extracting features from the SDP based on neural networks. <ref type="bibr" target="#b13">Xu et al. (2015a)</ref> learned robust relation representations from SDP through a CNN, and proposed a straightforward negative sampling strategy to improve the assignment of subjects and objects. <ref type="bibr" target="#b6">Liu et al. (2015)</ref> proposed a recursive neural network designed to model the subtrees, and CNN to capture the most important features on the shortest dependency path. <ref type="bibr" target="#b14">Xu et al. (2015b)</ref> picked up heterogeneous information along the left and right sub-path of the SDP respectively, leveraging recurrent neural networks with long short term memory units. We propose BRCNN to model the SDP, which can pick up bidirectional information with a combination of LSTM and CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>763</head><p>In this paper, we proposed a novel bidirectional neural network BRCNN, to improve the performance of relation classification. The BRCNN model, consisting of two RCNNs, learns features along SDP and inversely at the same time. Information of words and dependency relations are used utilizing a two-channel recurrent neural network with LSTM units. The features of dependency units in SDP are extracted by a convolution layer.</p><p>We demonstrate the effectiveness of our model by evaluating the model on SemEval-2010 relation classification task. RCNN achieves a better performance at learning features along the shortest dependency path, compared with some common neural networks. A significant improvement is observed when BRCNN is used, outperforming state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The shortest dependency path representation for an example sentence from SemEval-08.</figDesc><graphic url="image-1.png" coords="2,94.68,62.81,408.18,121.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of BRCNN. Two-Channel recurrent neural networks with LSTM units pick up information along the shortest dependency path, and inversely at the same time. Convolution layers are applied to extract local features from the dependency units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For exam- ple, "kills nsub j − −−− → it" is distinct from "kills dob j − −− → it". The shortest dependency path is composed of many substructures like "w a r ab − − → w b ", which are hereinafter referred to as "dependency unit". Hid- den states of words and dependency relations in the SDP are obtained, utilizing two-channel recur- rent neural network. The hidden states of w a , w b and r ab are h a , h b and h ab , and the hidden state of the dependency unit d ab is [h a ⊕ h ab ⊕ h b ], where ⊕ denotes concatenate operation. Local features L ab for the dependency unit d ab can be extracted, utilizing a convolution layer upon the two-channel recurrent neural network . Formally, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>G . A coarse-grained so f tmax classifier is used to predict a (K+1)-class distribution y. For- mally,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For decoding (predicting the relation of an un- seen sample), the bidirectional model provides the (2K+1)-class distribution − → y and ← − y . The final (2K+1)-class distribution y test becomes the com- bination of − → y and ← − y . Formally,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). The dataset contains 8000 sentences for training, and 2717 for testing. We split 800 samples out of the training set for validation.</figDesc><table>Classifier 

Additional Information 
F 1 

SVM 
POS, WordNet, Prefixes and other morphological features, 
82.2 

(Rink and Harabagiu, 2010) 

dependency parse, Levin classed, PropBank, FanmeNet, 
NomLex-Plus, Google n-gram, paraphrases, TextRunner 
RNN 
Word embeddings 
74.8 

(Socher et al., 2011) 

+ POS, NER, WordNet 
77.6 
MVRNN 
Word embeddings 
79.1 

(Socher et al., 2012) 

+ POS, NER, WordNet 
82.4 
CNN 
Word embeddings 
69.7 

(Zeng et al., 2014) 

+ word position embeddings, WordNet 
82.7 
FCM 
Word embeddings 
80.6 

(Yu et al., 2014) 

+ dependency parsing, NER 
83.0 
CR-CNN 
Word embeddings 
82.8 

(dos Santos et al., 2015) 

+ word position embeddings 
84.1 
SDP-LSTM 
Word embeddings 
82.4 

(Xu et al., 2015b) 

+ POS + GR + WordNet embeddings 
83.7 
DepNN 
Word embeddings, WordNet 
83.0 

(Liu et al., 2015) 

Word embeddings, NER 
83.6 
depLCNN 
Word embeddings, WordNet, word around nominals 
83.7 

(Xu et al., 2015a) 

+ negative sampling from NYT dataset 
85.6 
BRCNN 
Word embeddings 
85.4 

(Our Model) 

+ POS, NER, WordNet embeddings 
86.3 

Table 1: Comparison of relation classification systems. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 compares our RCNN model with CNNs and RNNs.</head><label>2</label><figDesc></figDesc><table>Model 
F 1 
CNN 
81.8 
LSTM 
76.6 
Two-channel LSTM 81.5 
RCNN 
82.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparing RCNN with CNNs and 
RNNS. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparing different variants of our 
model. 

Bi-RCNN is a variant of BRCNN, which 
doesn't have the coarse-grained classifier. 
− 
→ 
S 
and 
← − 
S are shortest dependency paths described 
in section 2.4. As shown in </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language</title>
		<meeting>the conference on Human Language</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple customization of recursive neural networks for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1372" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 22. Association for Computational Linguistics</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions, page 22. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dependency-based neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Joint Conference on Natural Language Processing and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Joint Conference on Natural Language Processing and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Embeddings semantic similarity in tree kernels for domain adaption of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A re-examination of dependency path kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Joint Conference on Natural Language Processing</title>
		<meeting>the Third International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="841" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="95" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint at</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
