<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>klein@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<address>
									<postCode>94305-9040, 94305-9040</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Stanford University Stanford</orgName>
								<orgName type="institution" key="instit2">Stanford University Stanford</orgName>
								<address>
									<postCode>94305-9040, 94305-9040</postCode>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a generative model for the unsupervised learning of dependency structures. We also describe the multiplicative combination of this dependency model with a model of linear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for un-supervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention ( <ref type="bibr" target="#b3">Carroll and Charniak, 1992;</ref><ref type="bibr">Pereira and Sch- abes, 1992;</ref><ref type="bibr" target="#b2">Brill, 1993;</ref><ref type="bibr" target="#b19">Stolcke and Omohundro, 1994)</ref>. Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus <ref type="bibr" target="#b7">(Clark, 2001)</ref>, to use induction systems as a first stage in constructing large treebanks <ref type="bibr" target="#b20">(van Zaanen, 2000</ref>), to build better language models <ref type="bibr" target="#b1">(Baker, 1979;</ref><ref type="bibr" target="#b4">Chen, 1995)</ref>, and to examine cognitive issues in language learning ( <ref type="bibr" target="#b18">Solan et al., 2003</ref>). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures <ref type="bibr" target="#b1">(Baker, 1979;</ref><ref type="bibr" target="#b4">Chen, 1995)</ref>, and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated <ref type="bibr">(van Zaa- nen, 2000;</ref><ref type="bibr" target="#b7">Clark, 2001;</ref><ref type="bibr" target="#b11">Klein and Manning, 2002</ref>). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the cognitive plausibility of the induction mechanisms we present here; however, the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence undermines arguments based on "the poverty of the stimulus" <ref type="bibr" target="#b5">(Chomsky, 1965</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Unsupervised Dependency Parsing</head><p>Most recent progress in unsupervised parsing has come from tree or phrase-structure grammar based models <ref type="bibr" target="#b7">(Clark, 2001;</ref><ref type="bibr" target="#b11">Klein and Manning, 2002</ref>), but there are compelling reasons to reconsider unsupervised dependency parsing. First, most state-ofthe-art supervised parsers make use of specific lexical information in addition to word-class level information -perhaps lexical information could be a useful source of information for unsupervised methods. Second, a central motivation for using tree structures in computational linguistics is to enable the extraction of dependencies -function-argument and modification structures -and it might be more advantageous to induce such structures directly. Third, as we show below, for languages such as Chinese, which have few function words, and for which the definition of lexical categories is much less clear, dependency structures may be easier to detect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Representation and Evaluation</head><p>An example dependency representation of a short sentence is shown in <ref type="figure" target="#fig_0">figure 1(a)</ref>, where, following the traditional dependency grammar notation, the regent or head of a dependency is marked with the tail of the dependency arrow, and the dependent is marked with the arrowhead <ref type="bibr" target="#b13">(MelčukMelˇMelčuk, 1988</ref>). It will be important in what follows to see that such a representation is isomorphic (in terms of strong generative capacity) to a restricted form of phrase structure grammar, where the set of terminals and nonterminals is identical, and every rule is of the form X → X Y or X → Y X <ref type="bibr" target="#b14">(Miller, 1999)</ref>, giving the isomorphic representation of figure 1(a) shown in figure 1(b). 1 Depending on the model, part-of- speech categories may be included in the dependency representation, as shown here, or dependencies may be directly between words. Below, we will assume an additonal reserved nonterminal ROOT, whose sole dependent is the head of the sentence. This simplifies the notation, math, and the evaluation metric. A dependency analysis will always consist of exactly as many dependencies as there are words in the sentence. For example, in the dependency structure of figure 1(b), the dependencies are {(ROOT, fell), (fell, payrolls), (fell, in), (in, September), (payrolls, Factory)}. The quality of a hypothesized dependency structure can hence be evaluated by accuracy as compared to a gold-standard dependency structure, by reporting the percentage of dependencies shared between the two analyses.</p><p>In the next section, we discuss several models of dependency structure, and throughout this paper we report the accuracy of various methods at recovering gold-standard dependency parses from various corpora, detailed here. WSJ is the entire Penn English Treebank WSJ portion. WSJ10 is the subset of sentences which contained 10 words or less after the removal of punctuation. CTB10 is the sentences of the same length from the Penn Chinese treebank (v3). NEGRA10 is the same, for the German NE-GRA corpus, based on the supplied conversion of the NEGRA corpus into Penn treebank format. In most of the present experiments, the provided partsof-speech were used as the input alphabet, though we also present limited experimentation with synthetic parts-of-speech.</p><p>It is important to note that the Penn treebanks do not include dependency annotations; however, the automatic dependency rules from <ref type="bibr" target="#b8">(Collins, 1999)</ref> are sufficiently accurate to be a good benchmark for unsupervised systems for the time being (though see below for specific issues). Similar head-finding rules were used for Chinese experiments. The NE-GRA corpus, however, does supply hand-annotated dependency structures.</p><p>structures which specify orders of attachment among multiple dependents which share a common head. Where possible, we report an accuracy figure for both directed and undirected dependencies. Reporting undirected numbers has two advantages: first, it facilitates comparison with earlier work, and, more importantly, it allows one to partially obscure the effects of alternate analyses, such as the systematic choice between a modal and a main verb for the head of a sentence (in either case, the two verbs would be linked, but the direction would vary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dependency Models</head><p>The dependency induction task has received relatively little attention; the best known work is <ref type="bibr">Car- roll and Charniak (1992)</ref>, <ref type="bibr" target="#b21">Yuret (1998), and</ref><ref type="bibr" target="#b15">Paskin (2002)</ref>. All systems that we are aware of operate under the assumption that the probability of a dependency structure is the product of the scores of the dependencies (attachments) in that structure. Dependencies are seen as ordered (head, dependent) pairs of words, but the score of a dependency can optionally condition on other characteristics of the structure, most often the direction of the dependency (whether the arrow points left or right).</p><p>Some notation before we present specific models: a dependency d is a pair h, a of a head and argument, which are words in a sentence s, in a corpus S. For uniformity of notation with section 4, words in s are specified as size-one spans of s: for example the first word would be 0 s 1 . A dependency structure D over a sentence is a set of dependencies (arcs) which form a planar, acyclic graph rooted at the special symbol ROOT, and in which each word in s appears as an argument exactly once. For a dependency structure D, there is an associated graph G which represents the number of words and arrows between them, without specifying the words themselves (see <ref type="figure" target="#fig_1">figure 2)</ref>. A graph G and sentence s together thus determine a dependency structure. The   dependency structure is the object generated by all of the models that follow; the steps in the derivations vary from model to model. Existing generative dependency models intended for unsupervised learning have chosen to first generate a word-free graph G, then populate the sentence s conditioned on G. For instance, the model of <ref type="bibr" target="#b15">Paskin (2002)</ref>, which is broadly similar to the semiprobabilistic model in <ref type="bibr" target="#b21">Yuret (1998)</ref>, first chooses a graph G uniformly at random (such as <ref type="figure" target="#fig_1">figure 2</ref>), then fills in the words, starting with a fixed root symbol (assumed to be at the rightmost end), and working down G until an entire dependency structure D is filled in (figure 1a). The corresponding probabilistic model is</p><formula xml:id="formula_0">P(D) = P(s, G) = P(G)P(s|G) = P(G) (i, j,dir)∈G P( i−1 s i | j −1 s j , dir) .</formula><p>In Paskin (2002), the distribution P(G) is fixed to be uniform, so the only model parameters are the conditional multinomial distributions P(a|h, dir) that encode which head words take which other words as arguments. The parameters for left and right arguments of a single head are completely independent, while the parameters for first and subsequent arguments in the same direction are identified.</p><p>In those experiments, the model above was trained on over 30M words of raw newswire, using EM in an entirely unsupervised fashion, and at great computational cost. However, as shown in <ref type="figure" target="#fig_3">figure 3</ref>, the resulting parser predicted dependencies at below chance level (measured by choosing a random dependency structure). This below-random performance seems to be because the model links word pairs which have high mutual information (such as occurrences of congress and bill) regardless of whether they are plausibly syntactically related. In practice, high mutual information between words is often stronger between two topically similar nouns than between, say, a preposition and its object.</p><p>One might hope that the problem with this model is that the actual lexical items are too semantically charged to represent workable units of syntactic structure. If one were to apply the Paskin (2002) model to dependency structures parameterized simply on the word-classes, the result would be isomorphic to the "dependency PCFG" models described in <ref type="bibr" target="#b3">Carroll and Charniak (1992)</ref>. In these models, Carroll and Charniak considered PCFGs with precisely the productions (discussed above) that make them isomorphic to dependency grammars, with the terminal alphabet being simply partsof-speech. Here, the rule probabilities are equivalent to P(Y|X, right) and P(Y|X, left) respectively. <ref type="bibr">2</ref> The actual experiments in <ref type="bibr" target="#b3">Carroll and Charniak (1992)</ref> do not report accuracies that we can compare to, but they suggest that the learned grammars were of extremely poor quality. With hindsight, however, the main issue in their experiments appears to be not their model, but that they randomly initialized the production (attachment) probabilities. As a result, their learned grammars were of very poor quality and had high variance. However, one nice property of their structural constraint, which all dependency models share, is that the symbols in the grammar are not symmetric. Even with a grammar in which the productions are initially uniform, a symbol X can only possibly have non-zero posterior likelihood over spans which contain a matching terminal X. Therefore, one can start with uniform rewrites and let the interaction between the data and the model structure break the initial symmetry. If one recasts their experiments in this way, they achieve an accuracy of 44.7% on the Penn treebank, which is higher than choosing a random dependency structure, but lower than simply linking all adjacent words into a left-headed (and right-branching) structure (53.2%).</p><p>A huge limitation of both of the above models is that they are incapable of encoding even first-order valence facts. For example, the latter model learns that nouns to the left of the verb (usually subjects) attach to the verb. But then, given a NOUN NOUN VERB sequence, both nouns will attach to the verb -there is no way that the model can learn that verbs have exactly one subject. We now turn to an improved dependency model that addresses this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An Improved Dependency Model</head><p>The dependency models discussed above are distinct from dependency models used inside highperformance supervised probabilistic parsers in several ways. First, in supervised models, a head outward process is modeled <ref type="bibr" target="#b9">(Eisner, 1996;</ref><ref type="bibr" target="#b8">Collins, 1999</ref>). In such processes, heads generate a sequence of arguments outward to the left or right, conditioning on not only the identity of the head and direction of the attachment, but also on some notion of distance or valence. Moreover, in a head-outward model, it is natural to model stop steps, where the final argument on each side of a head is always the special symbol STOP. Models like Paskin (2002) avoid modeling STOP by generating the graph skeleton G first, uniformly at random, then populating the words of s conditioned on G. Previous work <ref type="bibr" target="#b8">(Collins, 1999</ref>) has stressed the importance of including termination probabilities, which allows the graph structure to be generated jointly with the terminal words, precisely because it does allow the modeling of required dependents. We propose a simple head-outward dependency model over word classes which includes a model of valence, which we call DMV (for dependency model with valence). We begin at the ROOT. In the standard way, each head generates a series of non-STOP arguments to one side, then a STOP argument to that side, then non-STOP arguments to the other side, then a second STOP.</p><p>For example, in the dependency structure in <ref type="figure" target="#fig_0">fig- ure 1</ref>, we first generate a single child of ROOT, here fell. Then we recurse to the subtree under fell. This subtree begins with generating the right argument in. We then recurse to the subtree under in (generating September to the right, a right STOP, and a left STOP). Since there are no more right arguments after in, its right STOP is generated, and the process moves on to the left arguments of fell.</p><p>In this process, there are two kinds of derivation events, whose local probability factors constitute the model's parameters. First, there is the decision at any point whether to terminate (generate STOP) or not: P STOP (STOP|h, dir, ad j ). This is a binary decision conditioned on three things: the head h, the direction (generating to the left or right of the head), and the adjacency (whether or not an argument has been generated yet in the current direction, a binary variable). The stopping decision is estimated directly, with no smoothing. If a stop is generated, no more arguments are generated for the current head to the current side. If the current head's argument generation does not stop, another argument is chosen using: P CHOOSE (a|h, dir). Here, the argument is picked conditionally on the identity of the head (which, recall, is a word class) and the direction. This term, also, is not smoothed in any way. Adjacency has no effect on the identity of the argument, only on the likelihood of termination. After an argument is generated, its subtree in the dependency structure is recursively generated.</p><p>Formally, for a dependency structure D, let each word h have left dependents deps D (h, l) and right dependents deps D (h, r). The following recursion defines the probability of the fragment D(h) of the dependency tree rooted at h:</p><formula xml:id="formula_1">P(D(h)) = dir∈{l,r} a∈deps D (h,dir) P STOP (¬STOP|h, dir, ad j ) P CHOOSE (a|h, dir)P(D(a)) P STOP (STOP|h, dir, ad j )</formula><p>One can view a structure generated by this derivational process as a "lexicalized" tree composed of the local binary and unary context-free configurations shown in <ref type="figure" target="#fig_4">figure 4</ref>. <ref type="bibr">3</ref> Each configuration equivalently represents either a head-outward derivation step or a context-free rewrite rule. There are four such configurations. <ref type="figure" target="#fig_4">Figure 4</ref>(a) shows a head h taking a right argument a. The tree headed by h contains h itself, possibly some right arguments of h, but no left arguments of h (they attach after all the right arguments). The tree headed by a contains a itself, along with all of its left and right children. <ref type="figure" target="#fig_4">Figure 4</ref>(b) shows a head h taking a left argument a -the tree headed by h must have already generated its right stop to do so. <ref type="figure" target="#fig_4">Figure 4</ref>(c) and <ref type="figure" target="#fig_4">figure 4(d)</ref> show the sealing operations, where STOP derivation steps are generated. The left and right marks on node labels represent left and right STOPs that have been generated. <ref type="bibr">4</ref> The basic inside-outside algorithm (Baker, 1979) can be used for re-estimation. For each sentence s ∈ S, it gives us c s (x : i, j ), the expected fraction of parses of s with a node labeled x extending from position i to position j . The model can be re-estimated from these counts. For example, to re-estimate an entry of P STOP <ref type="bibr">(STOP|h, left, non-adj)</ref> according to a current model , we calculate two quantities. <ref type="bibr">5</ref> The first is the (expected) number of trees headed by h whose rightmost edge i is strictly left of h. The second is the number of trees headed by h with rightmost edge i strictly left of h. The ratio is the MLE of that local probability factor:</p><formula xml:id="formula_2">P STOP (STOP|h, left, non-adj) = 񮽙 s∈S 񮽙 i&lt;loc(h) 񮽙 k c(h : i, k) 񮽙 s∈S 񮽙 i&lt;loc(h) 񮽙 k c(h : i, k)</formula><p>This can be intuitively thought of as the relative number of times a tree headed by h had already taken at least one argument to the left, had an opportunity to take another, but didn't. <ref type="bibr">6</ref> Initialization is important to the success of any local search procedure. We chose to initialize EM not with an initial model, but with an initial guess at posterior distributions over dependency structures (completions). For the first-round, we constructed a somewhat ad-hoc "harmonic" completion where all non-ROOT words took the same number of arguments, and each took other words as arguments in inverse proportion to (a constant plus) the distance between them. The ROOT always had a single argument and took each word with equal probability. This structure had two advantages: first, when testing multiple models, it is easier to start them all off in a common way by beginning with an M-step, and, second, it allowed us to point the model in the vague general direction of what linguistic dependency structures should look like.</p><p>On the WSJ10 corpus, the DMV model recovers a substantial fraction of the broad dependency trends: 43.2% of guessed directed dependencies were correct (63.7% ignoring direction). To our knowledge, this is the first published result to break the adjacent-word heuristic (at 33.6% for this corpus). Verbs are the sentence heads, prepositions take following noun phrases as arguments, adverbs attach to verbs, and so on. The most common source of discrepancy between the test dependencies and the model's guesses is a result of the model systematically choosing determiners as the heads of noun phrases, while the test trees have the rightmost noun as the head. The model's choice is supported by a good deal of linguistic research <ref type="bibr" target="#b0">(Abney, 1987)</ref>, and is sufficiently systematic that we also report the scores where the NP headship rule is changed to percolate determiners when present. On this adjusted metric, the score jumps hugely to 55.7% directed <ref type="bibr">(and 67.9% undirected)</ref>.</p><p>This model also works on German and Chinese at above-baseline levels (55.8% and 54.2% undirected, respectively), with no modifications whatsoever. In German, the largest source of errors is also the systematic postulation of determiner-headed nounphrases. In Chinese, the primary mismatch is that subjects are considered to be the heads of sentences rather than verbs. This dependency induction model is reasonably successful. However, our intuition is still that the model can be improved by paying more attention to syntactic constituency. To this end, after briefly recapping the model of <ref type="bibr" target="#b11">Klein and Manning (2002)</ref>, we present a combined model that exploits dependencies and constituencies. As we will see, this combined model finds correct dependencies more successfully than the model above, and finds constituents more successfully than the model of <ref type="bibr" target="#b11">Klein and Manning (2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distributional Constituency Induction</head><p>In linear distributional clustering, items (e.g., words or word sequences) are represented by characteristic distributions over their linear contexts (e.g., multinomial models over the preceding and following words, see <ref type="figure" target="#fig_6">figure 5</ref>). These context distributions are then clustered in some way, often using standard   data clustering methods. In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes. Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech <ref type="bibr" target="#b10">(Finch, 1993;</ref><ref type="bibr" target="#b17">Schütze, 1995;</ref><ref type="bibr" target="#b6">Clark, 2000)</ref>. A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose, and might therefore be put into the same class. <ref type="bibr" target="#b7">Clark (2001)</ref> and <ref type="bibr" target="#b11">Klein and Manning (2002)</ref> show that this approach can be successfully used for discovering syntactic constituents as well. However, as one might expect, it is easier to cluster word sequences (or word class sequences) than to tell how to put them together into trees. In particular, if one is given all contiguous subsequences (subspans) from a corpus of sentences, most natural clusters will not represent valid constituents (to the extent that constituency of a non-situated sequence is even a well-formed notion). For example, it is easy enough to discover that DET N and DET ADJ N are similar and that V PREP DET and V PREP DET ADJ are similar, but it is much less clear how to discover that the former pair are generally constituents while the latter pair are generally not. In <ref type="bibr" target="#b11">Klein and Manning (2002)</ref>, we proposed a constituent-context model (CCM) which solves this problem by building constituency decisions directly into the distributional model, by earmarking a single cluster d for non-constituents. During the calculation of cluster assignments, only a non-crossing subset of the observed word sequences can be assigned to other, constituent clusters. This integrated approach is empirically successful.</p><p>The CCM works as follows. Sentences are given as sequences s of word classes (parts-of-speech or otherwise). One imagines each sentence as a list of the O(n 2 ) index pairs i, j , each followed by the corresponding subspan i s j and linear context i−1 s i ∼ j s j +1 (see <ref type="figure" target="#fig_6">figure 5)</ref>. The model generates all constituent-context pairs, span by span.</p><p>The first stage is to choose a bracketing B for the sentence, which is a maximal non-crossing subset of the spans (equivalent to a binary tree). In the basic model, P(B) is uniform over binary trees. Then, for each i, j , the subspan and context pair ( i s j , i−1 s i ∼ j s j +1 ) is generated via a classconditional independence model:</p><formula xml:id="formula_3">P(s, B) = P(B) i, j P( i s j |b i j )P( i−1 s i ∼ j s j +1 |b i j )</formula><p>That is, all spans guess their sequences and contexts given only a constituency decision b. <ref type="bibr">7</ref> This is a model P(s, B) over hidden bracketings and observed sentences, and it is estimated via EM to maximize the sentence likelihoods P(s) over the training corpus. <ref type="figure" target="#fig_8">Figure 6</ref> shows the accuracy of the CCM model not only on English but for the Chinese and German corpora discussed above. <ref type="bibr">8</ref> Results are reported at convergence; for the English case, F 1 is monotonic during training, while for the others, there is an earlier peak.</p><p>Also shown is an upper bound (the target trees are not all binary and so any all-binary system will overpropose constituents). <ref type="bibr" target="#b11">Klein and Manning (2002)</ref> gives comparative numbers showing that the basic CCM outperforms other recent systems on the ATIS corpus (which many other constituency induction systems have reported on). While absolute numbers are hard to compare across corpora, all the systems compared to in <ref type="bibr" target="#b11">Klein and Manning (2002)</ref> parsed below a right-branching baseline, while the CCM is substantially above it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A Combined Model</head><p>The two models described above have some common ground. Both can be seen as models over lexicalized trees composed of the configurations in <ref type="figure" target="#fig_4">fig- ure 4</ref>. For the DMV, it is already a model over these structures. At the "attachment" rewrite for the CCM in (a/b), we assign the quantity:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P( i s k |true)P( i−1 s i ∼ k s k+1 |true) P( i s k |false)P( i−1 s i ∼ k s k+1 |false)</head><p>which is the odds ratio of generating the subsequence and context for span i, k as a constituent as opposed to a non-constituent. If we multiply all trees' attachment scores by</p><formula xml:id="formula_4">i, j P( i s j |false)P( i−1 s i ∼ j s j +1 |false)</formula><p>the denominators of the odds ratios cancel, and we are left with each tree being assigned the probability it would have received under the CCM. <ref type="bibr">9</ref> In this way, both models can be seen as generating either constituency or dependency structures. Of course, the CCM will generate fairly random dependency structures (constrained only by bracketings). Getting constituency structures from the DMV is also problematic, because the choice of which side to first attach arguments on has ramifications on constituency -it forces x-bar-like structures -even though it is an arbitrary convention as far as dependency evaluations are concerned. For example, if we attach right arguments first, then a verb with a left subject and a right object will attach the object first, giving traditional VPs, while the other attachment order gives subject-verb groups. To avoid this bias, we alter the DMV in the following ways. When using the dependency model alone, we allow each word to have even probability for either generation order (but in each actual head derivation, only one order occurs). When using the models together, better performance was obtained by releasing the one-side-attaching-first requirement entirely.</p><p>In <ref type="figure" target="#fig_8">figure 6</ref>, we give the behavior of the CCM constituency model and the DMV dependency model on both constituency and dependency induction. Unsurprisingly, their strengths are complementary. The CCM is better at recovering constituency, and the dependency model is better at recovering dependency structures. It is reasonable to hope that a combination model might exhibit the best of both. In the supervised parsing domain, for example, scoring a lexicalized tree with the product of a simple lexical dependency model and a PCFG model can outperform each factor on its respective metric ( <ref type="bibr" target="#b12">Klein and Manning, 2003)</ref>. <ref type="bibr">9</ref> This scoring function as described is not a generative model over lexicalized trees, because it has no generation step at which nodes' lexical heads are chosen. This can be corrected by multiplying in a "head choice" factor of 1/(k − j ) at each final "sealing" configuration (d). In practice, this correction factor was harmful for the model combination, since it duplicated a strength of the dependency model, badly.   In the combined model, we score each tree with the product of the probabilities from the individual models above. We use the inside-outside algorithm to sum over all lexicalized trees, similar to the situation in section 3. The tree configurations are shown in <ref type="figure" target="#fig_4">figure 4</ref>. For each configuration, the relevant scores from each model are multiplied together. For example, consider figure 4(a). From the CCM we must generate i s k as a constituent and its corresponding context. From the dependency model, we pay the cost of h taking a as a right argument (P CHOOSE ), as well as the cost of choosing not to stop (P STOP ). We then running the inside-outside algorithm over this product model. For the results, we can extract the sufficient statistics needed to reestimate both individual models. <ref type="bibr">10</ref> The models in combination were intitialized in the same way as when they were run individually. Sufficient statistics were separately taken off these individual completions. From then on, the resulting models were used together during re-estimation. <ref type="figure" target="#fig_8">Figure 6</ref> summarizes the results. The combined model beats the CCM on English F 1 : 77.6 vs. 71.9. The figure also shows the combination model's score when using word classes which were induced entirely automatically, using the simplest distributional clustering method of <ref type="bibr" target="#b17">Schütze (1995)</ref>. These classes show some degradation, e.g. 72.9 F 1 , but it is worth noting that these totally unsupervised numbers are better than the performance of the CCM model of <ref type="bibr" target="#b11">Klein and Manning (2002)</ref> running off of Penn treebank word classes. Again, if we modify the gold standard so as to make determiners the head of NPs, then this model with distributional tags scores 50.6% on directed and 64.8% on undirected dependency accuracy.</p><p>On the German data, the combination again outperforms each factor alone, though while the combination was most helpful at boosting constituency quality for English, for German it provided a larger boost to the dependency structures. Finally, on the Chinese data, the combination did substantially boost dependency accuracy over either single factor, but actually suffered a small drop in constituency. 11 Overall, the combination is able to combine the individual factors in an effective way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a successful new dependencybased model for the unsupervised induction of syntactic structure, which picks up the key ideas that have made dependency models successful in supervised statistical parsing work. We proceeded to show that it works cross-linguistically. We then demonstrated how this model could be combined with the previous best constituent-induction model to produce a combination which, in general, substantially outperforms either individual model, on either metric. A key reason that these models are capable of recovering structure more accurately than previous work is that they minimize the amount of hidden structure that must be induced. In particular, neither model attempts to learn intermediate, recursive categories with no direct connection to surface statistics. Our results here are just on the ungrounded induction of syntactic structure. Nonetheless, we see the investigation of what patterns can be recovered from corpora as important, both from a computational perspective and from a philosophical one. It demonstrates that the broad constituent and dependency structure of a language can be recovered quite successfully (individually or, more effectively, jointly) from a very modest amount of training data.</p><p>the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program. This work also benefited from an enormous amount of useful feedback, from many audiences and individuals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three kinds of parse structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dependency graph with skeleton chosen, but words not populated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Parsing performance (directed and undirected dependency accuracy) of various dependency models on various treebanks, along with baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Dependency configurations in a lexicalized tree: (a) right attachment, (b) left attachment, (c) right stop, (d) left stop. h and a are head and argument words, respectively, while i , j , and k are positions between words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Span</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The CCM model's generative process for the sentence in figure 1. (a) A binary tree-equivalent bracketing is chosen at random. (b) Each span generates its yield and context (empty spans not shown here). Derivations which are not coherent are given mass zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Parsing performance of the combined model on various treebanks, along with baselines.</figDesc></figure>

			<note place="foot" n="1"> Strictly, such phrase structure trees are isomorphic not to flat dependency structures, but to specific derivations of those</note>

			<note place="foot" n="2"> There is another, subtle distinction: in the Paskin work, a canonical ordering of multiple attachments was fixed, while in the Carroll and Charniak work all attachment orders are considered, giving a numerical bias towards structures where heads take more than one argument.</note>

			<note place="foot" n="3"> It is lexicalized in the sense that the labels in the tree are derived from terminal symbols, but in our experiments the terminals were word classes, not individual lexical items.</note>

			<note place="foot" n="4"> Note that the asymmetry of the attachment rules enforces the right-before-left attachment convention. This is harmless and arbitrary as far as dependency evaluations go, but imposes an x-bar-like structure on the constituency assertions made by this model. This bias/constraint is dealt with in section 5. 5 To simplify notation, we assume each word h occurs at most one time in a given sentence, between indexes loc(h) and loc(h) + 1). 6 As a final note, in addition to enforcing the right-argumentfirst convention, we constrained ROOT to have at most a single dependent, by a similar device.</note>

			<note place="foot" n="7"> As is typical of distributional clustering, positions in the corpus can get generated multiple times. Since derivations need not be consistent, the entire model is mass deficient when viewed as a model over sentences. 8 In Klein and Manning (2002), we reported results using unlabeled bracketing statistics which gave no credit for brackets which spanned the entire sentence (raising the scores) but macro-averaged over sentences (lowering the scores). The numbers here hew more closely to the standard methods used for evaluating supervised parsers, by being micro-averaged and including full-span brackets. However, the scores are, overall, approximately the same.</note>

			<note place="foot" n="10"> The product, like the CCM itself, is mass-deficient.</note>

			<note place="foot" n="7"> Acknowledgements This work was supported by a Microsoft Graduate Research Fellowship to the first author and by 11 This seems to be partially due to the large number of unanalyzed fragments in the Chinese gold standard, which leave a very large fraction of the posited bracketings completely unjudged.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The English Noun Phrase in its Sentential Aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, MIT</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trainable grammars for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">K</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Communication Papers for the 97th Meeting of the</title>
		<editor>D. H. Klatt and J. J. Wolf</editor>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="1979" />
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic grammar induction and parsing free text: A transformation-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 31</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="259" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two experiments on learning probabilistic dependency grammars from corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the Workshop Statistically-Based NLP Techniques</title>
		<editor>Carl Weir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, editors</editor>
		<meeting><address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian grammar induction for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 33</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Aspects of the Theory of Syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inducing syntactic categories by context distribution clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fourth Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised induction of stochastic contextfree grammars using distributional clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fifth Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 16</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Finding Structure in Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven Paul</forename><surname>Finch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A generative constituent-context model for improved grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 40</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast exact inference with a factored model for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Suzanna Becker, Sebastian Thrun, and Klaus Obermayer</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dependency Syntax: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">Aleksandrovich</forename><surname>Melčukmelˇmelčuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<pubPlace>Albany, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>State University of New York Press</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Strong Generative Capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>CSLI Publications</publisher>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grammatical bigrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inside-outside reestimation from partially bracketed corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 30</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributional part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 7</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic acquisition and efficient representation of syntactic structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Suzanna Becker, Sebastian Thrun, and Klaus Obermayer</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inducing probabilistic grammars by Bayesian model merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Omohundro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grammatical Inference and Applications: Proceedings of the Second International Colloquium on Grammatical Inference</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ABL: Alignment-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menno Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING 18</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="961" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Discovery of Linguistic Relations Using Lexical Attraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, MIT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
