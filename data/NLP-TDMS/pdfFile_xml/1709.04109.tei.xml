<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Empower Sequence Labeling with Task-Aware Neural Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>shang7@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
							<email>frankxu@sjtu.edu.cnfacebook</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
							<email>huangui@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
							<email>jianpeng@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">University of Southern California</orgName>
								<orgName type="institution" key="instit3">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Empower Sequence Labeling with Task-Aware Neural Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Linguistic sequence labeling is a general approach encompassing a variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neu-ral networks (NNs) make it possible to build reliable models without handcrafted features. However, in many cases, it is hard to obtain sufficient annotations to train these models. In this study, we develop a neural framework to extract knowledge from raw texts and empower the sequence labeling task. Besides word-level knowledge contained in pre-trained word embeddings, character-aware neural language models are incorporated to extract character-level knowledge. Transfer learning techniques are further adopted to mediate different components and guide the language model towards the key knowledge. Comparing to previous methods, these task-specific knowledge allows us to adopt a more concise model and conduct more efficient training. Different from most transfer learning methods, the proposed framework does not rely on any additional supervision. It extracts knowledge from self-contained order information of training sequences. Extensive experiments on benchmark datasets demonstrate the effectiveness of leveraging character-level knowledge and the efficiency of co-training. For example, on the CoNLL03 NER task, model training completes in about 6 hours on a single GPU, reaching F1 score of 91.71±0.10 without using any extra annotations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Linguistic sequence labeling is a fundamental framework. It has been applied to a variety of tasks including part-ofspeech (POS) tagging, noun phrase chunking and named entity recognition (NER) <ref type="bibr" target="#b14">(Ma and Hovy 2016;</ref><ref type="bibr" target="#b25">Sha and Pereira 2003)</ref>. These tasks play a vital role in natural language understanding and fulfill lots of downstream applications, such as relation extraction, syntactic parsing, and entity linking ( <ref type="bibr" target="#b12">Liu et al. 2017;</ref><ref type="bibr" target="#b13">Luo et al. 2015</ref>).</p><p>Traditional methods employed machine learning models like Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs), and have achieved relatively high performance. However, these methods have a heavy reliance on handcrafted features (e.g., whether a word is capitalized) and language-specific resources (e.g., gazetteers). Therefore, it could be difficult to apply them to new tasks or shift to new domains. To overcome this drawback, neural networks (NNs) have been proposed to automatically extract features during model learning. Nevertheless, considering the overwhelming number of parameters in NNs and the relatively small size of most sequence labeling corpus, annotations alone may not be sufficient to train complicated models. So, guiding the learning process with extra knowledge could be a wise choice.</p><p>Accordingly, transfer learning and multi-task learning have been proposed to incorporate such knowledge. For example, NER can be improved by jointly conducting other related tasks like entity linking or chunking ( <ref type="bibr" target="#b13">Luo et al. 2015;</ref><ref type="bibr" target="#b19">Peng and Dredze 2016)</ref>. After all, these approaches would require additional supervision on related tasks, which might be hard to get, or not even existent for low-resource languages or special domains.</p><p>Alternatively, abundant knowledge can be extracted from raw texts, and enhance a variety of tasks. Word embedding techniques represent words in a continuous space ( <ref type="bibr" target="#b18">Mikolov et al. 2013;</ref><ref type="bibr" target="#b20">Pennington, Socher, and Manning 2014)</ref> and retain the semantic relations among words. Consequently, integrating these embeddings could be beneficial to many tasks ( <ref type="bibr" target="#b12">Liu et al. 2017;</ref><ref type="bibr" target="#b11">Lample et al. 2016</ref>). Nonetheless, most embedding methods take a word as a basic unit, thus only obtaining word-level knowledge, while character awareness is also crucial and highly valued in most state-ofthe-art NN models.</p><p>Only recently, character-level knowledge has been leveraged and empirically verified to be helpful in numerous sequence labeling tasks ( <ref type="bibr" target="#b21">Peters et al. 2017;</ref><ref type="bibr" target="#b23">Rei 2017)</ref>. Directly adopting pre-trained language models, character-level knowledge can be integrated as context embeddings and demonstrate its potential to achieve the state-of-the-art <ref type="bibr">(Pe- ters et al. 2017</ref>). However, the knowledge extracted through pre-training is not task-specific, thus containing a large irrelevant portion. So, this approach would require a bigger model, external corpus and longer training. For example, one of its language models was trained on 32 GPUs for more than half a month, which is unrealistic in many situations.</p><p>In this paper, we propose an effective sequence labeling framework, LM-LSTM-CRF, which leverages both wordlevel and character-level knowledge in an efficient <ref type="bibr">way</ref> word-level lstm unit concat unit c0, c1,0 c1,1 c1,2 c1,3 c1,4 c1,5 c1, c2,0 c2,1 c2,2 c2,3 c2,4 c2,5 c2,</p><formula xml:id="formula_0">c3,0 c3, c4,0 c4,1 c4, c5,0 c5,1 c5,2 c5,3 c5,4 c5, x1 x2 x3 x4 x5 c NNP y2 NNP y1 , y3 CD y4 NNS y5</formula><p>backward-to-SL highway backward-to-LM highway forward-to-SL highway forward-to-LM highway x <ref type="figure">Figure 1</ref>: LM-LSTM-CRF Neural Architecture model with the sequence labeling task and conduct multitask learning to guide the language model towards taskspecific key knowledge. Besides the potential of training a better model, this strategy also poses a new challenge. Based on our experiments, when the tasks are discrepant, language models could be harmful to sequence labeling in a na¨ıvena¨ıve co-training setting. For this reason, we employ highway networks <ref type="bibr" target="#b28">(Srivastava, Greff, and Schmidhuber 2015)</ref> to transform the output of character-level layers into different semantic spaces, thus mediating and unifying these two tasks. For word-level knowledge, we choose to fine-tune pre-trained word embeddings instead of co-training or pretraining the whole word-level layers, because the majority of parameters in word-level layers come from the embedding layer and such co-training or pre-training cost lots of time and resources. We conduct experiments on the CoNLL 2003 NER task, the CoNLL 2000 chunking task, as well as the WSJ portion of the Penn Treebank POS tagging task. LM-LSTM-CRF achieves a significant improvement over the state-of-the-art. Also, our co-training strategy allows us to capture more useful knowledge with a smaller network, thus yielding much better efficiency without loss of effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LM-LSTM-CRF Framework</head><p>The neural architecture of our proposed framework, LM-LSTM-CRF, is visualized in <ref type="figure">Fig. 1</ref>. For a sentence with annotations y = (y 1 , . . . , y n ), its word-level input is marked as x = (x 1 , x 2 , . . . , x n ), where x i is the ith word; its character-level input is recorded as c = (c 0, , c 1,1 , c 1,2 , . . . , c 1, , c 2,1 , . . . , c n, ), where c i,j is the j-th character for word w i and c i, is the space character after w i . These notations are also summarized in <ref type="table" target="#tab_1">Table 1</ref>. Now, we first discuss the multi-task learning strategy and then introduce the architecture in a bottom-up fashion.</p><p>x word-level input</p><formula xml:id="formula_1">x i i-th word c character-level input c i,j j-th char in x i c i, space after x i c 0, space before x 1 y label sequence y i label of x i f i output of forward character-level LSTM at c i, r i</formula><p>output of backward character-level LSTM at c i, f L i output of forward-to-LM highway unit r L i output of backward-to-LM highway unit f N i output of forward-to-SL highway unit r N i output of backward-to-SL highway unit v i input of word-level bi-LSTM at x i z i output of word-level bi-LSTM at x i </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task Learning Strategy</head><p>As shown in <ref type="figure">Fig. 1</ref>, our language model and sequence labeling model share the same character-level layer, which fits the setting of multi-task learning and transfer learning. However, different from typical models of this setting, our two tasks are not strongly related. This discordance makes our problem more challenging. E.g., although a naive co-training setting, which directly uses the output from character-level layers, could be effective in several scenarios <ref type="bibr" target="#b32">(Yang, Salakhutdinov, and Cohen 2017)</ref>, for our two tasks, it would hurt the performance. This phenomenon would be further discussed in the experiment section.</p><p>To mediate these two tasks, we transform the output of character-level layers into different semantic spaces for different objectives. This strategy allows character-level layers to focus on general feature extraction and lets the transform layers select task-specific features. Hence, our language model can provide related knowledge to the sequence labeling, without forcing it to share the whole feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character-level Layer</head><p>Character-level neural language models are trained purely on unannotated sequence data but can capture the underlying style and structure. For example, it can mimic Shakespeare's writing and generate sentences of similar styles, or even master the grammar of programming languages (e.g., XML, L A T E X, and C) and generate syntactically correct codes <ref type="bibr" target="#b9">(Karpathy 2015)</ref>. Accordingly, we adopted the character-level Long Short Term Memory (LSTM) networks to process character-level input. Aiming to capture lexical features instead of remembering words' spelling, we adjust the prediction from the next character to the next word. As in <ref type="figure">Fig. 1</ref>, the character-level LSTM would only make predictions for the next word at word boundaries (i.e., space characters or c i, ).</p><p>Furthermore, we coupled two LSTM units to capture information in both forward and backward directions. Although it seems similar to the bi-LSTM unit, the outputs of these two units are processed and aligned differently. Specifically, we record the output of forward LSTM at c i, as f i , and the output of backward LSTM at c i, as r i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highway Layer</head><p>In computer vision, Convolutional Neural Networks (CNN) has been proved to be an effective feature extractor, but its output needs to be further transformed by fully-connected layers to achieve the state-of-the-art. Bearing this in mind, it becomes natural to stack additional layers upon the flat character-level LSTMs. More specifically, we employ highway units (Srivastava, Greff, and Schmidhuber 2015), which allow unimpeded information flowing across several layers. Typically, highway layers conduct nonlinear transformation as</p><formula xml:id="formula_2">m = H(n) = t g(W H n + b H ) + (1 − t) n , where</formula><p>is element-wise product, g(·) is a nonlinear transformation such as ReLU in our experiments, t = σ(W T n + b T ) is called transform gate and (1 − t) is called carry gate.</p><p>In our final architecture, there are four highway units, named forward-to-LM, forward-to-SL, backward-to-LM, and backward-to-SL. The first two transfer f i into f L i and f N i , and the last two transfer r i into r L i and r N i . f L i and r L i are used in the language model, while f N i and r N i are used in the sequence labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word-level Layer</head><p>Bi-LSTM is adopted as the word-level structure to capture information in both directions. As shown in <ref type="figure">Fig. 1</ref>, we concatenate f N i and r N i−1 with word embeddings and then feed them into the bi-LSTM. Note that, in the backward character-level LSTM, c i−1, is the space character before word x i , therefore, f N i would be aligned and concatenated with r N i−1 instead of r N i . For example, in <ref type="figure">Fig. 1</ref>, the word embeddings of 'Pierre' will be concatenated with the output of the forward-to-SL over '. . .Pierre ' and the output of the backward-to-SL over '. . .erreiP '.</p><p>As to word-level knowledge, we chose to fine-tune pretrained word embeddings, instead of co-training the whole word-level layer. This is because most parameters of our word-level model come from word embeddings, and finetuning pre-trained word embeddings have been verified to be effective in leveraging word-level knowledge <ref type="bibr" target="#b14">(Ma and Hovy 2016)</ref>. Besides, current word embedding methods can easily scale to the large corpus; pre-trained word embeddings are available in many languages and domains <ref type="bibr" target="#b2">(Fernandez, Yu, and Downey 2017)</ref>. However, this strategy cannot be applied to character-level layers, since the embedding layer of character-level layers contains very few parameters. Based on these considerations, we applied different strategies to leverage word-level knowledge from character-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF for Sequence Labeling</head><p>Label dependencies are crucial for sequence labeling tasks. For example, in NER task with BIOES annotation, it is not only meaningless but illegal to annotate I-PER after B-ORG (i.e., mixing the person and the organization). Therefore, jointly decoding a chain of labels can ensure the resulting label sequence to be meaningful. Conditional random field (CRF) has been included in most state-of-the-art models to capture such information and further avoid generating illegal annotations. Consequently, we build a CRF layer upon the word-level LSTM.</p><p>For training instance (x i , c i , y i ), we suppose the output of word-level LSTM is Z i = (z i,1 , z i,2 , . . . , z i,n ). CRF models describe the probability of generating the whole label sequence with regard to</p><formula xml:id="formula_3">(x i , c i ) or Z. That is, p(ˆ y|x i , c i ) or p(ˆ y|Z), wherê y = (ˆ y 1 , . . . , ˆ y n )</formula><p>is a generic label sequence. Similar to (Ma and Hovy 2016), we define this probability as follows.</p><formula xml:id="formula_4">p(ˆ y|x i , c i ) = n j=1 φ(ˆ y j−1 , ˆ y j , z j ) y ∈Y(Z) n j=1 φ(y j−1 , y j , z j )<label>(1)</label></formula><p>Here, Y(Z) is the set of all generic label sequences, φ(y j−1 , y j , z j ) = exp(W yj−1,yj z i + b yj−1,yj ), where W yj−1,yj and b yj−1,yj are the weight and bias parameters corresponding to the label pair (y j−1 , y j ). For training, we minimize the following negative loglikelihood.</p><formula xml:id="formula_5">J CRF = − i log p(y i |Z i )<label>(2)</label></formula><p>And for testing or decoding, we want to find the optimal sequence y * that maximizes the likelihood.</p><formula xml:id="formula_6">y * = arg max y∈Y(Z) p(y|Z)<label>(3)</label></formula><p>Although the denominator of Eq. 1 is complicated, we can calculate Eqs. 2 and 3 efficiently by the Viterbi algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Language Model</head><p>The language model is a family of models describing the generation of sequences. In a neural language model, the generation probability of the sequence x = (x 1 , ..., x n ) in the forward direction (i.e., from left to right) is defined as  </p><formula xml:id="formula_7">p f (x 1 , ..., x n ) = N i=1 p f (x i |x 1 , . . . , x i−1 )</formula><formula xml:id="formula_8">(x i |c 0, , . . . , c i−1,1 , . . . , c i−1, ) instead of p f (x i |x 1 , . . . , x i−1 )</formula><p>. This probability is assumed as</p><formula xml:id="formula_9">p f (x i |c 0, , . . . , c i−1, ) = exp(w T xi f N i−1 ) ˆ xj exp(w T ˆ xj f N i−1 )</formula><p>where w xi is the weight vector for predicting word x i . In order to extract knowledge in both directions, we also adopted a reversed-order language model, which calculates the generation probability from right to left as</p><formula xml:id="formula_10">p r (x 1 , ..., x n ) = N i=1 p r (x i |c i+1, , . . . , c n, ) where p r (x i |c i+1, , . . . , c n, ) = exp(w T xi r N i ) ˆ xj exp(w T ˆ xj r N i )</formula><p>The following negative log likelihood is applied as the objective function of our language model.</p><formula xml:id="formula_11">J LM = − i log p f (x i ) − i log p r (x i )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Model Learning</head><p>By combining Eqs. 2 and 4, we can write the joint objective function as</p><formula xml:id="formula_12">J = − i p(y i |Z i ) + λ log p f (x i ) + log p r (x i )<label>(5)</label></formula><p>where λ is a weight parameter. In our experiments, λ is always set to 1 without any tuning. In order to train the neural network efficiently, stochastic optimization has been adopted. And at each iteration, we sample a batch of training instances and perform an update according to the summand function of Eq. 5:</p><formula xml:id="formula_13">p(y i |Z i ) + λ log p f (x i ) + log p r (x i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Here  <ref type="table" target="#tab_1">Parameter POS NER chunking  character-level dimension  30  embedding  character-level  depth  1  LSTM  state size  300  Highway  depth  1  word-level  dimension  100  embedding</ref>   <ref type="table" target="#tab_3">Table 2</ref>. We report the accuracy for the WSJ dataset. And in the first two datasets, we adopt the official evaluation metric (microaveraged F 1 ), and use the BIOES scheme ( <ref type="bibr" target="#b22">Ratinov and Roth 2009)</ref>. Also, in all three datasets, rare words (i.e., frequency less than 5) are replaced by a special token (&lt;UNK&gt;).</p><note type="other">word-level depth 1 bi-LSTM state size 300 Optimization η 0 0.015 0.01</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Training</head><p>For a fair comparison, we didn't spend much time on tuning parameters but borrow the initialization, optimization method, and all related hyper-parameter values (except the state size of LSTM) from the previous work <ref type="bibr" target="#b14">(Ma and Hovy 2016)</ref>. For the hidden state size of LSTM, we expand it from 200 to 300, because introducing additional knowledge allows us to train a larger network. We will further discuss this change later. <ref type="table" target="#tab_5">Table 3</ref> summarizes some important hyperparameters. Since the CoNLL00 is similar to the CoNLL03 NER dataset, we conduct experiments with the same parameters on both tasks. Initialization. We use GloVe 100-dimension pre-trained word embeddings released by Stanford 1 and randomly initialize the other parameters (Glorot and Bengio 2010; Jozefowicz, Zaremba, and Sutskever 2015). Optimization. We employ mini-batch stochastic gradient descent with momentum. The batch size, the momentum and the learning rate are set to 10, 0.9 and η t = η0 1+ρt , where η 0 is the initial learning rate and ρ = 0.05 is the decay ratio. Dropout is applied in our model, and its ratio is fixed to 0.5. To increase stability, we use gradient clipping of 5.0. Network Structure. The hyper-parameters of characterlevel LSTM are set to the same value of word-level bi-LSTM. We fix the depth of highway layers as 1 to avoid an over-complicated model. Note that some baseline methods (e.g., <ref type="bibr" target="#b1">(Chiu and Nichols 2016;</ref><ref type="bibr" target="#b21">Peters et al. 2017)</ref>) incorporate the development set as a part of training. However, because we are using early stopping based on the evaluation on the development set, our model is trained purely on the training set.</p><p>Compared Methods We consider three classes of baseline sequence labeling methods in our experiments.</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison</head><p>In this section, we focus on the comparisons between LM-LSTM-CRF and previous state-of-the-arts, including both effectiveness and efficiency. As demonstrated in Tables 4, 5 and 7, LM-LSTM-CRF significantly outperforms all baselines without additional resources. Moreover, even for those baselines with extra resources, LM-LSTM-CRF beats most of them and is only slightly worse than TagLM (index 4) ( <ref type="bibr" target="#b21">Peters et al. 2017</ref>   <ref type="bibr" target="#b24">(Reimers and Gurevych 2017)</ref>. During our experiments, we discover that, when trained on CPU, LSTM-CNN-CRF only reaches 90.83 F 1 score on the NER dataset, but gets 91.37 F 1 score when trained on GPU. We conjecture that this performance gap is due to the difference of runtime environments. Therefore, we conduct all of our experiments on GPU. Additionally, we can observe that, although co-trained with language model, results of index 12 fails to outperform LSTM-CNN-CRF or LSTM-CRF. The reason of this phenomenon could be complicated and beyond the scope of this paper. However, it verified the effectiveness of our method, and demonstrated the contribution of outperforming these baselines.</p><p>NER First of all, we have to point out that the results of index 1, 4, 8, 10 and 11 are not directly comparable with others since their final models are trained on both training and development set, while others are trained purely on the training set. As mentioned before, LM-LSTM-CRF outperforms all baselines except TagLM (index 4). For a thorough comparison, we also compare to its variants, TagLM (index 5), TagLM (index 10) and TagLM (index 11). Both index 10 and 11 are trained on the CoNLL03 dataset alone, while index 11 utilizes language model and index 10 doesn't. Comparing F 1 scores of these two settings, we can find that TagLM (index   <ref type="table">Table 6</ref>: Training statistics of TagLM (index 4 and 5) and LM-LSTM-CRF on the CoNLL03 NER dataset.</p><p>11) even performs worse than TagLM (index 10) , which reveals that directly applying co-training might hurt the sequence labeling performance. We will also discuss this challenge later in the Highway Layers &amp; Co-training section. Besides, changing the forward language model from 4096-8192-1024 to LSTM-2048-512, TagLM (index 5) gets a lower F 1 score of 91.62±0.23. Comparing this score to ours (91.71±0.10), one can verify that pre-trained language model usually extracts a large portion of unrelated knowledge. Relieving such redundancy by guiding the language model with task-specific information, our model is able to conduct both effective and efficient learning.</p><p>POS Tagging Similar to the NER task, LM-LSTM-CRF outperforms all baselines on the WSJ portion of the PTB POS tagging task. Although the improvements over LSTM-CRF and CNN-LSTM-CRF are less obvious than those on the CoNLL03 NER dataset, considering the fact that the POS tagging task is believed to be easier than the NER task and current methods have achieved relatively high performance, this improvement could still be viewed as significant. Moreover, it is worth noting that for both NER and POS tagging tasks, LM-LSTM-CRF achieves not only higher F 1 scores, but also with smaller variances, which further verifies the superiority of our framework. Chunking In the chunking task, LM-LSTM-CRF also achieves relatively high F 1 scores, but with slightly higher variances. Considering the fact that this corpus is much smaller than the other two (only about 1/5 of WSJ or 1/2 of CoNLL03 NER), we can expect more variance due to the   Efficiency We implement LM-LSTM-CRF 5 based on the PyTorch library <ref type="bibr">6</ref> . Models has been trained on one GeForce GTX 1080 GPU, with training time recorded in <ref type="table" target="#tab_13">Table 8</ref>.</p><p>In terms of efficiency, the language model component in LM-LSTM-CRF only introduces a small number of parameters in two highway units and a soft-max layer, which may not have a very large impact on the efficiency. To control variables like infrastructures, we further reimplemented both baselines, and report their performance together with original implementations. From the results, these re-implementations achieve better efficiency comparing to the original ones, but yield relative worse performance. Also, LM-LSTM-CRF achieves the best performance, and takes twice the training time of the most efficient model, LSTM-CNN-CRF . Empirically, considering the difference among the implementations of these models, we think these methods have roughly the same efficiency.</p><p>Besides, we list the required time and resources for pretraining model index 4 and 5 on the NER task in Table 6 ( <ref type="bibr" target="#b7">Jozefowicz et al. 2016</ref>). Comparing to these language models pre-trained on external corpus, our model has no such reliance on extensive corpus, and can achieve similar performance with much more concise model and effi-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>To analyze the performance of LM-LSTM-CRF, we conduct additional experiments on the CoNLL03 NER dataset.</p><p>Hidden State Size To explore the effect of model size, we train our model with different hidden state sizes. For comparison, we also apply the same hidden state sizes to LSTM-CRF and LSTM-CNN-CRF. From <ref type="table" target="#tab_15">Table 9</ref>, one can easily observe that the F 1 score of LM-LSTM-CRF keeps increasing when the hidden state size grows, while LSTM-CNN-CRF has a peak at state size 200 and LSTM-CRF has a drop at state size 200. This phenomenon further verified our intuition of employing the language model to extract knowledge and prevent overfitting.</p><p>Highway Layers &amp; Co-training To elucidate the effect of language model <ref type="bibr">7</ref> and highway units, we compare LM-LSTM-CRF with its two variants, LM-LSTM-CRF NL and LM-LSTM-CRF NH. The first keeps highway units, but optimizes J CRF alone; the second jointly optimizes J CRF and J LM , but without highway units. As shown in <ref type="table" target="#tab_1">Table 10</ref>, LM-LSTM-CRF NH yields worse performance than LM-LSTM-CRF NL. This observation accords with previous comparison between TagLM (index 10) and TagLM (index 11) on the CoNLL03 NER dataset. We conjecture that it is because the NER task and the language model is not strongly related to each other. In summary, our proposed co-training strategy is effective and introducing the highway layers is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>There exist two threads of related work regarding the topics in this paper, which are sequence labeling and how to improve it with additional information.   <ref type="bibr" target="#b6">2016)</ref>. These models all incorporate character-level structure, and report meaningful improvement over pure wordlevel model. Also, CRF layer has also been demonstrated to be effective in capturing the dependency among labels. Our model is based on the success of LSTM-CRF model and is further modified to better capture the char-level information in a language model manner.</p><p>Leveraging Additional Information. Integrating wordlevel and character-level knowledge has been proved to be helpful to sequence labeling tasks. For example, word embeddings ( <ref type="bibr" target="#b18">Mikolov et al. 2013;</ref><ref type="bibr" target="#b20">Pennington, Socher, and Manning 2014)</ref> can be utilized by co-training or pre-training strategies ( <ref type="bibr" target="#b12">Liu et al. 2017;</ref><ref type="bibr" target="#b11">Lample et al. 2016</ref>). However, none of these models utilizes the character-level knowledge. Although directly adopting character-level pre-trained language models could be helpful ( <ref type="bibr" target="#b21">Peters et al. 2017</ref>). Such pretrained knowledge is not task-specific and requires a larger neural network, external corpus, and longer training. Our model leverages both word-level and character-level knowledge through a co-training strategy, which leads to a concise, effective, and efficient neural network. Besides, unlike other multi-task learning methods, our model has no reliance on any extra annotation ( <ref type="bibr" target="#b21">Peters et al. 2017</ref>) or any knowledge base ( <ref type="bibr" target="#b26">Shang et al. 2017</ref>). Instead, it extracts knowledge from the self-contained order information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a sequence labeling framework, LM-LSTM-CRF, which effectively leverages the language model to extract character-level knowledge from the selfcontained order information. Highway layers are incorporated to overcome the discordance issue of the naive cotraining Benefited from the effectively captured such taskspecific knowledge, we can build a much more concise model, thus yielding much better efficiency without loss of effectiveness (achieved the state-of-the-art on three benchmark datasets) . In the future, we plan to further extract and incorporate knowledge from other "unsupervised" learning principles and empower more sequence labeling tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Copyright c 2018 ,</head><label>2018</label><figDesc>Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Notation Table.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Dataset summary.</head><label>2</label><figDesc></figDesc><table>where p f (x i |x 1 , . . . , x i−1 ) is computed by NN. 
In this paper, our neural language model makes 
predictions for words but takes the character se-
quence as input. Specifically, we would calcu-
late 
p f </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Hyper-parameters of LM-LSTM-CRF. 

• CoNLL00 chunking defines eleven syntactic chunk types 
(e.g., NP, VP) in addition to Other. It only includes train-
ing and test sets. Following previous works (Peters et al. 
2017), we sampled 1000 sentences from training set as a 
held-out development set. 
• WSJ contains 25 sections and categorizes each word into 
45 POS tags. We adopt the standard split and use sections 
0-18 as training data, sections 19-21 as development data, 
and sections 22-24 as test data (Manning 2011). 
The corpus statistics are summarized in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Model with Language Model: Language models have been employed by some recent works to extract knowledge from raw text and thus enhancing sequence labeling task. TagLM (Peters et al. 2017) leverages pre- trained language models and shows the effectiveness with the large external corpus, but the large model scale and long training time make it hard to re-run this model. An- other work (Rei 2017) also incorporates the sequence la- beling task with the language model. For comparison, we tune the parameters of three most re- lated baselines (Ma and Hovy 2016; Lample et al. 2016; Rei 2017) 2 . , and report the statics of the best working pa- rameter setting. Besides, we index these models by number, and summarize the results in Tables 4, 5 and 7.</figDesc><table>Sequence Labeling Only. Without any additional su-
pervision or extra resources, LSTM-CRF (Lample et al. 
2016) and LSTM-CNN-CRF (Ma and Hovy 2016) are 
the current state-of-art methods. We also list some top re-
ported performance on each dataset (Collobert et al. 2011; 
Luo et al. 2015; Chiu and Nichols 2016; Yang, Salakhut-
dinov, and Cohen 2017; Peters et al. 2017; Manning 2011; 
Søgaard and Goldberg 2016; Sun 2014). 
• Joint Model with Other Supervised Tasks. There are 
several attempts (Luo et al. 2015; Yang, Salakhutdinov, 
and Cohen 2017) to enhance sequence labeling tasks 
by introducing additional annotations from other related 
tasks (e.g., enhance NER with entity linking labels). 
• Joint </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>F 1 score on the CoNLL03 NER dataset. We mark 
models adopting pre-trained word embedding as  †, and 
record models which leverage language models as  ‡. 

train a backward language model of the same size, in-
stead, chose a much smaller one (LSTM-2048-512 4 ). 
It is worth noting that, when either extra corpus or 
4096-8192-1024 is absent, LM-LSTM-CRF shows 
significant improvements over TagLM (index 5, 10 and 11). 
Also, LSTM-CNN-CRF outperforms LSTM-CRF in 
our experiments, which is different from </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy on the WSJ dataset. We mark models 
adopting pre-trained word embedding as  †, and record mod-
els which leverage language models as  ‡. 

Model 
CoNLL03 NER 
WSJ POS 
CoNLL00 Chunking 
h 
F1Score 
h Accuracy 
h 
F1Score 
LSTM-CRF 
46 
90.76 
37 
97.35 
26 
94.37 
LSTM-CNN-CRF 
7 
91.22 
21 
97.42 
6 
95.80 
LM-LSTM-CRF 
6 
91.71 
16 
97.53 
5 
95.96 
LSTM-CRF 
4 
91.19 
8 
97.44 
2 
95.82 
LSTM-CNN-CRF 
3 
90.98 
7 
96.98 
2 
95.51 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="true"><head>Table 7 :</head><label>7</label><figDesc>F 1 score on the CoNLL00 chunking dataset. We mark models adopting pre-trained word embedding as †, and record models which leverage language models as ‡.</figDesc><table>Ind &amp; Model 
F1score Module 
Time · Device 
15) LM-LSTM-CRF 91.71 
total 
6 
h·GTX 1080 

5) Peters et al. 2017 
91.62 
LSTM-2048-512 
320 
h·Telsa K40 
LSTM-2048-512 
320 
h·Telsa K40 

4) Peters et al. 2017 
91.93 
4096-8192-1024 14112 h·Telsa K40 
LSTM-2048-512 
320 
h·Telsa K40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Training time and performance of LSTM-CRF, 
LSTM-CNN-CRF and LM-LSTM-CRF on three datasets. 
Our re-implementations are marked with 

lack of training data. Still, LM-LSTM-CRF outperforms all 
baselines without extra resources, and most of the baselines 
trained with extra resources. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Effect of hidden state size of LSTM 

cient training. It verifies that our LM-LSTM-CRF model 
can effectively leverage the language model to extract task-
specific knowledge to empower sequence labeling. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>Effect of language model and highway 

of handcrafted features, there are attempts to build end-to-
end systems for sequence labeling tasks, such as BiLSTM-
CNN (Chiu and Nichols 2016), LSTM-CRF (Lample et 
al. 2016), and the current state-of-the-art method in NER 
and POS tagging tasks, LSTM-CNN-CRF (Ma and Hovy 
</table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="4"> LSTM-2048-512 is composed of a single-layer LSTM with 2048 hidden units and a 512-dimension projection unit.</note>

			<note place="foot" n="5"> https://github.com/LiyuanLucasLiu/ LM-LSTM-CRF 6 http://pytorch.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Junliang Guo, Cheng Cheng and all reviewers for comments on earlier drafts that led to substantial improvements in the final version. Research was sponsored in part by the U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov), and Google PhD Fellowship. The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Named entity recognition: A maximum entropy approach using global information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichols</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">;</forename><surname>Tacl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Natural language processing (almost) from scratch. JMLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vecshare: A framework for sharing word representation vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno type="arXiv">arXiv:1611.01587</idno>
		<title level="m">A joint many-task model: Growing a neural network for multiple nlp tasks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint named entity recognition and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: is it time for some linguistics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dredze</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00108</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurevych</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04457</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Søgaard, A. 2011</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure regularization for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2000 shared task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong</forename><surname>Kim Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning language in logic and CoNLL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong</forename><surname>Kim Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language learning at NAACL-HLT</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06345</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
