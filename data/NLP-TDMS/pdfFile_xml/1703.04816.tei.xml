<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T07:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Making Neural QA as Simple as Possible but not Simpler</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
							<email>dirk.weissenborn@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<addrLine>Alt-Moabit 91c</addrLine>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
							<email>georg.wiese@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<addrLine>Alt-Moabit 91c</addrLine>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
							<email>laura.seiffe@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution">DFKI</orgName>
								<address>
									<addrLine>Alt-Moabit 91c</addrLine>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Making Neural QA as Simple as Possible but not Simpler</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neu-ral baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the ex-tractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements , can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering is an important end-user task at the intersection of natural language processing (NLP) and information retrieval (IR). QA systems can bridge the gap between IR-based search engines and sophisticated intelligent assistants that enable a more directed information retrieval process. Such systems aim at finding precisely the piece of information sought by the user instead of documents or snippets containing the answer. A special form of QA, namely extractive QA, deals with the extraction of a direct answer to a question from a given textual context.</p><p>The creation of large-scale, extractive QA datasets ( <ref type="bibr" target="#b16">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b20">Trischler et al., 2017;</ref><ref type="bibr" target="#b14">Nguyen et al., 2016</ref>) sparked research interest into the development of end-to-end neural QA systems. A typical neural architecture consists of an embedding-, encoding-, interaction-and answer layer ( <ref type="bibr" target="#b21">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b25">Yu et al., 2017;</ref><ref type="bibr" target="#b23">Xiong et al., 2017;</ref><ref type="bibr" target="#b18">Seo et al., 2017;</ref><ref type="bibr" target="#b25">Yang et al., 2017;</ref>). Most such systems describe several innovations for the different layers of the architecture with a special focus on developing powerful interaction layer that aims at modeling word-by-word interaction between question and context.</p><p>Although a variety of extractive QA systems have been proposed, there is no competitive neural baseline. Most systems were built in what we call a top-down process that proposes a complex architecture and validates design decisions by an ablation study. Most ablation studies, however, remove only a single part of an overall complex architecture and therefore lack comparison to a reasonable neural baseline. This gap raises the question whether the complexity of current systems is justified solely by their empirical results.</p><p>Another important observation is the fact that seemingly complex questions might be answerable by simple heuristics. Let's consider the following example:</p><p>When did building activity occur on St. Kazimierz Church?</p><p>Building activity occurred in numerous noble palaces and churches <ref type="bibr">[...]</ref>. One of the best examples [..] are Krasinski Palace <ref type="bibr">(1677)</ref><ref type="bibr">(1678)</ref><ref type="bibr">(1679)</ref><ref type="bibr">(1680)</ref><ref type="bibr">(1681)</ref><ref type="bibr">(1682)</ref><ref type="bibr">(1683)</ref>, Wilanow Palace (1677-1696) and <ref type="bibr">St. Kazimierz Church (1688</ref><ref type="bibr">-1692</ref> Although it seems that evidence synthesis of multiple sentences is necessary to fully understand the relation between the answer and the question, answering this question is easily possible by applying a simple context/type matching heuristic. The heuristic aims at selecting answer spans that a) match the expected answer type (a time as indicated by "When") and b) are close to important question words ("St. Kazimierz Church"). The actual answer "1688-1692" would easily be extracted by such a heuristic.</p><p>In this work, we propose to use the aforementioned context/type matching heuristic as a guideline to derive simple neural baseline architectures for the extractive QA task. In particular, we develop a simple neural, bag-of-words (BoW)-and a recurrent neural network (RNN) baseline, namely FastQA. Crucially, both models do not make use of a complex interaction layer but model interaction between question and context only through computable features on the word level. FastQA's strong performance questions the necessity of additional complexity, especially in the interaction layer, which is exhibited by recently developed models. We address this question by evaluating the impact of extending FastQA with an additional interaction layer (FastQAExt) and find that it doesn't lead to systematic improvements. Finally, our contributions are the following: i) definition and evaluation of a BoW-and RNN-based neural QA baselines guided by a simple heuristic; ii) bottom-up evaluation of our FastQA system with increasing architectural complexity, revealing that the awareness of question words and the application of a RNN are enough to reach stateof-the-art results; iii) a complexity comparison between FastQA and more complex architectures as well as an in-depth discussion of usefulness of an interaction layer; iv) a qualitative analysis indicating that FastQA mostly follows our heuristic which thus constitutes a strong baseline for extractive QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Bag-of-Words Neural QA System</head><p>We begin by motivating our architectures by defining our proposed context/type matching heuristic: a) the type of the answer span should correspond to the expected answer type given by the question, and b) the correct answer should further be surrounded by a context that fits the question, or, more precisely, it should be surrounded by many question words. Similar heuristics were frequently implemented explicitly in traditional QA systems, e.g., in the answer extraction step of <ref type="bibr" target="#b13">Moldovan et al. (1999)</ref>, however, in this work our heuristic is merely used as a guideline for the construction of neural QA systems. In the following, we denote the hidden dimensionality of the model by n, the question tokens by Q = (q 1 , ..., q L Q ), and the context tokens by X = (x 1 , ..., x L X ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding</head><p>The embedding layer is responsible for mapping tokens x to their corresponding n-dimensional representation x. Typically this is done by mapping each word x to its corresponding word embedding x w (lookup-embedding) using an embedding matrix E, s.t. x w = Ex. Another approach is to embed each word by encoding their corresponding character sequence</p><formula xml:id="formula_0">x c = (c 1 , ..., c L X ) with C, s.t. x c = C(x c ) (char-embedding).</formula><p>In this work, we use a convolutional neural network for C of filter width 5 with max-pooling over time as explored by <ref type="bibr" target="#b18">Seo et al. (2017)</ref>, to which we refer the reader for additional details. Both approaches are combined via concatenation, s.t. the final em-</p><formula xml:id="formula_1">bedding becomes x = [x w ; x c ] ∈ R d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Type Matching</head><p>For the BoW baseline, we extract the span in the question that refers to the expected, lexical answer type (LAT) by extracting either the question word(s) (e.g., who, when, why, how, how many, etc.) or the first noun phrase of the question after the question words "what" or "which" (e.g., "what year did..."). 1 This leads to a correct LAT for most questions. We encode the LAT by concatenating the embedding of the first-and last word together with the average embedding of all words within the LAT. The concatenated representations are further transformed by a fully-connected layer followed by a tanh non-linearity intõ z ∈ R n . Note that we refer to a fully-connected layer in the following by FC, s.t.</p><formula xml:id="formula_2">FC(u) = W u + b, W ∈ R n×m , b ∈ R n , u ∈ R m .</formula><p>We similarly encode each potential answer span (s, e) in the context, i.e., all spans with a specified, maximum number of words (10 in this work), by concatenating the embedding of the first-and last word together with the average embedding of all words within the span. Because the surrounding context of a potential answer span can give important clues towards the type of an answer span, for instance, through nominal modifiers left of the span (e.g., "... president obama ...") or through an apposition right of the span (e.g., "... obama, president of..."), we additionally concatenate the average embeddings of the 5 words to the left and to the right of a span, respectively. The concatenated span representation, which comprises in total five different embeddings, is further transformed by a fully-connected layer with a tanh non-linearity intõ x s,e ∈ R n . Finally, the concatenation of the LAT representation, the span representation and their elementwise product, i.e., [˜ z; ˜ x s,e ; ˜ z ˜ x s,e ], serve as input to a feed-forward neural network with one hidden layer which computes the type score g type (s, e) for each span (s, e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Context Matching</head><p>In order to account for the number of surrounding words of an answer span as a measure for question to answer span match (context match), we introduce two word-in-question features. They are computed for each context word x j and explained in the following binary The binary word-in-question (wiq b ) feature is 1 for tokens that are part of the question and else 0. The following equation formally defines this feature where I denotes the indicator function:</p><formula xml:id="formula_3">wiq b j = I(∃i : x j = q i )<label>(1)</label></formula><p>weighted The wiq w j feature for context word x j is defined in Eq. 3, where Eq. 2 defines a basic similarity score between q i and x j based on their word-embeddings. It is motivated on the one hand by the intuition that question tokens which rarely appear in the context are more likely to be important for answering the question, and on the other hand by the fact that question words might occur as morphological variants, synonyms or related words in the context. The latter can be captured (softly) by using word embeddings instead of the words themselves whereas the former is captured by the application of the softmax operation in Eq. 3 which ensures that infrequent occurrences of words are weighted more heavily.</p><formula xml:id="formula_4">sim i,j = v wiq (x j q i ) , v wiq ∈ R n<label>(2)</label></formula><formula xml:id="formula_5">wiq w j = i softmax(sim i,· ) j<label>(3)</label></formula><p>A derivation that connects wiq w with the termfrequencies (a prominent information retrieval measure) of a word in the question and the context, respectively, is provided in Appendix A.</p><p>Finally, for each answer span (s, e) we compute the average wiq b and wiq w scores of the 5, 10 and 20 token-windows to the left and to the right of the respective (s, e)-span. This results in a total of 2 (kinds of features)×3 (windows)×2 (left/right) = 12 scores which are weighted by trainable scalar parameters and summed to compute the contextmatching score g ctxt (s, e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Span Scoring</head><p>The final score g for each span (s, e) is the sum of the type-and the context matching score:</p><formula xml:id="formula_6">g(s, e) = g type (s, e) + g ctxt (s, e).</formula><p>The model is trained to minimize the softmax-cross-entropy loss given the scores for all spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FastQA</head><p>Although our BoW baseline closely models our intended heuristic, it has several shortcomings. First of all, it cannot capture the compositionality of language making the detection of sensible answer spans harder. Furthermore, the semantics of a question is dramatically reduced to a BoW representation of its expected answer-type and the scalar word-in-question features. Finally, answer spans are restricted to a certain length.</p><p>To account for these shortcomings we introduce another baseline which relies on the application of a single bi-directional recurrent neural networks (BiRNN) followed by a answer layer that separates the prediction of the start and end of the answer span. <ref type="bibr" target="#b11">Lample et al. (2016)</ref> demonstrated that BiRNNs are powerful at recognizing named entities which makes them sensible choice for context encoding to allow for improved type matching. Context matching can similarly be achieved with a BiRNN by informing it of the locations of question tokens appearing in the context through our wiq-features. It is important to recognize that our model should implicitly learn to capture the heuristic, but is not limited by it.</p><p>On an abstract level, our RNN-based model, called FastQA, consists of three basic layers, namely the embedding-, encoding-and answer layer. Embeddings are computed as explained in §2.1. The other two layers are described in detail in the following. An illustration of the basic archi-  tecture is provided in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding</head><p>In the following, we describe the encoding of the context which is analogous to that of the question.</p><p>To allow for interaction between the two embeddings described in §2.1, they are first projected jointly to a n-dimensional representation (Eq. 4)) and further transformed by a single highway layer (Eq. 5) similar to <ref type="bibr" target="#b18">Seo et al. (2017)</ref>.</p><formula xml:id="formula_7">x = P x , P ∈ R n×d (4) g e = σ(FC(x )) , x = tanh FC x ˜ x = g e x + (1 − g e )x<label>(5)</label></formula><p>Because we want the encoder to be aware of the question words we feed the binary-and the weighted word-in-question feature of §2.3 in addition to the embedded context words as input. The complete input˜Xinput˜ input˜X ∈ R n+2×L X to the encoder is therefore defined as follows:</p><formula xml:id="formula_8">˜ X = ([˜ x 1 ; wiq b 1 ; wiq w 1 ], ..., [˜ x L X ; wiq b L X ; wiq w L X ])</formula><p>˜ X is fed to a bidirectional RNN and its output is again projected to allow for interaction between the features accumulated in the forward and backward RNN (Eq. 6). In preliminary experiments we found LSTMs <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) to perform best.</p><formula xml:id="formula_9">H = Bi-LSTM( ˜ X) , H ∈ R 2n×L X H = tanh(BH ) , B ∈ R n×2n<label>(6)</label></formula><p>We initialize the projection matrix B with [I n ; I n ], where I n denotes the n-dimensional identity matrix. It follows that H is the sum of the outputs from the forward-and backward-LSTM at the beginning of training.</p><p>As mentioned before, we utilize the same encoder parameters for both question and context, except the projection matrix B which is not shared. However, they are initialized the same way, s.t. the context and question encoding is identical at the beginning of training. Finally, to be able to use the same encoder for both question and context we fix the two wiq features to 1 for the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Layer</head><p>After encoding context</p><formula xml:id="formula_10">X to H = [h 1 , ..., h L X ] and the question Q to Z = [z 1 , ..., z L Q ], we first compute a weighted, n-dimensional question rep- resentatioñ z of Q (Eq. 7)</formula><p>. Note that this representation is context-independent and as such only computed once, i.e., there is no additional wordby-word interaction between context and question.</p><formula xml:id="formula_11">α = softmax(v q Z) , v q ∈ R n ˜ z = i α i z i<label>(7)</label></formula><p>The probability distribution p s for the start location of the answer is computed by a 2-layer feedforward neural network with a rectified-linear (ReLU) activated, hidden layer s j as follows:  The conditional probability distribution p e for the end location conditioned on the start location s is computed similarly by a feed-forward neural network with hidden layer e j as follows:</p><formula xml:id="formula_12">s j = ReLU (FC ([h j ; ˜ z; h j ˜ z])) p s (j) ∝ exp(v s s j ) , v s ∈ R n<label>(8</label></formula><formula xml:id="formula_13">e j = ReLU (FC ([h j ; h s ; ˜ z; h j ˜ z; h j h s ])) p e (j|s) ∝ exp(v e e j ) , v e ∈ R n<label>(9)</label></formula><p>The overall probability p of predicting an answer span (s, e) is p(s, e) = p s (s) · p e (e|s). The model is trained to minimize the cross-entropy loss of the predicted span probability p(s, e).</p><p>Beam-search During prediction time, we compute the answer span with the highest probability by employing beam-search using a beam-size of k. This means that ends for the top-k starts are predicted and the span with the highest overall probability is predicted as final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparison to Prior Architectures</head><p>Many neural architectures for extractive QA have been conceived very recently. Most of these systems can be broken down into four basic layers for which individual innovations were proposed. A high-level illustration of these systems is show in <ref type="figure" target="#fig_1">Figure 2</ref>. In the following, we compare our system in more detail with existing models.</p><p>Embedder The embedder is responsible for embedding a sequence of tokens into a sequence of ndimensional states. Our proposed embedder ( §2.1) is very similar to existing ones used for example in <ref type="bibr" target="#b18">Seo et al. (2017)</ref>; <ref type="bibr" target="#b25">Yang et al. (2017)</ref>.</p><p>Encoder Embedded tokens are further encoded by some form of composition function. A prominent type of encoder is the (bi-directional) recurrent neural network (RNN) which is also used in this work. Feeding additional word-level features similar to ours is rarely done with the exception of ; .</p><p>Interaction Layer Most research focused on the interaction layer which is responsible for wordby-word interaction between context and question. Different ideas were explored such as attention ( <ref type="bibr" target="#b21">Wang and Jiang, 2017;</ref><ref type="bibr" target="#b25">Yu et al., 2017)</ref>, coattention ( <ref type="bibr" target="#b23">Xiong et al., 2017)</ref>, bi-directional attention flow ( <ref type="bibr" target="#b18">Seo et al., 2017)</ref>, multi-perspective context matching ( ) or fine-grained gating ( <ref type="bibr" target="#b25">Yang et al., 2017</ref>). All of these ideas aim at enriching the encoded context with weighted states from the question and in some cases also from the context. These are gathered individually for each context state, concatenated with it and serve as input to an additional RNN. Note that this layer is omitted completely in FastQA and therefore constitutes the main simplification over previous work.</p><p>Answer Layer Finally, most systems divide the prediction the start and the end by another network. Their complexity ranges from using a single fully-connected layer ( <ref type="bibr" target="#b18">Seo et al., 2017;</ref>) to employing convolutional neural networks ( <ref type="bibr" target="#b20">Trischler et al., 2017</ref>) or recurrent, deep Highway-Maxout-Networks( <ref type="bibr" target="#b23">Xiong et al., 2017)</ref>. We further introduce beam-search to extract the most likely answer span with a simple 2-layer feed-forward network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FastQA Extended</head><p>To explore the necessity of the interaction layer and to be architecturally comparable to existing models we extend FastQA with an additional interaction layer (FastQAExt). In particular, we introduce representation fusion to enable the exchange of information in between passages of the context (intra-fusion), and between the question and the context (inter-fusion). Representation fusion is defined as the weighted addition between a state, i.e., its n-dimensional representation, and its respective co-representation. For each context state its corresponding co-representation is retrieved via attention from the rest of the context (intra) or the question (inter), respectively, and "fused" into its own representation. For the sake of brevity we describe technical details of this layer in Appendix B, because this extension is not the focus of this work but merely serves as a representative of the more complex architectures described in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>We conduct experiments on the following datasets.  <ref type="bibr">3</ref> contains 100k answerable questions from a total of 120k questions. The dataset is built from CNN news stories that were originally collected by <ref type="bibr" target="#b7">Hermann et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD</head><p>Performance on the SQuAD and NewsQA datasets is measured in terms of exact match (accuracy) and a mean, per answer token-based F1 measure which was originally proposed by <ref type="bibr">Ra- jpurkar et al. (2016)</ref> to also account for partial matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>BoW Model The BoW model is trained on spans up to length 10 to keep the computation tractable. This leads to an upper bound of about 95% accuracy on SQuAD and 87% on NewsQA. As pre-processing steps we lowercase all inputs and tokenize it using spacy <ref type="bibr">4</ref> . The binary word in question feature is computed on lemmas provided by spacy and restricted to alphanumeric words that are not stopwords. Throughout all experiments we use a hidden dimensionality of n = 150, dropout at the input embeddings with the same mask for all words ( <ref type="bibr" target="#b6">Gal and Ghahramani, 2015)</ref> and a rate of 0.2 and 300-dimensional fixed word-embeddings from Glove ( <ref type="bibr" target="#b15">Pennington et al., 2014</ref>). We employed ADAM ( <ref type="bibr" target="#b10">Kingma and Ba, 2015</ref>) for optimization with an initial learning-rate of 10 −3 which was halved whenever the F 1 measure on the development set dropped between epochs. We used mini-batches of size 32.</p><p>FastQA The pre-processing of FastQA is slightly simpler than that of the BoW model. We tokenize the input on whitespaces (exclusive) and non-alphanumeric characters (inclusive). The binary word in question feature is computed on the words as they appear in context. Throughout all experiments we use a hidden dimensionality of n = 300, variational dropout at the input embeddings with the same mask for all words ( <ref type="bibr" target="#b6">Gal and Ghahramani, 2015)</ref> and a rate of 0.5 and 300-dimensional fixed word-embeddings from Glove ( <ref type="bibr" target="#b15">Pennington et al., 2014</ref>). We employed ADAM ( <ref type="bibr" target="#b10">Kingma and Ba, 2015)</ref> for optimization with an initial learning-rate of 10 −3 which was halved whenever the F 1 measure on the development set dropped between checkpoints. Checkpoints occurred after every 1000 mini-batches each containing 64 examples.</p><p>Cutting Context Length Because NewsQA contains examples with very large contexts (up to more than 1500 tokens) we cut contexts larger than 400 tokens in order to efficiently train our models. We ensure that at least one, but at best all answers are still present in the remaining 400 tokens. Note that this restriction is only employed during training.   <ref type="table" target="#tab_4">Table 1</ref> shows the individual contributions of each model component that was incrementally added to a plain BiLSTM model without features, character embeddings and beam-search. We see that the most crucial performance boost stems from the introduction of either one of our features (≈ 15% F1). However, all other extensions also achieve notable improvements typically between 1 and 2% F1. Beam-search slightly improves results which shows that the most probable start is not necessarily the start of the best answer span.</p><p>In general, these results are interesting in many ways. For instance, it is surprising that a simple binary feature like wiq b can have such a dramatic effect on the overall performance. We believe that the reason for this is the fact that an encoder without any knowledge of the actual question has to account for every possible question that might be asked, i.e., it has to keep track of the entire context around each token in its recurrent state. An informed encoder, on the other hand, can selectively keep track of question related information. It can further abstract over concrete entities to their respective types because it is rarely the case that many entities of the same type occur in the question. For example, if a person is mentioned in the question the context encoder only needs to remember that the "question-person" was mentioned but not the concrete name of the person.</p><p>Another interesting finding is the fact that additional character based embeddings have a notable effect on the overall performance which was already observed by <ref type="bibr" target="#b18">Seo et al. (2017)</ref>; <ref type="bibr" target="#b25">Yu et al. (2017)</ref>. We see further improvements when employing representation fusion to allow for more interaction. This shows that a more sophisticated interaction layer can help. However, the differences are not substantial, indicating that this extension does not offer any systematic advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Comparing to State-of-the-Art</head><p>Our neural BoW baseline achieves good results on both datasets <ref type="table" target="#tab_4">(Tables 3 and 1)</ref>  <ref type="bibr">5</ref> . For instance, it outperforms a feature rich logistic-regression baseline on the SQuAD development set <ref type="table" target="#tab_4">(Table 1)</ref> and nearly reaches the BiLSTM baseline system (i.e., FastQA without character embeddings and features). It shows that more than half or more than a third of all questions in SQuAD or NewsQA, respectively, are (partially) answerable by a very simple neural BoW baseline. However, the gap to state-of-the-art systems is quite large (≈ 20%F1) which indicates that employing</p><formula xml:id="formula_14">Model Test F1 Exact Logistic Regression 1 51.0 40.4 Match-LSTM 2</formula><p>73.7 64.7 Dynamic Chunk Reader <ref type="bibr">3</ref> 71.0 62.5 Fine-grained Gating <ref type="bibr">4</ref> 73.3 62.5 Multi-Perspective Matching <ref type="bibr">5</ref> 75</p><note type="other">.1 65.5 Dynamic Coattention Networks 6 75.9 66.2 Bidirectional Attention Flow 7 77.3 68.0 r-net 8 77.9 69.5 FastQA w/ beam-size k = 5 77.1 68.4 FastQAExt k = 5</note><p>78.9 70.8   <ref type="table" target="#tab_5">Tables 2 and 3</ref> clearly demonstrate the strength of the FastQA system. It is very competitive to previously established stateof-the-art results on the two datasets and even improves those for NewsQA. This is quite surprising when considering the simplicity of FastQA putting existing systems and the complexity of the datasets, especially SQuAD, into perspective. Our extended version FastQAExt achieves even slightly better results outperforming all reported results prior to submitting our model on the very competitive SQuAD benchmark.</p><p>In parallel to this work <ref type="bibr" target="#b3">Chen et al. (2017)</ref> introduced a very similar model to FastQA, which relies on a few more hand-crafted features and a 3-layer encoder instead of a single layer in this work. These changes result in slightly better performance which is in line with the observations in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Do we need additional interaction?</head><p>In order to answer this question we compare FastQA, a system without a complex word-byword interaction layer, to representative models that have an interaction layer, namely FastQAExt and the Dynamic Coattention Network (DCN, <ref type="bibr" target="#b23">Xiong et al. (2017)</ref>). We measured both timeand space-complexity of FastQAExt and a reimplementation of the DCN in relation to FastQA and found that FastQA is about twice as fast as the other two systems and requires 2 − 4× less memory compared to FastQAExt and DCN, respectively <ref type="bibr">6</ref> .</p><p>In addition, we looked for systematic advantages of FastQAExt over FastQA by comparing SQuAD examples from the development set that were answered correctly by FastQAExt and incorrectly by FastQA (589 FastQAExt wins) against FastQA wins (415). We studied the average question-and answer length as well as the question types for these two sets but could not find any systematic difference. The same observation was made when manually comparing the kind of reasoning that is needed to answer a certain question. This finding aligns with the marginal empirical improvements, especially for NewsQA, between the two systems indicating that FastQAExt seems to generalize slightly better but does not offer a particular, systematic advantage. Therefore, we argue that the additional complexity introduced by the interaction layer is not necessarily justified by the incremental performance improvements presented in §7.2, especially when memory or run-time constraints exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Qualitative Analysis</head><p>Besides our empirical evaluations this section provides a qualitative error inspection of predictions for the SQuAD development dataset. We analyse 55 errors made by the FastQA system in detail and highlight basic abilities that are missing to reach human level performance.</p><p>We found that most errors are based on a lack of either syntactic understanding or a fine-grained semantic distinction between lexemes with similar meanings. Other error types are mostly related to annotation preferences, e.g., answer is good but there is a better, more specific one, or ambiguities within the question or context. high temperature for Fresno ... set on July 8, 1905, while the official record low ... set on January <ref type="bibr">6,</ref><ref type="bibr">1913</ref> A prominent type of mistake is a lack of finegrained understanding of certain answer types (Ex. 1). Another error is the lack of co-reference resolution and context sensitive binding of abbreviations (Ex. 2). We also find that the model sometimes struggles to capture basic syntactic structure, especially with respect to nested sentences where important separators like punctuation and conjunctions are being ignored <ref type="bibr">(Ex. 3)</ref>.</p><p>A manual examination of errors reveals that about 35 out of 55 mistakes (64%) can directly be attributed to the plain application of our heuristic. A similar analysis reveals that about 44 out of 50 (88%) analyzed positive cases are covered by our heuristic as well. We therefore believe that our model and, wrt. empirical results, other models as well mostly learn a simple context/type matching heuristic.</p><p>This finding is important because it reveals that an extractive QA system does not have to solve the complex reasoning types of  that were used to classify SQuAD instances ( <ref type="bibr" target="#b16">Rajpurkar et al., 2016)</ref>, in order to achieve current state-ofthe-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>The creation of large scale cloze datasets such the DailyMail/CNN dataset ( <ref type="bibr" target="#b7">Hermann et al., 2015)</ref> or the Children's Book Corpus ( <ref type="bibr" target="#b8">Hill et al., 2016)</ref> paved the way for the construction of end-to-end neural architectures for reading comprehension. A thorough analysis by , however, revealed that the DailyMail/CNN was too easy and still quite noisy. New datasets were constructed to eliminate these problems including SQuAD <ref type="bibr">(Ra- jpurkar et al., 2016)</ref>, <ref type="bibr">NewsQA (Trischler et al., 2017)</ref> and <ref type="bibr">MsMARCO (Nguyen et al., 2016)</ref>.</p><p>Previous question answering datasets such as MCTest ( <ref type="bibr" target="#b17">Richardson et al., 2013)</ref> and TREC-QA ( <ref type="bibr" target="#b4">Dang et al., 2007</ref>) were too small to successfully train end-to-end neural architectures such as the models discussed in §4 and required different approaches. Traditional statistical QA systems (e.g., <ref type="bibr" target="#b5">Ferrucci (2012)</ref>) relied on linguistic pre-processing pipelines and extensive exploitation of external resources, such as knowledge bases for feature-engineering. Other paradigms include template matching or passage retrieval <ref type="bibr" target="#b1">(Andrenucci and Sneiders, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we introduced a simple, context/type matching heuristic for extractive question answering which serves as guideline for the development of two neural baseline system. Especially FastQA, our RNN-based system turns out to be an efficient neural baseline architecture for extractive question answering. It combines two simple ingredients necessary for building a currently competitive QA system: a) the awareness of question words while processing the context and b) a composition function that goes beyond simple bag-of-words modeling. We argue that this important finding puts results of previous, more complex architectures as well as the complexity of recent QA datasets into perspective. In the future we want to extend the FastQA model to address linguistically motivated error types of §7.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Weighted Word-in-Question to Term Frequency</head><p>In this section we explain the connection between the weighted word-in-question feature ( §2.3) defined in Eq. 3 and the term frequency (tf) of a word occurring in the question Q = (q 1 , ..., q L Q ) and context X = (x 1 , ..., x L X ), respectively. To facilitate this analysis, we repeat the equations at this point:</p><formula xml:id="formula_15">sim i,j = v wiq (x j q i ) , v wiq ∈ R n (2) wiq w j = i softmax(sim i,· ) j<label>(3)</label></formula><p>Let us assume that we re-define the similarity score sim i,j of Eq. 2 as follows:</p><formula xml:id="formula_16">sim i,j = 0 if q i = x j − inf else<label>(10)</label></formula><p>Given the new (discrete) similarity score we can derive the following equation for the wiq w feature for context word x j . Note that we refer to the term frequency of a word z in the context and question by tf(z|C) and tf(z|Q), respectively. Our derived formula shows that wiq w of context word x j would become a simple combination of the term frequencies of x j within the context and question if our similarity score is redefined as in Eq. 10. Note that this holds true for any finite value chosen in Eq. 10 and not just 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Representation Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Intra-Fusion</head><p>It is well known that recurrent neural networks have a limited ability to model long-term dependencies. This limitation is mainly due to the information bottleneck that is posed by the fixed size of the internal RNN state. Hence, it is hard for our proposed baseline model to answer questions that require synthesizing evidence from different text passages. Such passages are typically connected via co-referent entities or events. Consider the following example from the NewsQA dataset <ref type="bibr" target="#b20">(Trischler et al., 2017)</ref>:</p><p>Where is Brittanee Drexel from?</p><p>The mother of a 17-year-old Rochester, New York high school student ... says she did not give her daughter permission to go on the trip. Brittanee Marie Drexel's mom says</p><p>To correctly answer this question the representations of Rochester, New York should contain the information that it refers to Brittanee Drexel. This connection can, for example, be established through the mention of mother and its co-referent mention mom. Fusing information from the context representation h mom into h mother allows crucial information about the mentioning of Brittanee Drexel to flow close to the correct answer. We enable the model to find co-referring mentions via a normalized similarity measure β (Eq. 11). For each context state we retrieve its co-state using β (Eq. 12) and finally fuse the representations of each state with their respective co-state representations via a gated addition (Eq. 13). We call this procedure associative representation fusion.</p><formula xml:id="formula_17">ˆ β j,k = I(j = k) v β (h j h k ) β j = softmax( ˆ β j,· )<label>(11)</label></formula><formula xml:id="formula_18">h co j = k β j,k h k (12) h * j = FUSE(h j , h co j ) = g β h j + (1 − g β )h co j (13) g β = σ(FC([h j ; h co j ]))</formula><p>We initialize v β with 1, s.t. ˆ β j,k is initially identical to the dot-product between hidden states.</p><p>We further introduce recurrent representation fusion to sequentially propagate information gathered by associative fusion between neighbouring tokens, e.g., between the representation of mother containing additional information about Brittanee Drexel and those representations of Rochester, New York. This is achieved via a recurrent</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of FastQA system on example question from SQuAD. The two word-in-question features (wiq b , wiq w ) are presented with varying degrees of activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the basic architecture which underlies most existing neural QA systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>softmax(sim i,· ) j = i exp(sim i,j ) j exp(sim i,j ) = i I(x j = q i ) j I(x j = q i ) = i I(x j = q i ) tf(q i |C) = i, q i =x j 1 tf(q i |C) = tf(x j |Q) tf(x j |C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>SQuAD results on development set for 
increasingly complex architectures. 1 Rajpurkar 
et al. (2016) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Official SQuAD leaderboard of single-
model systems on test set from 2016/12/29, the 
date of submitting our model. 1 Rajpurkar et al. 
(2016), 2 Wang and Jiang (2017), 3 Yu et al. (2017), 
4 Yang et al. (2017), 5 Wang et al. (2017), 6 Xiong 
et al. (2017), 7 Seo et al. (2017), 8 not published. 
Note that systems are regularly uploaded and im-
proved on SQuAD. 

Model 
Dev 
Test 
F1 
Exact 
F1 
Exact 

Match-LSTM 1 
48.9 
35.2 
48.0 
33.4 
BARB 2 
49.6 
36.1 
48.3 
34.1 

Neural BoW Baseline 37.6 
25.8 
36.6 
24.1 
FastQA k = 5 
56.4 43.7 
55.7 
41.9 
FastQAExt k = 5 
56.1 
43.7 56.1 42.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on the NewsQA dataset. 
1 Wang and Jiang (2017) was re-implemented by 
2 Trischler et al. (2017). 

more complex composition functions than averag-
ing, such as RNNs in FastQA, are indeed neces-
sary to achieve good performance. 
Results presented in </table></figure>

			<note place="foot" n="1"> More complex heuristics can be employed here but for the sake of simplicity we chose a very simple approach.</note>

			<note place="foot" n="2"> https://rajpurkar.github.io/ SQuAD-explorer/ 3 https://datasets.maluuba.com/NewsQA/ 4 http://spacy.io</note>

			<note place="foot" n="5"> We did not evaluate the BoW baseline on the SQuAD test set because it requires submitting the model to Rajpurkar et al. (2016) and we find that comparisons on NewsQA and the SQuAD development set give us enough insights.</note>

			<note place="foot" n="6"> We implemented all models in TensorFlow (Abadi et al., 2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Sebastian Riedel, Philippe Thomas, Leonhard Hennig and Omer Levy for comments on an early draft of this work as well as the anonymous reviewers for their insightful comments. This research was supported by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (01IW14002), BBDC (01IS14013E), and Software Campus (01IS12050, sub-project GeNIE).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Inter-Fusion</head><p>The representation fusion between question and context states is very similar to the intra-fusion procedure. It is applied on top of the context representations after intra-fusion has been employed. Associative fusion is performed via attention weights γ (Eq. 16) between question states z i and context states˜hstates˜ states˜h j ). The co-state is computed for each context state via γ (Eq. 17).</p><p>Note, because the softmax normalization is applied over the all context tokens for each question word, γ i will be close to zero for most context positions and therefore, its co-state will be close to a zero-vector. Therefore, only question related context states will receive a non-empty co-state. The rest of inter-fusion follows the same procedure as for intra-fusion and the resulting context representations serve as input to the answer layer.</p><p>In contrast to existing interaction layers which typically combine representations retrieved through attention by concatenation and feed them as input to an additional RNN (e.g., LSTM), our approach can be considered a more light-weight version of interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Generative Question Answering</head><p>Although our system was designed to answer question by extracting answers from a given context it can also be employed for generative question answering datasets such as the Microsoft Machine Reading Comprehension (MsMARCO, Nguyen et al. <ref type="formula">(2016)</ref>   <ref type="bibr" target="#b21">Wang and Jiang (2017)</ref>.</p><p>100k real world queries from the Bing search engine and human generated answers that are based on relevant web documents. Because we focus on extractive question answering in this work, we limit the queries for training to queries whose answers are directly extractable from the given web documents. We found that 67.2% of all queries fall into this category. Evaluation, however, is performed on the entire development and test set, respectively, which makes it impossible to answer the subset of Yes/No questions (≈ 7%) properly. For the sake of simplicity, we concatenate all given paragraphs and treat them as a single document. Since most queries in MsMARCO are lower-cased we also lower-cased the context. The official scoring measure of MsMARCO for generative models is ROUGE-L and BLEU-1. Even though our model is extractive we use our extracted answers as if they were generated. The results are shown in <ref type="table">Table 4</ref>. The strong performance of our purely extractive system on the generative MsMARCO dataset is notable. It shows that answers to Bing queries can mostly be extracted directly from web documents without the need for a more complex generative approach. Since this was only an initial experiment on generative QA using extractive QA and the methodology used for training, pre-and post-processing on this dataset for the other models, especially for <ref type="bibr" target="#b21">Wang and Jiang (2017)</ref>, is unclear, the comparability to the other QA systems is limited.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TensorFlow: LargeScale Machine Learning on Heterogeneous Distributed Systems</title>
		<editor>Ilya Sutskever, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Oriol Vinyals, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<meeting><address><addrLine>Dan Man, Rajat Monga, Sherry Moore, Derek Murray, Jonathon Shlens, Benoit Steiner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automated question answering: Review of the main approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Andrenucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eriks</forename><surname>Sneiders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>In ICITA</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">Reading Wikipedia to Answer OpenDomain Questions. ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Overview of the TREC 2007 Question Answering Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>TREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">This is Watson&quot;. IBM Journal of Research and Development</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<title level="m">Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Goldilocks Principle: Reading Children&apos;s Books with Explicit Memory Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m">LONG SHORT-TERM MEMORY. Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">Neural Architectures for Named Entity Recognition. NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06275v1[cs.CL</idno>
		<title level="m">Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lasso: A tool for surfing the answer net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ms Marco: a Human Generated Machine Reading Comprehension Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bi-Directional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hananneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05284</idno>
		<title level="m">ReasoNet: Learning to Stop Reading in Machine Comprehension</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">NewsQA: A Machine Comprehension Dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Machine Comprehension Using Match-LSTM and Answer Pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-Perspective Context Matching for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dynamic Coattention Networks for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Words or Characters? Fine-grained Gating for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
