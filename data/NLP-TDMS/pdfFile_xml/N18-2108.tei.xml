<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T07:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Higher-order Coreference Resolution with Coarse-to-fine Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 1 -6, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
							<email>kentonl@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
							<email>luheng@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Higher-order Coreference Resolution with Coarse-to-fine Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of NAACL-HLT 2018</title>
						<meeting>NAACL-HLT 2018 <address><addrLine>New Orleans, Louisiana</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="687" to="692"/>
							<date type="published">June 1 -6, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a fully differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilin-ear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent coreference resolution systems have heavily relied on first order models <ref type="bibr">(Clark and Man- ning, 2016a;</ref><ref type="bibr" target="#b9">Lee et al., 2017)</ref>, where only pairs of entity mentions are scored by the model. These models are computationally efficient and scalable to long documents. However, because they make independent decisions about coreference links, they are susceptible to predicting clusters that are locally consistent but globally inconsistent. <ref type="figure" target="#fig_0">Fig- ure 1</ref> shows an example from <ref type="bibr" target="#b15">Wiseman et al. (2016)</ref> that illustrates this failure case. The plurality of <ref type="bibr">[you]</ref> is underspecified, making it locally compatible with both <ref type="bibr">[I]</ref> and [all of you], while the full cluster would have mixed plurality, resulting in global inconsistency.</p><p>We introduce an approximation of higher-order inference that uses the span-ranking architecture from <ref type="bibr" target="#b9">Lee et al. (2017)</ref> in an iterative manner. At each iteration, the antecedent distribution is used as an attention mechanism to optionally update existing span representations, enabling later coreferSpeaker 1: Um and <ref type="bibr">[I]</ref> think that is what's -Go ahead Linda. Speaker 2: Well and uh thanks goes to <ref type="bibr">[you]</ref> and to the media to help us... So our hat is off to [all of you] as well. ence decisions to softly condition on earlier coreference decisions. For the example in <ref type="figure" target="#fig_0">Figure 1</ref>, this enables the linking of <ref type="bibr">[you]</ref> and [all of you] to depend on the linking of <ref type="bibr">[I]</ref> and <ref type="bibr">[you]</ref>.</p><p>To alleviate computational challenges from this higher-order inference, we also propose a coarseto-fine approach that is learned with a single endto-end objective. We introduce a less accurate but more efficient coarse factor in the pairwise scoring function. This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor. Intuitively, the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function.</p><p>Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark. We observe a significant increase in average F1 with a second-order model, but returns quickly diminish with a third-order model. Additionally, our analysis shows that the coarse-to-fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>687</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Task definition We formulate the coreference resolution task as a set of antecedent assignments y i for each of span i in the given document, following <ref type="bibr" target="#b9">Lee et al. (2017)</ref>. The set of possible assignments for each y i is Y(i) = {, 1, . . . , i − 1}, a dummy antecedent and all preceding spans. Non-dummy antecedents represent coreference links between i and y i . The dummy antecedent represents two possible scenarios: (1) the span is not an entity mention or (2) the span is an entity mention but it is not coreferent with any previous span. These decisions implicitly define a final clustering, which can be recovered by grouping together all spans that are connected by the set of antecedent predictions.</p><p>Baseline We describe the baseline model ( <ref type="bibr" target="#b9">Lee et al., 2017)</ref>, which we will improve to address the modeling and computational limitations discussed previously. The goal is to learn a distribution P (y i ) over antecedents for each span i :</p><formula xml:id="formula_0">P (y i ) = e s(i,y i ) y ∈Y(i) e s(i,y ) (1)</formula><p>where s(i, j) is a pairwise score for a coreference link between span i and span j. The baseline model includes three factors for this pairwise coreference score: (1) s m (i), whether span i is a mention, (2) s m (j), whether span j is a mention, and (3) s a (i, j) whether j is an antecedent of i:</p><formula xml:id="formula_1">s(i, j) = s m (i) + s m (j) + s a (i, j)<label>(2)</label></formula><p>In the special case of the dummy antecedent, the score s(i, ) is instead fixed to 0. A common component used throughout the model is the vector representations g i for each possible span i. These are computed via bidirectional LSTMs <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) that learn context-dependent boundary and head representations. The scoring functions s m and s a take these span representations as input:</p><formula xml:id="formula_2">s m (i) = w m FFNN m (g i ) (3) s a (i, j) = w a FFNN a ([g i , g j , g i • g j , φ(i, j)]) (4)</formula><p>where • denotes element-wise multiplication, FFNN denotes a feed-forward neural network, and the antecedent scoring function s a (i, j) includes explicit element-wise similarity of each span g i • g j and a feature vector φ(i, j) encoding speaker and genre information from the metadata and the distance between the two spans. The model above is factored to enable a twostage beam search. A beam of up to M potential mentions is computed (where M is proportional to the document length) based on the spans with the highest mention scores s m (i). Pairwise coreference scores are only computed between surviving mentions during both training and inference.</p><p>Given supervision of gold coreference clusters, the model is learned by optimizing the marginal log-likelihood of the possibly correct antecedents. This marginalization is required since the best antecedent for each span is a latent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Higher-order Coreference Resolution</head><p>The baseline above is a first-order model, since it only considers pairs of spans. First-order models are susceptible to consistency errors as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Unlike in sentence-level semantics, where higher-order decisions can be implicitly modeled by the LSTMs, modeling these decisions at the document-level requires explicit inference due to the potentially very large surface distance between mentions.</p><p>We propose an inference procedure that allows the model to condition on higher-order structures, while being fully differentiable. This inference involves N iterations of refining span representations, denoted as g n i for the representation of span i at iteration n. At iteration n, g n i is computed with an attention mechanism that averages over previous representations g n−1 j weighted according to how likely each mention j is to be an antecedent for i, as defined below.</p><p>The baseline model is used to initialize the span representation at g 1 i . The refined span representations allow the model to also iteratively refine the antecedent distributions P n (y i ):</p><formula xml:id="formula_3">P n (y i ) = e s(g n i ,g n y i ) y∈Y(i) e s(g n i ,g n y ))<label>(5)</label></formula><p>where s is the coreference scoring function of the baseline architecture. The scoring function uses the same parameters at every iteration, but it is given different span representations. At each iteration, we first compute the expected antecedent representation a n i of each span i by using the current antecedent distribution P n (y i ) as an attention mechanism:</p><formula xml:id="formula_4">a n i = y i ∈Y(i) P n (y i ) · g n y i<label>(6)</label></formula><p>The current span representation g n i is then updated via interpolation with its expected antecedent representation a n i :</p><formula xml:id="formula_5">f n i = σ(W f [g n i , a n i ])<label>(7)</label></formula><formula xml:id="formula_6">g n+1 i = f n i • g n i + (1 − f n i ) • a n i<label>(8)</label></formula><p>The learned gate vector f n i determines for each dimension whether to keep the current span information or to integrate new information from its expected antecedent. At iteration n, g n i is an element-wise weighted average of approximately n span representations (assuming P n (y i ) is peaked), allowing P n (y i ) to softly condition on up to n other spans in the predicted cluster.</p><p>Span-ranking can be viewed as predicting latent antecedent trees <ref type="bibr">(Fernandes et al., 2012;</ref><ref type="bibr" target="#b10">Martschat and Strube, 2015)</ref>, where the predicted antecedent is the parent of a span and each tree is a predicted cluster. By iteratively refining the span representations and antecedent distributions, another way to interpret this model is that the joint distribution i P N (y i ) implicitly models every directed path of up to length N + 1 in the latent antecedent tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Coarse-to-fine Inference</head><p>The model described above scales poorly to long documents. Despite heavy pruning of potential mentions, the space of possible antecedents for every surviving span is still too large to fully consider. The bottleneck is in the antecedent score s a (i, j), which requires computing a tensor of size</p><formula xml:id="formula_7">M × M × (3|g| + |φ|).</formula><p>This computational challenge is even more problematic with the iterative inference from Section 3, which requires recomputing this tensor at every iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Heuristic antecedent pruning</head><p>To reduce computation, <ref type="bibr" target="#b9">Lee et al. (2017)</ref> heuristically consider only the nearest K antecedents of each span, resulting in a smaller input of size M × K × (3|g| + |φ|).</p><p>The main drawback to this solution is that it imposes an a priori limit on the maximum distance of a coreference link. The previous work only considers up to K = 250 nearest mentions, whereas coreference links can reach much further in natural language discourse.  <ref type="figure">Figure 2</ref>: Comparison of accuracy on the development set for the two antecedent pruning strategies with various beams sizes K. The distance-based heuristic pruning performance drops by almost 5 F1 when reducing K from 250 to 50, while the coarse-to-fine pruning results in an insignificant drop of less than 0.2 F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Coarse-to-fine antecedent pruning</head><p>We instead propose a coarse-to-fine approach that can be learned end-to-end and does not establish an a priori maximum coreference distance. The key component of this coarse-to-fine approach is an alternate bilinear scoring function:</p><formula xml:id="formula_8">s c (i, j) = g i W c g j (9)</formula><p>where W c is a learned weight matrix. In contrast to the concatenation-based s a (i, j), the bilinear s c (i, j) is far less accurate. A direct replacement of s a (i, j) with s c (i, j) results in a performance loss of over 3 F1 in our experiments. However, s c (i, j) is much more efficient to compute. Computing s c (i, j) only requires manipulating matrices of size M × |g| and M × M . Therefore, we instead propose to use s c (i, j) to compute a rough sketch of likely antecedents. This is accomplished by including it as an additional factor in the model:</p><formula xml:id="formula_9">s(i, j) = s m (i) + s m (j) + s c (i, j) + s a (i, j) (10)</formula><p>Similar to the baseline model, we leverage this additional factor to perform an additional beam pruning step. The final inference procedure involves a three-stage beam search:   Third stage The overall coreference s(i, j) is computed based on the remaining span pairs. The soft higher-order inference from Section 3 is computed in this final stage. While the maximum-likelihood objective is computed over only the span pairs from this final stage, this coarse-to-fine approach expands the set of coreference links that the model is capable of learning. It achieves better performance while using a much smaller K (see <ref type="figure">Figure 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We use the English coreference resolution data from the CoNLL-2012 shared task ( <ref type="bibr" target="#b14">Pradhan et al., 2012</ref>) in our experiments. The code for replicating these results is publicly available. <ref type="bibr">1</ref> Our models reuse the hyperparameters from <ref type="bibr" target="#b9">Lee et al. (2017)</ref>, with a few exceptions mentioned below. In our results, we report two improvements that are orthogonal to our contributions.</p><p>• We used embedding representations from a language model ( <ref type="bibr" target="#b13">Peters et al., 2018</ref>) at the input to the LSTMs (ELMo in the results).</p><p>• We changed several hyperparameters:</p><p>1. increasing the maximum span width from 10 to 30 words. 2. using 3 highway LSTMs instead of 1. 3. using GloVe word embeddings <ref type="bibr">(Pen- nington et al., 2014</ref>) with a window size 1 https://github.com/kentonl/e2e-coref of 2 for the head word embeddings and a window size of 10 for the LSTM inputs.</p><p>The baseline model considers up to 250 antecedents per span. As shown in <ref type="figure">Figure 2</ref>, the coarse-to-fine model is quite insensitive to more aggressive pruning. Therefore, our final model considers only 50 antecedents per span. On the development set, the second-order model (N = 2) outperforms the first-order model by 0.8 F1, but the third order model only provides an additional 0.1 F1 improvement. Therefore, we only compute test results for the secondorder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We report the precision, recall, and F1 of the the MUC, B 3 , and CEAF φ 4 metrics using the official CoNLL-2012 evaluation scripts. The main evaluation is the average F1 of the three metrics.</p><p>Results on the test set are shown in <ref type="table">Table 1</ref>. We include performance of systems proposed in the past 3 years for reference. The baseline relative to our contributions is the span-ranking model from <ref type="bibr" target="#b9">Lee et al. (2017)</ref> augmented with both ELMo and hyperparameter tuning, which achieves 72.3 F1. Our full approach achieves 73.0 F1, setting a new state of the art for coreference resolution.</p><p>Compared to the heuristic pruning with up to 250 antecedents, our coarse-to-fine model only computes the expensive scores s a (i, j) for 50 antecedents. Despite using far less computation, it outperforms the baseline because the coarse scores s c (i, j) can be computed for all antecedents, enabling the model to potentially predict a coreference link between any two spans in the document. As a result, we observe a much higher recall when adopting the coarse-to-fine approach.</p><p>We also observe further improvement by including the second-order inference (Section 3). The improvement is largely driven by the overall increase in precision, which is expected since the higher-order inference mainly serves to rule out inconsistent clusters. It is also consistent with findings from Martschat and Strube (2015) who report mainly improvements in precision when modeling latent trees to achieve a similar goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In addition to the end-to-end span-ranking model ( <ref type="bibr" target="#b9">Lee et al., 2017</ref>) that our proposed model builds upon, there is a large body of literature on coreference resolvers that fundamentally rely on scoring span pairs ( <ref type="bibr" target="#b11">Ng and Cardie, 2002;</ref><ref type="bibr">Bengt- son and Roth, 2008;</ref><ref type="bibr" target="#b5">Denis and Baldridge, 2008;</ref><ref type="bibr">Fernandes et al., 2012;</ref><ref type="bibr" target="#b6">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b16">Wiseman et al., 2015;</ref><ref type="bibr" target="#b3">Clark and Manning, 2016a</ref>).</p><p>Motivated by structural consistency issues discussed above, significant effort has also been devoted towards cluster-level modeling. Since global features are notoriously difficult to define ( <ref type="bibr" target="#b15">Wiseman et al., 2016)</ref>, they often depend heavily on existing pairwise features or architectures <ref type="bibr" target="#b1">(Björkelund and Kuhn, 2014;</ref><ref type="bibr">Manning, 2015, 2016b</ref>). We similarly use an existing pairwise span-ranking architecture as a building block for modeling more complex structures. In contrast to <ref type="bibr" target="#b15">Wiseman et al. (2016)</ref> who use highly expressive recurrent neural networks to model clusters, we show that the addition of a relatively lightweight gating mechanism is sufficient to effectively model higher-order structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a state-of-the-art coreference resolution system that models higher order interactions between spans in predicted clusters. Additionally, our proposed coarse-to-fine approach alleviates the additional computational cost of higherorder inference, while maintaining the end-to-end learnability of the entire model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of consistency errors to which firstorder span-ranking models are susceptible. Span pairs (I, you) and (you, all of you) are locally consistent, but the span triplet (I, you, all of you) is globally inconsistent. Avoiding this error requires modeling higherorder structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>First</head><label></label><figDesc>stage Keep the top M spans based on the mention score s m (i) of each span. Second stage Keep the top K antecedents of each remaining span i based on the first three fac- tors, s m (i) + s m (j) + s c (i, j).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>MUC</head><label></label><figDesc></figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS-1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award. We also thank the UW NLP group for helpful conversations and comments on the work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity-centric coreference resolution with model stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Specialized models and ranking for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cícero Nogueira Dos Santos, and Ruy Luiz Milidiú. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long Short-term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Latent structures for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart M</forename><surname>Shieber</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning anaphoricity and antecedent ranking features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
