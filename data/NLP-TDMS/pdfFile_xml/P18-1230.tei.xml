<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Glosses into Neural Word Sense Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15 -20, 2018. 2018. 2473</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
							<email>luofuli@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
							<email>tianyu0421@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Glosses into Neural Word Sense Disambiguation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2473" to="2482"/>
							<date type="published">July 15 -20, 2018. 2018. 2473</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word Sense Disambiguation (WSD) aims to identify the correct meaning of poly-semous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance ( <ref type="bibr">Raganato et al., 2017b,a)</ref>, there are still two major challenges.</p><p>Firstly, supervised methods ( <ref type="bibr" target="#b3">Iacobacci et al., 2016</ref>) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts <ref type="bibr">1</ref> . Recent neural-based methods <ref type="bibr">(Kågebäck and Salomons- son, 2016;</ref>) solve this problem by building a unified model for all the polysemous words, but they still can't beat the best word expert system.</p><p>Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet <ref type="bibr">(Miller, 1995)</ref> which are widely used in the knowledge-based methods. The gloss, which extensionally defines a word sense meaning, plays a key role in the well-known Lesk algorithm <ref type="bibr">(Lesk, 1986)</ref>. Recent studies ( <ref type="bibr">Banerjee and Pedersen, 2002;</ref><ref type="bibr" target="#b1">Basile et al., 2014</ref>) have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of Lesk algorithm.</p><p>To this end, our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words. We further consider extending the original gloss through its semantic relations in our framework. As shown in <ref type="figure">Figure 1</ref>, the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation. Therefore, we integrate not only the original gloss but also the related glosses of hypernyms and hyponyms into the neural network. Figure 1: The hypernym (green node) and hyponyms (blue nodes) for the 2nd sense bed 2 of bed, which means a plot of ground in which plants are growing, rather than the bed for sleeping in. The figure shows that bed 2 is a kind of plot 2 , and bed 2 includes f lowerbed 1 , seedbed 1 , etc.</p><p>In this paper, we propose a novel model GAS: a gloss-augmented WSD neural network which is a variant of the memory network ( <ref type="bibr" target="#b18">Sukhbaatar et al., 2015b;</ref><ref type="bibr">Kumar et al., 2016;</ref><ref type="bibr" target="#b20">Xiong et al., 2016)</ref>. GAS jointly encodes the context and glosses of the target word and models the semantic relationship between the context and glosses in the memory module. In order to measure the inner relationship between glosses and context more accurately, we employ multiple passes operation within the memory as the re-reading process and adopt two memory updating mechanisms.</p><p>The main contributions of this paper are listed as follows:</p><p>• To the best of our knowledge, our model is the first to incorporate the glosses into an end-to-end neural WSD model. In this way, our model can benefit from not only massive labeled data but also rich lexical knowledge.</p><p>• In order to model semantic relationship of context and glosses, we propose a glossaugmented neural network (GAS) in an improved memory network paradigm.</p><p>• We further expand the gloss through its semantic relations to enrich the gloss information and better infer the context. We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet.</p><p>• The experimental results on several English all-words WSD benchmark datasets show that our model outperforms the state-of-theart systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Knowledge-based, supervised and neural-based methods have already been applied to WSD task <ref type="bibr" target="#b12">(Navigli, 2009)</ref>. Knowledge-based WSD methods mainly exploit two kinds of knowledge to disambiguate polysemous words: 1) The gloss, which defines a word sense meaning, is mainly used in Lesk algorithm <ref type="bibr">(Lesk, 1986)</ref> and its variants.</p><p>2) The structure of the semantic network, whose nodes are synsets 2 and edges are semantic relations, is mainly used in graph-based algorithms <ref type="bibr" target="#b11">(Agirre et al., 2014;</ref><ref type="bibr" target="#b2">Moro et al., 2014)</ref>.</p><p>Supervised methods ( <ref type="bibr">Ia- cobacci et al., 2016)</ref> usually involve each target word as a separate classification problem (often called word expert) and train classifiers based on manual designed features.</p><p>Although word expert supervised WSD methods perform best in terms of accuray, they are less flexible than knowledge-based methods in the allwords WSD task ( ). To deal with this problem, recent neural-based methods aim to build a unified classifier which shares parameters among all the polysemous words. <ref type="bibr" target="#b4">Kågebäck and Salomonsson (2016)</ref> leverages the bidirectional long short-term memory network which shares model parameters among all the polysemous words.  transfers the WSD problem into a neural sequence labeling task. However, none of the neural-based methods can totally beat the best word expert supervised methods on English all-words WSD datasets.</p><p>What's more, all of the previous supervised methods and neural-based methods rarely take the lexical resources like WordNet <ref type="bibr">(Fellbaum, 1998)</ref> into consideration. Recent studies on sense embeddings have proved that lexical resources are helpful. <ref type="bibr">Chen et al. (2015)</ref> trains word sense embeddings through learning sentence level embeddings from glosses using a convolutional neural networks. <ref type="bibr" target="#b15">Rothe and Schütze (2015)</ref> extends word embeddings to sense embeddings by using the constraints and semantic relations in WordNet. They achieve an improvement of more than 1% in WSD performance when using sense embeddings as WSD features for SVM classifier. This work shows that integrating structural information of lexical resources can help to word expert supervised methods. However, sense embeddings can only indirectly help to WSD (as SVM classifier features).  shows that the coarse-grained semantic labels in WordNet can help to WSD in a multi-task learning framework. As far as we know, there is no study directly integrates glosses or semantic relations of the WordNet into an end-to-end model.</p><p>In this paper, we focus on how to integrate glosses into a unified neural WSD system. Memory network <ref type="bibr" target="#b18">(Sukhbaatar et al., 2015b;</ref><ref type="bibr">Kumar et al., 2016;</ref><ref type="bibr" target="#b20">Xiong et al., 2016</ref>) is initially proposed to solve question answering problems. Recent researches show that memory network obtains the state-of-the-art results in many NLP tasks such as sentiment classification ( <ref type="bibr">Li et al., 2017</ref>) and analysis <ref type="bibr">(Gui et al., 2017)</ref>, poetry generation ( <ref type="bibr" target="#b21">Zhang et al., 2017)</ref>, spoken language understanding ( <ref type="bibr">Chen et al., 2016</ref>), etc. Inspired by the success of memory network used in many NLP tasks, we introduce it into WSD. We make some adaptations to the initial memory network in order to incorporate glosses and capture the inner relationship between the context and glosses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Incorporating Glosses into Neural Word Sense Disambiguation</head><p>In this section, we first give an overview of the proposed model GAS: a gloss-augmented WSD neural network which integrates the context and the glosses of the target word into a unified framework. After that, each individual module is described in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture of GAS</head><p>The overall architecture of the proposed model is shown in <ref type="figure">Figure 2</ref>. It consists of four modules:</p><p>• Context Module: The context module encodes the local context (a sequence of surrounding words) of the target word into a distributed vector representation.</p><p>• Gloss Module: Like the context module, the gloss module encodes all the glosses of the target word into a separate vector representations of the same size. In other words, we can get |s t | word sense representations according to |s t | 3 senses of the target word, where |s t | is the sense number of the target word w t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gloss Module</head><p>Context Module Memory Module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scoring Module</head><p>Figure 2: Overview of Gloss-augmented Memory Network for Word Sense Disambiguation.</p><p>• Memory Module: The memory module is employed to model the semantic relationship between the context embedding and gloss embedding produced by context module and gloss module respectively.</p><p>• Scoring Module: In order to benefit from both labeled contexts and gloss knowledge, the scoring module takes the context embedding from context module and the last step result from the memory module as input. Finally it generates a probability distribution over all the possible senses of the target word.</p><p>Detailed architecture of the proposed model is shown in <ref type="figure">Figure 3</ref>. The next four sections will show detailed configurations in each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Module</head><p>Context module encodes the context of the target word into a vector representation, which is also called context embedding in this paper.</p><p>We leverage the bidirectional long short-term memory network (Bi-LSTM) for taking both the preceding and following words of the target word into consideration.</p><p>The input of this module [x 1 , . . . , x t−1 , x t+1 , . . . , x Tx ] is a sequence of words surrounding the target word x t , where T x is the length of the context. After applying a lookup operation over the pre-trained word embedding matrix M ∈ R D×V , we transfer a one hot vector x i into a D-dimensional vector. Then, the forward LSTM reads the segment (x 1 , . . . , x t−1 ) on the left of the target word x t and calculates a sequence of forward hidden states</p><formula xml:id="formula_0">( − → h 1 , . . . , − → h t−1 ).</formula><p>The backward LSTM reads the segment (x Tx , . . . , x t+1 ) on the right of the target word x t and calculates a sequence of backward hidden states (  <ref type="figure">Figure 3</ref>: Detailed architecture of our proposed model, which consists of a context module, a gloss module, a memory module and a scoring module. The context module encodes the adjacent words surrounding the target word into a vector c. The gloss module encodes the original gloss or extended glosses into a vector g i . In the memory module, we calculate the inner relationship (as attention) between context c and each gloss g i and then update the memory as m i at pass i. In the scoring module, we make final predictions based on the last pass attention of memory module and the context vector c. Note that GAS only uses the original gloss, while GAS ext uses the entended glosses through hypernymy and hyponymy relations. In other words, the relation fusion layer (grey dotted box) only belongs to GAS ext .</p><formula xml:id="formula_1">← − h Tx , . . . , ← − h t+1 ). The context vec- tor c is finally concatenated as c = [ − → h t−1 : ← − h t+1 ]<label>(1)</label></formula><p>where : is the concatenation operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gloss Module</head><p>The gloss module encodes each gloss of the target word into a fixed size vector like the context vector c, which is also called gloss embedding. We further enrich the gloss information by taking semantic relations and their associated glosses into consideration. This module contains a gloss reader layer and a relation fusion layer. Gloss reader layer generates a vector representations for a gloss. Relation fusion layer aims at modeling the semantic relations of each gloss in the expanded glosses list which consists of related glosses of the original gloss. Our model GAS with extended glosses is denoted as GAS ext . GAS only encodes the original gloss, while GAS ext encodes the expanded glosses from hypernymy and hyponymy relations (details in <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Gloss Reader Layer</head><p>Gloss reader layer contains two parts: gloss expansion and gloss encoder. Gloss expansion is to enrich the original gloss information through its hypernymy and hyponymy relations in WordNet. Gloss encoder is to encode each gloss into a vector representation.</p><p>Gloss Expansion: We only expand the glosses of nouns and verbs via their corresponding hypernyms and hyponyms. There are two reasons: One is that most of polysemous words (about 80%) are nouns and verbs; the other is that the most frequent relations among word senses for nouns and verbs are the hypernymy and hyponymy relations <ref type="bibr">4</ref> .</p><p>The original gloss is denoted as g 0 . Breadthfirst search method with a limited depth K is employed to extract the related glosses. The glosses of hypernyms within K depth are denoted as</p><formula xml:id="formula_2">[g −1 , g −2 , . . . , g −L 1 ].</formula><p>The glosses of hyponyms within K depth are denoted as [g +1 , g +2 , . . . , g +L 2 ] 5 . Note that g +1 and g −1 are the glosses of the nearest word sense.</p><p>Gloss Encoder: We denote the j-th 6 gloss in <ref type="bibr">4</ref> In WordNet, more than 95% of relations for nouns and 80% for verbs are hypernymy and hyponymy relations. <ref type="bibr">5</ref> Since one synset has one or more direct hypernyms and hyponyms, L1 &gt;= K and L2 &gt;= K. <ref type="bibr">6</ref> Since GAS don't have gloss expansion, j is always 0 and gi = g i 0 . See more in <ref type="figure">Figure 3</ref>. the expanded glosses list for i th sense of the target word as a sequence of G words. Like the context encoder, the gloss encoder also leverages Bi-LSTM units to process the words sequence of the gloss. The gloss representation g i j is computed as the concatenation of the last hidden states of the forward and backward LSTM.</p><formula xml:id="formula_3">g i j = [ − → h i,j G : ← − h i,j 1 ]<label>(2)</label></formula><p>where j ∈ [−L 1 , . . . , −1, 0, +1, . . . , +L 2 ] and : is the concatenation operator .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Relation Fusion Layer</head><p>Relation fusion layer models the hypernymy and hyponymy relations of the target word sense. A forward LSTM is employed to encode the hypernyms' glosses of i th sense , . . . , g i +1 , g i 0 ) as a sequence of backward hidden states</p><formula xml:id="formula_4">(g i −L 1 , . . . , g i −1 , g i 0 ) as a sequence of forward hidden states ( − → h i −L 1 , . . . , − → h i −1 , − → h i 0</formula><formula xml:id="formula_5">( ← − h i +L 2 , . . . , ← − h i +1 , ← − h i 0 )</formula><p>. In order to highlight the original gloss g i 0 , the enhanced i th sense representation is concatenated as the final state of the forward and backward LSTM.</p><formula xml:id="formula_6">g i = [ − → h i 0 : ← − h i 0 ]<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Memory Module</head><p>The memory module has two inputs: the context vector c from the context module and the gloss vectors {g 1 , g 2 , . . . , g |st| } from the gloss module, where |s t | is the number of word senses. We model the inner relationship between the context and glosses by attention calculation. Since onepass attention calculation may not fully reflect the relationship between the context and glosses (details in Section 4.4.2), the memory module adopts a repeated deliberation process. The process repeats reading gloss vectors in the following passes, in order to highlight the correct word sense for the following scoring module by a more accurate attention calculation. After each pass, we update the memory to refine the states of the current pass. Therefore, memory module contains two phases: attention calculation and memory update. Attention Calculation: For each pass k, the attention e k i of gloss g i is generally computed as</p><formula xml:id="formula_7">e k i = f (g i , m k−1 , c)<label>(4)</label></formula><p>where m k−1 is the memory vector in the (k − 1)-th pass while c is the context vector. The scoring function f calculates the semantic relationship of the gloss and context, taking the vector set (g i , m k−1 , c) as input. In the first pass, the attention reflects the similarity of context and each gloss. In the next pass, the attention reflects the similarity of adapted memory and each gloss. A dot product is applied to calculate the similarity of each gloss vector and context (or memory) vector.</p><p>We treat c as m 0 . So, the attention α k i of gloss g i at pass k is computed as a dot product of g i and m k−1 :</p><formula xml:id="formula_8">e k i = g i · m k−1<label>(5)</label></formula><formula xml:id="formula_9">α k i = exp(e k i ) |st| j=1 exp(e j i )<label>(6)</label></formula><p>Memory Update: After calculating the attention, we store the memory state in u k which is a weighted sum of gloss vectors and is computed as</p><formula xml:id="formula_10">u k = n i=1 α k i g i (7)</formula><p>where n is the hidden size of LSTM in the context module and gloss module. And then, we update the memory vector m k from last pass memory m k−1 , context vector c, and memory state u k . We propose two memory update methods:</p><p>• Linear: we update the memory vector m k by a linear transformation from m k−1</p><formula xml:id="formula_11">m k = Hm k−1 + u k (8)</formula><p>where H ∈ R 2n×2n .</p><p>• Concatenation: we get a new memory for kth pass by taking both the gloss embedding and context embedding into consideration</p><formula xml:id="formula_12">m k = ReLU (W [m k−1 : u k : c] + b) (9)</formula><p>where : is the concatenation operator, W ∈ R n×6n and b ∈ R 2n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Scoring Module</head><p>The scoring module calculates the scores for all the related senses {s 1 t , s 2 t , . . . , s p t } corresponding to the target word x t and finally outputs a sense probability distribution over all senses.</p><p>The overall score for each word sense is determined by gloss attention α T M i from the last pass in the memory module, where T M is the number of passes in the memory module. The e T M ( α T M without Softmax) is regarded as the gloss score.</p><formula xml:id="formula_13">score g = e T M<label>(10)</label></formula><p>Meanwhile, a fully-connected layer is employed to calculate the context score.</p><formula xml:id="formula_14">score c = W xt c + b xt (11)</formula><p>where W xt ∈ R |st|×2n , b xt ∈ R |st| , |s t | is the number of senses for the target word x t and n is the number of hidden units in the LSTM. It's noteworthy that in Equation 11, each ambiguous word x t has its corresponding weight matrix W xt and bias b xt in the scoring module.</p><p>In order to balance the importance of background knowledge and labeled data, we introduce a parameter λ ∈ R N 7 in the scoring module which is jointly learned during the training process. The probability distributionˆydistributionˆ distributionˆy over all the word senses of the target word is calculated as:</p><formula xml:id="formula_15">ˆ y = Sof tmax(λ xt score c + (1 − λ xt )score g )</formula><p>where λ xt is the parameter for word x t , and λ xt ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>During training, all model parameters are jointly learned by minimizing a standard crossentropy loss betweenˆybetweenˆ betweenˆy and the true label y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Evaluation Dataset: we evaluate our model on several English all-words WSD datasets. For fair comparison, we use the benchmark datasets proposed by <ref type="bibr" target="#b14">Raganato et al. (2017b)</ref> which includes five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions. They are Senseval-2 (SE2), Senseval-3 task 1 (SE3), SemEval-07 task 17 (SE7), SemEval-13 task 12 (SE13), and SemEval-15 task 13 (SE15). Following by , we choose SE7, the smallest test set as the development (validation) set, which consists of 455 labeled instances. The last four test sets consist of 6798 labeled instances with four types of target words, namely nouns, verbs, adverbs and adjectives. We extract word sense glosses from WordNet3.0 because <ref type="bibr" target="#b14">Raganato et al. (2017b)</ref> maps all the sense annotations 8 from its original version to 3.0.</p><p>Training Dataset: We choose SemCor 3.0 as the training set, which was also used by , <ref type="bibr" target="#b14">Raganato et al. (2017b)</ref>, <ref type="bibr" target="#b3">Iacobacci et al. (2016)</ref>, , etc. It consists of 226,036 sense annotations from 352 documents, which is the largest manually annotated corpus for WSD. Note that all the systems listed in <ref type="table">Table 1</ref> are trained on SemCor 3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use the validation set (SE7) to find the optimal settings of our framework: the hidden state size n, the number of passes |T M |, the optimizer, etc. We use pre-trained word embeddings with 300 dimensions 9 , and keep them fixed during the training process. We employ 256 hidden units in both the gloss module and the context module, which means n=256. Orthogonal initialization is used for weights in LSTM and random uniform initialization with range [-0.1, 0.1] is used for others. We assign gloss expansion depth K the value of 4. We also experiment with the number of passes |T M | from 1 to 5 in our framework, finding |T M | = 3 performs best. We use Adam optimizer <ref type="bibr">(Kingma and Ba, 2014</ref>) in the training process with 0.001 initial learning rate. In order to avoid overfitting, we use dropout regularization and set drop rate to 0.5. Training runs for up to 100 epochs with early stopping if the validation loss doesn't improve within the last 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Systems to be Compared</head><p>In this section, we describe several knowledgebased methods, supervised methods and neuralbased methods which perform well on the English all-words WSD datasets for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Knowledge-based Systems</head><p>• Lesk ext+emb : Basile et al. <ref type="formula" target="#formula_1">(2014)</ref> is a variant of Lesk algorithm <ref type="bibr">(Lesk, 1986)</ref> by using a word similarity function defined on a distributional semantic space to calculate the gloss-context overlap. This work shows that glosses are important to WSD and enriching <ref type="table">Table 1</ref>: F1-score (%) for fine-grained English all-words WSD on the test sets. Bold font indicates best systems. The * represents the neural network models using external knowledge. The fives blocks list the MFS baseline, two knowledge-based systems, two supervised systems (feature-based), three neuralbased systems and our models, respectively.</p><p>. gloss information via its semantic relations can help to WSD.</p><p>• Babelfy: Moro et al. <ref type="formula" target="#formula_1">(2014)</ref> exploits the semantic network structure from BabelNet and builds a unified graph-based architecture for WSD and Entity Linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Supervised Systems</head><p>The supervised systems mentioned in this paper refers to traditional feature-based systems which train a dedicated classifier for every word individually (word expert).</p><p>• IMS: Zhi and Ng (2010) selects a linear Support Vector Machine (SVM) as its classifier and makes use of a set of features surrounding the target word within a limited window, such as POS tags, local words and local collocations.</p><p>• IMS +emb : Iacobacci et al. <ref type="formula" target="#formula_1">(2016)</ref> selects IMS as the underlying framework and makes use of word embeddings as features which makes it hard to beat in most of WSD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Neural-based Systems</head><p>Neural-based systems aim to build an end-to-end unified neural network for all the polysemous words in texts.</p><p>• Bi-LSTM: Kågebäck and Salomonsson (2016) leverages a bidirectional LSTM network which shares model parameters among all words. Note that this model is equivalent to our model if we remove the gloss module and memory module of GAS.</p><p>• Bi-LSTM +att.+LEX and its variant Bi-LSTM +att.+LEX+P OS : Raganato et al. (2017a) transfers WSD into a sequence learning task and propose a multi-task learning framework for WSD, POS tagging and coarse-grained semantic labels (LEX). These two models have used the external knowledge, for the LEX is based on lexicographer files in WordNet.</p><p>Moreover, we introduce MFS baseline, which simply selects the most frequent sense in the training data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">English all-words results</head><p>In this section, we show the performance of our proposed model in the English all-words task. Table 1 shows the F1-score results on the four test sets mentioned in Section 4.1. The systems in the first four blocks are implemented by <ref type="bibr">Raganato et al. (2017a,b)</ref> except for the single Bi-LSTM model. The last block lists the performance of our proposed model GAS and its variant GAS ext which extends the gloss module in GAS.</p><p>GAS and GAS ext achieves the state-of-theart performance on the concatenation of all test datasets. Although there is no one system always performs best on all the test sets 10 , we can find that GAS ext with concatenation memory updating strategy achieves the best results 70.6 on the concatenation of the four test datasets. Compared with other three neural-based methods in the Context: He plays a pianist in the film Glosses Pass 1 Pass 2 Pass 3 Pass 4 Pass 5 g 1 : participate in games or sport g 2 : perform music on a instrument g 3 : act a role or part <ref type="table">Table 2</ref>: An example of attention weights in the memory module within 5 passes. Darker colors mean that the attention weight is higher. Case studies show that the proposed multi-pass operation can recognize the correct sense by enlarging the attention gap between correct senses and incorrect ones.  <ref type="table">Table 3</ref>: F1-score (%) of different passes from 1 to 5 on the test data sets. It shows that appropriate number of passes can boost the performance as well as avoid over-fitting of the model. . fourth block, we can find that our best model outperforms the previous best neural network models ( ) on every individual test set. The IMS +emb , which trains a dedicated classifier for each word individually (word expert) with massive manual designed features including word embeddings, is hard to beat for neural networks models. However, our best model can also beat IMS +emb on the SE3, SE13 and SE15 test sets.</p><p>Incorporating glosses into neural WSD can greatly improve the performance and extending the original gloss can further boost the results. Compared with the Bi-LSTM baseline which only uses labeled data, our proposed model greatly improves the WSD task by 2.2% F1-score with the help of gloss knowledge. Furthermore, compared with the GAS which only uses original gloss as the background knowledge, GAS ext can further improve the performance with the help of the extended glosses through the semantic relations. This proves that incorporating extended glosses through its hypernyms and hyponyms into the neural network models can boost the performance for WSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Multiple Passes Analysis</head><p>To better illustrate the influence of multiple passes, we give an example in <ref type="table">Table 2</ref>. Consider the situation that we meet an unknown word x 11 , we look 11 x refers to word play in reality. up from the dictionary and find three word senses and their glosses corresponding to x.</p><p>We try to figure out the correct meaning of x according to its context and glosses of different word senses by the proposed memory module. In the first pass, the first sense is excluded, for there are no relevance between the context and g 1 . But the g 2 and g 3 may need repeated deliberation, for word pianist is similar to the word music and role in the two glosses. By re-reading the context and gloss information of the target word in the following passes, the correct word sense g 3 attracts much more attention than the other two senses. Such rereading process can be realized by multi-pass operation in the memory module.</p><p>Furthermore, <ref type="table">Table 3</ref> shows the effectiveness of multi-pass operation in the memory module. It shows that multiple passes operation performs better than one pass, though the improvement is not significant. The reason of this phenomenon is that for most target words, one main word sense accounts for the majority of their appearances. Therefore, in most circumstances, one-pass inference can lead to the correct word senses. Case studies in <ref type="table">Table 2</ref> show that the proposed multipass inference can help to recognize the infrequent senses like the third sense for word play. In <ref type="table">Ta- ble 3</ref>, with the increasing number of passes, the F1-score increases. However, when the number of passes is larger than 3, the F1-score stops increasing or even decreases due to over-fitting. It shows that appropriate number of passes can boost the performance as well as avoid over-fitting of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we seek to address the problem of integrating the glosses knowledge of the ambiguous word into a neural network for WSD. We further extend the gloss information through its semantic relations in WordNet to better infer the context. In this way, we not only make use of labeled context data but also exploit the background knowledge to disambiguate the word sense. Results on four English all-words WSD data sets show that our best model outperforms the existing methods.</p><p>There is still one challenge left for the future. We just extract the gloss, missing the structural properties or graph information of lexical resources. In the next step, we will consider integrating the rich structural information into the neural network for Word Sense Disambiguation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>a plot of ground in which plants</head><label></label><figDesc>are growing a small area of ground covered by specific vegetation flowerbed1 a bed in which flowers are growing seedbed1 a bed where seedlings are grown before transplanting turnip_bed1 a bed in which turnips are growing Example sentence the gardener planted a bed of roses Hypernymy Hyponymy plot2</figDesc></figure>

			<note place="foot" n="1"> If there are N polysemous words in texts, they need to train N classifiers individually.</note>

			<note place="foot" n="2"> A synset is a set of words that denote the same sense.</note>

			<note place="foot" n="3"> st is the sense set {s 1 t , s 2 t , . . . , s p t } corresponding to the target word xt</note>

			<note place="foot" n="7"> N is the number of polysemous words in the training corpora.</note>

			<note place="foot" n="8"> The original WordNet version of SE2, SE3, SE7, SE13, SE15 are 1.7, 1.7.1, 2.1, 3.0 and 3.0, respectively. 9 We download the pre-trained word embeddings from https://github.com/stanfordnlp/GloVe, and we select the smaller Wikipedia 2014 + Gigaword 5.</note>

			<note place="foot" n="10"> Because the source of the four datasets are extremely different which belongs to different domains.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the Lei Sha, Jiwei Tan, Jianmin Zhang and Junbing Liu for their instructive suggestions and invaluable help. The research work is supported by the National Science Foundation of China under Grant No. 61772040 and No. 61751201. The contact authors are Baobao Chang and Zhifang Sui.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>66.0 63.8 67.1 67.7 49.8 73.1 80.5 65.5</idno>
		<title level="m">SE3 SE13 SE15 Noun Verb Adj Adv All MFS baseline 65</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lesk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basile</surname></persName>
		</author>
		<idno>0 63.7 66.2 64.6 70.0 51.1 51.7 80.6 64.2</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babelfy (moro</surname></persName>
		</author>
		<idno>0 63.5 66.4 70.3 68.9 50.7 73.2 79.8 66.4</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">67</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ims (zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng ; Iacobacci</surname></persName>
		</author>
		<idno>72.2 70.4 65.9 71.5 71.9 56.6 75.9 84.7 70.1</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
	<note>9 69.3 65.3 69.5 70.5 55.8 75.6 82.9 68.9 IMS +emb</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Lstm (</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salomonsson</forename><surname>Kågebäck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
	<note>1 68.4 64.8 68.3 69.5 55.9 76.2 82.4 68.4</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bi-Lstm+att</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>+lex (raganato</surname></persName>
		</author>
		<idno>72.0 69.4 66.4 72.4 71.6 57.1 75.6 83.2 69.9</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bi-Lstm+att</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>+lex+p Os (raganato</surname></persName>
		</author>
		<idno>72.0 69.1 66.9 71.5 71.5 57.5 75.0 83.8 69.9</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<idno>72.0 70.0 66.7 71.6 71.7 57.4 76.5 83.5 70.1</idno>
	</analytic>
	<monogr>
		<title level="j">GAS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<idno>72.1 70.2 67.0 71.8 72.1 57.2 76.0 84.4 70.3</idno>
	</analytic>
	<monogr>
		<title level="j">GAS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasext</surname></persName>
		</author>
		<idno>72.4 70.1 67.1 72.1 71.9 58.1 76.4 84.7 70.4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasext</surname></persName>
		</author>
		<idno>72.2 70.5 67.2 72.6 72.2 57.7 76.6 85.0 70.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random walks for knowledge-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References Eneko Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Oier Lpez De Lacalle, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word sense disambiguation:a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural sequence learning models for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><forename type="middle">Delli</forename><surname>Bovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A unified evaluation framework and empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01127</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Will repeated reading benefit natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National CCF Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Deliberation networks: Sequence generation beyond one-pass decoding</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Flexible and creative chinese poetry generation using neural memory pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1364" to="1373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden, System Demonstrations</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
