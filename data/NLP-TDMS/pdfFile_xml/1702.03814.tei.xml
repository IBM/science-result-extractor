<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilateral Multi-Perspective Matching for Natural Language Sentences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<email>zhigwang@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
							<email>whamza@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
							<email>raduf@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM T.J. Watson Research Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bilateral Multi-Perspective Matching for Natural Language Sentences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Natural language sentence matching is a fundamental technology for a variety of tasks. Previous approaches either match sentences from a single direction or only apply single granular (word-byword or sentence-by-sentence) matching. In this work, we propose a bilateral multi-perspective matching (BiMPM) model. Given two sentences P and Q, our model first encodes them with a BiL-STM encoder. Next, we match the two encoded sentences in two directions P against Q and Q against P. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fixed-length matching vector. Finally, based on the matching vector, a decision is made through a fully connected layer. We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language sentence matching (NLSM) is the task of comparing two sentences and identifying the relationship between them. It is a fundamental technology for a variety of tasks. For example, in a paraphrase identification task, NLSM is used to determine whether two sentences are paraphrase or not <ref type="bibr">[Yin et al., 2015]</ref>. For a natural language inference task, NLSM is utilized to judge whether a hypothesis sentence can be inferred from a premise sentence <ref type="bibr" target="#b0">[Bowman et al., 2015]</ref>. For question answering and information retrieval tasks, NLSM is employed to assess the relevance between query-answer pairs and rank all the candidate answers [ <ref type="bibr" target="#b3">Wang et al., 2016d]</ref>. For machine comprehension tasks, NLSM is used for matching a passage with a question and pointing out the correct answer span .</p><p>With the renaissance of neural network models <ref type="bibr" target="#b0">[LeCun et al., 2015;</ref><ref type="bibr" target="#b1">Peng et al., 2015a;</ref>, two types of deep learning frameworks were proposed for NLSM.</p><p>The first framework is based on the "Siamese" architecture <ref type="bibr" target="#b0">[Bromley et al., 1993]</ref>. In this framework, the same neural network encoder (e.g., a CNN or a RNN) is applied to two input sentences individually, so that both of the two sentences are encoded into sentence vectors in the same embedding space. Then, a matching decision is made solely based on the two sentence vectors <ref type="bibr" target="#b0">[Bowman et al., 2015;</ref><ref type="bibr" target="#b2">Tan et al., 2015]</ref>. The advantage of this framework is that sharing parameters makes the model smaller and easier to train, and the sentence vectors can be used for visualization, sentence clustering and many other purposes <ref type="bibr" target="#b3">[Wang et al., 2016c]</ref>. However, a disadvantage is that there is no explicit interaction between the two sentences during the encoding procedure, which may lose some important information. To deal with this problem, a second framework "matchingaggregation" has been proposed <ref type="bibr" target="#b3">[Wang and Jiang, 2016;</ref><ref type="bibr" target="#b3">Wang et al., 2016d]</ref>. Under this framework, smaller units (such as words or contextual vectors) of the two sentences are firstly matched, and then the matching results are aggregated (by a CNN or a LSTM) into a vector to make the final decision. The new framework captures more interactive features between the two sentences, therefore it acquires significant improvements. However, the previous "matchingaggregation" approaches still have some limitations. First, some of the approaches only explored the word-by-word matching <ref type="bibr" target="#b2">[Rocktäschel et al., 2015]</ref>, but ignored other granular matchings (e.g., phrase-by-sentence); Second, the matching is only performed in a single direction (e.g., matching P against Q) <ref type="bibr" target="#b3">[Wang and Jiang, 2015]</ref>, but neglected the reverse direction (e.g., matching Q against P ).</p><p>In this paper, to tackle these limitations, we propose a bilateral multi-perspective matching (BiMPM) model for NLSM tasks. Our model essentially belongs to the "matchingaggregation" framework. Given two sentences P and Q, our model first encodes them with a bidirectional Long ShortTerm Memory Network (BiLSTM). Next, we match the two encoded sentences in two directions P → Q and P ← Q. In each matching direction, let's say P → Q, each time step of Q is matched against all time-steps of P from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fixed-length matching vector. Finally, based on the matching vector, a decision is made through a fully connected layer. We evaluate our model on three NLSM tasks: paraphrase identification, natural lan-guage inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks.</p><p>In following parts, we start with a brief definition of the NLSM task (Section 2), followed by the details of our model (Section 3). Then we evaluate our model on standard benchmark datasets (Section 4). We talk about related work in Section 5, and conclude this work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>Formally, we can represent each example of the NLSM task as a triple (P, Q, y), where P = (p 1 , ..., p j , ..., p M ) is a sentence with a length M , Q = (q 1 , ..., q i , ..., q N ) is the second sentence with a length N , y ∈ Y is the label representing the relationship between P and Q, and Y is a set of taskspecific labels. The NLSM task can be represented as estimating a conditional probability Pr (y|P, Q) based on the training set, and predicting the relationship for testing examples by y * = arg max y∈Y Pr(y|P, Q). Concretely, for a paraphrase identification task, P and Q are two sentences, Y = {0, 1}, where y = 1 means that P and Q are paraphrase of each other, and y = 0 otherwise. For a natural language inference task, P is a premise sentence, Q is a hypothesis sentence, and Y = {entailment, contradiction, neutral} where entailment indicates Q can be inferred from P , contradiction indicates Q cannot be true condition on P , and neutral means P and Q are irrelevant to each other. In an answer sentence selection task, P is a question, Q is a candidate answer, and Y = {0, 1} where y = 1 means Q is a correct answer for P , and y = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first give a high-level overview of our model in Sub-section 3.1, and then give more details about our novel multi-perspective matching operation in Subsection 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>We propose a bilateral multi-perspective matching (BiMPM) model to estimate the probability distribution Pr(y|P, Q). Our model belongs to the "matching-aggregation" framework <ref type="bibr" target="#b3">[Wang and Jiang, 2016]</ref>. Contrarily to previous "matching-aggregation" approaches, our model matches P and Q in two directions (P → Q and P ← Q). In each individual direction, our model matches the two sentences from multiple perspectives. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of our model. Given a pair of sentences P and Q, the BiMPM model estimates the probability distribution Pr(y|P, Q) through the following five layers.</p><p>Word Representation Layer. The goal of this layer is to represent each word in P and Q with a d-dimensional vector. We construct the d-dimensional vector with two components: a word embedding and a character-composed embedding. The word embedding is a fixed vector for each individual word, which is pre-trained with GloVe <ref type="bibr" target="#b2">[Pennington et al., 2014]</ref> or word2vec <ref type="bibr" target="#b1">[Mikolov et al., 2013]</ref>. The charactercomposed embedding is calculated by feeding each character (represented as a character embedding) within a word</p><formula xml:id="formula_0">í µí² " í µí² # í µí² $ í µí² % …... …...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Representation Layer</head><p>Matching Layer</p><formula xml:id="formula_1">... ... ... ... …... …... …... …...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Representation Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>softmax</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction Layer</head><p>Pr (y|í µí±, í µí±)  into a Long Short-Term Memory Network (LSTM) <ref type="bibr">[Hochre- iter and Schmidhuber, 1997]</ref>, where the character embeddings are randomly initialized and learned jointly with other network parameters from NLSM tasks. The output of this layer are two sequences of word vectors P :</p><formula xml:id="formula_2">[p 1 , ..., p M ] and Q : [q 1 , ..., q N ].</formula><p>Context Representation Layer. The purpose of this layer is to incorporate contextual information into the representation of each time step of P and Q. We utilize a bi-directional LSTM (BiLSTM) to encode contextual embeddings for each time-step of P .</p><formula xml:id="formula_3">− → h p i = − −−− → LSTM( − → h p i−1 , p i ) i = 1, ..., M ← − h p i = ← −−− − LSTM( ← − h p i+1 , p i ) i = M, ..., 1<label>(1)</label></formula><p>Meanwhile, we apply the same BiLSTM to encode Q:</p><formula xml:id="formula_4">− → h q j = − −−− → LSTM( − → h q j−1 , q j ) j = 1, ..., N ← − h q j = ← −−− − LSTM( ← − h q j+1 , q j ) j = N, ..., 1<label>(2)</label></formula><p>Matching Layer. This is the core layer within our model. The goal of this layer is to compare each contextual embedding (time-step) of one sentence against all contextual embeddings (time-steps) of the other sentence. As shown in <ref type="figure" target="#fig_0">Figure  1</ref>, we will match the two sentences P and Q in two directions: match each time-step of P against all time-steps of Q, and match each time-step of Q against all time-steps of P . To match one time-step of a sentence against all time-steps of the other sentence, we design a multi-perspective matching operation ⊗. We will give more details about this operation in Sub-section 3.2. The output of this layer are two sequences of matching vectors (right above the operation ⊗ in <ref type="figure" target="#fig_0">Figure  1</ref>), where each matching vector corresponds to the matching result of one time-step against all time-steps of the other sentence.</p><p>Aggregation Layer. This layer is employed to aggregate the two sequences of matching vectors into a fixed-length matching vector. We utilize another BiLSTM model, and apply it to the two sequences of matching vectors individually. Then, we construct the fixed-length matching vector by concatenating (the four green) vectors from the last time-step of the BiLSTM models.</p><p>Prediction Layer. The purpose of this layer is to evaluate the probability distribution Pr(y|P, Q). To this end, we employ a two layer feed-forward neural network to consume the fixed-length matching vector, and apply the sof tmax function in the output layer. The number of nodes in the output layer is set based on each specific task described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-perspective Matching Operation</head><p>We define the multi-perspective matching operation ⊗ in following two steps:</p><p>First, we define a multi-perspective cosine matching function f m to compare two vectors</p><formula xml:id="formula_5">m = f m (v 1 , v 2 ; W )<label>(3)</label></formula><p>where v 1 and v 2 are two d-dimensional vectors, W ∈ l×d is a trainable parameter with the shape l × d, l is the number of perspectives, and the returned value m is a l-dimensional</p><formula xml:id="formula_6">vector m = [m 1 , ..., m k , ..., m l ].</formula><p>Each element m k ∈ m is a matching value from the k-th perspective, and it is calculated by the cosine similarity between two weighted vectors</p><formula xml:id="formula_7">m k = cosine(W k • v 1 , W k • v 2 )<label>(4)</label></formula><p>where • is the element-wise multiplication, and W k is the k-th row of W , which controls the k-th perspective and assigns different weights to different dimensions of the ddimensional space.</p><p>Second, based on f m , we define four matching strategies to compare each time-step of one sentence against all time-steps of the other sentence. To avoid repetition, we only define these matching strategies for one matching direction P → Q. The readers can infer equations for the reverse direction easily.</p><p>(1) Full-Matching. <ref type="figure" target="#fig_1">Figure 2</ref> (a) shows the diagram of this matching strategy. In this strategy, each forward (or backward) contextual embedding</p><formula xml:id="formula_8">− → h p i (or ← − h p i )</formula><p>is compared with the last time step of the forward (or backward) representation of the other sentence</p><formula xml:id="formula_9">− → h q N (or ← − h q 1 ). − → m f ull i = f m ( − → h p i , − → h q N ; W 1 ) ← − m f ull i = f m ( ← − h p i , ← − h q 1 ; W 2 )<label>(5)</label></formula><p>(2) Maxpooling-Matching. <ref type="figure" target="#fig_1">Figure 2 (</ref>  </p><formula xml:id="formula_10">f m ( − → h p i , − → h q j ; W 3 ) ← − m max i = max j∈(1...N ) f m ( ← − h p i , ← − h q j ; W 4 )</formula><p>where max</p><formula xml:id="formula_11">j∈(1...N )</formula><p>is element-wise maximum. </p><formula xml:id="formula_12">− → α i,j = cosine( − → h p i , − → h q j ) j = 1, ..., N ← − α i,j = cosine( ← − h p i , ← − h q j ) j = 1, ..., N<label>(7)</label></formula><p>Then, we take</p><formula xml:id="formula_13">− → α i,j (or ← − α i,j ) as the weight of − → h q j (or ← − h q j )</formula><p>, and calculate an attentive vector for the entire sentence Q by weighted summing all the contextual embeddings of Q:</p><formula xml:id="formula_14">− → h mean i = N j=1 − → α i,j · − → h q j N j=1 − → α i,j ← − h mean i = N j=1 ← − α i,j · ← − h q j N j=1 ← − α i,j<label>(8)</label></formula><p>Finally, we match each forward (or backward) contextual embedding of − → h p i (or ← − h p i ) with its corresponding attentive vector:</p><formula xml:id="formula_15">− → m att i = f m ( − → h p i , − → h mean i ; W 5 ) ← − m att i = f m ( ← − h p i , ← − h mean i ; W 6 )<label>(9)</label></formula><p>(4) Max-Attentive-Matching. <ref type="figure" target="#fig_1">Figure 2 (d)</ref> shows the diagram of this matching strategy. This strategy is similar to the Attentive-Matching strategy. However, instead of taking the weighed sum of all the contextual embeddings as the attentive vector, we pick the contextual embedding with the highest cosine similarity as the attentive vector. Then, we match each contextual embedding of the sentence P with its new attentive vector.</p><p>We apply all these four matching strategies to each timestep of the sentence P , and concatenate the generated eight vectors as the matching vector for each time-step of P . We also perform the same process for the reverse matching direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. We will first introduce the general setting of our BiMPM models in Sub-section 4.1. Then, we demonstrate the properties of our model through some ablation studies in Sub-section 4.2. Finally, we compare our model with state-of-the-art models on some standard benchmark datasets in Sub-section 4.3, 4.4 and 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>We initialize word embeddings in the word representation layer with the 300-dimensional GloVe word vectors pretrained from the 840B Common Crawl corpus <ref type="bibr" target="#b2">[Pennington et al., 2014]</ref>. For the out-of-vocabulary (OOV) words, we initialize the word embeddings randomly. For the charactercomposed embeddings, we initialize each character as a 20-dimensional vector, and compose each word into a 50-dimensional vector with a LSTM layer. We set the hidden size as 100 for all BiLSTM layers. We apply dropout to every layers in <ref type="figure" target="#fig_0">Figure 1</ref>, and set the dropout ratio as 0.1. To train the model, we minimize the cross entropy of the training set, and use the ADAM optimizer <ref type="bibr" target="#b0">[Kingma and Ba, 2014]</ref> to update parameters. We set the learning rate as 0.001. During training, we do not update the pre-trained word embeddings. For all the experiments, we pick the model which works the best on the dev set, and then evaluate it on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Properties</head><p>To demonstrate the properties of our model, we choose the paraphrase identification task, and experiment on the "Quora Question Pairs" dataset <ref type="bibr">1</ref> . This dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other. We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the dev set, and sample another 5,000 paraphrases and 5,000 non-paraphrases as the test set. We keep the remaining instances as the training set 2 . First, we study the influence of our multi-perspective cosine matching function in Eq.(3). We vary the number of perspectives l among {1, 5, 10, 15, 20} 3 , and keep the other options unchanged. We also build a baseline model by replacing Eq. <ref type="formula" target="#formula_5">(3)</ref> with the vanilla cosine similarity function. <ref type="figure" target="#fig_3">Figure  3</ref> shows the performance curve on the dev set, where l = 0 corresponds to the performance of our baseline model. We can see that, even if we only utilize one perspective (l = 1), our model gets a significant improvement. When increasing the number of perspectives, the performance improves significantly. Therefore, our multi-perspective cosine matching function is really effective for matching vectors.</p><p>Second, to check the effectiveness of bilateral matching, we build two ablation models to matching sentences in only a single direction: 1) "Only P → Q" which only matches P against Q; 2) "Only P ← Q" which only matches Q against P . <ref type="table">Table 1</ref> shows the performance on the dev set. Comparing the two ablation models with the "Full Model", we can observe that single direction matching hurts the performance for about 1 percent. Therefore, matching sentences in two directions is really necessary for acquiring better performance.</p><p>Third, we evaluate the effectiveness of different matching strategies. To this end, we construct four ablation models (w/o Full-Matching, w/o Maxpooling-Matching, w/o Attentive-Matching, w/o Max-Attentive-Matching) by eliminating a matching strategy at each time. <ref type="table">Table 1</ref> shows the performance on the dev set. We can see that eliminating any of the matching strategies would hurt the performance significantly.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Paraphrase Identification</head><p>In this Sub-section, we compare our model with state-of-theart models on the paraphrase identification task. We still experiment on the "Quora Question Pairs" dataset, and use the same dataset partition as Sub-section 4.2. This dataset is a brand-new dataset, and no previous results have been published yet. Therefore, we implemented three types of baseline models.</p><p>First, under the Siamese framework, we implement two baseline models: "Siamese-CNN" and "Siamese-LSTM". Both of the two models encode two input sentences into sentence vectors with a neural network encoder, and make a decision based on the cosine similarity between the two sentence vectors. But they implement the sentence encoder with a CNN and a LSTM respectively. We design the CNN and the LSTM model according to the architectures in <ref type="bibr" target="#b3">[Wang et al., 2016c]</ref>.</p><p>Second, based on the two baseline models, we implement two more baseline models "Multi-Perspective-CNN" and "Multi-Perspective-LSTM". In these two models, we change the cosine similarity calculation layer with our multiperspective cosine matching function in Eq. <ref type="formula" target="#formula_5">(3)</ref>, and apply a fully-connected layer (with sigmoid function on the top) to make the prediction.</p><p>Third, we re-implement the "L.D.C." model proposed by <ref type="bibr" target="#b3">[Wang et al., 2016d]</ref>, which is a model under the "matchingaggregation" framework and acquires the state-of-the-art performance on several tasks. <ref type="table" target="#tab_3">Table 2</ref> shows the performances of all baseline models and our "BiMPM" model. We can see that "Multi-Perspective-CNN" (or "Multi-Perspective-LSTM") works much better than "Siamese-CNN" (or "Siamese-LSTM"), which further indicates that our multi-perspective cosine matching funcModels Accuracy <ref type="bibr" target="#b0">[Bowman et al., 2015]</ref> 77.6 <ref type="bibr" target="#b2">[Vendrov et al., 2015]</ref> 81.4 <ref type="bibr" target="#b1">[Mou et al., 2015]</ref> 82.1 <ref type="bibr" target="#b2">[Rocktäschel et al., 2015]</ref> 83.5 [  85.0 [  85.1 <ref type="bibr" target="#b3">[Wang and Jiang, 2015]</ref> 86.1 <ref type="bibr" target="#b0">[Cheng et al., 2016]</ref> 86.3 <ref type="bibr" target="#b1">[Parikh et al., 2016]</ref> 86.8 <ref type="bibr">[Munkhdalai and Yu, 2016]</ref> 87.3 <ref type="bibr" target="#b2">[Sha et al., 2016]</ref> 87.5   <ref type="table">(Single)</ref> 87.7   <ref type="table">(Ensemble)</ref> 88.3</p><p>Only P → Q 85.6 Only P ← Q 86.3 BiMPM 86.9 BiMPM (Ensemble)</p><p>88.8 tion (Eq. <ref type="formula" target="#formula_5">(3)</ref>) is very effective for matching vectors. Our "BiMPM" model outperforms the "L.D.C." model by more than two percent. Therefore, our model is very effective for the paraphrase identification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments on Natural Language Inference</head><p>In this Sub-section, we evaluate our model on the natural language inference task over the SNLI dataset <ref type="bibr" target="#b0">[Bowman et al., 2015]</ref>. We test four variations of our model on this dataset, where "Only P → Q" and "Only P ← Q" are the single direction matching models described in Sub-section 4.2, "BiMPM" is our full model, and "BiMPM (Ensemble)" is an ensemble version of our "BiMPM" model. We design the ensemble model by simply averaging the probability distributions <ref type="bibr" target="#b1">[Peng et al., 2015b;</ref><ref type="bibr" target="#b2">Peng et al., 2017]</ref> of four "BiMPM" models, and each of the "BiMPM" model has the same architecture, but is initialized with a different seed. <ref type="table" target="#tab_4">Table 3</ref> shows the performances of the state-of-the-art models and our models. First, we can see that "Only P ← Q" works significantly better than "Only P → Q", which tells us that, for natural language inference, matching the hypothesis against the premise is more effective than the other way around. Second, our "BiMPM" model works much better than "Only P ← Q", which reveals that matching premise against the hypothesis can also bring some benefits. Finally, comparing our models with all the state-of-the-art models, we can observe that our single model "BiMPM" is on par with the state-of-the-art single models, and our 'BiMPM (Ensemble)" works much better than "  (Ensemble)". Therefore, our models achieve the state-of-the-art performance in both single and ensemble scenarios for the natural language inference task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TREC-QA</head><p>WikiQA MAP MRR MAP MRR  0.695 0.763 0.652 0.665 <ref type="bibr" target="#b2">[Tan et al., 2015]</ref> 0.728 0.832 --Wang and Itty. <ref type="bibr">[2015]</ref> 0.746 0.820 --[ <ref type="bibr" target="#b2">Santos et al., 2016]</ref> 0.753 0.851 0.689 0.696 <ref type="bibr">[Yin et al., 2015]</ref> --0.692 0.711 <ref type="bibr" target="#b1">[Miao et al., 2016]</ref> --0.689 0.707 [ <ref type="bibr" target="#b3">Wang et al., 2016d]</ref> 0.771 0.845 0.706 0.723 <ref type="bibr" target="#b2">[He and Lin, 2016]</ref> 0.777 0.836 0.709 0.723 <ref type="bibr" target="#b2">[Rao et al., 2016]</ref> 0.801 0.877 0.701 0.718 [  --0.734 0.742 <ref type="bibr" target="#b3">[Wang and Jiang, 2016]</ref> --0.743 0.755 BiMPM 0.802 0.875 0.718 0.731 <ref type="table">Table 4</ref>: Performance for answer sentence selection on TREC-QA and WikiQA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiments on Answer Sentence Selection</head><p>In this Sub-section, we study the effectiveness of our model for answer sentence selection tasks. The answer sentence selection task is to rank a list of candidate answer sentences based on their similarities to the question, and the performance is measured by the mean average precision (MAP) and mean reciprocal rank (MRR). We experiment on two datasets: TREC-QA [ <ref type="bibr" target="#b3">Wang et al., 2007]</ref> and WikiQA . Experimental results of the state-of-the-art models <ref type="bibr">4</ref> and our "BiMPM" model are listed in <ref type="table">Table 4</ref>, where the performances are evaluated with the standard trec eval-8.0 script 5 . We can see that the performance from our model is on par with the state-of-the-art models. Therefore, our model is also effective for answer sentence selection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Natural language sentence matching (NLSM) has been studied for many years. Early approaches focused on designing hand-craft features to capture n-gram overlapping, word reordering and syntactic alignments phenomena <ref type="bibr" target="#b0">[Heilman and Smith, 2010;</ref><ref type="bibr" target="#b3">Wang and Ittycheriah, 2015]</ref>. This kind of method can work well on a specific task or dataset, but it's hard to generalize well to other tasks. With the availability of large-scale annotated datasets <ref type="bibr" target="#b0">[Bowman et al., 2015]</ref>, many deep learning models were proposed for NLSM. The first kind of framework is based the Siamese architecture <ref type="bibr" target="#b0">[Bromley et al., 1993]</ref>, where sentences are encoded into sentence vectors based on some neural network encoders, and then the relationship between two sentences was decided solely based on the two sentence vectors <ref type="bibr" target="#b0">[Bowman et al., 2015;</ref><ref type="bibr" target="#b2">Tan et al., 2015]</ref>. However, this kind of framework ignores the fact that the lower level interactive features between two <ref type="bibr">4</ref> [ <ref type="bibr" target="#b2">Rao et al., 2016]</ref> pointed out that there are two versions of TREC-QA dataset: raw-version and clean-version. In this work, we utilized the clean-version. Therefore, we only compare with approaches reporting performance on this dataset. 5 http://trec.nist.gove/trec eval/ sentences are indispensable. Therefore, many neural network models were proposed to match sentences from multiple level of granularity <ref type="bibr">[Yin et al., 2015;</ref><ref type="bibr" target="#b3">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b3">Wang et al., 2016d]</ref>. Experimental results on many tasks have proofed that the new framework works significantly better than the previous methods. Our model also belongs to this framework, and we have shown its effectiveness in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a bilateral multi-perspective matching (BiMPM) model under the "matching-aggregation" framework. Different from the previous "matchingaggregation" approaches, our model matches sentences P and Q in two directions (P → Q and P ← Q). And, in each individual direction, our model matches the two sentences from multiple perspectives. We evaluated our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture for Bilateral Multi-Perspective Matching (BiMPM) Model, where ⊗ is the multi-perspective matching operation described in sub-section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagrams for different matching strategies, where fm is the multi-perspective cosine matching function in Eq.(3), the input includes one time step of one sentence (left orange block) and all the time-steps of the other sentence (right blue blocks), and the output is a vector of matching values (top green block) calculated by Eq.(3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>)</head><label></label><figDesc>Attentive-Matching. Figure 2 (c) shows the diagram of this matching strategy. We first calculate the cosine similar- ities between each forward (or backward) contextual embed- ding − → h p i (or ← − h p i ) and every forward (or backward) contextual embeddings of the other sentence − → h q j (or ← − h q j ):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1Figure 3 :</head><label>3</label><figDesc>Figure 3: Influence of the multi-perspective cosine matching function in Eq.(3) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance for paraphrase identification on the Quora 
dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance for natural language inference on the SNLI 
dataset. 

</table></figure>

			<note place="foot" n="2"> We will release our source code and the dataset partition at https://zhiguowang.github.io/ . 3 Due to practical limitations, we did not experiment with more perspectives.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<idno>arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<editor>Lin, 2016] Hua He and Jimmy Lin</editor>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<publisher>Heilman and Smith</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">IJPRAI</biblScope>
			<biblScope unit="page" from="436" to="444" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization. LeCun et al., 2015] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05573</idno>
		<idno>arXiv:1606.01933</idno>
	</analytic>
	<monogr>
		<title level="m">Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference</title>
		<editor>Munkhdalai and Yu, 2016] Tsendsuren Munkhdalai and Hong Yu</editor>
		<imprint>
			<publisher>Xi Peng</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="3880" to="3888" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward personalized modeling: Incremental and ensemble alignment for sequential faces in the wild</title>
		<idno type="arXiv">arXiv:1509.06664</idno>
		<idno>arXiv:1512.08849</idno>
	</analytic>
	<monogr>
		<title level="m">Zhiguo Wang and Abraham Ittycheriah. Faq-based question answering via word alignment</title>
		<editor>Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou</editor>
		<imprint>
			<publisher>Wang and Jiang</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Shuohang Wang and Jing Jiang. Learning natural language inference with lstm</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah. Sentence similarity learning by lexical decomposition and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang ; Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing Jiang ;</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01747</idno>
		<idno>arXiv:1512.05193</idno>
	</analytic>
	<monogr>
		<title level="m">Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<editor>Yang et al., 2015] Yi Yang, Wen-tau Yih, and Christopher Meek</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>A compare-aggregate model for matching text sequences</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
