<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Relation Classification via Hierarchical Recurrent Neural Network with Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>December 11-17 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguang</forename><surname>Xiao</surname></persName>
							<email>xiaomg@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
							<email>liucong3@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Relation Classification via Hierarchical Recurrent Neural Network with Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
						<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers <address><addrLine>Osaka, Japan</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1254" to="1263"/>
							<date type="published">December 11-17 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic relation classification remains a challenge in natural language processing. In this paper , we introduce a hierarchical recurrent neural network that is capable of extracting information from raw sentences for relation classification. Our model has several distinctive features: (1) Each sentence is divided into three context subsequences according to two annotated nom-inals, which allows the model to encode each context subsequence independently so as to selectively focus as on the important context information; (2) The hierarchical model consists of two recurrent neural networks (RNNs): the first one learns context representations of the three context subsequences respectively, and the second one computes semantic composition of these three representations and produces a sentence representation for the relationship classification of the two nominals. (3) The attention mechanism is adopted in both RNNs to encourage the model to concentrate on the important information when learning the sentence representations. Experimental results on the SemEval-2010 Task 8 dataset demonstrate that our model is comparable to the state-of-the-art without using any hand-crafted features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic relation classification is an important task in natural language processing, which has attracted great attention in recent years. The goal is to identify the semantic relationship between a pair of nominals marked in a sentence. For instance, in the sentence "The software <ref type="bibr">[company]</ref> e 1 addressed the problem with the <ref type="bibr">[publication]</ref> e 2 of a fix on Saturday", the marked nominals of company and publication are of relationship Product-Producer(e 2 , e 1 ). Most conventional models focus on machine learning and feature design, which have been shown to obtain performance improvements <ref type="bibr" target="#b9">(Kambhatla, 2004;</ref><ref type="bibr" target="#b23">Tratz and Hovy, 2010;</ref><ref type="bibr" target="#b16">Rink and Harabagiu, 2010)</ref>.</p><p>Recently, neural network approaches have been widely used for relation classification, which aim at reducing the need of hand-crafted features. These approaches are broadly divided into two categories: one explores the effectiveness of using dependency paths and its attached subtrees between two nominals, and various neural networks are adopted to model the shortest dependency paths and dependency subtrees ( <ref type="bibr" target="#b26">Xu et al., 2015a;</ref><ref type="bibr" target="#b27">Xu et al., 2015b</ref>; <ref type="bibr" target="#b10">Liu et al., 2015)</ref>; the other exploits deep neural networks to learn syntactic and semantic features from raw sentences ( <ref type="bibr" target="#b31">Zeng et al., 2014;</ref><ref type="bibr" target="#b13">Dos Santos et al., 2015;</ref><ref type="bibr" target="#b32">Zhang et al., 2015)</ref>, which has been proved effective, but inevitably suffers from irrelevant parts. Our paper introduces an attentive neural network that selectively focuses on useful information on raw sentences.</p><p>Context information of the annotated nominals has been widely believed to be useful for relation classification ( <ref type="bibr" target="#b32">Zhang et al., 2015;</ref><ref type="bibr" target="#b22">Thang Vu et al., 2016)</ref>. In this work, we further explore the effectiveness of context information around the annotated nominals in a sentence. In our model, a sentence with two marked nominals is divided into three context subsequences according to two marked nominals: the left context subsequence, the middle context subsequence and the right context subsequence. This method is similar to <ref type="bibr">Pei et al. (2015)</ref> and Thang <ref type="bibr" target="#b22">Vu et al. (2016)</ref>, which have showed that contextual information is effectively obtained by deep learning techniques. Instead of combining the middle context subsequence with the left and right context subsequences, respectively, as in <ref type="bibr" target="#b22">(Thang Vu et al., 2016)</ref>, we propose to learn context representations via recurrent neural networks that work on each context subsequence independently. For example, the sentence "The software <ref type="bibr">[company]</ref> e 1 addressed the problem with the <ref type="bibr">[publication]</ref> e 2 of a fix on Saturday" is split into three subsequences: " The software", "addressed the problem with the" and "of a fix on Saturday". And the marked nominals of <ref type="bibr">[company]</ref> e 1 and [publication] e 2 are not included in any context subsequence. As a result, the sentence is divided into five parts: three context subsequences and two annotated nominals. Our sentence representations are leant hierarchically from context subsequences to sentences using a hierarchical recurrent neural network, which firstly learns the context representation of each context subsequence independently, and then encodes the semantics of context subsequences into a sentence representation for the relation classification. Furthermore, we introduce the attention mechanism ( <ref type="bibr" target="#b0">Bahdanau, 2014;</ref><ref type="bibr" target="#b19">Rush, 2015;</ref><ref type="bibr" target="#b17">Rocktäschel et al., 2016</ref>) that encourages the model to focus on the important information. Experimental results demonstrate that our model is comparable to the state-of-the-art with a single model that works on the raw sentences.</p><p>In the rest of this paper, we review recurrent neural networks in Section 2. We provide details about our model in Section 3. Section 4 presents our experiments and their results. Finally, we make a conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent Neural Networks</head><p>Recurrent neural networks (RNNs) <ref type="bibr" target="#b3">(Elman, 1990;</ref><ref type="bibr" target="#b11">Mikolov et al., 2010</ref>) project a sequence of inputs x 1 , . . . , x T to a sequence of outputs y 1 , . . . , y T via an affine transformation followed by a non-linear function. At timestep t, a standard RNN computes the new hidden vector as</p><formula xml:id="formula_0">h t = f (W x t + U h t−1 + b) (1)</formula><p>where W is trained matrix transforming the current input x t into the current state linearly, U is also trained matrix connecting the previous state h t−1 with the current state, and b is a bias term, and f is a non-linear function (e.g., tanh). However, RNNs with the above form may suffer from gradient exploding or vanishing problem <ref type="bibr">(Ben- gio et al., 1994;</ref><ref type="bibr" target="#b7">Hochreiter, 1997</ref>) during training when it is trained with the backpropagation through time algorithm <ref type="bibr" target="#b18">(Rumelhart et al., 1986;</ref><ref type="bibr" target="#b24">Werbos, 1990;</ref><ref type="bibr" target="#b25">Williams and Zipser, 1995)</ref>. To address this problem, long short-term memory network (LSTM) was proposed in <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref> where the architecture of a standard RNN was modified to avoid vanishing or exploding gradients. Many LSTM variants have been proposed, and here we adopt the version of <ref type="bibr" target="#b29">Zaremba and Sutskever (2014a)</ref>.</p><p>The LSTM model comprises a memory cell that can store information over a long period of time, and three gates that allow it to control the flow of information into and out of the cell: input gate, forget gate, and output gate. Concretely, the LSTM unit at time step t encompasses a collection of vectors: an input gate i t , a forget gate f t , an output gate o t , a memory cell c t , and a hidden state h t . The unit accepts an input vector x t , the previous hidden state h t−1 , and the memory cell c t−1 and computes the new vectors using the following equations:</p><formula xml:id="formula_1">i t = σ(W (i) x t + U (i) h t−1 + b (i) ) f t = σ(W (f ) x t + U (f ) h t−1 + b (f ) ) o t = σ(W (o) x t + U (o) h t−1 + b (o) )<label>(2)</label></formula><formula xml:id="formula_2">u t = tanh(W (u) x t + U (u) h t−1 + b (u) ) c t = i t u t + f t c t−1 h t = o t tanh(c t )</formula><p>where σ denotes the element-wise application of the logistic function, denotes the element-wise multiplication of two vectors, W and U are weight matrices, and b are bias vectors.</p><formula xml:id="formula_3">[x k ] [x j ]</formula><p>...</p><formula xml:id="formula_4">x j-1 x 2 x 1 ... x k-1 x j+2 x j+1</formula><p>... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we introduce the proposed neural model that learns distributed representations from raw sentences. These representations serving as features are further used for relation classification. An overview of our model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Given a sentence with two annotated nominals, the sentence is firstly divided into five parts (three context subsequences and two annotated nominals) based on the two marked nominals (Section 3.1). Next, the model computes the distributed representations for the context subsequences using a bidirectional LSTM that works on word vectors (Section 3.2). Lastly, these distributed context representations are further encoded into a sentence representation via a bidirectional RNN (Section 3.3). Furthermore, we extend this model with a neural attention that encourages the model to focus on important information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context Subsequences</head><p>In most cases different contexts have different functions for the meaning of sentences. Some recent work fell into the idea that the middle context contains the most relevant information for relation classification, combining the middle context with the left and right context respectively ( <ref type="bibr" target="#b32">Zhang et al., 2015;</ref><ref type="bibr" target="#b22">Thang Vu et al., 2016</ref>). We instead model each context part independently, which allows the model to automatically identify contexts that contain useful information. Given a sentence s and its annotated nominals e 1 and e 2 , the sentence first is split into five parts according to the two annotated nominals: the left context subsequence, entity e 1 , the middle context subsequence, entity e 2 and the right context subsequence. Preprocessing the sentence in such a way allows the model to encode each context subsequence independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Subsequence Composition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Word Encoder</head><p>A bidirectional LSTM (Bi-LSTM) ( <ref type="bibr" target="#b5">Graves and Schmidhuber, 2005;</ref><ref type="bibr" target="#b4">Graves et al., 2013</ref>) is applied to independently encoding each of the three context subsequences. A bidirectional LSTM consists of two LSTMs: the forward and backward LSTMs. They are run in parallel: the forward LSTM inputs the words from x 1 to x T , and the backward LSTM inputs in an reverse order from x T back to x 1 . At time step t, we obtain the hidden state (denoted as h t ) of the bidirectional LSTM by concatenating the forward hidden state (denoted as − → h t ) and the backward one (denoted as</p><formula xml:id="formula_5">← − h t ), i.e., h t = [ − → h t , ← − h t ]</formula><p>. Bi-LSTM can summarize the information from the whole context subsequence centered around words, which let the model understand the meaning of words comprehensively. Given a sentence s divided into the left context subsequence c 1 , the middle context subsequence c 2 and the right context subsequence c 3 , we assume that the sentence s contains L words and the context subsequence c i has T i words, where i ∈ <ref type="bibr">[1,</ref><ref type="bibr">3]</ref>. The input to Bi-LSTM is a context subsequence c i : [x i1 , . . . , x iT i ] where x it is the word vector for word w it . At time step t, the encoder produces a hidden state h it which gathers the information of the whole context subsequence c i centered around w it . The equations are following:</p><formula xml:id="formula_6">− → h it = −−−−→ LST M (x it ) (3) ← − h it = ←−−−− LST M (x it )<label>(4)</label></formula><formula xml:id="formula_7">h it = concat( − → h it , ← − h it )<label>(5)</label></formula><p>where concat is concatenation function, i.e.,</p><formula xml:id="formula_8">h it = [ − → h it ; ← − h it ]</formula><p>. Note that our model encodes the left, middle and right context subsequence independently but with one Bi-LSTM. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Word-level Attention</head><p>Due to the fact that raw sentences contain more information than the shortest dependency paths, there may be some irrelevant information in raw sentences. For concentrating on these words that are important to predict the relationship of entities, it can be a good strategy to pay more attention on these words. To encourage such behavior, this paper introduces a word-level attention mechanism. The attention mechanism enables the model to differently attend over the hidden vectors of Bi-LSTM along a context subsequence, and produces a weighted representation m i of them as follows:</p><formula xml:id="formula_9">z it = tanh(W (w) h it + W (c) r i + b) α it = exp(v z z it ) T i j=1 exp(v z z ij )<label>(6)</label></formula><formula xml:id="formula_10">m i = T i t=1 α it h it</formula><p>where W (w) and W (c) are weight matrices, b is a bias vector, v z is a weight vector and v z is its transformation, and r i is an external context vector that is randomly initialized and jointly optimized during training.</p><p>The attention representation z it corresponding to the t-th word w it in the context subsequence c i is computed via a non-linear combination of the hidden state h it and the external context vector r i . The attention weight α it for the t-th word w it in the context subsequence c i is a probability that is the normalized weight of z it (parameterized by v z ) through a softmax layer, reflecting the importance of the t-th word w it with respect to the meaning of the context subsequence c i in classifying the relationship of two entities. The external context vector r i not only represents the high-level meaning of the context subsequence c i , but also allows the model to identify that the word w it is in the context subsequence c i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Composition</head><p>After establishing an attention-based Bi-LSTM (Section 3.2) to capture the meaning of three context subsequences, resulting in three context representations, there is one difficulty that how to further obtain the semantic composition of these context representations plus two representations of marked nominals. Note that there are five semantic representations. The most common approach is that a multilayer perceptron (MLP) is adopted to take these representations as input and compute semantic compositionality for them. In this work, we adopt a Bi-RNN to integrate syntactics and semantics of three context subsequences and two annotated nominals into sentence representation s, which is further fed into a classifier for relation classification. We propose to learn sentence representations via Bi-RNNs for two reasons: (1) a sentence containing two annotated nominals divided into three context subsequences that are ordered as in the sentence, can be treated as a short sequence that consists of five tokens; (2) recurrent neural networks are competent enough to model the semantics of these context subsequences and their inherent relations, which is important to obtain the semantic meaning of the sentence. The experimental results demonstrate that Bi-RNNs significantly outperform MLP.</p><p>Let Y be a matrix containing five column vectors [m 1 , m e 1 , m 2 , m e 2 , m 3 ], where m i (i ∈ <ref type="bibr">[1,</ref><ref type="bibr">3]</ref>) is the representation of the context subsequence c i , and m e 1 and m e 2 are the representations of annotated nominals e 1 and e 2 . 2 To obtain compositional vector representations for sentences, we iterate the following sequence of equations:</p><formula xml:id="formula_11">− → h j = −−−→ RN N (y j ) (7) ← − h j = ←−−− RN N (y j )<label>(8)</label></formula><formula xml:id="formula_12">h j = concat( − → h j , ← − h j )<label>(9)</label></formula><p>where y j ∈ Y (j ∈ <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>) is the j-th column vector in Y.</p><p>Note that the sentence only contains five elements, our model do not make any assumptions about the type of RNNs used in this subsection. But as far as comparison goes, LSTMs performs better than the standard RNNs.</p><p>To selectively focus on the important context subsequences, it is an alternative solution to applying neural attention to the hidden vectors of the above Bi-RNNs, similar to Subsection 3.2.2. We also make further extensions such as average pooling and max pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>A fully connected softmax layer is used as classifier for classification. It produces the probability distribution p over relation types conditioned on the sentence representation s:</p><formula xml:id="formula_13">p = sof tmax(W (s) s + b (s) )<label>(10)</label></formula><p>The training objective is to minimize the cross-entropy error between the ground truth and predicted label. The parameters of our model are optimized using AdaGrad (Duchi et al., 2011) with a learning rate of 0.01, a mini-batch size of 5 and a L 2 regularization coefficient of 10 −6 . The details are described further in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>In our experiments, we evaluate our model on the SemEval-2010 Task 8 dataset ( <ref type="bibr" target="#b6">Hendrickx et al., 2010)</ref>, which is one of the most widely used benchmarks for relation classification. The dataset contains 10,717 annotated sentences divided into 8,000 sentences for training and 2717 for testing. Each sentence is annotated with each of nine different relationship and an artificial relation Other, and each relationship has two direction except for the undirected relation Other. The nine directed relations are Cause-Effect, Instrument-Agency, Product-Producer, Content-Container, Entity-Origin, EntityDestination, Component-Whole, Member-Collection, and Message-Topic.</p><p>The official evaluation metric is the macro-averaged F1-score (excluding Other), and takes into consideration the directionality. We use the official scorer to test the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>We tune the hyperparameters for our model using 5-fold cross-validation. We pretrain 200-dimensional word embeddings using word2vec ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) on the English Wikipedia corpus, and randomly initialize other hyperparameters. We set the LSTM dimension to be 200. We apply dropout only on the word embeddings and outputs of LSTM as in ( <ref type="bibr">Zaremba et al., 2014b)</ref>, and the dropout rate is 0.2.</p><p>To enable a direct comparison with the previous work, we use the same features: position features, WordNet hypernyms and NER. WordNet hypernyms and NER were obtained using the tool of Ciaramita and Al-tun (2006). <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model features F1</head><p>Bi    Here we apply neural attention to the hidden states of RNNs (Bi-LSTM and Bi-RNN). To ensure the number of parameters comparable, we adopt a two-layer full-connected neural network with the hidden size of 600 dimension and a non-linear function of tanh to serve as MLP. And the hidden size of the standard RNN is 350-dimensional. From <ref type="table">Table 1a</ref>, we find that both the combinations of Bi-LSTM with Bi-RNN and Bi-LSTM outperform the combination of Bi-LSTM and MLP without any features. In particular, the combination of Bi-LSTM and Bi-LSTM achieves the best result 83.90% without any feature, and its F1-score is about 1.5% higher than the model of Bi-LSTM+MLP. The results indicate that the neural architecture of two Bi-LSTMs effectively captures semantic meanings of these context subsequences and their inherent relations, and obtains more robust sentence representations for relation classification. In this paper, we tackle the relation classification task using the combination of two BiLSTMs.</p><p>The comparison of different methods <ref type="table">Table 1b</ref> shows experiments for our model with various methods for the hidden vectors of Bi-LSTMs. We begin with the model using the concatenation of the final state of forward and backward LSTMs. And then we replace concatenation operation with average pooling, max-pooling and neural attention respectively. Not surprisingly, processing the hidden vectors of Bi-LSTMs via neural attention achieves the best result, which gives an improvement of 2.23 percentage points in F1-score over max-pooling. We suspect that this is due to the attention model being run in a more focused way that makes it easier to capture large important information from contexts. We also consider the impact of features for these methods. Results in <ref type="table">Table 1b</ref> show that by adding features the F1-scores of all methods improve, which hints that three features are useful for relation classification.   <ref type="table">Table 2</ref>: Experimental results of our model against other models.   <ref type="figure">CNN)</ref>. And they also propose connectionist bi-directional recurrent neural networks (R-RNN) that adds a connection to the hidden states of bi-directional recurrent neural networks. We observe in <ref type="table">Table 2</ref> that our model is comparable to the state-of-the-art (previous best result is 84.0% obtained by depLCNN + NS) without any features, whereas depLCNN works on the shortest dependency</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison with State-of-the-art Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Feature Set F1 <ref type="table">CR-CNN  - 82.8  +position features  84.1  R-RNN  +position features, position indicators, entity flag 83.4  ER-CNN  +position features, extended middle context  84.2  ER-CNN + R-RNN +all features, voting scheme</ref> 84.9</p><p>Our model -84.1 +position features 84.5 <ref type="table">Table 3</ref>: Comparison of ranking models (no lexical features).</p><p>paths, which consist of most relevant information and avoid negative effect from irrelevant parts in the sentences. This result suggests that our model automatically focuses on important information related to determining the relationship of two entities. The F1-score is improved by adding three features but not as obvious as in ( <ref type="bibr" target="#b31">Zeng et al., 2014;</ref><ref type="bibr" target="#b27">Xu et al., 2015b</ref>) <ref type="figure">(CNN, depLCNN)</ref>. We argue that this is due to BiLSTMs being able to learn position information on sequences and lexical features leading to overfitting as in ( <ref type="bibr" target="#b28">Yu et al., 2014;</ref><ref type="bibr" target="#b10">Liu et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Comparison of ranking models</head><p>For fair comparison, we also replace the softmax layer with a ranking layer to train our model, as proposed in <ref type="bibr" target="#b13">(Dos Santos et al., 2015)</ref>. We use training settings following Thang <ref type="bibr" target="#b22">Vu et al. (2016)</ref>. More details about ranking layer are described in <ref type="bibr" target="#b13">(Dos Santos et al., 2015;</ref><ref type="bibr" target="#b22">Thang Vu et al., 2016)</ref>.</p><p>From <ref type="table">Table 3</ref>, we observe that our model outperform the state-of-the-art without any feature, whereas previous work's best reported performance is 83.9% in ER-CNN using word embeddings of size 400. Combining ER-CNN and R-RNN using a voting scheme achieves a state-of-the-art result of 84.9 in F1-score, which is presented by <ref type="bibr" target="#b22">(Thang Vu et al., 2016</ref>). But our model reaches a new state-of-the-art result with a single model when position features are added, and outperforms the model of ER-CNN that learns context representations for two contexts of the combinations of the middle context with the left and right context respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduce a hierarchical recurrent neural network model that learns useful features from raw sentences for relation classification. We further extend the model with neural attention at two different levels that provides significant improvements over the concatenation, average pooling and max-pooling. Our model shows comparable performance to the state-of-the-art on the SemEval-2010 Task 8 dataset without using any costly hand-crafted features. In addition, the models presented here are general hierarchical models, and are therefore suitable for hierarchical structures, such as paragraphs and documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of our model. Given a sentence consisting of L words, it is divided into the left context subsequence [x 1 , . . . , x j−1 ], the middle context subsequence [x j+1 , . . . , x k−1 ] and the right context subsequence [x k+1 , . . . , x L ]. [x j ] and [x k ] represent the marked nominals e 1 and e 2 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1: (a) F1-scores on the test data for various neural network architectures. We also test these models with three features of position features, WordNet and NER. (b) The comparison of different methods on SemEval-2010 Task 8 test set. Here the neural network architecture is the combination of two Bi-LSTMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 3 .</head><label>3</label><figDesc>1 The Effect of Different Components The effect of neural network architectures We first analyze the effect of different neural network architectures of the combinations of Bi-LSTM with MLP, a standard Bi-RNN and Bi-LSTM separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2 compares our model with several start-of-art models. The SVM model (Rink and Harabagiu, 2010) is used for relation classification by combining lexical and semantic features. It extracts these hand-crafted features from sentences with the use of many external resources. Socher et al. (2012) ex- tend the recursive neural networks with matrix-vector spaces (MV-RNN), and use MV-RNN to learn representations along the constituency tree for relation classification. Yu et al. (2014) propose factor- based compositional embedding models (FCM) for relation classification. It learns representations for the substructures of an annotated sentence, which are further used for classification. Zeng et al. (2014) exploit a convolutional neural network (CNN) to extract lexical and sentence level features for relation classification. And they design position features to specify the target nouns in the sentence, which leads to better performance for their model. CR-CNN outperforms the state-of-art by using a new ranking loss function and omitting the representation of the Other class for diminishing its effect, as proposed by (Dos Santos et al., 2015). Zhang et al. (2015) utilized bidirectional LSTMs (BLSTM) to capture the sentence level features and concatenated them and lexical level features to form the finally feature vector for rela- tion classification. Liu et al. (2015) design a dependency-based framework (DepNN) to learn semantic representations of the augmented dependency paths that are the combination of the shortest dependency paths and their dependency subtrees. Xu et al. (2015a) build multiple LSTMs to model the different chan- nels of word vectors, POS, grammatical relations, and WordNet along the shortest dependency paths and achieves an F1-score of 83.7 (SDP-LSTM). Xu et al. (2015b) propose to learn a robust representation us- ing a convolutional neural network that works on the dependency path between subjects and objects, and propose a negative sampling strategy (NS) to address the relation directionality (DepLCNN). Thang Vu et al. (2016) design extended middle context and present a new context representation for convolutional neural networks for relation classification (ER-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We adopt Bi-LSTM to encode each context subsequence separately even if it contains few words, such as one word.</note>

			<note place="foot" n="2"> We use an additional tanh layer to map the word vectors of annotated nominals e1 and e2 to the dimensionality of the hidden size of the Bi-LSTM.</note>

			<note place="foot" n="3"> sourceforge.net/projects/supersensetag/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by National Science Foundation of China (grant 61472459). We thank the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Net-works</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Preslavnakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions</title>
		<meeting>the ACL 2004 on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Dependency-Based Neural Network for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Honza&quot; ˇ Cernock´yCernock´y, and Sanjeev Khudanpur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop at ICLR</title>
		<meeting>the Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero Nogueira Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Wenzhe Pei</title>
		<imprint/>
	</monogr>
	<note>Tao Ge</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An Effective Neural Network Model for Graph-based Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning about Entailment with Neural Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR2016</title>
		<meeting>ICLR2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The Pdp Research</forename><surname>Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Distributed Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1986" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Sig-nal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combining Recurrent and Convolutional Neural Networks for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Isi: automatic classification of relations between nominals using a maximum entropy classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="222" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yves Chauvain and David E. Rumelhart: &quot;Back-Propagation: Theory, Architectures and Applications</title>
		<imprint>
			<publisher>Lawrence Erlbaum Publishers</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Workshop on Learning Semantics</title>
		<meeting>the NIPS Workshop on Learning Semantics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014b. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bidirectional Long Short-Term Memory Networks for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
