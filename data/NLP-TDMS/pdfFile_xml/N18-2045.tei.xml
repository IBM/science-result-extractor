<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 1 -6, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of NAACL-HLT 2018</title>
						<meeting>NAACL-HLT 2018 <address><addrLine>New Orleans, Louisiana</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="278" to="283"/>
							<date type="published">June 1 -6, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While neural networks have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA)-extraction of fine-grained opinion polarity w.r.t. a pre-defined set of aspects-remains a difficult task. Motivated by recent advances in memory-augmented models for machine reading, we propose a novel architecture, utilising external &quot;memory chains&quot; with a delayed memory update mechanism to track entities. On a TABSA task, the proposed model demonstrates substantial improvements over state-of-the-art approaches, including those using external knowledge bases. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Targeted aspect-based sentiment analysis (TABSA) is the task of identifying fine-grained opinion polarity towards a specific aspect associated with a given target. The task requires classification of opinions on different entities across a range of different attributes, with the expectation that there will be no overt opinion expressed on a given entity for many attributes. This can be seen in Example (1), e.g., where opinions on the aspects SAFETY and PRICE are expressed for entity LOC1 but not entity LOC2: <ref type="bibr">2</ref> (1) LOC1 is your best bet for secure although expensive and LOC2 is too far.  <ref type="bibr">2</ref> Note that in our dataset, all entity mentions have been pre-nomalised to LOCn, where n is an index.</p><p>The earliest work on (T)ABSA relied heavily on feature engineering ( <ref type="bibr" target="#b19">Wagner et al., 2014;</ref><ref type="bibr">Kir- itchenko et al., 2014</ref>), but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect-specific word and sentence representations ( <ref type="bibr" target="#b17">Tang et al., 2016a)</ref>.</p><p>Despite these successes, keeping track of multiple entity-aspect pairs remains a difficult task, even for an LSTM. As reported in <ref type="bibr" target="#b15">Saeidi et al. (2016)</ref>, a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research.</p><p>More recently, successful works in (T)ABSA have explored the idea of leveraging external memory ( <ref type="bibr" target="#b18">Tang et al., 2016b;</ref><ref type="bibr" target="#b2">Chen et al., 2017</ref>). Their models are largely based on memory networks ( , originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance ( <ref type="bibr" target="#b16">Seo et al., 2017;</ref><ref type="bibr" target="#b7">Henaff et al., 2017)</ref>.</p><p>However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the two tasks: on the Children's Book Test corpus (CBT), for example, competitive models take as input a window of text, centred around candidate entities, with crucial information contained within that window ( <ref type="bibr" target="#b8">Hill et al., 2015;</ref><ref type="bibr" target="#b7">Henaff et al., 2017)</ref>. In TABSA, given the fine-grained nature of the task, it is common practice for models to operate at the word-rather than chunk/sentencelevel <ref type="table">. It is not uncommon to see examples like  Example (1)</ref>, where the sentence starts with LOC1, but the negative PRICE sentiment towards the entity is not expressed until much later. Moreover, phrases such as best bet and although play the role of triggers, indicating that succeeding tokens bear aspect/sentiment signal. This key difference necessitates the ability to model the delayed activation of memory updates.</p><p>In this work, we propose a novel model architecture for TABSA, augmented with multiple "memory chains", and equipped with a delayed memory update mechanism, to keep track of numerous entities independently. We evaluate the effectiveness of the proposed model over the task of TABSA, and achieve substantial improvements over a number of baselines, including one incorporating external knowledge bases, setting a new state of the art in both sentiment classification and aspect detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Task description. In TABSA, a sentence s typically consists of a sequence of words: {w 1 , . . . , w i , . . . , w m } where w i denotes words interleaved with one or more targets (t), which we assume to be pre-identified as with LOC1 and LOC2 in Example (1). Following <ref type="bibr" target="#b15">Saeidi et al. (2016)</ref>, we frame the task as a 3-class classification problem: given a sentence s, a pre-identified set of target entities T and fixed set of aspects A, predict the sentiment polarity y ∈ {positive, negative, none} over the full set of target-aspect pairs {(t, a) : t ∈ T, a ∈ A}. For example, (LOC1,SAFETY) has goldstandard polarity positive, while (LOC1,TRANSIT-LOCATION) has polarity none.</p><p>Proposed model. To this end, we design a neural network architecture, capable of tracking and updating the states of entities at the right time with external memory, making it a natural fit for the task. Specifically, our model maintains a number of "memory chains" h j , one for each entity with the key k j and dynamically updates the states (h j ) of them as it progresses through the sentence with the help of the delay recurrence d j , taking previous activations into account. An illustration of our model is provided in <ref type="figure">Figure 1</ref>. <ref type="figure">Figure 1</ref>: Illustration of our model with a single memory chain at time i. σ, φ and GRU represent Equations <ref type="formula">(2)</ref>, <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_2">(4)</ref>, while circled nodes L, C, and + depict the location, content terms, Hadamard product, and addition, resp.</p><formula xml:id="formula_0">w i φ GRU key k j delay d j i−1 ˜ h j i d j i σ L C update gate g j i + memory h j i−1 h j i</formula><p>Delayed memory update. Update of each memory chain is controlled by a gating mechanism, consisting of three components: the "content" term w i · h j i−1 , the "location" term w i · k j and the "delay" term v·d j i where d j i carries knowledge regarding previous activation of the gate and v is a trainable parameter vector. All three terms may lead to the activation of g j i , but differ in how they turn the gate on. While the "location" term causes the gate to open for memory chains whose keys (k j ) match the input, the "content" term triggers the activation when the content of the entities (h j i−1 ) matches the input. The delay term models how and when the gate was turned on in the past with a GRU ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>) and how past activations should influence the current one.</p><p>More formally, with arrows denoting processing direction, the update gate is defined as:</p><formula xml:id="formula_1">− → g j i = σ(w i · − → h j i−1 + w i · k j + − → v · − → d j i ) (2) where − → g j i</formula><p>is the update gate value for the j-th memory at time i, 3 k j is the embedding for the jth entity (key), − → h j i−1 is the hidden memory representation responsible for keeping track of the state of the j-th entity (content), and σ is the sigmoid activation function. The delay recurrence</p><formula xml:id="formula_2">− → d j i is defined as: − → ˜ h j i = φ( − → U − → h j i−1 + − → V k j + − → W w i ) (3) − → d j i = −−→ GRU( − → ˜ h j i , − → d j i−1 )<label>(4)</label></formula><p>where − → ˜ h j i is the new candidate memory vector to be incorporated into the existing memory</p><formula xml:id="formula_3">− → h j i−1</formula><p>to form the new memory − → h j i , φ is the parametric ReLU activation function ( <ref type="bibr" target="#b5">He et al., 2015)</ref>, and − → U , − → V and − → W are trainable weight matrices. Once the update gate value has been computed, the j-th memory is then updated according to the intensity of − → g j i :</p><formula xml:id="formula_4">− → ˚ h j i = − → h j i−1 + − → g j i − → ˜ h j i (5)</formula><p>where is the Hadamard product, and − → ˚ h j i is the unnormalised memory representation for the j-th entity.</p><p>Essentially, gate − → g j i determines how much the j-th memory should be updated, factoring in three elements: (1) how similar the current input w i is to the entity being tracked (k j ); (2) how related the current input w i is to the state of the j-th entity ( − → h j i−1 ); and <ref type="formula">(3)</ref> how past activation should influence the current one. Update of the memory of an entity is only triggered when the gate is activated.</p><p>Normalisation. Following the update, the model performs a normalisation step, allowing the memory to forget:</p><formula xml:id="formula_5">− → h j i = − → ˚ h j i / − → ˚ h j i where − → ˚ h j i denotes the Euclidean norm of − → ˚ h j i . As all information stored in − → h j i</formula><p>is constrained to be of unit length, when new information − → ˜ h j i is added to the existing memory − → h j i−1 , the cosine distance between the original and updated memory decreases, allowing the model to forget information deemed out-of-date.</p><p>Bi-directionality. We apply the above steps both forward and backward over the sentence, enabling the model to capture sentiment terms appearing before and after its associated entity. The memory representation incorporating contexts from both directions is obtained by</p><formula xml:id="formula_6">h j i = − → h j i + ← − h j i , with ← − h j i computed analogously to − → h j i .</formula><p>Final classifier. Our model predicts the sentiment polarityˆypolarityˆ polarityˆy to the given target t and aspect a embeddings by incorporating the states of all tracked entities in the form of a weighted sum u:</p><formula xml:id="formula_7">p j = softmax (k j ) W att t a<label>(6)</label></formula><formula xml:id="formula_8">u = j p j h j m<label>(7)</label></formula><p>where [ ] denotes concatenation, m is sentence length, and W att is a trainable weight matrix.</p><p>Here, the values of both t and a take the embedding values of their corresponding words (i.e. t and a are drawn from the same embedding matrix as are the input words w i ). In the case of multi-word aspect expressions (e.g. TRANSIT-LOCATION), we take the mean of the embeddings of the constituent words. We then transform u to get:</p><formula xml:id="formula_9">ˆ y = softmax(Rφ(Hu + a))<label>(8)</label></formula><p>Training is carried out based on cross entropy loss.</p><formula xml:id="formula_10">L = CrossEntropy(y, ˆ y)<label>(9)</label></formula><p>Comparision with EntNet. While our model is largely inspired by Recurrent Entity Networks (EntNets: Henaff et al. <ref type="formula" target="#formula_8">(2017)</ref>), it differs in three main respects. First, we explicitly model the delay of activation of the update gates g j with the GRU in Equations <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_2">(4)</ref> as opposed to making h j i implicitly assume the same responsibility in EntNets. Admittedly, for EntNets on bAbI and CBT, given the coarse-grained nature and the difference in the granularity of inputs (sentences vs. words), the demand for modelling delayed memory update is less obvious. With this delayed gate activation mechanism, we essentially decouple the duty of capturing transitions of activations between steps from the task of entity state tracking. That is, h j t is now dedicated to keeping track of the state of the j-th entity only and released from the burden of monitoring the activation of the update gate. Second, tailoring to the task of TABSA, we incorporate not only the target t but also the aspect a when trying to determine the attention in the softmax function. Third, the proposed model is bi-directional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Dataset. To test the effectiveness of our model, we use Sentihood, a dataset constructed by <ref type="bibr" target="#b15">Saeidi et al. (2016)</ref> for the purpose of detecting aspects and identifying sentiments for each targetaspect pair, consisting of 5, 215 sentences, 3, 862 of which contain a single target, and the remainder multiple targets. Each sentence is annotated with a list of tuples {(t, a, y)} with each identifying the sentiment polarity y towards a specific aspect a of   <ref type="formula" target="#formula_9">(2018)</ref>, resp; Bold = best performance; "-" = not reported; † = average performance over 5 runs.</p><p>a given target t in s. Ultimately, given a sentence s, we are interested in both detecting the mention of an aspect a for target t (a label other than none), and also identifying the specific sentiment y w.r.t. the target-aspect pair. A detailed description of the task is presented in Section 2.</p><p>Model configuration. We initialise our model with GloVe (300-D, trained on 42B tokens, 1.9M vocab, not updated during training: Pennington et al. <ref type="formula" target="#formula_2">(2014)</ref>) <ref type="bibr">4</ref> and pre-process the corpus with tokenisation using NLTK ( <ref type="bibr" target="#b0">Bird et al., 2009</ref>) and case folding. Training is carried out over 800 epochs with the FTRL optimiser ( <ref type="bibr" target="#b13">McMahan et al., 2013</ref>) and a batch size of 128 and learning rate of 0.05.</p><p>We use the following hyper-parameters for weight matrices in both directions: R ∈ R 300×3 , H, U, V, W are all matrices of size R 300×300 , v ∈ R 300 , and hidden size of the GRU in Equation <ref type="formula" target="#formula_2">(4)</ref> is 300. Dropout is applied to the output of φ in the final classifier (Equation <ref type="formula" target="#formula_9">(8)</ref>) with a rate of 0.2. Moreover, we employ the technique introduced by <ref type="bibr" target="#b4">Gal and Ghahramani (2016)</ref> where the same dropout mask is applied to the input w i at every step with a rate of 0.2. Lastly, to curb overfitting, we regularise the last layer (Equation <ref type="formula" target="#formula_9">(8)</ref>) with an L 2 penalty on its weights: λR where λ = 0.001.</p><p>We empirically set the number of memory chains to 6, with the keys of two of them set to the same embeddings as the target words LOC1 and LOC2, resp., and the other 4 chains with free key embeddings which are updated during training, and therefore free to capture any entities. 5 4 http://nlp.stanford.edu/data/glove. 42B.300d.zip <ref type="bibr">5</ref> In line with the findings of <ref type="bibr" target="#b7">Henaff et al. (2017)</ref> that tying key vectors damages model performance, we observed similar performance deterioration when using tied keys only. While we also experimented with various configurations (all Consistent with <ref type="bibr" target="#b15">Saeidi et al. (2016)</ref>, we tackle the data unbalanced problem (none positive + negative) by sampling the same number of training instances within a batch randomly from each class.</p><p>Evaluation. We benchmark against baseline systems presented in the works of <ref type="bibr" target="#b15">Saeidi et al. (2016)</ref> and <ref type="bibr" target="#b12">Ma et al. (2018)</ref>: <ref type="formula">(1)</ref> LR: a logistic regression classifier with n-gram and POS tag features; (2) LSTM-Final: a biLSTM taking the final states as representations; (3) LSTM-Loc: a biLSTM taking the states at the location where target t is mentioned as representations; (4) LSTM+TA+SA: a biLSTM equipped with complex target and sentence-level attention mechanisms; (5) SenticLSTM: an improved version of (4) incorporating the SenticNet external knowledge base <ref type="bibr" target="#b1">(Cambria et al., 2016)</ref>. We additionally implement a bi-directional EntNet with the same hyper-parameter settings and GloVe embeddings as our model ( <ref type="bibr" target="#b7">Henaff et al., 2017)</ref>.</p><p>In terms of evaluation, we adopt the standard 70/10/20 train/validation/test split, and report the test performance corresponding to the model with the best validation score. Following <ref type="bibr" target="#b15">Saeidi et al. (2016)</ref>, we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT-LOCATION, and SAFETY) and employ the following evaluation metrics: macro-average F 1 and AUC for aspect detection ignoring the none class, and accuracy and macro-average AUC for sentiment classification. Following <ref type="bibr" target="#b12">Ma et al. (2018)</ref>, we also report strict accuracy for aspect detection, as the fraction of sentences where all aspects are detected correctly.</p><p>tied vs. all free), this hybrid setup results in the best performance on the validation set.   <ref type="figure">Figure 2</ref>: Example of the gate value g t averaged across memory chains, forward and backward, in EntNet vs. our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The experimental results are presented in <ref type="table">Table 1</ref>.</p><p>State-of-the-art results. Our model achieves state-of-the-art results for both aspect detection and sentiment classification. It is impressive that the proposed model, equipped only with domainindependent general-purpose GloVe embeddings, outperforms SenticLSTM, an approach heavily reliant on external knowledge bases and domainspecific embeddings.</p><p>EntNet vs. our model. We see consistent performance gains for our model in both aspect detection and sentiment classification, compared to EntNet, esp. for aspect detection, underlining the benefit of delayed update gate activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>To better understand what the model has learned, we visualise the average gate value g t in <ref type="figure">Figure 2</ref>, where colour intensity indicates how much memory is updated. Observe that, while updated less by the mention of LOC1, our model carries out memory updates upon seeing lovely town and plenty of restaurants, key phrases associated with aspects such as GENERAL and DINNING. Perhaps even more importantly, despite the distance between LOC1 and the final portion of the sentence, our model recognises the relevance to TRANSIT-LOCATION and keeps the update gates open to track this particular aspect, as opposed to EntNet where the last phase is overlooked. The ultimate prediction for the TRANSIT-LOCATION aspect of LOC1 is correct with our model (positive), but not detected by EntNet (none), resulting in a false negative. More interestingly, with EntNet, once distant from a target, it can be frequently observed 2 3 4 5 6 7 8 9 10 76 77 78 79 # of memory chains Aspect detection F 1 <ref type="figure">Figure 3</ref>: Sensitivity study of model performance to # of memory chains n. Note that we report average performance over 5 runs with standard deviation. that the activation rate of g t tends to drop, a tendency not so apparent with our model.</p><p>In <ref type="figure">Figure 3</ref>, we further study the sensitivity of model performance to the number of memory chains n (2 of which are constrained to track LOC1 and LOC2, the rest are unconstrained chains). Observe that, when n &lt; 5, the model suffers from insufficient capacity (not enough memory chains) to capture the various aspects required by the task, with aspect detection F 1 remaining below 78. In particular, when n = 2 (no unconstrained chains), model performance drops substantially to a F 1 of 76.6 ± 0.4. Once n ≥ 5, aspect detection F 1 increases to around 78, and is quite stable even with as many as n = 10 chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have proposed a model which is capable of dynamically tracking entities with a delayed memory update mechanism, and demonstrated the effectiveness of the method over the task of targeted aspect-based sentiment analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>LOC1</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="3"> While − → g j i could instead be a vector for finer-grained control, following Henaff et al. (2017), we use a scalar for simplicity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valuable feedback, and gratefully acknowledge the support of Australian Government Research Training Program Scholarship and National Computational Infrastructure (NCI Australia). This work was also supported in part by the Australian Research Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Senticnet 4: A semantic resource for sentiment analysis based on conceptual primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2666" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
	<note>Copenhagen</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS 2014 Deep Learning and Representation Learning Workshop</title>
		<meeting>the NIPS 2014 Deep Learning and Representation Learning Workshop<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Usa</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">San</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puerto</forename><surname>Rico</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ireland</forename><surname>Dublin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32rd AAAI Conference on Artificial Intelligence (AAAI 2018)</title>
		<meeting>the 32rd AAAI Conference on Artificial Intelligence (AAAI 2018)<address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ad click prediction: A view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentihood: Targeted aspect based sentiment analysis dataset for urban neighbourhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Query-reduction networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DCU: Aspect-based polarity classification for SemEval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsab</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasha</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamia</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
