<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Narrative Modeling with Memory Chains and Semantic Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15 -20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne Victoria</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Narrative Modeling with Memory Chains and Semantic Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="278" to="284"/>
							<date type="published">July 15 -20, 2018</date>
						</imprint>
					</monogr>
					<note>278</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction , our model demonstrates superior performance to a collection of competitive baselines, setting a new state of the art. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Story narrative comprehension has been a longstanding challenge in artificial intelligence <ref type="bibr">(Wino- grad, 1972;</ref><ref type="bibr" target="#b20">Turner, 1994;</ref><ref type="bibr" target="#b17">Schubert and Hwang, 2000</ref>). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour <ref type="bibr">(Charniak, 1972)</ref>. Of particular interest in this paper is the work by <ref type="bibr" target="#b13">Mostafazadeh et al. (2016)</ref> on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features ( <ref type="bibr" target="#b19">Schwartz et al., 2017;</ref><ref type="bibr">Chaturvedi et al., 2017)</ref>, or representation learning via deep learning models <ref type="bibr" target="#b10">(Mihaylov and Frank, 2017;</ref><ref type="bibr">Bugert et al., 2017;</ref><ref type="bibr" target="#b15">Mostafazadeh et al., 2017)</ref>. Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. The current state-of-the-art approach of <ref type="bibr">Chaturvedi et al. (2017)</ref> is based on understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency. <ref type="bibr">Chaturvedi et al. (2017)</ref> adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation.</p><p>Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by <ref type="bibr" target="#b5">Henaff et al. (2017)</ref> for reasoning-focused question answering and clozestyle reading comprehension, we introduce a novel multi-task learning objective, encouraging each chain to focus on a particular aspect. While still making use of external linguistic resources, we do not extract or design features from them but rather utilise such tools to generate labels. The generated labels are then used to guide training so that each chain focuses on tracking a particular aspect. At test time, our model is free of feature engineering such that, once trained, it can be easily deployed to unseen data without preprocessing. Moreover, our approach also differs in the lack of a ROC Stories language model component, eliminating the need for large, domain-specific training corpora.</p><p>Evaluated on the task of Story Cloze Test, our model outperforms a collection of competitive baselines, achieving state-of-the-art performance under more modest data requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In the story cloze test, given a story of length L, consisting of a sequence of context sentences c = {c 1 , c 2 , . . . , c L }, we are interested in predicting the coherent ending to the story out of two ending options o 1 and o 2 . Following previous studies ( <ref type="bibr" target="#b19">Schwartz et al., 2017)</ref>, we frame this problem as a binary classification task. Assuming o 1 and o 2 are the logical and inconsistent endings respectively with their associated labels being y 1 and y 2 , we assign y 1 = 1 and y 2 = 0. At test time, given a pair of possible endings, the system returns the one with the higher score as the prediction. In this section, we first describe the model architecture and then detail how we identify aspect-triggering words in text and incorporate them in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proposed Model</head><p>First, to take neighbouring contexts into account, we process context sentences and ending options at the word level with a bi-directional gated recurrent unit ("Bi-GRU": <ref type="bibr" target="#b1">Chung et al. (2014)</ref></p><formula xml:id="formula_0">): − → h i = −−→ GRU(w i , − → h i−1 )</formula><p>where w i is the embedding of the i-th word in the story or ending option. The backward hidden representation ← − h i is computed analogously but with a different set of GRU parameters. We simply take the sum h i = − → h i + ← − h i as the representation at time i. An ending option, denoted o, is represented by taking the sum of the final states of the same Bi-GRU over the ending option word sequence.</p><p>This representation is then taken as input to a Recurrent Entity Network ("EntNet": <ref type="bibr" target="#b5">Henaff et al. (2017)</ref>), capable of tracking the state of the world with external memory. Developed in the context of machine reading comprehension, EntNet maintains a number of "memory chains" -one for each entity -and dynamically updates the representations of them as it progresses through a story. Here, we borrow the concept of "entity" and use each chain to track a unique aspect. An illustration of EntNet is provided in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_1">h i φ key k j σ L C update gate g j i ˜ m j i + memory m j i−1 m j i</formula><p>Figure 2: Illustration of EntNet with a single memory chain at time i. φ and σ represent Equations 1 and 2, while circled nodes L, C, and + depict the location, content terms, Hadamard product, and addition, resp.</p><p>Memory update candidate. At every time step i, the internal memory update candidate − → ˜ m j i for the j-th memory chain is computed as:</p><formula xml:id="formula_2">− → ˜ m j i = φ( − → U − → m j i−1 + − → V k j + − → W h i ) (1)</formula><p>where k j is the embedding for the j-th entity (key), Memory update. The update of each memory chain is controlled by a gating mechanism:</p><formula xml:id="formula_3">− → U , − → V and − → W</formula><formula xml:id="formula_4">− → g j i = σ(h i · k j + h i · − → m j i−1 ) (2) − → ˚ m j i = − → m j i−1 + − → g j i − → ˜ m j i (3)</formula><p>where denotes Hadamard product (resulting in the unnormalised memory representation − → ˚ m j i ), and σ is the sigmoid function. The gate − → g j i controls how much the memory chain should be updated, a decision factoring in two elements: (1) the "location" term, quantifying how related the current input h i (the output of the Bi-GRU at time i) is to the key k j of the entity being tracked; and (2) the "content" term, measuring the similarity between the input and the current state − → m j i−1 of the entity tracked by the j-th memory chain.</p><p>Normalisation. We normalise each memory representation:</p><formula xml:id="formula_5">− → m j i = − → ˚ m j i / − → ˚ m j i where − → ˚ m j i de- notes the Euclidean norm of − → ˚ m j i .</formula><p>In doing so, we allow the model to forget in the sense that, as − → m j i is constrained to be lying on the surface of the unit sphere, adding new information carried by − → ˜ m j i and then performing normalisation inevitably causes forgetting in that the cosine distance between the original and updated memory decreases.</p><p>Bi-directionality. In contrast to EntNet, we apply the above steps in both directions, scanning a story both forward and backward, with arrows denoting the processing direction. ← − m j i is computed analogously to − → m j i but with a different set of parameters, and m j i = − → m j i + ← − m j i . We further define g j i to be the average of the gate values of both directions at time i for the j-th chain:</p><formula xml:id="formula_6">g j i = ( − → g j i + ← − g j i )/2<label>(4)</label></formula><p>which will be used for semantic supervision.</p><p>Final classifier. The final predictionˆypredictionˆ predictionˆy to an ending option given its context story is performed by incorporating the last states of all memory chains in the form of a weighted sum u:</p><formula xml:id="formula_7">p j = softmax((k j ) W att o)<label>(5)</label></formula><formula xml:id="formula_8">u = j p j m j T (6)</formula><p>where T denotes the total number of words in a story and W att is a trainable weight matrix. We then transform u to get the final prediction:</p><formula xml:id="formula_9">ˆ y = σ(Rφ(Hu + o))<label>(7)</label></formula><p>where R and H are trainable weight matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantically Motivated Memory Chains</head><p>In order to encourage each chain to focus on a particular aspect, we supervise the activation of each update gate (Equation 2) with binary labels. Intuitively, for the sentiment chain, a sentimentbearing word (e.g. amazing) receives a label of 1, allowing the model to open the gate and therefore change the internal state relevant to this aspect, while a neutral one (e.g. school) should not trigger the activation with an assigned label of 0. To achieve this, we use three memory chains to independently track each of: (1) event sequence, (2) sentiment trajectory, and (3) topical consistency. To supervise the memory update gates of these chains, we design three sequences of binary labels:</p><formula xml:id="formula_10">l j = {l j 1 , l j 2 , . . . , l j T } for j ∈ [1, 3</formula><p>] representing event, sentiment, and topic, and l j i ∈ {0, 1}. The label at time i for the j-th aspect is only assigned a value of 1 if the word is a trigger for that particular aspect: l j i = 1; otherwise l j i = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Ricky fell while hiking in the woods <ref type="table">Table 1: An example of the linguistically moti- vated memory chain supervision binary labels.</ref> to open its gate only when it sees a trigger bearing information semantically sensitive to that particular aspect. Note that at test time, we do not apply such supervision. This effectively becomes a binary tagging task for each memory chain independently and results in individual memory chains being sensitive to only a specific set of triggers bearing one of the three types of signals. While largely inspired by <ref type="bibr">Chaturvedi et al. (2017)</ref>, our approach differs in how we extract and use such features. Also note that, while still making use of external linguistic resources to detect trigger words, our approach lets the memory chains decide how such words should influence the final prediction, as opposed to the handcrafted conditional probability features of <ref type="bibr">Chaturvedi et al. (2017)</ref>.</p><formula xml:id="formula_11">l Event 0 1 0 1 0 0 1 l Sentiment 0 1 0 0 0 0 0 l T opic 1 1 0 1 0 0 1</formula><p>To identify the trigger words, we use external linguistic tools, and mark trigger words for each aspect with a label of 1. An example is presented in <ref type="table">Table 1</ref>, noting that the same word can act as trigger for multiple aspects.</p><p>Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR ( <ref type="bibr" target="#b2">Das et al., 2010)</ref>, and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following <ref type="bibr">Chaturvedi et al. (2017)</ref>, we utilise a pre-compiled list of sentiment words ( <ref type="bibr" target="#b6">Liu et al., 2005</ref>). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser ( <ref type="bibr" target="#b0">Chen and Manning, 2014</ref>) and include negation words as trigger words.</p><p>Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following <ref type="bibr">Chaturvedi et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Loss</head><p>In addition to the cross entropy loss of the final prediction of right/wrong endings, we also take into account the memory update gate supervision of each chain by adding the second term. More formally, the model is trained to minimise the loss:</p><formula xml:id="formula_12">L = XEntropy(y, ˆ y) + α i,j XEntropy(l j i , g j i )</formula><p>wherê y and g j i are defined in Equations 7 and 4 respectively, y is the gold label for the current ending option o, and l j i is the semantic supervision binary label at time i for the j-th memory chain. In our experiments, we empirically set α to 0.5 based on development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Dataset. To test the effectiveness of our model, we employ the Story Cloze Test dataset of <ref type="bibr" target="#b13">Mostafazadeh et al. (2016)</ref>. The development and test set each consist of 1,871 4-sentence stories, each with a pair of ending options. Consistent with previous studies, we split the development set into a training and validation set (for early stopping), resulting in 1,683 and 188 in each set, resp. Note that while most current approaches make use of the much larger training set, comprised of 100K 5-sentence ROC stories (with coherent endings only, also released as part of the dataset) to train a language model, we make no use of this data.</p><p>Model configuration. We initialise our model with word2vec embeddings (300-D, pre-trained on 100B Google News articles, not updated during training: <ref type="bibr">Mikolov et al. (2013a,b)</ref>). In addition to the three supervised chains, we also add a "free" chain, unconstrained to any semantic aspect.</p><p>Training is carried out over 200 epochs with the FTRL optimiser ( <ref type="bibr" target="#b8">McMahan et al., 2013</ref>) and a batch size of 128 and learning rate of 0.1. We use the following hyper-parameters for weight matrices in both directions: R ∈ R 300×1 , H, U, V, W are all matrices of size R 300×300 , and hidden size of the Bi-GRU is 300. Dropout is applied to the output of φ in the final classifier (Equation 7) with a rate of 0.2. Moreover, we employ the technique introduced by <ref type="bibr" target="#b3">Gal and Ghahramani (2016)</ref> where the same dropout mask is applied at every step to the input w i to the Bi-GRU and the input h i to the memory chains with rates of 0.5 and 0.2 respectively. Lastly, to curb overfitting, we regularise the last layer (Equation 7) with an L 2 penalty on its weights: λR where λ = 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc. SD DSSM 58.5 -TBMIHAYLOV 72.8 -MSAP</p><p>75.2 -HCM 77.6 -EntNet † 77.6 ±0.5 Our model † 78.5 ±0.5 <ref type="table">Table 2</ref>: Performance on the Story Cloze test set. Bold = best performance, † = ave. accuracy over 5 runs, SD = standard deviation, "-" = not reported.</p><p>Evaluation. Following previous studies, we evaluate the performance in terms of coherent ending prediction accuracy: #correct #total . Here, we report the average accuracy over 5 runs with different random seeds. Echoing <ref type="bibr" target="#b9">Melis et al. (2018)</ref>, we also observe some variance in model performance even if training is carried out with the same random seed, which is largely due to the non-deterministic ordering of floating-point operations in our environment (Tensorflow ( <ref type="bibr">Abadi et al., 2015</ref>) with a single GPU). Therefore, to account for the randomness, we further train our model 5 times for each random seed and select the model with the best validation performance.</p><p>We benchmark against a collection of strong baselines, including the top-3 systems of the 2017 LSDSem workshop shared task <ref type="bibr" target="#b15">(Mostafazadeh et al., 2017</ref>  <ref type="bibr" target="#b10">Mihaylov and Frank, 2017)</ref>. The first two primarily rely on a simple logistic regression classifier, both taking linguistic and probability features from a ROC Stories domain-specific neural language model. TBMIHAYLOV is LSTMbased; we also include DSSM ( <ref type="bibr" target="#b13">Mostafazadeh et al., 2016)</ref>. We additionally implement a bi-directional EntNet ( <ref type="bibr" target="#b5">Henaff et al., 2017</ref>) with the same hyper-parameter settings as our model and no semantic supervision. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Discussion</head><p>The experimental results are shown in <ref type="table">Table 2</ref>. EntNet vs. our model. We see clear improvements of the proposed method over EntNet, an absolute gain of 0.9% in accuracy. This validates the hypothesis that encouraging each memory chain to focus on a unique, well-defined aspect is beneficial.</p><p>Discussion. To better understand the benefits of the proposed method, we visualise the learned word representations (output representation of the Bi-GRU, h i ) and keys between EntNet and our model in <ref type="figure">Figure 3</ref>. With EntNet, while sentiment words form a loose cluster, words bearing event and topic signal are placed in close proximity and are largely indistinguishable. With our model, on the other hand, the degree of separation is much clearer. The intersection of a small portion of the event and topic groups is largely due to the fact that both aspects include verbs. Another interesting perspective is the location of the automatically learned keys (displayed as diamonds): while all the keys with EntNet end up overlapping, indicating little difference among them, the keys learned by our method demonstrate semantic diversity, with each placed within its respective cluster. Note that the free key is learned to complement the event key, a difficult challenge given the two disjoint clusters.</p><p>Ablation study. We further perform a ablation study to analyse the utility of each component in <ref type="table">Table 3</ref>. The uni-directional variant, with its performance down to 77.8, is inferior to its bi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc. SD Our model 78.5 ±0.5 -bi-directionality 77.8 ±0.7 -all semantic supervision 77.6 ±0.5 -event sequence 78.7 ±0.2 -sentiment trajectory 75.9 ±0.4 -topical consistency 77.3 ±0.4 -free chain 77.0 ±0.4 <ref type="table">Table 3</ref>: Ablation study. Average accuracy over 5 runs on the Story Cloze test set (all subject to the same model selection criterion as our model).</p><p>Bold: best performance. SD: standard deviation.</p><p>directional cousin. Removing all semantic supervision (resulting in 4 free memory chains) is also damaging to the accuracy (dropping to 77.6).</p><p>Among the different types of semantic supervision, we see various degrees of performance degradation, with removing sentiment trajectory being the most detrimental, reflecting its value to the task. Interestingly, we observe improvement when removing event sequence supervision from consideration. We suspect that this is mainly due to the noise introduced by the rather inaccurate FrameNet representation output of SEMAFOR (F 1 = 61.4% in frame identification as reported in <ref type="bibr" target="#b2">Das et al. (2010)</ref>). While it is possible that replacing SEMAFOR with the SemLM approach (with the extended frame definition to include explicit discourse markers, e.g. but) in <ref type="bibr" target="#b16">Peng and Roth (2016)</ref> and <ref type="bibr">Chaturvedi et al. (2017)</ref> may potentially reduce the amount of noise, we leave this exercise for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have proposed a novel model for tracking various semantic aspects with external memory chains. While requiring less domainspecific training data, our model achieves stateof-the-art performance on the task of ROC Story Cloze ending prediction, beating a collection of strong baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Story Cloze Test example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are trainable parameters, and φ is the parametric ReLU (He et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>): MSAP (Schwartz et al., 2017), HCM (Chaturvedi et al., 2017) 2 , and TBMIHAYLOV (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: t-SNE scatter plot of aspect-triggering words (output representations h i by Bi-GRU, 500 randomly sampled from each aspect) and the learned keys. EntNet (left) vs. our model (right). Best viewed in colour.</figDesc></figure>

			<note place="foot" n="1"> Code available at http://github.com/liufly/ narrative-modeling.</note>

			<note place="foot" n="2"> We take the performance reported in a subsequent paper, 3.2% better than the original submission to the shared task. 3 EntNet is also subject to the same model selection criterion described above.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valuable feedback, and gratefully acknowledge the support of Australian Government Research Training Program Scholarship. This work was also supported in part by the Australian Research Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS 2014 Deep Learning and Representation Learning Workshop</title>
		<meeting>the NIPS 2014 Deep Learning and Representation Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic frame-semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010)</title>
		<meeting><address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="948" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS 2016)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS 2016)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV 2015)</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV 2015)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opinion observer: analyzing and comparing opinions on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on World Wide Web (WWW 2005)</title>
		<meeting>the 14th International Conference on World Wide Web (WWW 2005)<address><addrLine>Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ad click prediction: A view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2013)</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2013)<address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR 2018</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Story cloze ending selection baselines and data examination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Learning Representations</title>
		<meeting>the 1st International Conference on Learning Representations<address><addrLine>Scottsdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR 2013</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NIPS 2013)</title>
		<meeting>the 27th Annual Conference on Neural Information Processing Systems (NIPS 2013)<address><addrLine>Stateline, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2016)</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LSDSem 2017 shared task: The story cloze test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two discourse driven language models for semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="290" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Episodic logic meets little red riding hood: A comprehensive, natural representation for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><forename type="middle">Hee</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lucja Iwanska and</title>
		<meeting><address><addrLine>Stuart C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shapiro</surname></persName>
		</author>
		<title level="m">Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language</title>
		<imprint>
			<biblScope unit="page" from="111" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Creative Process: A Computer Model of Storytelling and Creativity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Lawrence Erlbaum, Hillsdale, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="191" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
