<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Embedding via Dynamic Mapping Matrix</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
							<email>guoliang.ji@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
							<email>shizhu.he@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
							<email>lhxu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<email>kliu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<email>jzhao@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph Embedding via Dynamic Mapping Matrix</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="687" to="696"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper , we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments , we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs such as WordNet <ref type="bibr" target="#b0">(Miller 1995)</ref>, <ref type="bibr">Freebase (Bollacker et al. 2008</ref>) and <ref type="bibr">Yago (Suchanek et al. 2007</ref>) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&amp;A), etc. They usually contain huge amounts of structured data as the form of triplets (head entity, relation, tail entity)(denoted as (h, r, t)), where relation models the relationship between the two entities. As most knowledge graphs have been built either collaboratively or (partly) automatically, they often suffer from incompleteness. Knowledge graph completion is to predict relations between entities based on existing triplets in a knowledge graph. In the past decade, much work based on symbol and logic has been done for knowledge graph completion, but they are neither tractable nor enough convergence for large scale knowledge graphs. Recently, a powerful approach for this task is to encode every element (entities and relations) of a knowledge graph into a low-dimensional embedding vector space. These methods do reasoning over knowledge graphs through algebraic operations (see section "Related Work").</p><p>Among these methods, <ref type="bibr">TransE (Bordes et al. 2013</ref>) is simple and effective, and also achieves state-of-the-art prediction performance. It learns low-dimensional embeddings for every entity and relation in knowledge graphs. These vector embeddings are denoted by the same letter in boldface. The basic idea is that every relation is regarded as translation in the embedding space. For a golden triplet (h, r, t), the embedding h is close to the embedding t by adding the embedding r, that is h + r ≈ t. TransE is suitable for 1-to-1 relations, but has flaws when dealing with 1-to-N, N-to-1 and N-to-N relations. TransH ( <ref type="bibr" target="#b4">Wang et al. 2014</ref>) is proposed to solve these issues. TransH regards a relation as a translating operation on a relation-specific hyperplane, which is characterized by a norm vector w r and a translation vector d r . The embeddings h and t are first projected to the hyperplane of relation r to obtain vectors h ⊥ = h − w r hw r and t ⊥ = t − w r tw r , and then h ⊥ + d r ≈ t ⊥ . Both in TransE and TransH, the embeddings of entities and relations are in the same space. However, entities and relations are different types objects, it is insufficient to model them in the same space. TransR/CTransR ( <ref type="bibr" target="#b5">Lin et al. 2015</ref>) set a mapping matrix M r and a vector r for every relation r. In TransR, h and t are projected to the aspects that relation r focuses on through the ma-  <ref type="figure">Figure 1</ref>: Simple illustration of TransD. Each shape represents an entity pair appearing in a triplet of relation r. M rh and M rt are mapping matrices of h and t, respectively. h ip , t ip (i = 1, 2, 3), and r p are projection vectors. h i⊥ and t i⊥ (i = 1, 2, 3) are projected vectors of entities. The projected vectors satisfy h i⊥ + r ≈ t i⊥ (i = 1, 2, 3).</p><p>trix M r and then M r h + r ≈ M r t. CTransR is an extension of TransR by clustering diverse headtail entity pairs into groups and learning distinct relation vectors for each group. TransR/CTransR has significant improvements compared with previous state-of-the-art models. However, it also has several flaws: (1) For a typical relation r, all entities share the same mapping matrix M r . However, the entities linked by a relation always contains various types and attributes. For example, in triplet (friedrich burklein, nationality, germany), friedrich burklein and germany are typical different types of entities. These entities should be projected in different ways; (2) The projection operation is an interactive process between an entity and a relation, it is unreasonable that the mapping matrices are determined only by relations; and (3) Matrix-vector multiplication makes it has large amount of calculation, and when relation number is large, it also has much more parameters than TransE and TransH. As the complexity, TransR/CTransR is difficult to apply on largescale knowledge graphs.</p><p>In this paper, we propose a novel method named TransD to model knowledge graphs. <ref type="figure">Figure 1</ref> shows the basic idea of TransD. In TransD, we define two vectors for each entity and relation. The first vector represents the meaning of an entity or a relation, the other one (called projection vector) represents the way that how to project a entity embedding into a relation vector space and it will be used to construct mapping matrices. Therefore, every entity-relation pair has an unique mapping matrix. In addition, TransD has no matrixby-vector operations which can be replaced by vectors operations. We evaluate TransD with the task of triplets classification and link prediction. The experimental results show that our method has significant improvements compared with previous models.</p><p>Our contributions in this paper are: (1)We propose a novel model TransD, which constructs a dynamic mapping matrix for each entity-relation pair by considering the diversity of entities and relations simultaneously. It provides a flexible style to project entity representations to relation vector space; (2) Compared with TransR/CTransR, TransD has fewer parameters and has no matrixvector multiplication. It is easy to be applied on large-scale knowledge graphs like <ref type="bibr">TransE and TransH; and (3)</ref> In experiments, our approach outperforms previous models including TransE, TransH and TransR/CTransR in link prediction and triplets classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Before proceeding, we define our mathematical notations. We denote a triplet by (h, r, t) and their column vectors by bold lower case letters h, r, t; matrices by bold upper case letters, such as M; tensors by bold upper case letters with a hat, such as M. Score function is represented by f r (h, t). For a golden triplet (h, r, t) that corresponds to a true fact in real world, it always get a relatively higher score, and lower for an negative triplet. Other notations will be described in the appropriate sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TransE, TransH and TransR/CTransR</head><p>As mentioned in Introduction section, <ref type="bibr">TransE (Bordes et al. 2013</ref>) regards the relation r as translation from h to t for a golden triplet (h, r, t). Hence, (h+r) is close to (t) and the score function is</p><formula xml:id="formula_0">f r (h, t) = −−h + r − t 2 2 .<label>(1)</label></formula><p>TransE is only suitable for 1-to-1 relations, there remain flaws for 1-to-N, N-to-1 and N-to-N relations.</p><p>To solve these problems, TransH ( <ref type="bibr" target="#b4">Wang et al. 2014</ref>) proposes an improved model named translation on a hyperplane. On hyperplanes of different relations, a given entity has different representations. Similar to TransE, TransH has the score function as follows:</p><formula xml:id="formula_1">f r (h, t) = −−h ⊥ + r − t ⊥ 2 2 .<label>(2)</label></formula><p>Model #Parameters # Operations (Time complexity) Unstructured ( <ref type="bibr" target="#b6">Bordes et al. 2012;</ref> O(Nem) O(Nt) SE ( <ref type="bibr" target="#b8">Bordes et al. 2011)</ref> O(Nem + 2Nrn 2 )(m = n) O(2m 2 Nt) SME(linear) ( <ref type="bibr" target="#b6">Bordes et al. 2012;</ref> O(Nem + Nrn + 4mk + 4k)(m = n) O(4mkNt) SME (bilinear) ( <ref type="bibr" target="#b6">Bordes et al. 2012;</ref> O(Nem + Nrn + 4mks + 4k)(m = n) O(4mksNt) LFM ( <ref type="bibr" target="#b9">Jenatton et al. 2012;</ref><ref type="bibr" target="#b10">Sutskever et al. 2009</ref>) <ref type="bibr" target="#b4">Wang et al. 2014</ref>) In order to ensure that h ⊥ and t ⊥ are on the hyperplane of r, TransH restricts w r = 1. Both TransE and TransH assume that entities and relations are in the same vector space. But relations and entities are different types of objects, they should not be in the same vector space. TransR/CTransR ( <ref type="bibr" target="#b5">Lin et al. 2015</ref>) is proposed based on the idea. TransR set a mapping matrix M r for each relation r to map entity embedding into relation vector space. Its score function is:</p><formula xml:id="formula_2">O(Nem + Nrn 2 )(m = n) O((m 2 + m)Nt) SLM (Socher et al. 2013) O(Nem + Nr(2k + 2nk))(m = n) O((2mk + k)Nt) NTN (Socher et al. 2013) O(Nem + Nr(n 2 s + 2ns + 2s))(m = n) O(((m 2 + m)s + 2mk + k)Nt) TransE (Bordes et al. 2013) O(Nem + Nrn)(m = n) O(Nt) TransH (</formula><formula xml:id="formula_3">O(Nem + 2Nrn)(m = n) O(2mNt) TransR (Lin et al. 2015) O(Nem + Nr(m + 1)n) O(2mnNt) CTransR (Lin et al. 2015) O(Nem + Nr(m + d)n) O(2mnNt) TransD (this paper) O(2Nem + 2Nrn) O(2nNt)</formula><formula xml:id="formula_4">f r (h, t) = −−M r h + r − M r t 2 2 . (3)</formula><p>where M r ∈ R m×n , h, t ∈ R n and r ∈ R m . CTransR is an extension of TransR. As head-tail entity pairs present various patterns in different relations, CTransR clusters diverse head-tail entity pairs into groups and sets a relation vector for each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other Models</head><p>Unstructured. Unstructured model ( <ref type="bibr" target="#b6">Bordes et al. 2012;</ref>) ignores relations, only models entities as embeddings. The score function is </p><formula xml:id="formula_5">f r (h, t) = −−h − t 2 2 .<label>(4</label></formula><formula xml:id="formula_6">f r (h, t) = −−M rh h − M rt t 1 (5)</formula><p>Semantic Matching Energy (SME). SME model ( <ref type="bibr" target="#b6">Bordes et al. 2012;</ref>) encodes each named symbolic object (entities and relations) as a vector. Its score function is a neural network that captures correlations between entities and relations via matrix operations. Parameters of the neural network are shared by all relations. SME defines two semantic matching energy functions for optimization, a linear form</p><formula xml:id="formula_7">g η = M η1 e η + M η2 r + b η (6)</formula><p>and a bilinear form</p><formula xml:id="formula_8">g η = (M η1 e η ) ⊗ (M η2 r) + b η (7)</formula><p>where η = {lef t, right}, e lef t = h, e right = t and ⊗ is the Hadamard product. The score function is</p><formula xml:id="formula_9">f r (h, t) = g lef t g right (8)</formula><p>In <ref type="formula">(</ref> </p><formula xml:id="formula_10">f r (h, t) = u r f (M r1 h + M r2 t + b r )<label>(9)</label></formula><p>where M r1 , M r2 and b r are parameters indexed by relation r, f () is tanh operation.</p><p>Neural Tensor Network (NTN). NTN model ( <ref type="bibr" target="#b11">Socher et al. 2013</ref>) extends SLM model by considering the second-order correlations into nonlinear neural networks. The score function is</p><formula xml:id="formula_11">f r (h, t) = u r f (h W r t + M r h t + b r ) (10)</formula><p>where W r represents a 3-way tensor, M r denotes the weight matrix, b r is the bias and f () is tanh operation. NTN is the most expressive model so far, but it has so many parameters that it is difficult to scale up to large knowledge graphs. <ref type="table" target="#tab_1">Table 1</ref> lists the complexity of all the above models. The complexity (especially for time) of TransD is much less than TransR/CTransR and is similar to TransE and TransH. Therefore, TransD is effective and train faster than TransR/CTransR. Beyond these embedding models, there is other related work of modeling multi-relational data, such as matrix factorization, recommendations, etc. In experiments, we refer to the results of RESCAL presented in ( <ref type="bibr" target="#b5">Lin et al. 2015</ref>) and compare with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>We first define notations. Triplets are represented as (h i , r i , t i )(i = 1, 2, . . . , n t ), where h i denotes a head entity, t i denotes a tail entity and r i denotes a relation. Their embeddings are denoted by h i , r i , t i (i = 1, 2, . . . , n t ). We use ∆ to represent golden triplets set, and use ∆ to denote negative triplets set. Entities set and relations set are denoted by E and R, respectively. We use I m×n to denote the identity matrix of size m × n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multiple Types of Entities and Relations</head><p>Considering the diversity of relations, CTransR segments triplets of a specific relation r into several groups and learns a vector representation for each group. However, entities also have various types. <ref type="figure" target="#fig_1">Figure 2</ref> shows several kinds of head and tail entities of relation location.location.partially containedby in FB15k. In both TransH and TransR/CTransR, all types of entities share the same mapping vectors/matrices. However, different types of entities have different attributes and functions, it is insufficient to let them share the same transform parameters of a relation. And for a given relation, similar entities should have similar mapping matrices and otherwise for dissimilar entities. Furthermore, the mapping process is a transaction between entities and relations that both have various types. Therefore, we propose a more fine-grained model TransD, which considers different types of both entities and relations, to encode knowledge graphs into embedding vectors via dynamic mapping matrices produced by projection vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TransD</head><p>Model In TransD, each named symbol object (entities and relations) is represented by two vectors. The first one captures the meaning of entity (relation), the other one is used to construct mapping matrices. For example, given a triplet (h, r, t), its vectors are h, h p , r, r p , t, t p , where subscript p marks the projection vectors, h, h p , t, t p ∈ R n and r, r p ∈ R m . For each triplet (h, r, t), we set two mapping matrices M rh , M rt ∈ R m×n to project entities from entity space to relation space. They are defined as follows:</p><formula xml:id="formula_12">M rh = r p h p + I m×n (11) M rt = r p t p + I m×n<label>(12)</label></formula><p>Therefore, the mapping matrices are determined by both entities and relations, and this kind of operation makes the two projection vectors interact sufficiently because each element of them can meet every entry comes from another vector. As we initialize each mapping matrix with an identity matrix, we add the I m×n to M rh and M rh . With the mapping matrices, we define the projected vectors as follows:</p><formula xml:id="formula_13">h ⊥ = M rh h, t ⊥ = M rt t<label>(13)</label></formula><p>Then the score function is</p><formula xml:id="formula_14">f r (h, t) = −−h ⊥ + r − t ⊥ 2 2<label>(14)</label></formula><p>In experiments, we enforce constrains as h 2 ≤ 1, t 2 ≤ 1, r 2 ≤ 1, h ⊥ 2 ≤ 1 and t ⊥ 2 ≤ 1.</p><p>Training Objective We assume that there are n t triplets in training set and denote the ith triplet by (h i , r i , t i )(i = 1, 2, . . . , n t ). Each triplet has a label y i to indicate the triplet is positive (y i = 1) or negative (y i = 0). Then the golden and negative triplets are denoted by ∆ = {(h j , r j , t j ) | y j = 1} and ∆ = {(h j , r j , t j ) | y j = 0}, respectively. Before training, one important trouble is that knowledge graphs only encode positive training triplets, they do not contain negative examples. Therefore, we obtain ∆ from knowledge graphs and generate ∆ as follows:</p><formula xml:id="formula_15">∆ = {(h l , r k , t k ) | h l = h k ∧ y k = 1} ∪ {(h k , r k , t l ) | t l = t k ∧ y k = 1}</formula><p>. We also use two strategies "unif" and "bern" described in ( <ref type="bibr" target="#b4">Wang et al. 2014</ref>) to replace the head or tail entity.</p><p>Let us use ξ and ξ to denote a golden triplet and a corresponding negative triplet, respectively. Then we define the following margin-based ranking loss as the objective for training:</p><formula xml:id="formula_16">L = ξ∈∆ ξ ∈∆ [γ + f r (ξ ) − f r (ξ)] +<label>(15)</label></formula><p>where <ref type="bibr">[x]</ref> + max (0, x), and γ is the margin separating golden triplets and negative triplets. The process of minimizing the above objective is carried out with stochastic gradient descent (SGD). In order to speed up the convergence and avoid overfitting, we initiate the entity and relation embeddings with the results of TransE and initiate all the transfer matrices with identity matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Connections with TransE, TransH and TransR/CTransR</head><p>TransE is a special case of TransD when the dimension of vectors satisfies m = n and all projection vectors are set zero. TransH is related to TransD when we set m = n. Under the setting, projected vectors of entities can be rewritten as follows:</p><formula xml:id="formula_17">h ⊥ = M rh h = h + h p hr p<label>(16)</label></formula><formula xml:id="formula_18">t ⊥ = M rt t = t + t p tr p<label>(17)</label></formula><p>Hence, when m = n, the difference between TransD and TransH is that projection vectors are determinded only by relations in TransH, but TransD's projection vectors are determinded by both entities and relations. As to TransR/CTransR, TransD is an improvement of it. TransR/CTransR directly defines a mapping matrix for each relation, TransD consturcts two mapping matrices dynamically for each triplet by setting a projection vector for each entity and relation. In addition, TransD has no matrix-vector multiplication operation which can be replaced by vector operations. Without loss of generality, we assume m ≥ n, the projected vectors can be computed as follows:</p><formula xml:id="formula_19">h ⊥ = M rh h = h p hr p + h , 0 (18) t ⊥ = M rt t = t p tr p + t , 0<label>(19)</label></formula><p>Therefore, TransD has less calculation than TransR/CTransR, which makes it train faster and can be applied on large-scale knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results Analysis</head><p>We evaluate our apporach on two tasks: triplets classification and link prediction. Then we show the experiments results and some analysis of them.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Triplets Classification</head><p>Triplets classification aims to judge whether a given triplet (h, r, t) is correct or not, which is a binary classification task. Previous work <ref type="bibr" target="#b11">(Socher et al. 2013;</ref><ref type="bibr" target="#b4">Wang et al. 2014;</ref><ref type="bibr" target="#b5">Lin et al. 2015)</ref> had explored this task. In this paper ,we use three datasets WN11, FB13 and FB15k to evaluate our approach. The test sets of WN11 and FB13 provided by <ref type="bibr" target="#b11">(Socher et al. 2013</ref>) contain golden and negative triplets. As to FB15k, its test set only contains correct triplets, which requires us to construct negative triplets. In this parper, we construct negative triplets following the same setting used for FB13 ( <ref type="bibr" target="#b11">Socher et al. 2013</ref>).</p><p>For triplets classification, we set a threshold δ r for each relation r. δ r is obtained by maximizing the classification accuracies on the valid set. For a given triplet (h, r, t), if its score is larger than δ r , it will be classified as positive, otherwise negative.</p><p>We compare our model with several previous embedding models presented in Related Work section. As we construct negative triplets for FB15k by ourselves, we use the codes of TransE, TransH and TransR/CTransR provied by ( <ref type="bibr" target="#b5">Lin et al. 2015</ref>) to evaluate the datasets instead of reporting the results of ( <ref type="bibr">Wang et al.2014;</ref><ref type="bibr" target="#b5">Lin et al. 2015</ref>) directly.</p><p>In this experiment, we optimize the objective with ADADELTA SGD <ref type="bibr" target="#b13">(Zeiler 2012</ref>). We select the margin γ among {1, 2, 5, 10}, the dimension of entity vectors m and the dimension of relation vectors n among {20, 50, 80, 100}, and the mini-batch size B among {100, 200, 1000, 4800}. The best configuration obtained by valid set are:γ = 1, m, n = 100, B = 1000 and taking L 2 as dissimilarity on WN11; γ = 1, m, n = 100, B = 200 and taking L 2 as dissimilarity on FB13; γ = 2, m, n = 100, B = 4800 and taking L 1 as dissimilarity on FB15k. For all the three datasets, We traverse to training for 1000 rounds. As described in Related Work section, TransD trains much faster than TransR (On our PC, TransR needs 70 seconds and TransD merely spends 24 seconds a round on FB15k). <ref type="table" target="#tab_6">Table 3</ref> shows the evaluation results of triplets classification. On WN11, we found that there are 570 entities appearing in valid and test sets but not appearing in train set, we call them "NULL Entity". In valid and test sets, there are 1680 (6.4%) triplets containing "NULL Entity". In NTN(+E), these entity embeddings can be obtained by word embedding. In TransD, how-   <ref type="formula" target="#formula_1">(2)</ref> On FB13, the classification accuracy of TransD achieves 89.1%, which is significantly higher than that of TransE, TransH and TransR/CTransR and is near to the performance of NTN(+E) (90.0%); and (3) Under most circumstances, the "bern" sampling method works better than "unif". <ref type="figure">Figure 3</ref> shows the prediction accuracy of different relations. On the three datasets, different relations have different prediction accuracy: some are higher and the others are lower. Here we focus on the relations which have lower accuracy. On WN11, the relation similar to obtains accuracy 51%, which is near to random prediction accuracy. In the view of intuition, similar to can be inferred from other information. However, the number of entity pairs linked by relation similar to is only 1672, which accounts for 1.5% in all train data, and prediction of the relation needs much information about entities. Therefore, the insufficient of train data is the main cause. On FB13, the accuracies of relations cuase of death and gender are lower than that of other relations because they are difficult to infer from other imformation, especially cuase of death. Relation gender may be inferred from a person's name <ref type="bibr" target="#b11">(Socher et al. 2013</ref>), but we learn a vector for each name, not for the words included in the names, which makes the Accuracy(%) of "bern" Accuracy(%) of "unif"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FB15K</head><p>Figure 3: Classification accuracies of different relations on the three datasets. For FB15k, each triangle represent a relation, in which the red triangles represent the relations whose accuracies of "bern" or "unif" are lower than 50% and the blacks are higher than 50%. The red line represents the function y = x. We can see that the most relations are in the lower part of the red line.</p><p>names information useless for gender. On FB15k, accuracies of some relations are lower than 50%, for which some are lack of train data and some are difficult to infer. Hence, the ability of reasoning new facts based on knowledge graphs is under a certain limitation, and a complementary approach is to extract facts from plain texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Link Prediction</head><p>Link prediction is to predict the missing h or t for a golden triplet (h, r, t). In this task, we remove the head or tail entity and then replace it with all the entities in dictionary in turn for each triplet in test set. We first compute scores of those corrupted triplets and then rank them by descending order; the rank of the correct entity is finally stored. The task emphasizes the rank of the correct entity instead of only finding the best one entity. Similar to ( <ref type="bibr" target="#b3">Bordes et al. 2013</ref>), we report two measures as our evaluation metrics: the average rank of all correct entites (Mean Rank) and the proportion of correct entities ranked in top 10 (Hits@10). A lower Mean Rank and a higher Hits@10 should be achieved by a good embedding model. We call the evaluation setting "Raw'. Noting the fact that a corrupted triplet may also exist in knowledge graphs, the corrupted triplet should be regard as a correct triplet. Hence, we should remove the corrupted triplets included in train, valid and test sets before ranking. We call this evaluation setting "Filter". In this paper, we will report evaluation results of the two settings .</p><p>In this task, we use two datasets: WN18 and FB15k. As all the data sets are the same, we refer to their experimental results in this paper. On WN18, we also use ADADELTA SGD (Zeiler 2012) for optimization. We select the margin γ among {0.1, 0.5, 1, 2}, the dimension of entity vectors m and the dimension of relation vectors n among {20, 50, 80, 100}, and the mini-batch size B among {100, 200, 1000, 1400}. The best configuration obtained by valid set are:γ = 1, m, n = 50, B = 200 and taking L 2 as dissimilarity. For both the two datasets, We traverse to training for 1000 rounds.</p><p>Experimental results on both WN18 and FB15k are shown in <ref type="table" target="#tab_8">Table 4</ref>. From <ref type="table" target="#tab_8">Table 4</ref>, we can conclude that: (1) TransD outperforms other baseline embedding models (TransE, TransH and TransR/CTransR), especially on sparse dataset, i.e., FB15k; (2) Compared with CTransR, TransD is a more fine-grained model which considers the multiple types of entities and relations simultaneously, and it achieves a better performance. It indicates that TransD handles complicated internal correlations of entities and relations in knowledge graphs better than CTransR; (3) The "bern" sampling trick can reduce false negative labels than "unif".</p><p>For the comparison of Hits@10 of different kinds of relations, <ref type="table" target="#tab_9">Table 5</ref> shows the detailed results by mapping properties of relations 1 on FB15k. From <ref type="table" target="#tab_9">Table 5</ref>, we can see that TransD outperforms TransE, TransH and TransR/CTransR significantly in both "unif" and "bern" settings. TransD achieves better performance than CTransR in all types of relations (1-to-1, 1-to-N, N-to-1 and N-to-N). For N-to-N relations in predicting both head and tail, our approach improves the Hits@10 by almost 7.4% than CTransR. In particular, for   N-to-1 relations (predicting head) and 1-to-N relations (predicting tail), TransD improves the accuracy by 9.0% and 14.7% compared with previous state-of-the-art results, respectively. Therefore, the diversity of entities and relations in knowledge grahps is an important factor and the dynamic mapping matrix is suitable for modeling knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Properties of Projection Vectors</head><p>As mentioned in Section "Introduction", TransD is based on the motivation that each mapping matrix is determined by entity-relation pair dynamically. These mapping matrices are constructed with projection vectors of entities and relations. Here, we analysis the properties of projection vectors. We seek the similar objects (entities and relations) for a given object (entities and relations) by projection vectors. As WN18 has the most entities (40,943 entities which contains various types of words. FB13 also has many entities, but the most are person's names) and FB15k has the most relations (1,345 relations), we show the similarity of projection vectors on them. <ref type="table">Table 6</ref> and 7 show that the same category objects have similar projection vectors. The similarity of projection vectors of different types of entities and relations indicates the rationality of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We introduced a model TransD that embed knowledge graphs into continues vector space for their completion. TransD has less complexity and more flexibility than TransR/CTransR. When learning embeddings of named symbol objects (entities or relations), TransD considers the diversity of them both. Extensive experiments show that TransD outperforms TrasnE, TransH and TransR/CTransR on two tasks including triplets classification and link prediction. As shown in Triplets Classification section, not all new facts can be deduced from the exist-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datesets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WN18 Entities and Definitions</head><p>upset VB 4 cause to overturn from an upright or normal position srbija NN 1 a historical region in central and northern Yugoslavia</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similar Entities and Definitions</head><p>sway VB 4 cause to move back and forth montenegro NN 1 a former country bordering on the Adriatic Sea shift VB 2 change place or direction constantina NN 1 a Romanian resort city on the Black Sea flap VB 3 move with a thrashing motion lappland NN 1 a region in northmost Europe inhabited by Lapps fluctuate VB 1 cause to fluctuate or move in a wavelike pattern plattensee NN 1 a large shallow lake in western Hungary leaner NN 1 (horseshoes) the throw of a horseshoe so as to lean against (but not encircle) the stake brasov NN 1 a city in central Romania in the foothills of the Transylvanian Alps <ref type="table">Table 6</ref>: Entity projection vectors similarity (in descending order) computed on WN18. The similarity scores are computed with cosine function.  ing triplets in knowledge graphs, such as relations gender, place of place, parents and children. These relations are difficult to infer from all other information, but they are also useful resource for practical applications and incomplete, i.e. the place of birth attribute is missing for 71% of all people included in FreeBase <ref type="bibr" target="#b19">(Nickel, et al. 2015</ref>). One possible way to obtain these new triplets is to extract facts from plain texts. We will seek methods to complete knowledge graphs with new triplets whose entities and relations come from plain texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datesets</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Bordes et al.2014), matrices of the bilinear form are replaced by tensors. Latent Factor Model (LFM). LFM model (Je- natton et al. 2012; Sutskever et al. 2009) en- codes each entity into a vector and sets a ma- trix for every relation. It defines a score function f r (h, t) = h M r t, which incorporates the inter- action of the two entity vectors in a simple and effecitve way. Single Layer Model (SLM). SLM model is de- signed as a baseline of Neural Tensor Network (Socher et al. 2013). The model constructs a non- linear neural network to represent the score func- tion defined as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multiple types of entities of relation location.location.partially containedby.</figDesc><graphic url="image-3.png" coords="4,314.36,157.91,205.54,172.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>FB15k Relation /location/statistical region/rent50 2./measurement unit/dated money value/currency Similar relations /location/statistical region/rent50 3./measurement unit/dated money value/currency /location/statistical region/rent50 1./measurement unit/dated money value/currency /location/statistical region/rent50 4./measurement unit/dated money value/currency /location/statistical region/rent50 0./measurement unit/dated money value/currency /location/statistical region/gdp nominal./measurement unit/dated money value/currency Relation /sports/sports team/roster./soccer/football roster position/player Similar relations /soccer/football team/current roster./sports/sports team roster/player /soccer/football team/current roster./soccer/football roster position/player /sports/sports team/roster./sports/sports team roster/player /basketball/basketball team/historical roster./sports/sports team roster/player /sports/sports team/roster./basketball/basketball historical roster position/player</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>= r r r r 1⊥ h 2⊥ h 3⊥ h 1⊥ t 2⊥ t 3⊥ t</head><label></label><figDesc></figDesc><table>Entity Space 

Relation Space 

r 

1 

h 

1 

t 

2 

h 

2 

t 

3 

h 

3 

t 

i 

i 

m n 
rh 
p i p 

m n 
rt 
p i p 

× 

× 

= 
+ 

= 
+ 

M 
r h 
I 

M 
r t 
I 

⊤ 

⊤ 

( 
) 

1, 2,3 
i </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Complexity (the number of parameters and the number of multiplication operations in an epoch) 
of several embedding models. N e and N r represent the number of entities and relations, respectively. 
N t represents the number of triplets in a knowledge graph. m is the dimension of entity embedding 
space and n is the dimension of relation embedding space. d denotes the average number of clusters of a 
relation. k is the number of hidden nodes of a neural network and s is the number of slice of a tensor. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Datesets used in the experiments.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results of Triplets Classifi-
cation(%). "+E" means that the results are com-
bined with word embedding. 

ever, they are only initialized randomly. There-
fore, it is not fair for TransD, but we also achieve 
the accuracy 86.4% which is higher than that of 
NTN(+E) (86.2%). From Table 3, we can con-
clude that: (1) On WN11, TransD outperforms any 
other previous models including TransE, TransH 
and TransR/CTransR, especially NTN(+E); </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 4 : Experimental results on link prediction.</head><label>4</label><figDesc></figDesc><table>Tasks 
Prediction Head (Hits@10) 
Prediction Tail (Hits@10) 
Relation Category 
1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N 
Unstructured (Bordes et al. 2012) 
34.5 
2.5 
6.1 
6.6 
34.3 
4.2 
1.9 
6.6 
SE (Bordes et al. 2011) 
35.6 
62.6 
17.2 
37.5 
34.9 
14.6 
68.3 
41.3 
SME (linear) (Bordes et al.2012) 
35.1 
53.7 
19.0 
40.3 
32.7 
14.9 
61.6 
43.3 
SME (Bilinear) (Bordes et al. 2012) 
30.9 
69.6 
19.9 
38.6 
28.2 
13.1 
76.0 
41.8 
TransE (Bordes et al. 2013) 
43.7 
65.7 
18.2 
47.2 
43.7 
19.7 
66.7 
50.0 
TransH (unif) (Wang et al. 2014) 
66.7 
81.7 
30.2 
57.4 
63.7 
30.1 
83.2 
60.8 
TransH (bern) (Wang et al. 2014) 
66.8 
87.6 
28.7 
64.5 
65.5 
39.8 
83.3 
67.2 
TransR (unif) (Lin et al. 2015) 
76.9 
77.9 
38.1 
66.9 
76.2 
38.4 
76.2 
69.1 
TransR (bern) (Lin et al. 2015) 
78.8 
89.2 
34.1 
69.2 
79.2 
37.4 
90.4 
72.1 
CTransR (unif) (Lin et al. 2015) 
78.6 
77.8 
36.4 
68.0 
77.4 
37.8 
78.0 
70.3 
CTransR (bern) (Lin et al. 2015) 
81.5 
89.0 
34.7 
71.2 
80.8 
38.6 
90.1 
73.8 
TransD (unif) 
80.7 
85.8 
47.1 
75.6 
80.0 
54.5 
80.7 
77.9 
TransD (bern) 
86.1 
95.5 
39.8 
78.5 
85.4 
50.6 
94.4 
81.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Experimental results on FB15K by mapping properities of relations (%). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Relation projection vectors similarity computed on FB15k. The similarity scores are computed with cosine function.</figDesc><table></table></figure>

			<note place="foot" n="1"> Mapping properties of relations follows the same rules in (Bordes et al. 2013)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">YAGO: A core of semantic Knowledge Unifying WordNet and Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Durán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learing with multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="259" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling Relational Data using Bayesian Clustered Tensor Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
	<note>Tenenbaum</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ununier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ADADELTA: AN ADAP-TIVE LEARNING RATE METHOD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A threeway model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: Scalable Machine Learning for Linked Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Analysis of Tensor Models for Learning from Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tensor Factorization for Multi-Relational Learning. Machine Learning and Knowledge Discovery in Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Review of Relational Machine Learning for Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
