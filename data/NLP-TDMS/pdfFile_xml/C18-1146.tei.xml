<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Infused Copy Mechanisms for Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 20-26, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
							<email>kqsong@knights.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
							<email>lin.zhao@us.bosch.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<email>feiliu@cs.ucf.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Dept</orgName>
								<orgName type="department" key="dep2">Computer Science Dept</orgName>
								<orgName type="laboratory">Research and Tech. Center Robert Bosch LLC Sunnyvale</orgName>
								<orgName type="institution">University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816, 94085</postCode>
									<region>FL, CA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Infused Copy Mechanisms for Abstractive Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
						<meeting>the 27th International Conference on Computational Linguistics <address><addrLine>Santa Fe, New Mexico, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1717" to="1729"/>
							<date type="published">August 20-26, 2018</date>
						</imprint>
					</monogr>
					<note>1717</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Seq2seq learning has produced promising results on summarization. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the syntactic structure of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the system, and our proposed approach compares favorably to state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed increasing interest in abstractive summarization. The systems seek to condense source texts to summaries that are concise, grammatical, and preserve the important meaning of the original texts ( <ref type="bibr" target="#b34">Nenkova and McKeown, 2011</ref>). The task encompasses a number of high-level text operations, e.g., paraphrasing, generalization, text reduction and reordering <ref type="bibr" target="#b20">(Jing and McKeown, 1999</ref>), posing a considerable challenge to natural language understanding.</p><p>Src A Mozambican man suspect of murdering Jorge Microsse, director of Maputo central prison, has escaped from the city's police headquarters, local media reported on Tuesday. Ref Mozambican suspected of killing Maputo prison director escapes Sys mozambican man arrested for murder Src An Alaska father who was too drunk to drive had his 11-year-old son take the wheel, authorities said. Ref Drunk Alaska dad has 11 year old drive home Sys alaska father who was too drunk to drive <ref type="table">Table 1</ref>: Example source sentences, reference and system summaries produced by a neural attentive seq-to-seq model. System summaries fail to preserve summary-worthy content of the source (e.g., main verbs) despite their syntactic importance.</p><p>The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive summarization ( <ref type="bibr" target="#b41">Rush et al., 2015;</ref><ref type="bibr" target="#b42">See et al., 2017;</ref><ref type="bibr" target="#b38">Paulus et al., 2017)</ref>. While the results are impressive, individual system summaries can appear unreliable and fail to preserve the meaning of the source texts. <ref type="table">Table 1</ref> presents two examples. In these cases, the syntactic structure of source sentences is relatively rare but perfectly normal. The first sentence contains two appositional phrases ("suspect of murdering Jorge Microsse," "director of Maputo central prison") and the second sentence has a relative clause ("who was too drunk to drive"), both located between the subject and the main verb. The system, however, fails to identify the main verb in both cases; it instead chooses to focus on the first few words of the source sentences. We observe that rare syntactic constructions of the source can pose problems for neural summarization systems, possibly for two reasons. First, similar to rare words, certain syntactic constructions do not occur frequently enough in the training data to allow the system to learn the patterns. Second, neural summarization systems are not explicitly informed of the syntactic structure of the source sentences and they tend to bias towards sequential recency.  <ref type="figure">Figure 1</ref>: An example dependency parse tree created for the source sentence in <ref type="table">Table 1</ref>. If important dependency edges such as "father ← had" can be preserved in the summary, the system summary is likely to preserve the meaning of the original.</p><p>In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary-worthy content and compose summaries that preserve the important meaning of the source texts. We present structure-infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences. For example, if important parts of the source syntactic structure, such as a dependency edge from the main verb to the subject ("father" ← "had," shown in <ref type="figure">Figure 1</ref>), can be preserved in the summary, the "missing verb" issue in <ref type="table">Table 1</ref> can be effectively alleviated. Our model therefore learns to recognize important source words and source dependency relations and strives to preserve them in the summaries. Our research contributions include the following:</p><p>• we introduce novel neural architectures that encourage salient source words/relations to be preserved in summaries. The framework naturally combines the dependency parse tree structure with the copy mechanism of an abstractive summarization system. To the best of our knowledge, this is the first attempt at comparing various neural architectures for this purpose;</p><p>• we study the effectiveness of several important components, including the vocabulary size, a coveragebased regularizer <ref type="bibr" target="#b42">(See et al., 2017)</ref>, and a beam search with reference mechanism (Tan et al., 2017); • through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective. Our approach surpasses state-of-the-art published systems on the benchmark dataset. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an "extract-and-compress" framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents <ref type="bibr" target="#b10">(Daume III and Marcu, 2002;</ref><ref type="bibr" target="#b1">Berg- Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b45">Thadani and McKeown, 2013;</ref><ref type="bibr" target="#b11">Durrett et al., 2016)</ref>, or a pipeline approach that combines generic sentence compression <ref type="bibr" target="#b31">(McDonald, 2006;</ref><ref type="bibr" target="#b9">Clarke and Lapata, 2008;</ref><ref type="bibr" target="#b13">Filippova et al., 2015</ref>) with a sentence pre-selection or post-selection process ( <ref type="bibr" target="#b47">Zajic et al., 2007;</ref><ref type="bibr">Galanis and Androut- sopoulos, 2010;</ref><ref type="bibr" target="#b46">Wang et al., 2013;</ref><ref type="bibr" target="#b23">Li et al., 2013;</ref><ref type="bibr" target="#b24">Li et al., 2014)</ref>. Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly ( <ref type="bibr" target="#b21">Kikuchi et al., 2016;</ref><ref type="bibr" target="#b5">Chen et al., 2016;</ref><ref type="bibr" target="#b50">Zhou et al., 2017;</ref><ref type="bibr" target="#b44">Tan et al., 2017;</ref><ref type="bibr" target="#b38">Paulus et al., 2017</ref>). Most systems adopt a "cut-andstitch" scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries. The resulting summary sentences can contain misleading information (e.g., "mozambican man arrested for murder" flips the meaning of the original) or grammatical errors (e.g., verbless, as in "alaska father who was too drunk to drive").</p><p>Natural language generation (NLG)-based abstractive summarization <ref type="bibr" target="#b3">(Carenini and Cheung, 2008;</ref><ref type="bibr" target="#b15">Gerani et al., 2014;</ref><ref type="bibr" target="#b12">Fabbrizio et al., 2014;</ref><ref type="bibr" target="#b28">Liu et al., 2015;</ref><ref type="bibr" target="#b43">Takase et al., 2016</ref>) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspecific templates built using a text planner or an OpenIE system ( <ref type="bibr" target="#b40">Pighin et al., 2014</ref>). In particular, <ref type="bibr" target="#b2">Cao et al. (2018)</ref> leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries. Different from the above approaches, this paper seeks to directly incorporate source-side syntactic structure in the copy mechanism of an abstractive sentence summarization system. It learns to recognize important source words and relations during training, while striving to preserve them in the summaries at test time to aid reproduction of factual details. Our intent of incorporating source syntax in summarization is different from that of neural machine translation (NMT) ( <ref type="bibr" target="#b25">Li et al., 2017a;</ref><ref type="bibr" target="#b6">Chen et al., 2017)</ref>, in part because NMT does not handle the information loss from source to target. In contrast, a summarization system must selectively preserve source content to render concise and grammatical summaries. We specifically focus on sentence summarization, where the goal is to reduce the first sentence of an article to a title-like summary. We believe even for this reasonably simple task there remains issues unsolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and preserves the meaning of the source sentence. A source word is replaced by its Glove embedding <ref type="bibr">(Pen- nington et al., 2014</ref>) before it is fed to the system; the vector is denoted by x i (i ∈ [S]; 'S' for source). Similarly, a summary word is denoted by y t (t ∈ [T ]; 'T' for target). If a word does not appear in the input vocabulary, it is replaced by a special 'unk' token. We begin this section by describing the basic summarization framework, followed by our new copy mechanisms used to encourage source words and dependency relations to be preserved in the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Basic Framework</head><p>We build an encoder-decoder architecture for this work. An encoder condenses the entire source text to a continuous vector; it also learns a vector representation for each unit of the source text (e.g., words as units). In this work we use a two-layer stacked bi-directional Long Short-Term Memory <ref type="bibr" target="#b19">(Hochreiter and Schmidhuber, 1997</ref>) networks as the encoder, where the input to the second layer is the concatenation of hidden states from the forward and backward passes of the first layer. We obtain the hidden states of the second layer; they are denoted by h e i . The source text vector is constructed by averaging over all h e i and passing the vector through a feedforward layer with tanh activation to convert from the encoder hidden states to an initial decoder hidden state (h d 0 ). This process is illustrated in Eq. <ref type="bibr">(2)</ref>.</p><formula xml:id="formula_0">h e i = f e (h e i−1 , x i ) h d t = f d (h d t−1 , y t−1 )<label>(1)</label></formula><formula xml:id="formula_1">h d 0 = tanh(W h 0 1 S S i=1 h e i + b h 0 )<label>(2)</label></formula><p>A decoder unrolls the summary by predicting one word at a time. During training, the decoder takes as input the embeddings of ground truth summary words, denoted by y t , while at test time y t are embeddings of system predicted summary words (i.e., teacher forcing). We implement an LSTM decoder with the attention mechanism. A context vector c t is used to encode the source words that the system attends to for generating the next summary word. It is defined in Eqs (3-5), where [·||·] denotes the concatenation of two vectors. The α matrix measures the strength of interaction between the decoder hidden states {h d t } and encoder hidden states {h e i }. To predict the next word, the context vector c t and h d t are concatenated and used as input to build a new vector h d t (Eq. <ref type="formula">(6)</ref>). h d t is a surrogate for semantic meanings carried at time step t of the decoder. It is subsequently used to compute a probability distribution over the output vocabulary (Eq. <ref type="formula">(7)</ref>).</p><formula xml:id="formula_2">e t,i = v tanh(W e [h d t ||h e i ] + b e ) (3) α t,i = exp(e t,i ) S i =1 exp(e t,i )<label>(4)</label></formula><formula xml:id="formula_3">c t = S i=1 α t,i h e i (5) h d t = tanh(W h [h d t ||c t ] + b h ) (6) P vocab (w) = softmax(W y h d t + b y ) (7) [x i ||s e i ] h e i h d t c t ↵ t,i h d t c t ↵ t,i x i</formula><p>[h e i ||s e i ] <ref type="figure">Figure 2</ref>: System architectures for 'Struct+Input' (left) and <ref type="bibr">'Struct+Hidden' (right)</ref>. A critical question we seek to answer is whether the structural embeddings (s e i ) should be supplied as input to the encoder (left) or be exempted from encoding and directly concatenated with the encoder hidden states (right).</p><p>The copy mechanism <ref type="bibr" target="#b42">See et al., 2017</ref>) allows words in the source sequence to be selectively copied to the target sequence. It expands the search space for summary words to include both the output vocabulary and the source text. The copy mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT ( <ref type="bibr" target="#b30">Luong et al., 2015b</ref>) and text summarization ( <ref type="bibr" target="#b17">Gu et al., 2016;</ref><ref type="bibr" target="#b7">Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b48">Zeng et al., 2017)</ref>.</p><p>Our copy mechanism employs a 'switch' to estimate the likelihood of generating a word from the vocabulary (p gen ) vs. copying it from the source text (1 − p gen ). The basic model is similar to that of the pointer-generator networks ( <ref type="bibr" target="#b42">See et al., 2017)</ref>. The switch is a feedforward layer with sigmoid activation (Eq. <ref type="formula">(8)</ref>). At time step t, its input is a concatenation of the decoder hidden state h d t , context vector c t , and the embedding of the previously generated word y t−1 . For predicting the next word, we combine the generation and copy probabilities, shown in Eq. <ref type="formula" target="#formula_4">(9)</ref>. If a word w appears once or more in the input text, its copy probability ( i:w i =w α t,i ) is the sum of the attention weights over all its occurrences. If w appears in both the vocabulary and source text, P (w) is a weighted sum of the two probabilities.</p><formula xml:id="formula_4">p gen =σ(W z [h d t ||c t ||y t−1 ])+b z ) (8) P (w)=p gen P vocab (w)+(1−p gen ) i:w i =w α t,i<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure-Infused Copy Mechanisms</head><p>The aforementioned copy mechanism attends to source words based on their "semantic" importance encoded in {α t,i }, which measures the semantic relatedness of the encoder hidden state h e i and the decoder hidden state h d t (Eq. <ref type="formula" target="#formula_2">(4)</ref>). However, the source syntactic structure is ignored. This is problematic, because it hurts the system's ability to effectively identify summary-worthy source words that are syntactically important. We next propose three strategies to inject source syntactic structure to the copy mechanism.  Inspired by compressive summarization via structured prediction <ref type="bibr" target="#b1">(Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013)</ref>, we hypothesize that structural labels, such as the incoming dependency arc and the depth in a dependency parse tree, can be helpful to predict word importance. We consider six categories of structural labels in this work; they are presented in <ref type="table" target="#tab_1">Table 2</ref>. Each structural label is mapped to a fixed-length, trainable structural embedding. However, a critical question remains as to where the structural embeddings should be injected in the existing neural architecture. This problem has not yet been systematically investigated. In this work, we compare two settings:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Shallow Combination</head><p>• Struct+Input concatenates structural embeddings of position i (flattened into one vector s e i ) with the source word embedding x i and uses them as a new form of input to the encoder: • Struct+Hidden concatenates structural embeddings of position i (flattened) with the encoder hidden state h e i and uses them as a new form of hidden states:</p><formula xml:id="formula_5">x i ⇒ [x i ||s e i ]; h e i h d t c t ↵ t,i 񮽙 t,i x i g e i 񮽙 t,i h e i h d t ↵ t,i 񮽙 t,i x i [g e i ||g e p,i ] 񮽙 t,i c t 񮽙 t,i</formula><formula xml:id="formula_6">h e i ⇒ [h e i ||s e i ].</formula><p>The architectural difference is illustrated in <ref type="figure">Figure 2</ref>. Structural embeddings are important complements to existing neural architectures. However, it is unclear whether they should be supplied as input to the encoder or be left out of the encoding process and directly concatenated with the encoder hidden states. This is a critical question we seek to answer by comparing the two settings. Note that an alternative setting is to separately encode words and structural labels using two RNN encoders, we consider this as a subproblem of the "Struct+Input" case.</p><p>The above models complement state-of-the-art by combining semantic and structural signals to determine summary-worthy content. Intuitively, a source word is copied to the summary for two reasons: it contains salient semantic content, or it serves a critical syntactic role in the source sentence. Without explicitly modeling the two factors, 'semantics' can outweigh 'structure,' resulting in summaries that fail to keep the original meaning intact. In the following we propose a two-way mechanism to separately model the "semantic" and "structural" importance of source words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">2-Way Combination (+Word)</head><p>Our new architecture involves two attention matrices that are parallel to each other, denoted by α and β. α t,i is defined as previously in Eq. <ref type="figure" target="#fig_1">(3-4)</ref>. It represents the "semantic" aspect, calculated as the strength of interaction between the encoder hidden state h e i and the decoder hidden state h d t . In contrast, β t,i measures the "structural" importance of the i-th input word to generating the t-th output word, calculated by comparing the structure-enhanced embedding g e i with the decoder hidden state h d t (Eq. <ref type="figure">(10-11)</ref>). We use g e i = [s e i ||x i ] as a primitive (unencoded) representation of the i-th source word. We define δ t,i ∝ α t,i + β t,i as a weighted sum of α t,i and β t,i , where a trainable coefficient is introduced to balance the contribution from both sides <ref type="bibr">(Eq. (12)</ref>). Merging semantic and structural salience at this stage allows us to acquire an accurate estimate of how important the i-th source word is to predicting the t-th output word. δ t,i replaces α t,i to become the new attention value. It is used to calculate the context vector c t (Eq. <ref type="formula" target="#formula_0">(13)</ref>). A reliable estimate of c t is crucial as it is used to estimate the generation probability over the vocabulary (P vocab (w), Eq. (6-7)), the switch value (p gen , Eq. <ref type="formula">(8)</ref>), and ultimately used to predict the next word (P (w), Eq. (9)).</p><formula xml:id="formula_7">f t,i = u tanh(W f [g e i ||h d t ] + b f )<label>(10)</label></formula><formula xml:id="formula_8">β t,i = exp(f t,i ) S i =1 exp(f t,i )<label>(11)</label></formula><formula xml:id="formula_9">δ t,i = α t,i + β t,i S i =1 (α t,i + β t,i )<label>(12)</label></formula><formula xml:id="formula_10">c t = S i=1 δ t,i h e i (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">2-Way Combination (+Relation)</head><p>We observe that salient source relations also play a critical role in predicting the next word. For example, if a dependency edge ("father" nsubj ←−− "had") is salient and "father" is selected to be included in the summary, it is likely that "had" will be selected next such that a salient source relation ("nsubj") is preserved in the summary. Because summary words tend to follow the word order of the original, we assume selecting a source word and including it in the summary has an impact on its subsequent source words, but not the reverse. </p><formula xml:id="formula_11">񮽙 t,j 񮽙 t,i 񮽙 t,i e ↵ t,j e ↵ t,j</formula><p>In this formulation we use β t,i to capture the saliency of the dependency edge pointing to the i-th source word. Thus, an edge w j ← w i has its salience score saved in β t,j ; and conversely, an edge w j → w i has its salience score in β t,i . β is calculated in the same way as described in Eq. <ref type="figure">(10-11)</ref>. However, we replace g e i with [g e i ||g e p,i ] so that a dependency edge is characterized by the embeddings of its two endpoints (g e p,i is the parent embedding). The architectural difference between "Struct+2Way+Word" and "Struct+2Way+Relation" is illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>To obtain the likelihood of w j being selected to the summary prior to time step t, we define α t,j = t−1 t =0 α t ,j that sums up the individual probabilities up to time step t-1. Assume there is a dependency edge w j → w i (j&lt;i) whose salience score is denoted by β t,i . At time step t, we calculate α t,j β t,i (or α t,j β t,j for edge w j ← w i ) as the probability of w i being selected to the summary, given that one of its prior words w j (j&lt;i) is included in the summary and there is a dependency edge connecting the two. By summing the impact over all its previous words, we obtain the likelihood of the i-th source word being included to the summary at time step t in order to preserve salient source relations; this is denoted by γ t,i <ref type="bibr">(Eq. (15)</ref>). Next, we define δ t,i ∝ α t,i + γ t,i as a weighted combination of semantic and structural salience (Eq. (16)). δ t,i replace α t,i to become the new attention values used to estimate the context vector c t (Eq. <ref type="formula" target="#formula_0">(13)</ref>). Finally, the calculation of generation probabilities P vocab (w), switch value p gen , and probabilities for predicting the next word P (w) remains the same as previously (Eq. <ref type="figure">(6-9)</ref>).</p><formula xml:id="formula_12">α t,j = t−1 t =0 α t ,j<label>(14)</label></formula><formula xml:id="formula_13">γ t,i = j:j&lt;i α t,j β t,i if w j → w i α t,j β t,j if w j ← w i<label>(15)</label></formula><formula xml:id="formula_14">δ t,i = α t,i + γ t,i S i =1 (α t,i + γ t,i )<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Objective and Beam Search</head><p>We next describe our learning objective, including a coverage-based regularizer ( <ref type="bibr" target="#b42">See et al., 2017)</ref>, and a beam search with reference mechanism ( <ref type="bibr" target="#b44">Tan et al., 2017)</ref>. We want to investigate the effectiveness of these techniques on sentence summarization, which has not been explored in previous work.</p><p>Learning objective. Our training proceeds by minimizing a per-target-word cross-entropy loss function. A regularization term is applied to the α matrix. Recall that α t,i ∈ [0, 1] measures the interaction strength between the t-th output word and the i-th input word. Naturally, we expect a 1-to-1 mapping between the two words. The coverage-based regularizer, proposed by See et al., <ref type="bibr">(2017)</ref>, encourages this behavior by tracking the historical attention values attributed to the i-th input word (up to time step t-1), denoted by α t,i = t−1 t =0 α t ,i . The approach then takes the minimum between α t,i and α t,i , which has the practical effect of forcing α t,i (∀t) to be close to either 0 or 1, otherwise a penalty will be applied. The regularizer Ω is defined in Eq. <ref type="formula" target="#formula_0">(17)</ref>, where M is the size of the mini-batch, S and T are the lengths of the source and target sequences. For two-way copy mechanisms, δ replaces α to become the new attention values, we therefore apply regularization to δ instead of α. When the regularizer applies, the objective becomes minimizing (L + Ω).</p><formula xml:id="formula_15">Ω=λ M m=1 1 T (m) S (m) T (m) t=1 S (m) i=1 min( α t,i ,α t,i )<label>(17)</label></formula><p>Beam search with reference. During testing, we employ greedy search to generate system summary sequences. For the task of summarization, the ground truth summary sequences are usually close to the source texts. This property can be leveraged in beam search. <ref type="bibr" target="#b44">Tan et al., (2017)</ref> describe a beam search with reference mechanism that rewards system summaries that have a high degree of bigram overlap with the source texts. We describe it in Eq. <ref type="formula" target="#formula_0">(18)</ref>, where where S(w) denotes the score of word w. B(y &lt;t , x) measures the number of bigrams shared by the system summary (up to time step t-1) and the source text; {y &lt;t , w} adds a word w to the end of the system summary. The shorter the source text (measured by length S), the more weight a shared bigram will add to the score of the current word w. A hyperparameter η controls the degree of closeness between the system summary and the source text.</p><formula xml:id="formula_16">S(w)=logP (w)+η B({y &lt;t ,w},x)−B(y &lt;t ,x) S<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed structure-infused copy mechanisms for summarization in this section. We describe the dataset, experimental settings, baselines, and finally, evaluation results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>We evaluate our proposed models on the Gigaword summarization dataset <ref type="bibr" target="#b35">(Parker, 2011;</ref><ref type="bibr" target="#b41">Rush et al., 2015)</ref>. The task is to reduce the first sentence of an article to a title-like summary. We obtain dependency parse trees for source sentences using the Stanford neural network parser <ref type="bibr" target="#b4">(Chen and Manning, 2014</ref>). We also use the standard train/valid/test data splits. Following <ref type="bibr" target="#b41">(Rush et al., 2015)</ref>, the train and valid splits are pruned 2 to improve the data quality. Spurious pairs that are repetitive, overly long/short, and pairs whose source and summary sequences have little word overlap are removed. No pruning is performed for instances in the test set. The processed corpus contains 4,018K training instances. We construct two (non-overlapped) validation sets: "valid-4096" contains 4,096 randomly sampled instances from the valid split; it is used for hyperparameter tuning and early stopping. "valid-2000" is used for evaluation; it allows the models to be trained and evaluated on pruned instances. Finally, we report results on the standard Gigaword test set ( <ref type="bibr" target="#b41">Rush et al., 2015</ref>) containing 1,951 instances ("test-1951").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We use the Xavier scheme (Glorot and Bengio, 2010) for parameter initialization, where weights are initialized using a Gaussian distribution W i,j ∼ N (0, σ), σ = 2 n in +nout ; n in and n out are numbers of the input and output units of the network; biases are set to be 0. We further implement two techniques to accelerate mini-batch training. First, all training instances are sorted by the source sequence length and partitioned into mini-batches. The shorter sequences are padded to have the same length as the longest sequence in the batch. All batches are shuffled at the beginning of each epoch. Second, we introduce a variable-length batch vocabulary containing only source words of the current mini-batch and words of the output vocabulary. P (w) in Eq. (9) only needs to be calculated for words in the batch vocabulary. It is magnitudes smaller than a direct combination of the input and output vocabularies. Finally, our input vocabulary contains the most frequent 70K words in the source texts and summaries. The output vocabulary contains 5K words by default. More network parameters are presented in <ref type="table">Table 3</ref>.</p><p>Input vocabulary size 70K Output vocabulary size 5K (default) Dim. of word embeddings 100 Dim. of structural embeddings 16 Num. of encoder/decoder hidden units 256 Adam optimizer <ref type="bibr" target="#b22">(Kingma and Ba, 2015)</ref> lr = 1e-4 <ref type="bibr" target="#b36">(Pascanu et al., 2013)</ref> g ∈ [-5, 5] <ref type="table">Table 3</ref>: Parameter settings of our summarization system.  <ref type="table">Table 4</ref>: Results on the Gigaword valid-2000 set (full-length F1). Models implementing the structure-infused copy mechanisms ("Struct+*") outperform the baseline.</p><note type="other">Coeff. for coverage-based regularizer λ = 1 Coeff. for beam search with reference η ≈ 13.5 Beam size K = 5 Minibatch size M = 64 Early stopping criterion (max 20 epochs) valid. loss Gradient clipping</note><formula xml:id="formula_17">Gigaword Valid-2000 System R-1 R-2 R-L</formula><p>S: the government filed another round of criminal charges in a widening stock options scandal T: options scandal widens B: government files more charges in stock options scandal I: another round of criminal charges in stock options scandal H: charges filed in stock options scandal W: another round of criminal charges in stock options scandal R: government files another round of criminal charges in options scandal <ref type="table">Table 5</ref>: Example system summaries. 'S:' source; 'T:' target; 'B:' baseline; 'I:' Struct+Input; 'H:' Struct+Hidden; 'W:' 2Way+Word; "R:" 2Way+Relation. "2Way+Relation" is able to preserve important source relations in the summary, e.g., "government nsubj ← −− − files," "files dobj −−→ round," and "round nmod − −− → charges."</p><p>S: red cross negotiators from rivals north korea and south korea held talks wednesday on emergency food shipments to starving north koreans and agreed to meet again thursday T: koreas meet in beijing to discuss food aid from south eds B: north korea , south korea agree to meet again I: north korea , south korea meet again H: north korea , south korea meet on emergency food shipments W: north korea , south korea hold talks on food shipments R: north korea , south korea hold talks on emergency food shipments <ref type="table">Table 6</ref>: Example system summaries. "Struct+Hidden" and "2Way+Relation" successfully preserve salient source words ("emergency food shipments"), which are missed out by other systems. We observe that copying "hold talks" from the source also makes the resulting summaries more informative than using the word "meet."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>ROUGE results on valid set. We first report results on the Gigaword valid-2000 dataset in <ref type="table">Table 4</ref>. We present R-1, R-2, and R-L scores <ref type="bibr" target="#b27">(Lin, 2004</ref>) that respectively measures the overlapped unigrams, bigrams, and longest common subsequences between the system and reference summaries 3 . Our baseline system ("Baseline") implements the seq2seq architecture with the basic copy mechanism (Eq. <ref type="figure">(1-9)</ref>). It is a strong baseline that resembles the pointer-generator networks described in ( <ref type="bibr" target="#b42">See et al., 2017</ref>). The structural models ("Struct+*") differ from the baseline only on the structure-infused copy mechanisms. All models are evaluated without the coverage regularizer or beam search ( §3.3) to ensure fair comparison. Overall, we observe that models equipped with the structure-infused copy mechanisms are superior to the baseline, suggesting that combining source syntactic structure with the copy mechanism is effective. We found that the "Struct+Hidden" architecture, which directly concatenates structural embeddings with the encoder hidden states, outperforms "Struct+Input" despite that the latter requires more parameters. "Struct+2Way+Word" also demonstrates strong performance, achieving 43.21%, 21.84%, and 40.86% F 1 scores, for R-1, R-2, and R-L respectively.</p><p>ROUGE results on test set. We compare our proposed approach with a range of state-of-the-art neural summarization systems. Results on the standard Gigaword test set ("test-1951") are presented in <ref type="table">Table 7</ref>. Details about these systems are provided in <ref type="table" target="#tab_5">Table 8</ref>. Overall, our proposed approach with structureinfused pointer networks perform strongly, yielding ROUGE scores that are on-par with or surpassing state-of-the-art published systems. Notice that the scores on the valid-2000 dataset are generally higher than those of test-1951. This is because the (source, summary) pairs in the Gigaword test set are not pruned (see §4.1). In some cases, none (or very few) of the summary words appear in the source. This may cause difficulties to the systems equipped with the copy mechanism. The "Struct+2Way+Word" architecture that respectively models the semantic and syntactic importance of source words achieves the highest scores. It outperforms its counterpart of "Struct+2Way+Relation," which seeks to preserve source dependency relations in summaries. We conjecture that the imperfect dependency parse trees generated <ref type="bibr">Gigaword Test-1951</ref> System R-1 R-2 R-L ABS <ref type="bibr" target="#b41">(Rush et al., 2015)</ref> 29.55 11.32 26.42 ABS+ <ref type="bibr" target="#b41">(Rush et al., 2015)</ref> 29.76 11.88 26.96 Luong-NMT <ref type="bibr" target="#b8">(Chopra et al., 2016)</ref> 33.10 14.45 30.71 RAS-LSTM <ref type="bibr" target="#b8">(Chopra et al., 2016)</ref> 32.55 14.70 30.03 RAS-Elman <ref type="bibr" target="#b8">(Chopra et al., 2016)</ref> 33.78 15.97 31.15 ASC+FSC1 <ref type="bibr" target="#b32">(Miao and Blunsom, 2016)</ref> 34.17 15.94 31.92 lvt2k-1sent  32.67 15.59 30.64 lvt5k-1sent  35.30 16.64 32.62 Multi-Task ( <ref type="bibr" target="#b37">Pasunuru et al., 2017)</ref> 32.75 15.35 30.82 DRGD <ref type="bibr" target="#b26">(Li et al., 2017b)</ref> 36  <ref type="table">Table 7</ref>: Results on the Gigaword test-1951 set (fulllength F1). Models with structure-infused copy mechanisms ("Struct+*") perform well. Their R-2 F-scores are on-par with or outperform state-of-the-art published systems.</p><p>ABS and ABS+ <ref type="bibr" target="#b41">(Rush et al., 2015)</ref> are the first work introducing an encoder-decoder architecture for summarization. Luong-NMT <ref type="bibr" target="#b8">(Chopra et al., 2016)</ref> is a re-implementation of the attentive stacked LSTM encoder-decoder of <ref type="bibr" target="#b29">Luong et al. (2015a)</ref>. RAS-LSTM and RAS-Elman <ref type="bibr" target="#b8">(Chopra et al., 2016)</ref> describe a convolutional attentive encoder that ensures the decoder focuses on appropriate words at each step of generation. ASC+FSC1 <ref type="bibr" target="#b32">(Miao and Blunsom, 2016)</ref> presents a generative auto-encoding sentence compression model jointly trained on labelled/unlabelled data. lvt2k-1sent and lvt5k-1sent  address issues in the attentive encoder-decoder framework, including modeling keywords, capturing sentence-toword structure, and handling rare words. Multi-Task w/ Entailment ( <ref type="bibr" target="#b37">Pasunuru et al., 2017)</ref> combines entailment with summarization in a multi-task setting. DRGD <ref type="bibr" target="#b26">(Li et al., 2017b</ref>) describes a deep recurrent generative decoder learning latent structure of summary sequences via variational inference. by the parser may affect the "Struct+2Way+Relation" results. However, because the Gigaword dataset does not provide gold-standard annotations for parse trees, we could not easily verify this and will leave it for future work. In <ref type="table">Table 5</ref> and 6, we present system summaries produced by various models.  <ref type="table">Table 9</ref>: Informativeness, fluency, and faithfulness scores of summaries. They are rated by Amazon turkers on a Likert scale of 1 (worst) to 5 (best). We choose to evaluate Struct+2Way+Relation (as oppose to 2Way+Word) because it focuses on preserving source relations in the summaries.</p><p>Linguistic quality. To further gauge the summary quality, we hire human workers from the Amazon Mechanical Turk platform to rate summaries on a Likert scale of 1 to 5 according to three criteria (Zhang and Lapata, 2017): fluency (is the summary grammatical and well-formed?), informativeness (to what extent is the meaning of the original sentence preserved in the summary?), and faithfulness (is the summary accurate and faithful to the original?). We sample 100 instances from the test set and employ 5 turkers to rate each summary; their averaged scores are presented in <ref type="table">Table 9</ref>. We found that "Struct+2Way+Relation" outperforms "Struct+Input" on all three criteria. It also compares favorably to ground-truth summaries on "fluency" and "faithfulness." On the other hand, the ground-truth summaries, corresponding to article titles, are judged as less satisfying according to human raters.</p><p>Dependency relations. We investigate the source dependency relations preserved in the summaries in <ref type="table">Table 10</ref>. A source relation is considered preserved if both its words appear in the summary. We observe that the models implementing structure-infused copy mechanisms (e.g., "Struct+2Way+Word") are more likely to preserve important dependency relations in the summaries, including nsubj, dobj, amod, nmod, and nmod:poss. Dependency relations that are less important (mark, case, conj, cc, det) are less likely to be preserved. These results show that our structure-infused copy mechanisms can learn to recognize the importance of dependency relations and selectively preserve them in the summaries.</p><p>Coverage and reference beam. In <ref type="figure">Figure 11</ref>, we investigate the effect of applying the coverage regularizer ("coverage") and reference-based beam search ("ref beam") ( §3.3) to our models. The coverage regularizer is applied in a second training stage, where the system is trained for an extra 5 epochs with coverage and the model yielding the lowest validation loss is selected. Both coverage and ref beam can improve the system performance. Our observation suggests that ref beam is an effective addition to shorten the gap between different systems.  <ref type="table">Table 10</ref>: Percentages of source dependency relations (of various types) preserved in the system summaries.  Output vocabulary size. Finally, we investigate the impact of the output vocabulary size on the summarization performance in <ref type="table" target="#tab_1">Table 12</ref>. All our models by default use an output vocabulary of 5K words in order to make the results comparable to state-of-the-art-systems. However, we observe that there is a potential to further boost the system performance (17.25→17.62 R-2 F1-score, w/o coverage or ref beam) if we had chosen to use a larger vocabulary (10K) and can endure a slightly longer training time <ref type="bibr">(1.2x)</ref>.</p><p>In <ref type="table" target="#tab_1">Table 12</ref>, we further report the percentages of reference summary words covered by the output vocabulary ("InVcb") and covered by either the output vocabulary or the source text ("InVcb+Src"). The gap between the two conditions shortens as the size of the output vocabulary is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigated structure-infused copy mechanisms that combine source syntactic structure with the copy mechanism of an abstractive summarization system. We compared various system architectures and showed that our models can effectively preserve salient source relations in summaries. Results on benchmark datasets showed that the structural models are on-par with or surpass state-of-theart published systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: System architectures for 'Struct+2Way+Word' (left) and 'Struct+2Way+Relation' (right). βt,i (left) measures the structural importance of the i-th source word; βt,i (right) measures the saliency of the dependency edge pointing to the i-th source word. g e p,i is the structural embedding of the parent. In both cases δt,i replaces αt,i to become the new attention value used to estimate the context vector ct.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Six categories of structural labels. Example labels are 
generated for word 'had' in Figure 1. Relative word positions 
are discretized into ten buckets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 8 : Existing summarization methods.</head><label>8</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Effects of applying the coverage regularizer and 
the reference beam search to structural models, evaluated on 
test-1951. Combining both yields the highest scores. 

|V | 
R-2 
Train Speed InVcb InVcb+Src 
1K 
13.99 
2.5h/epoch 
60.57 
76.04 
2K 
15.35 
2.7h/epoch 
69.71 
80.72 
5K 
17.25 
3.2h/epoch 
79.98 
86.51 
10K 17.62 
3.8h/epoch 
88.26 
92.18 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 12 :</head><label>12</label><figDesc></figDesc><table>Results of the "Struct+2Way+Relation" system 
trained using output vocabularies of various sizes (|V |), eval-
uated on test-1951 w/o coverage or ref beam. The training 
speed is calculated as the elapsed time (hours) per epoch, 
tested on a GTX 1080Ti GPU card. 

</table></figure>

			<note place="foot" n="1"> We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ</note>

			<note place="foot" n="2"> https://github.com/facebookarchive/NAMAS/blob/master/dataset/filter.py</note>

			<note place="foot" n="3"> w/ ROUGE options: -n 2 -m -w 1.2 -c 95 -r 1000</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and robust compressive summarization with dual decomposition and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faithful to the original: Fact aware neural abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extractive vs. NLG-based abstractive summarization of evaluative text: The effect of corpus controversiality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference (INLG)</title>
		<meeting>the Fifth International Natural Language Generation Conference (INLG)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A noisy-channel model for document compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning-based single-document summarization with compression and anaphoricity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hybrid approach to multi-document summarization of opinions in reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">Di</forename><surname>Fabbrizio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">J</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Natural Language Generation Conference (INLG)</title>
		<meeting>the 8th International Natural Language Generation Conference (INLG)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An extractive supervised two-stage method for sentence compression</title>
	</analytic>
	<monogr>
		<title level="m">Dimitrios Galanis and Ion Androutsopoulos</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bita</forename><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-tosequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The decomposition of human-written summary sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving multi-documents summarization by sentence compression based on expanded constituent parse tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep recurrent generative decoder for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ROUGE: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Text Summarization Branches Out</title>
		<meeting>ACL Workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic summarization. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">English Gigaword fifth edition LDC2011T07. Philadelphia: Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards improving abstractive summarization via entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization</title>
		<meeting>the Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modelling events through memory-based, open-ie patterns for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cornolti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A neural attention model for sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural headline generation on abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graph-based attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sentence compression with joint structural inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient summarization with read-again and copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
