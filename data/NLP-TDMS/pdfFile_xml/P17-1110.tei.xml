<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Multi-Criteria Learning for Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30 -August 4, 2017. July 30 -August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
							<email>xinchichen13@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Multi-Criteria Learning for Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1193" to="1203"/>
							<date type="published">July 30 -August 4, 2017. July 30 -August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/P17-1110</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However , it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP). Currently, the state-ofthe-art methods are based on statistical supervised learning algorithms, and rely on a large-scale annotated corpus whose cost is extremely expensive. Although there have been great achievements in building CWS corpora, they are somewhat incompatible due to different segmentation criteria. As shown in <ref type="table" target="#tab_1">Table 1</ref>, given a sentence "姚明进 入总决赛 (YaoMing reaches the final)", the two commonly-used corpora, PKU's People's Daily (PKU) ( <ref type="bibr" target="#b29">Yu et al., 2001</ref>) and Penn Chinese Treebank (CTB) <ref type="bibr" target="#b10">(Fei, 2000)</ref>, use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora.  Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging <ref type="bibr">(Ji- ang et al., 2009;</ref><ref type="bibr" target="#b28">Sun and Wan, 2012;</ref><ref type="bibr" target="#b27">Qiu et al., 2013;</ref><ref type="bibr" target="#b20">Li et al., , 2016</ref>. These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks <ref type="bibr" target="#b8">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Luong et al., 2015;</ref><ref type="bibr" target="#b5">Chen et al., 2016)</ref>.</p><p>In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria. Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning <ref type="bibr" target="#b3">(Caruana, 1997;</ref><ref type="bibr" target="#b1">Ben-David and Schuller, 2003)</ref>, where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features. Inspired by the success of adversarial strategy on domain adaption ( <ref type="bibr" target="#b0">Ajakan et al., 2014;</ref><ref type="bibr">Ga- nin et al., 2016;</ref><ref type="bibr" target="#b2">Bousmalis et al., 2016)</ref>, we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria. Finally, we exploit the eight segmentation criteria on the five simplified Chi-nese and three traditional Chinese corpora. Experiments show that our models are effective to improve the performance for CWS. We also observe that traditional Chinese could benefit from incorporating knowledge from simplified Chinese.</p><p>The contributions of this paper could be summarized as follows.</p><p>• Multi-criteria learning is first introduced for CWS, in which we propose three sharedprivate models to integrate multiple segmentation criteria.</p><p>• An adversarial strategy is used to force the shared layer to learn criteria-invariant features, in which an new objective function is also proposed instead of the original cross-entropy loss.</p><p>• We conduct extensive experiments on eight CWS corpora with different segmentation criteria, which is by far the largest number of datasets used simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">General Neural Model for Chinese Word Segmentation</head><p>Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B, M, E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering ( <ref type="bibr" target="#b30">Zheng et al., 2013;</ref><ref type="bibr" target="#b25">Pei et al., 2014;</ref><ref type="bibr">Chen et al., 2015a,b)</ref>. Specifically, given a sequence with n characters X = {x 1 , . . . , x n }, the aim of CWS task is to figure out the ground truth of labels Y * = {y * 1 , . . . , y * n }:</p><formula xml:id="formula_0">Y * = arg max Y ∈L n p(Y |X),<label>(1)</label></formula><p>where L = {B, M, E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural network. In this  <ref type="figure">Figure 1</ref>: General neural architecture for Chinese word segmentation.</p><p>paper, we adopt the bi-directional long short-term memory neural networks followed by CRF as the tag inference layer. <ref type="figure">Figure 1</ref> illustrates the general architecture of CWS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding layer</head><p>In neural models, the first step usually is to map discrete language symbols to distributed embedding vectors. Formally, we lookup embedding vector from embedding matrix for each character x i as e x i ∈ R de , where d e is a hyper-parameter indicating the size of character embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature layers</head><p>We adopt bi-directional long short-term memory (Bi-LSTM) as feature layers. While there are numerous LSTM variants, here we use the LSTM architecture used by <ref type="bibr" target="#b16">(Jozefowicz et al., 2015)</ref>, which is similar to the architecture of (Graves, 2013) but without peep-hole connections.</p><p>LSTM LSTM introduces gate mechanism and memory cell to maintain long dependency information and avoid gradient vanishing. Formally, LSTM, with input gate i, output gate o, forget gate f and memory cell c, could be expressed as:</p><formula xml:id="formula_1">    i i o i f i ˜ c i     =     σ σ σ ϕ     ( W g [ e x i h i−1 ] + b g ) ,<label>(2)</label></formula><formula xml:id="formula_2">c i = c i−1 ⊙ f i + ˜ c i ⊙ i i ,<label>(3)</label></formula><formula xml:id="formula_3">h i = o i ⊙ ϕ(c i ),<label>(4)</label></formula><p>where W g ∈ R (de+d h )×4d h and b g ∈ R 4d h are trainable parameters. d h is a hyper-parameter, in-dicating the hidden state size. Function σ(·) and ϕ(·) are sigmoid and tanh functions respectively.</p><p>Bi-LSTM In order to incorporate information from both sides of sequence, we use bi-directional LSTM (Bi-LSTM) with forward and backward directions. The update of each Bi-LSTM unit can be written precisely as follows:</p><formula xml:id="formula_4">h i = − → h i ⊕ ← − h i ,<label>(5)</label></formula><formula xml:id="formula_5">= Bi-LSTM(e x i , − → h i−1 , ← − h i+1 , θ),<label>(6)</label></formula><p>where − → h i and ← − h i are the hidden states at position i of the forward and backward LSTMs respectively; ⊕ is concatenation operation; θ denotes all parameters in Bi-LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference Layer</head><p>After extracting features, we employ conditional random fields (CRF) ( <ref type="bibr" target="#b18">Lafferty et al., 2001</ref>) layer to inference tags. In CRF layer, p(Y |X) in Eq (1) could be formalized as:</p><formula xml:id="formula_6">p(Y |X) = Ψ(Y |X) ∑ Y ′ ∈L n Ψ(Y ′ |X) .<label>(7)</label></formula><p>Here, Ψ(Y |X) is the potential function, and we only consider interactions between two successive labels (first order linear chain CRFs):</p><formula xml:id="formula_7">Ψ(Y |X) = n ∏ i=2 ψ(X, i, y i−1 , y i ),<label>(8)</label></formula><formula xml:id="formula_8">ψ(x, i, y ′ , y) = exp(s(X, i) y + b y ′ y ),<label>(9)</label></formula><p>where b y ′ y ∈ R is trainable parameters respective to label pair (y ′ , y). Score function s(X, i) ∈ R |L| assigns score for each label on tagging the i-th character:</p><formula xml:id="formula_9">s(X, i) = W ⊤ s h i + b s ,<label>(10)</label></formula><p>where h i is the hidden state of Bi-LSTM at position i; W s ∈ R d h ×|L| and b s ∈ R |L| are trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Criteria Learning for Chinese Word Segmentation</head><p>Although neural models are widely used on CWS, most of them cannot deal with incompatible criteria with heterogonous segmentation criteria simultaneously.</p><p>Inspired by the success of multi-task learning <ref type="bibr" target="#b3">(Caruana, 1997;</ref><ref type="bibr" target="#b1">Ben-David and Schuller, 2003;</ref><ref type="bibr" target="#b21">Liu et al., 2016a</ref>,b), we regard the heterogenous criteria as multiple "related" tasks, which could improve the performance of each other simultaneously with shared information.</p><p>Formally, assume that there are M corpora with heterogeneous segmentation criteria. We refer D m as corpus m with N m samples:</p><formula xml:id="formula_10">D m = {(X (m) i , Y (m) i )} Nm i=1 ,<label>(11)</label></formula><p>where X m i and Y m i denote the i-th sentence and the corresponding label in corpus m.</p><p>To exploit the shared information between these different criteria, we propose three sharing models for CWS task as shown in <ref type="figure">Figure 2</ref>. The feature layers of these three models consist of a private (criterion-specific) layer and a shared (criterioninvariant) layer. The difference between three models is the information flow between the task layer and the shared layer. Besides, all of these three models also share the embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model-I: Parallel Shared-Private Model</head><p>In the feature layer of Model-I, we regard the private layer and shared layer as two parallel layers. For corpus m, the hidden states of shared layer and private layer are:</p><formula xml:id="formula_11">h (s) i =Bi-LSTM(e x i , − → h (s) i−1 , ← − h (s) i+1 , θ s ), (12) h (m) i =Bi-LSTM(e x i , − → h (m) i−1 , ← − h (m) i+1</formula><p>, θ m ), (13) and the score function in the CRF layer is computed as:</p><formula xml:id="formula_12">s (m) (X, i) = W (m) s ⊤ [ h (s) i h (m) i ] + b (m) s ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_13">W (m) s ∈ R 2d h ×|L| and b (m) s</formula><p>∈ R |L| are criterion-specific parameters for corpus m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model-II: Stacked Shared-Private Model</head><p>In the feature layer of Model-II, we arrange the shared layer and private layer in stacked manner. The private layer takes output of shared layer as input. For corpus m, the hidden states of shared layer and private layer are:</p><formula xml:id="formula_14">h (s) i = Bi-LSTM(ex i , − → h (s) i−1 , ← − h (s) i+1 , θs),<label>(15)</label></formula><formula xml:id="formula_15">h (m) i = Bi-LSTM( [ ex i h (s) i ] , − → h (m) i−1 , ← − h (m) i+1 , θm)<label>(16)</label></formula><p>and the score function in the CRF layer is computed as:</p><formula xml:id="formula_16">s (m) (X, i) = W (m) s ⊤ h (m) i + b (m) s ,<label>(17)</label></formula><p>where <ref type="figure">Figure 2</ref>: Three shared-private models for multi-criteria learning. The yellow blocks are the shared Bi-LSTM layer, while the gray block are the private Bi-LSTM layer. The yellow circles denote the shared embedding layer. The red information flow indicates the difference between three models.</p><formula xml:id="formula_17">W (m) s ∈ R 2d h ×|L| and b (m) s ∈ R |L| are criterion-specific parameters for corpus m. CRF CRF Task A Task B X (A) X (B) Y (B) Y (A) (a) Model-I CRF CRF Task A Task B X (A) X (B) Y (B) Y (A) (b) Model-II CRF CRF Task A Task B X (A) X (B) Y (B) Y (A) (c) Model-III</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model-III: Skip-Layer Shared-Private Model</head><p>In the feature layer of Model-III, the shared layer and private layer are in stacked manner as Model-II. Additionally, we send the outputs of shared layer to CRF layer directly. The Model III can be regarded as a combination of Model-I and Model-II. For corpus m, the hidden states of shared layer and private layer are the same with Eq <ref type="formula" target="#formula_0">(15)</ref> and <ref type="formula" target="#formula_0">(16)</ref>, and the score function in CRF layer is computed as the same as <ref type="bibr">Eq (14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective function</head><p>The parameters of the network are trained to maximize the log conditional likelihood of true labels on all the corpora. The objective function J seg can be computed as:</p><formula xml:id="formula_18">Jseg(Θ m , Θ s ) = M ∑ m=1 Nm ∑ i=1 log p(Y (m) i |X (m) i ; Θ m , Θ s ),<label>(18)</label></formula><p>where Θ m and Θ s denote all the parameters in private and shared layers respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Incorporating Adversarial Training for Shared Layer</head><p>Although  <ref type="figure">Figure 3</ref>: Architecture of Model-III with adversarial training strategy for shared layer. The discriminator firstly averages the hidden states of shared layer, then derives probability over all possible criteria by applying softmax operation after a linear transformation.</p><p>layer via adversarial training ( <ref type="bibr" target="#b12">Goodfellow et al., 2014</ref>). Therefore, besides the task loss for CWS, we additionally introduce an adversarial loss to prevent criterion-specific feature from creeping into shared space as shown in <ref type="figure">Figure 3</ref>. We use a criterion discriminator which aims to recognize which criterion the sentence is annotated by using the shared features.</p><p>Specifically, given a sentence X with length n, we refer to h </p><formula xml:id="formula_19">X = 1 n ∑ n i h (s)</formula><p>x i . The criterion discriminator computes the probability p(·|X) over all criteria as:</p><formula xml:id="formula_20">p(·|X; Θ d , Θ s ) = softmax(W ⊤ d h (s) X + b d ),<label>(19)</label></formula><p>where Θ d indicates the parameters of criterion discriminator W d ∈ R d h ×M and b d ∈ R M ; Θ s denotes the parameters of shared layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adversarial loss function</head><p>The criterion discriminator maximizes the cross entropy of predicted criterion distribution p(·|X) and true criterion.</p><formula xml:id="formula_21">max Θ d J 1 adv (Θ d ) = M ∑ m=1 Nm ∑ i=1 log p(m|X (m) i ; Θ d , Θ s ).<label>(20)</label></formula><p>An adversarial loss aims to produce shared features, such that a criterion discriminator cannot reliably predict the criterion by using these shared features. Therefore, we maximize the entropy of predicted criterion distribution when training shared parameters.</p><formula xml:id="formula_22">max Θ s J 2 adv (Θ s ) = M ∑ m=1 Nm ∑ i=1 H ( p(m|X (m) i ; Θ d , Θ s ) ) ,<label>(21)</label></formula><p>where H(p) = − ∑ i p i log p i is an entropy of distribution p.</p><p>Unlike ( <ref type="bibr" target="#b11">Ganin et al., 2016)</ref>, we use entropy term instead of negative cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>Finally, we combine the task and adversarial objective functions.</p><formula xml:id="formula_23">J (Θ; D) = Jseg(Θ m , Θ s ) + J 1 adv (Θ d ) + λJ 2 adv (Θ s ),<label>(22)</label></formula><p>where λ is the weight that controls the interaction of the loss terms and D is the training corpora. The training procedure is to optimize two discriminative classifiers alternately as shown in Algorithm 1. We use Adam ( <ref type="bibr" target="#b17">Kingma and Ba, 2014</ref>) with minibatchs to maximize the objectives.</p><p>Notably, when using adversarial strategy, we firstly train 2400 epochs (each epoch only trains on eight batches from different corpora), then we only optimize J seg (Θ m , Θ s ) with Θ s fixed until convergence (early stop strategy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIG-HAN2005 ( <ref type="bibr" target="#b9">Emerson, 2005</ref>) and SIGHAN2008 <ref type="bibr" target="#b15">(Jin and Chen, 2008)</ref>. <ref type="table" target="#tab_5">Table 2</ref> gives the details of the eight datasets. Among these datasets, AS, CI-TYU and CKIP are traditional Chinese, while the Algorithm 1 Adversarial multi-criteria learning for CWS task.</p><p>1: for i = 1; i &lt;= n_epoch; i + + do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p># Train tag predictor for CWS </p><formula xml:id="formula_24">B = {X, Y } bm 1 ∈ D m 12: Θ d += α∇ Θ d J (Θ; B) 13:</formula><p>end for 14: end for remains, MSRA, PKU, CTB, NCC and SXU, are simplified Chinese. We use 10% data of shuffled train set as development set for all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Configurations</head><p>For hyper-parameter configurations, we set both the character embedding size d e and the dimensionality of LSTM hidden states d h to 100. The initial learning rate α is set to 0.01. The loss weight coefficient λ is set to 0.05. Since the scale of each dataset varies, we use different training batch sizes for datasets. Specifically, we set batch sizes of AS and MSR datasets as 512 and 256 respectively, and 128 for remains. We employ dropout strategy on embedding layer, keeping 80% inputs (20% dropout rate).</p><p>For initialization, we randomize all parameters following uniform distribution at (−0.05, 0.05). We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit ( <ref type="bibr" target="#b24">Mikolov et al., 2013)</ref>. Following previous work <ref type="bibr" target="#b7">(Chen et al., 2015b;</ref><ref type="bibr" target="#b25">Pei et al., 2014)</ref>, all experiments including baseline results are using pre-trained character embedding with bigram feature.  (2) In the second block, our proposed three models based on multi-criteria learning boost performance. Model-I gains 0.75% improvement on averaging F-measure score compared with Bi-LSTM result (94.14%). Only the performance on MSRA drops slightly. Compared to the baseline results (Bi-LSTM and stacked Bi-LSTM), the proposed models boost the performance with the help of exploiting information across these heterogeneous segmentation criteria. Although various criteria have different segmentation granularities, there are still some underlying information shared. For instance, MSRA and CTB treat family name and last name as one token "宁 泽 涛 (NingZeTao)", whereas some other datasets, like PKU, regard them as two tokens, "宁 (Ning)" and "泽涛 (ZeTao)". The partial boundaries (before "宁 (Ning)" or after "涛 (Tao)") can be shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Overall Results</head><p>(3) In the third block, we introduce adversarial training. By introducing adversarial training, the performances are further boosted, and Model-I is slightly better than Model-II and Model-III. The adversarial training tries to make shared layer keep criteria-invariant features. For instance, as shown in <ref type="table" target="#tab_4">Table 3</ref>, when we use shared information, the performance on MSRA drops (worse than baseline result). The reason may be that the shared parameters bias to other segmentation criteria and introduce noisy features into shared parameters. When we additionally incorporate the adversarial strategy, we observe that the performance on MSRA is improved and outperforms the baseline results. We could also observe the improvements on other datasets. However, the boost from the adversarial strategy is not significant. The main reason might be that the proposed three sharing models implicitly attempt to keep invariant features by shared parameters and learn discrepancies by the task layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Speed</head><p>To further explore the convergence speed, we plot the results on development sets through epochs. <ref type="figure">Figure 4</ref> shows the learning curve of Model-I without incorporating adversarial strategy. As shown in <ref type="figure">Figure 4</ref>, the proposed model makes progress gradually on all datasets. After about 1000 epochs, the performance becomes stable and convergent.</p><p>We also test the decoding speed, and our models process 441.38 sentences per second averagely. As the proposed models and the baseline models (Bi-LSTM and stacked Bi-LSTM) are nearly in the same complexity, all models are nearly the same efficient. However, the time consumption of training process varies from model to model. For the models without adversarial training, it costs about 10 hours for training (the same for stacked Bi-LSTM to train eight datasets), whereas it takes about 16 hours for the models with adversarial training. All the experiments are conducted on the hardware with Intel(R) Xeon(R) CPU E5-2643 v3 @ 3.40GHz and NVIDIA GeForce GTX TITAN X.   <ref type="table" target="#tab_4">Table 3</ref>: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of our proposed three models without adversarial training. The third block consists of our proposed three models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV recall rate respectively. The maximum F values in each block are highlighted for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Error Analysis</head><p>We further investigate the benefits of the proposed models by comparing the error distributions between the single-criterion learning (baseline model Bi-LSTM) and multi-criteria learning (Model-I and Model-I with adversarial training) as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. According to the results, we could observe that a large proportion of points lie above diagonal lines in <ref type="figure" target="#fig_3">Figure 5a</ref> and <ref type="figure" target="#fig_3">Figure 5b</ref>, which implies that performance benefit from integrating knowledge and complementary information from other corpora. As shown in <ref type="table" target="#tab_4">Table 3</ref>, on the test set of CITYU, the performance of Model-I and its adversarial version (Model-I+ADV) boost from 92.17% to 95.59% and 95.42% respectively.</p><p>In addition, we observe that adversarial strategy is effective to prevent criterion specific features from creeping into shared space. For instance, the segmentation granularity of personal name is often different according to heterogenous criteria. With the help of adversarial strategy, our models could correct a large proportion of mistakes on personal name. <ref type="table" target="#tab_8">Table 4</ref> lists the examples from 2333-th and 89-th sentences in test sets of PKU and MSRA datasets respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Knowledge Transfer</head><p>We also conduct experiments of whether the shared layers can be transferred to the other related tasks or domains. In this section, we investigate the ability of knowledge transfer on two experiments: (1) simplified Chinese to traditional Chinese and (2) formal texts to informal texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Simplified Chinese to Traditional Chinese</head><p>Traditional Chinese and simplified Chinese are two similar languages with slightly difference on character forms (e.g. multiple traditional characters might map to one simplified character   As we can see, the average performance is boosted by 0.41% on F-measure score (from 93.78% to 94.19%), which indicates that shared features learned from simplified Chinese segmentation criteria can help to improve performance on traditional Chinese. Like MSRA, as AS dataset is relatively large (train set of 5.4M tokens), the features learned by shared parameters might bias to other datasets and thus hurt performance on such large dataset AS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Formal Texts to Informal Texts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Dataset</head><p>We use the NLPCC 2016 dataset 2 ( <ref type="bibr" target="#b26">Qiu et al., 2016</ref>) to evaluate our model on micro-blog texts. The NLPCC 2016 data are provided by the shared task in the 5th CCF Conference on Natural Language Processing &amp; Chinese Computing (NLPCC 2016): Chinese Word Segmentation and POS Tagging for micro-blog Text. Unlike the popular used newswire dataset, the NLPCC 2016 dataset is collected from Sina Weibo 3 , which consists of the informal texts from micro-blog with the various topics, such as finance, sports, entertainment, and so on. The information of the dataset is shown in <ref type="table" target="#tab_10">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words</head><p>Chars <ref type="table" target="#tab_1">Word Types Char Types  Sents OOV Rate  Train  421,166 688,743  43,331  4,502 20,135  - Dev  43,697  73,246  11,187  2,879  2,052  6.82%  Test  187,877 315,865  27,804  3,911  8,592</ref> 6.98%   <ref type="table" target="#tab_5">(Table  2</ref>) and are fixed for NLPCC dataset. Here, we conduct Model-I without incorporating adversarial training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Results</head><p>Formal documents (like the eight datasets in <ref type="table" target="#tab_5">Table  2</ref>) and micro-blog texts are dissimilar in many aspects. Thus, we further investigate that if the formal texts could help to improve the performance of micro-blog texts. <ref type="table" target="#tab_11">Table 7</ref> gives the results of Model-I on the NLPCC 2016 dataset under the help of the eight datasets in <ref type="table" target="#tab_5">Table 2</ref>. Specifically, we firstly train the model on the eight datasets, then we train on the NLPCC 2016 dataset alone with shared parameters fixed. The baseline model is Bi-LSTM which is trained on the NLPCC 2016 dataset alone.</p><p>As we can see, the performance is boosted by 0.30% on F-measure score (from 93.94% to 94.24%), and we could also observe that the OOV recall rate is boosted by 3.97%. It shows that the shared features learned from formal texts can help to improve the performance on of micro-blog texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Works</head><p>There are many works on exploiting heterogeneous annotation data to improve various NLP tasks. <ref type="bibr" target="#b14">Jiang et al. (2009)</ref> proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. <ref type="bibr" target="#b28">Sun and Wan (2012)</ref> proposed a structurebased stacking model to reduce the approximation error, which makes use of structured features such as sub-words. These models are unidirectional aid and also suffer from error propagation problem. <ref type="bibr" target="#b27">Qiu et al. (2013)</ref> used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets.  proposed a coupled sequence labeling model which could directly learn and infer two heterogeneous annotations.  also utilize multiple corpora using coupled sequence labeling model. These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features.</p><p>Our proposed models use deep neural networks, which can easily share information with hidden shared layers. <ref type="bibr" target="#b5">Chen et al. (2016)</ref> also adopted neural network models for exploiting heterogeneous annotations based on neural multi-view model, which can be regarded as a simplified version of our proposed models by removing private hidden layers.</p><p>Unlike the above models, we design three sharing-private architectures and keep shared layer to extract criterion-invariance features by introducing adversarial training. Moreover, we fully exploit eight corpora with heterogeneous segmentation criteria to model the underlying shared information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions &amp; Future Works</head><p>In this paper, we propose adversarial multi-criteria learning for CWS by fully exploiting the underlying shared knowledge across multiple heterogeneous criteria. Experiments show that our proposed three shared-private models are effective to extract the shared information, and achieve significant improvements over the single-criterion methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>X</head><label></label><figDesc>as shared features for X in one of the sharing models. Here, we compute h (s) X by simply averaging the hidden states of shared layer h (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>;</head><label></label><figDesc>m &lt;= M ; m + + do 4: # Randomly pick data from corpus m 5: B = {X, Y } bm 1 ∈ D m 6: Θ s += α∇ Θ s J (Θ; B) 7: Θ m += α∇ Θ m J (Θ; B) 8: end for 9: # Train criterion discriminator 10: for m = 1; m &lt;= M ; m + + do 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Models</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: Convergence speed of Model-I without adversarial training on development sets of eight datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Illustration of the different segmentation 
criteria. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 shows</head><label>3</label><figDesc>the experiment results of the pro- posed models on test sets of eight CWS datasets, which has three blocks. (1) In the first block, we can see that the per- formance is boosted by using Bi-LSTM, and the</figDesc><table>Datasets 

Words 
Chars 
Word Types 
Char Types 
Sents 
OOV Rate 

Sighan05 
MSRA 
Train 
2.4M 
4.1M 
88.1K 
5.2K 
86.9K 
-
Test 
0.1M 
0.2M 
12.9K 
2.8K 
4.0K 
2.60% 

AS 
Train 
5.4M 
8.4M 
141.3K 
6.1K 
709.0K 
-
Test 
0.1M 
0.2M 
18.8K 
3.7K 
14.4K 
4.30% 

Sighan08 

PKU 
Train 
1.1M 
1.8M 
55.2K 
4.7K 
47.3K 
-
Test 
0.2M 
0.3M 
17.6K 
3.4K 
6.4K 
-

CTB 
Train 
0.6M 
1.1M 
42.2K 
4.2K 
23.4K 
-
Test 
0.1M 
0.1M 
9.8K 
2.6K 
2.1K 
5.55% 

CKIP 
Train 
0.7M 
1.1M 
48.1K 
4.7K 
94.2K 
-
Test 
0.1M 
0.1M 
15.3K 
3.5K 
10.9K 
7.41% 

CITYU 
Train 
1.1M 
1.8M 
43.6K 
4.4K 
36.2K 
-
Test 
0.2M 
0.3M 
17.8K 
3.4K 
6.7K 
8.23% 

NCC 
Train 
0.5M 
0.8M 
45.2K 
5.0K 
18.9K 
-
Test 
0.1M 
0.2M 
17.5K 
3.6K 
3.6K 
4.74% 

SXU 
Train 
0.5M 
0.9M 
32.5K 
4.2K 
17.1K 
-
Test 
0.1M 
0.2M 
12.4K 
2.8K 
3.7K 
5.12% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Details of the eight datasets. 

performance of Bi-LSTM cannot be improved by 
merely increasing the depth of networks. In addi-
tion, although the F value of LSTM model in (Chen 
et al., 2015b) is 97.4%, they additionally incorpo-
rate an external idiom dictionary. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 : Segmentation cases of personal names.</head><label>4</label><figDesc></figDesc><table>Models 
AS 
CKIP CITYU 
Avg. 
Baseline(Bi-LSTM) 94.20 93.06 
94.07 
93.78 
Model-I  *  
94.12 93.24 
95.20 
94.19 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance on 3 traditional Chinese da-
tasets. Model-I  *  means that the shared parameters 
are trained on 5 simplified Chinese datasets and 
are fixed for traditional Chinese datasets. Here, we 
conduct Model-I without incorporating adversarial 
training strategy. 

nal Chinese datasets under the help of 5 simpli-
fied Chinese datasets. Specifically, we firstly train 
the model on simplified Chinese datasets, then 
we train traditional Chinese datasets independently 
with shared parameters fixed. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 : Statistical information of NLPCC 2016 dataset.</head><label>6</label><figDesc></figDesc><table>Models 
P 
R 
F 
OOV 
Baseline(Bi-LSTM) 93.56 94.33 93.94 70.75 
Model-I  *  
93.65 94.83 94.24 74.72 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Performances on the test set of NLPCC 
2016 dataset. Model-I  *  means that the shared pa-
rameters are trained on 8 Chinese datasets </table></figure>

			<note place="foot" n="2"> https://github.com/FudanNLP/ NLPCC-WordSeg-Weibo 3 http://www.weibo.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We appreciate the contribution from Jingjing Gong and Jiacheng Xu. Besides, we would like to thank the anonymous reviewers for their valuable comments. This work is partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), Shanghai Municipal Science and Technology Commission on (No. 16JC1420401).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4446</idno>
		<title level="m">Domain-adversarial neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting task relatedness for multiple task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="567" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting heterogeneous annotations for weibo word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National CCF Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="495" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural network for heterogeneous annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The part-of-speech tagging guidelines for the penn chinese treebank (3.0)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xia Fei</surname></persName>
		</author>
		<ptr target="http://www.cis.upenn.edu/˜chinese/segguide.3rd.ch.pdf" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth SIGHAN Workshop on Chinese Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coupled sequence labeling on heterogeneous annotations: Pos tagging as a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast coupled sequence labeling on heterogeneous annotations via context-aware pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Baobao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the NLPCC-ICCPOL 2016 shared task: Chinese word segmentation for micro-blog texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Processing of Oriental Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint chinese word segmentation and pos tagging on heterogeneous annotated corpora with multiple task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="658" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Processing norms of modern Chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
