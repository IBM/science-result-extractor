<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grammar as a Foreign Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
							<email>vinyals@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
							<email>lukaszkaiser@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><forename type="middle">Koo</forename><surname>Google</surname></persName>
							<email>terrykoo@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><forename type="middle">Sutskever</forename><surname>Google</surname></persName>
							<email>ilyasu@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton Google</surname></persName>
						</author>
						<title level="a" type="main">Grammar as a Foreign Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Syntactic constituency parsing is a fundamental problem in linguistics and natural language processing that has a wide range of applications. This problem has been the subject of intense research for decades, and as a result, there exist highly accurate domain-specific parsers. The computational requirements of traditional parsers are cubic in sentence length, and while linear-time shift-reduce constituency parsers improved in accuracy in recent years, they never matched state-of-the-art. Furthermore, standard parsers have been designed with parsing in mind; the concept of a parse tree is deeply ingrained into these systems, which makes these methods inapplicable to other problems.</p><p>Recently, Sutskever et al. <ref type="bibr" target="#b0">[1]</ref> introduced a neural network model for solving the general sequenceto-sequence problem, and Bahdanau et al. <ref type="bibr" target="#b1">[2]</ref> proposed a related model with an attention mechanism that makes it capable of handling long sequences well. Both models achieve state-of-the-art results on large scale machine translation tasks (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>). Syntactic constituency parsing can be formulated as a sequence-to-sequence problem if we linearize the parse tree (cf. <ref type="figure">Figure 2</ref>), so we can apply these models to parsing as well.</p><p>Our early experiments focused on the sequence-to-sequence model of Sutskever et al. <ref type="bibr" target="#b0">[1]</ref>. We found this model to work poorly when we trained it on standard human-annotated parsing datasets (1M tokens), so we constructed an artificial dataset by labelling a large corpus with the BerkeleyParser.</p><p>.  <ref type="figure">Figure 1</ref>: A schematic outline of a run of our LSTM+A model on the sentence "Go.". See text for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Go</head><p>To our surprise, the sequence-to-sequence model matched the BerkeleyParser that produced the annotation, having achieved an F1 score of 90.5 on the test set (section 23 of the WSJ).</p><p>We suspected that the attention model of Bahdanau et al. <ref type="bibr" target="#b1">[2]</ref> might be more data efficient and we found that it is indeed the case. We trained a sequence-to-sequence model with attention on the small human-annotated parsing dataset and were able to achieve an F1 score of 88.3 on section 23 of the WSJ without the use of an ensemble and 90.5 with an ensemble, which matches the performance of the BerkeleyParser (90.4) when trained on the same data.</p><p>Finally, we constructed a second artificial dataset consisting of only high-confidence parse trees, as measured by the agreement of two parsers. We trained a sequence-to-sequence model with attention on this data and achieved an F1 score of 92.1 on section 23 of the WSJ, matching the state-of-the-art. This result did not require an ensemble, and as a result, the parser is also very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LSTM+A Parsing Model</head><p>Let us first recall the sequence-to-sequence LSTM model. The Long Short-Term Memory model of <ref type="bibr" target="#b4">[5]</ref> is defined as follows. Let x t , h t , and m t be the input, control state, and memory state at timestep t. Given a sequence of inputs (x 1 , . . . , x T ), the LSTM computes the h-sequence (h 1 , . . . , h T ) and the m-sequence (m 1 , . . . , m T ) as follows.</p><formula xml:id="formula_0">i t = sigm(W 1 x t + W 2 h t−1 ) i t = tanh(W 3 x t + W 4 h t−1 ) f t = sigm(W 5 x t + W 6 h t−1 ) o t = sigm(W 7 x t + W 8 h t−1 ) m t = m t−1 f t + i t i t h t = m t o t</formula><p>The operator denotes element-wise multiplication, the matrices W 1 , . . . , W 8 and the vector h 0 are the parameters of the model, and all the nonlinearities are computed element-wise.</p><p>In a deep LSTM, each subsequent layer uses the h-sequence of the previous layer for its input sequence x. The deep LSTM defines a distribution over output sequences given an input sequence:</p><formula xml:id="formula_1">P (B|A) = T B t=1 P (B t |A 1 , . . . , A T A , B 1 , . . . , B t−1 ) ≡ T B t=1 softmax(W o · h T A +t ) δ Bt ,</formula><p>The above equation assumes a deep LSTM whose input sequence is x = (A 1 , . . . , A T A , B 1 , . . . , B T B ), so h t denotes t-th element of the h-sequence of topmost LSTM. is a Kronecker delta with a dimension for each output symbol, so softmax(W o · h T A +t ) δ Bt is precisely the B t 'th element of the distribution defined by the softmax. Every output sequence terminates with a special end-of-sequence token which is necessary in order to define a distribution over sequences of variable lengths. We use two different sets of LSTM parameters, one for the input sequence and one for the output sequence, as shown in <ref type="figure">Figure 1</ref>. Stochastic gradient descent is used to maximize the training objective which is the average over the training set of the log probability of the correct output sequence given the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention Mechanism</head><p>An important extension of the sequence-to-sequence model is by adding an attention mechanism.</p><p>We adapted the attention model from <ref type="bibr" target="#b1">[2]</ref> which, to produce each output symbol B t , uses an attention mechanism over the encoder LSTM states. Similar to our sequence-to-sequence model described in the previous section, we use two separate LSTMs (one to encode the sequence of input words A i , and another one to produce or decode the output symbols B i ). Recall that the encoder hidden states are denoted (h 1 , . . . , h T A ) and we denote the hidden states of the decoder by</p><formula xml:id="formula_2">(d 1 , . . . , d T B ) := (h T A +1 , . . . , h T A +T B ).</formula><p>To compute the attention vector at each output time t over the input words (1, . . . , T A ) we define:</p><formula xml:id="formula_3">u t i = v T tanh(W 1 h i + W 2 d t ) a t i = softmax(u t i ) d t = T A i=1 a t i h i</formula><p>The vector v and matrices W 1 , W 2 are learnable parameters of the model. The vector u t has length T A and its i-th item contains a score of how much attention should be put on the i-th hidden encoder state h i . These scores are normalized by softmax to create the attention mask a t over encoder hidden states. In all our experiments, we use the same hidden dimensionality (256) at the encoder and the decoder, so v is a vector and W 1 and W 2 are square matrices. Lastly, we concatenate d t with d t , which becomes the new hidden state from which we make predictions, and which is fed to the next time step in our recurrent model.</p><p>In Section 4 we provide an analysis of what the attention mechanism learned, and we visualize the normalized attention vector a t for all t in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linearizing Parsing Trees</head><p>To apply the model described above to parsing, we need to design an invertible way of converting the parse tree into a sequence (linearization). We do this in a very simple way following a depth-first traversal order, as depicted in <ref type="figure">Figure 2</ref>.</p><p>We use the above model for parsing in the following way. First, the network consumes the sentence in a left-to-right sweep, creating vectors in memory. Then, it outputs the linearized parse tree using information in these vectors. As described below, we use 3 LSTM layers, reverse the input sentence and normalize part-of-speech tags. An example run of our LSTM+A model on the sentence "Go." is depicted in <ref type="figure">Figure 1</ref> (top gray edges illustrate attention).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameters and Initialization</head><p>Sizes. In our experiments we used a model with 3 LSTM layers and 256 units in each layer, which we call LSTM+A. Our input vocabulary size was 90K and we output 128 symbols.</p><p>Dropout. Training on a small dataset we additionally used 2 dropout layers, one between LSTM 1 and LSTM 2 , and one between LSTM 2 and LSTM 3 . We call this model LSTM+A+D.</p><p>POS-tag normalization. Since part-of-speech (POS) tags are not evaluated in the syntactic parsing F1 score, we replaced all of them by "XX" in the training data. This improved our F1 score by about 1 point, which is surprising: For standard parsers, including POS tags in training data helps significantly. All experiments reported below are performed with normalized POS tags.</p><p>Input reversing. We also found it useful to reverse the input sentences but not their parse trees, similarly to <ref type="bibr" target="#b0">[1]</ref>. Not reversing the input had a small negative impact on the F1 score on our development set (about 0.2 absolute). All experiments reported below are performed with input reversing.</p><p>Pre-training word vectors. The embedding layer for our 90K vocabulary can be initialized randomly or using pre-trained word-vector embeddings. We pre-trained skip-gram embeddings of size 512 using word2vec <ref type="bibr" target="#b5">[6]</ref> on a 10B-word corpus. These embeddings were used to initialize our network but not fixed, they were later modified during training. We discuss the impact of pre-training in the experimental section.</p><p>We do not apply any other special preprocessing to the data. In particular, we do not binarize the parse trees or handle unaries in any specific way. We also treat unknown words in a naive way: we map all words beyond our 90K vocabulary to a single UNK token. This potentially underestimates our final results, but keeps our framework task-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data</head><p>We trained the model described above on 2 different datasets. For one, we trained on the standard WSJ training dataset. This is a very small training set by neural network standards, as it contains only 40K sentences (compared to 60K examples even in MNIST). Still, even training on this set, we managed to get results that match those obtained by domain-specific parsers.</p><p>To match state-of-the-art, we created another, larger training set of ∼11M parsed sentences (250M tokens). First, we collected all publicly available treebanks. We used the OntoNotes corpus version 5 <ref type="bibr" target="#b6">[7]</ref>, the English Web Treebank <ref type="bibr" target="#b7">[8]</ref> and the updated and corrected Question Treebank <ref type="bibr" target="#b8">[9]</ref>. 1 Note that the popular Wall Street Journal section of the Penn Treebank <ref type="bibr" target="#b9">[10]</ref> is part of the OntoNotes corpus. In total, these corpora give us ∼90K training sentences (we held out certain sections for evaluation, as described below).</p><p>In addition to this gold standard data, we use a corpus parsed with existing parsers using the "tri-training" approach of <ref type="bibr" target="#b10">[11]</ref>. In this approach, two parsers, our reimplementation of BerkeleyParser <ref type="bibr" target="#b11">[12]</ref> and a reimplementation of ZPar <ref type="bibr" target="#b12">[13]</ref>, are used to process unlabeled sentences sampled from news appearing on the web. We select only sentences for which both parsers produced the same parse tree and re-sample to match the distribution of sentence lengths of the WSJ training corpus. Re-sampling is useful because parsers agree much more often on short sentences. We call the set of ∼11 million sentences selected in this way, together with the ∼90K golden sentences described above, the high-confidence corpus.</p><p>After creating this corpus, we made sure that no sentence from the development or test set appears in the corpus, also after replacing rare words with "unknown" tokens. This operation guarantees that we never see any test sentence during training, but it also lowers our F1 score by about 0.5 points. We are not sure if such strict de-duplication was performed in previous works, but even with this, we still match state-of-the-art.  <ref type="bibr" target="#b15">[16]</ref> semi-supervised 92.4 92.1 In earlier experiments, we only used one parser, our reimplementation of BerkeleyParser, to create a corpus of parsed sentences. In that case we just parsed ∼7 million senteces from news appearing on the web and combined these parsed sentences with the ∼90K golden corpus described above. We call this the BerkeleyParser corpus.</p><note type="other">Set WSJ 22 WSJ 23 baseline LSTM+D WSJ only &lt; 70 &lt; 70 LSTM+A+D WSJ only 88.7 88.3 LSTM+A+D ensemble WSJ only 90.7 90.5 baseline LSTM BerkeleyParser corpus 91.0 90.5 LSTM+A high-confidence corpus 92.8 92.1 Petrov et al. (2006) [12] WSJ only 91.1 90.4 Zhu et al. (2013) [13] WSJ only N/A 90.4 Petrov et al. (2010) ensemble [14] WSJ only 92.5 91.8 Zhu et al. (2013) [13] semi-supervised N/A 91.3 Huang &amp; Harper (2009) [15] semi-supervised N/A 91.3 McClosky et al. (2006)</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>We use the standard EVALB tool 2 for evaluation and report F1 scores on our developments set (section 22 of the Penn Treebank) and the final test set (section 23) in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>First, let us remark that our training setup differs from those reported in previous works. To the best of our knowledge, no standard parsers have ever been trained on datasets numbering in the hundreds of millions of tokens, and it would be hard to do due to efficiency problems. We therefore cite the semi-supervised results, which are analogous in spirit but use less data. <ref type="table" target="#tab_1">Table 1</ref> shows performance of our models on the top and results from other papers at the bottom. We compare to variants of the BerkeleyParser that use self-training on unlabeled data <ref type="bibr" target="#b14">[15]</ref>, or built an ensemble of multiple parsers <ref type="bibr" target="#b13">[14]</ref>, or combine both techniques. We also include the best linear-time parser in the literature, the transition-based parser of <ref type="bibr" target="#b12">[13]</ref>.</p><p>It can be seen that, when training on WSJ only, a baseline LSTM does not achieve any reasonable score, even with dropout and early stopping. But a single attention model gets to 88.3 and an ensemble of 5 LSTM+A+D models achieves 90.5 matching a single-model BerkeleyParser on WSJ 23. When trained on the large high-confidence corpus, a single LSTM+A model achieves 92.1 and so matches the best previous single model result.</p><p>Generating well-formed trees. The LSTM+A model trained on WSJ dataset only produced malformed trees for 25 of the 1700 sentences in our development set (1.5% of all cases), and the model trained on full high-confidence dataset did this for 14 sentences (0.8%). In these few cases where LSTM+A outputs a malformed tree, we simply add brackets to either the beginning or the end of the tree in order to make it balanced. It is worth noting that all 14 cases where LSTM+A produced unbalanced trees were sentences or sentence fragments that did not end with proper punctuation. There were very few such sentences in the training data, so it is not a surprise that our model cannot deal with them very well.</p><p>Score by sentence length. An important concern with the sequence-to-sequence LSTM was that it may not be able to handle long sentences well. We determine the extent of this problem by partitioning the development set by length, and evaluating BerkeleyParser, a baseline LSTM model without attention, and LSTM+A on sentences of each length. The results, presented in <ref type="figure" target="#fig_2">Figure 3</ref>, are surprising. The difference between the F1 score on sentences of length upto 30 and that upto 70 is 1.3 for the BerkeleyParser, 1.7 for the baseline LSTM, and 0.7 for LSTM+A. So already the baseline LSTM has similar performance to the BerkeleyParser, it degrades with length only slightly. Surprisingly, LSTM+A shows less degradation with length than BerkeleyParser -a full O(n 3 ) chart parser that uses a lot more memory. Beam size influence. Our decoder uses a beam of a fixed size to calculate the output sequence of labels. We experimented with different settings for the beam size. It turns out that it is almost irrelevant. We report report results that use beam size 10, but using beam size 2 only lowers the F1 score of LSTM+A on the development set by 0.2, and using beam size 1 lowers it by 0.5. Beam sizes above 10 do not give any additional improvements.</p><p>Dropout influence. We only used dropout when training on the small WSJ dataset and its influence was significant. A single LSTM+A model only achieved an F1 score of 86.5 on our development set, that is over 2 points lower than the 88.7 of a LSTM+A+D model. Pre-training influence. As described in the previous section, we initialized the word-vector embedding with pre-trained word vectors obtained from word2vec. To test the influence of this initialization, we trained a LSTM+A model on the high-confidence corpus, and a LSTM+A+D model on the WSJ corpus, starting with randomly initialized word-vector embeddings. The F1 score on our development set was 0.4 lower for the LSTM+A model and 0.3 lower for the LSTM+A+D model (88.4 vs 88.7). So the effect of pre-training is consistent but small.</p><p>Performance on other datasets. The WSJ evaluation set has been in use for 20 years and is commonly used to compare syntactic parsers. But it is not representative for text encountered on the web <ref type="bibr" target="#b7">[8]</ref>. Even though our model was trained on a news corpus, we wanted to check how well it generalizes to other forms of text. To this end, we evaluated it on two additional datasets: QTB 1000 held-out sentences from the Question Treebank <ref type="bibr" target="#b8">[9]</ref>; WEB the first half of each domain from the English Web Treebank <ref type="bibr" target="#b7">[8]</ref> (8310 sentences).</p><p>LSTM+A trained on the high-confidence corpus (which only includes text from news) achieved an F1 score of 95.7 on QTB and 84.6 on WEB. Our score on WEB is higher both than the best score reported in <ref type="bibr" target="#b7">[8]</ref> (83.5) and the best score we achieved with an in-house reimplementation of BerkeleyParser trained on human-annotated data <ref type="bibr">(84.4)</ref>. We managed to achieve a slightly higher score (84.8) with the in-house BerkeleyParser trained on a large corpus. On QTB, the 95.7 score of LSTM+A is also lower than the best score of our in-house BerkeleyParser (96.2). Still, taking into account that there were only few questions in the training data, these scores show that LSTM+A managed to generalize well beyond the news language it was trained on.</p><p>Parsing speed. Our LSTM+A model, running on a multi-core CPU using batches of 128 sentences on a generic unoptimized decoder, can parse over 120 sentences from WSJ per second for sentences of all lengths (using beam-size 1). This is better than the speed reported for this batch size in <ref type="figure" target="#fig_3">Figure  4</ref> of <ref type="bibr" target="#b16">[17]</ref> at 100 sentences per second, even though they run on a GPU and only on sentences of under 40 words. Note that they achieve 89.7 F1 score on this subset of sentences of section 22, while our model at beam-size 1 achieves a score of 93.2 on this subset. On the bottom, we show outputs for four consecutive time steps, where the attention mask moves to the right. As can be seen, every time a terminal node is consumed, the attention pointer moves to the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>As shown in this paper, the attention mechanism was a key component especially when learning from a relatively small dataset. We found that the model did not overfit and learned the parsing function from scratch much faster, which resulted in a model which generalized much better than the plain LSTM without attention.</p><p>One of the most interesting aspects of attention is that it allows us to visualize to interpret what the model has learned from the data. For example, in <ref type="bibr" target="#b1">[2]</ref> it is shown that for translation, attention learns an alignment function, which certainly should help translating from English to French. <ref type="figure" target="#fig_3">Figure 4</ref> shows an example of the attention model trained only on the WSJ dataset. From the attention matrix, where each column is the attention vector over the inputs, it is clear that the model focuses quite sharply on one word as it produces the parse tree. It is also clear that the focus moves from the first word to the last monotonically, and steps to the right deterministically when a word is consumed.</p><p>On the bottom of <ref type="figure" target="#fig_3">Figure 4</ref> we see where the model attends (black arrow), and the current output being decoded in the tree (black circle). This stack procedure is learned from data (as all the parameters are randomly initialized), but is not quite a simple stack decoding. Indeed, at the input side, if the model focuses on position i, that state has information for all words after i (since we also reverse the inputs). It is worth noting that, in some examples (not shown here), the model does skip words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The task of syntactic constituency parsing has received a tremendous amount of attention in the last 20 years. Traditional approaches to constituency parsing rely on probabilistic context-free grammars (CFGs). The focus in these approaches is on devising appropriate smoothing techniques for highly lexicalized and thus rare events <ref type="bibr" target="#b17">[18]</ref> or carefully crafting the model structure <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b11">[12]</ref> partially alleviate the heavy reliance on manual modeling of linguistic structure by using latent variables to learn a more articulated model. However, their model still depends on a CFG backbone and is thereby potentially restricted in its capacity.</p><p>Early neural network approaches to parsing, for example by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> also relied on strong linguistic insights. <ref type="bibr" target="#b21">[22]</ref> introduced Incremental Sigmoid Belief Networks for syntactic parsing. By constructing the model structure incrementally, they are able to avoid making strong independence assumptions but inference becomes intractable. To avoid complex inference methods, <ref type="bibr" target="#b22">[23]</ref> propose a recurrent neural network where parse trees are decomposed into a stack of independent levels. Unfortunately, this decomposition breaks for long sentences and their accuracy on longer sentences falls quite significantly behind the state-of-the-art. <ref type="bibr" target="#b23">[24]</ref> used a tree-structured neural network to score candidate parse trees. Their model however relies again on the CFG assumption and furthermore can only be used to score candidate trees rather than for full inference.</p><p>Our LSTM model significantly differs from all these models, as it makes no assumptions about the task. As a sequence-to-sequence prediction model it is somewhat related to the incremental parsing models, pioneered by <ref type="bibr" target="#b24">[25]</ref> and extended by <ref type="bibr" target="#b25">[26]</ref>. Such linear time parsers however typically need some task-specific constraints and might build up the parse in multiple passes. Relatedly, <ref type="bibr" target="#b12">[13]</ref> present excellent parsing results with a single left-to-right pass, but require a stack to explicitly delay making decisions and a parsing-specific transition strategy in order to achieve good parsing accuracies. The LSTM in contrast uses its short term memory to model the complex underlying structure that connects the input-output pairs.</p><p>Recently, researchers have developed a number of neural network models that can be applied to general sequence-to-sequence problems. <ref type="bibr" target="#b26">[27]</ref> was the first to propose a differentiable attention mechanism for the general problem of handwritten text synthesis, although his approach assumed a monotonic alignment between the input and output sequences. Later, <ref type="bibr" target="#b1">[2]</ref> introduced a more general attention model that does not assume a monotonic alignment, and applied it to machine translation, and <ref type="bibr" target="#b27">[28]</ref> applied the same model to speech recognition. <ref type="bibr" target="#b28">[29]</ref> used a convolutional neural network to encode a variable-sized input sentence into a vector of a fixed dimension and used a RNN to produce the output sentence. Essentially the same model has been used by <ref type="bibr" target="#b29">[30]</ref> to successfully learn to generate image captions. Finally, already in 1990 <ref type="bibr" target="#b30">[31]</ref> experimented with applying recurrent neural networks to the problem of syntactic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we have shown that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing with relatively little effort or tuning. In addition, while we found the model of Sutskever et al. <ref type="bibr" target="#b0">[1]</ref> to not be particularly data efficient, the attention model of Bahdanau et al. <ref type="bibr" target="#b1">[2]</ref> was found to be highly data efficient, as it has matched the performance of the BerkeleyParser when trained on a small human-annotated parsing dataset. Finally, we showed that synthetic datasets with imperfect labels can be highly useful, as our models have substantially outperformed the models that have been used to create their training data. We suspect it is the case due to the different natures of the teacher model and the student model: the student model has likely viewed the teacher's errors as noise which it has been able to ignore. This approach was so successful that we obtained a new state-of-the-art result in syntactic constituency parsing with a single attention model, which also means that the model is exceedingly fast. This work shows that domain independent models with excellent learning algorithms can match and even outperform domain specific models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Example parsing task and its linearization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of sentence length on the F1 score on WSJ 22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attention matrix. Shown on top is the attention matrix where each column is the attention vector over the inputs. On the bottom, we show outputs for four consecutive time steps, where the attention mask moves to the right. As can be seen, every time a terminal node is consumed, the attention pointer moves to the right.</figDesc><graphic url="image-1.png" coords="7,157.50,81.86,336.61,336.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>F1 scores of various parsers on the development and test set. See text for discussion.</figDesc><table></table></figure>

			<note place="foot" n="1"> All treebanks are available through the Linguistic Data Consortium (LDC): OntoNotes (LDC2013T19), English Web Treebank (LDC2012T13) and Question Treebank (LDC2012R121).</note>

			<note place="foot" n="2"> http://nlp.cs.nyu.edu/evalb/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8206</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2007</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL. ACL</title>
		<imprint>
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<title level="m">Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Questionbank: Creating a corpus of parse-annotated questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCL &amp; ACL&apos;06</title>
		<meeting>ICCL &amp; ACL&apos;06</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ambiguity-aware ensemble training for semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL&apos;14</title>
		<meeting>ACL&apos;14</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. ACL</title>
		<imprint>
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and accurate shift-reduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. ACL</title>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-training PCFG grammars with latent annotations across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL. ACL</title>
		<imprint>
			<date type="published" when="2006-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparser, better, faster gpu parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three generative, lexicalised models for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the ACL</title>
		<meeting>the 35th Annual Meeting of the ACL</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="1997-07" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>423-430. ACL</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the ACL</title>
		<meeting>the 41st Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2003-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inducing history representations for broad coverage statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the ACL (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the ACL (ACL&apos;04), Main Volume</meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Constituent parsing with incremental sigmoid belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. ACL</title>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A linear observed time statistical parser based on maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the ACL (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the ACL (ACL&apos;04), Main Volume</meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent nn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">First results. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A neural network for learning how to parse tree adjoining grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">B.S.Eng Thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
