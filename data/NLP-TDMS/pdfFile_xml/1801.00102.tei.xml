<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
							<email>at.luu@i2r.a-star.edu.sg*asschui@ntu.edu.sgφ</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<orgName type="institution">A*Star</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><surname>Cheung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">†φ Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameteri-zation of our model also enjoys a ≈ 3 times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly inter-pretable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Inference (NLI) is a pivotal and fundamental task in language understanding and artificial intelligence. More concretely, given a premise and hypothesis, NLI aims to detect whether the latter entails or contradicts the former. As such, NLI is also commonly known as Recognizing Textual Entailment (RTE). NLI is known to be a significantly challenging task for machines whose success often depends on a wide repertoire of reasoning techniques.</p><p>In recent years, we observe a steep improvement in NLI systems, largely contributed by the release of the largest publicly available corpus for NLI -the Stanford Natural Language Inference (SNLI) corpus <ref type="bibr" target="#b2">(Bowman et al., 2015</ref>) which comprises 570K hand labeled sentence pairs. This has improved the feasibility of training complex neural models, given the fact that neural models often require a relatively large amount of training data.</p><p>Highly competitive neural models for NLI are mostly based on soft-attention alignments, popularized by <ref type="bibr" target="#b18">(Parikh et al., 2016;</ref><ref type="bibr" target="#b22">Rocktäschel et al., 2015)</ref>. The key idea is to learn an alignment of sub-phrases in both sentences and learn to compare the relationship between them. Standard feed-forward neural networks are commonly used to model similarity between aligned (decomposed) sub-phrases and then aggregated into the final prediction layers.</p><p>Alignment between sentences has become a staple technique in NLI research and many recent state-of-the-art models such as the Enhanced Sequential Inference Model (ESIM) <ref type="bibr" target="#b4">(Chen et al., 2017b</ref>) also incorporate the alignment strategy. The difference here is that ESIM considers a nonparameterized comparison scheme, i.e., concatenating the subtraction and element-wise product of aligned sub-phrases, along with two original sub-phrases, into the final comparison vector. A bidirectional LSTM is then used to aggregate the compared alignment vectors.</p><p>This paper presents a new neural model for NLI. There are several new novel components in our work. Firstly, we propose a compare, compress and propagate (ComProp) architecture where compressed alignment features are propagated to upper layers (such as a RNN-based encoder) for enhancing representation learning. Secondly, in order to achieve an efficient propagation of alignment features, we propose alignment factorization layers to reduce each alignment vector to a single scalar valued feature. Each scalar valued feature is used to augment the base word representation, allowing the subsequent RNN encoder layers to benefit from not only global but also cross sentence information.</p><p>There are several major advantages to our proposed architecture. Firstly, our model is relatively compact, i.e., we compress alignment feature vectors and augment them to word representations instead. This is to avoid large alignment (or match) vectors being propagated across the network. As a result, our model is more parameter efficient compared to ESIM since the width of the middle layers of the network is now much smaller. To the best of our knowledge, this is the first work that explicitly employs such a paradigm.</p><p>Secondly, the explicit usage of compression enables improved interpretabilty since each alignment pair is compressed to a scalar and hence, can be easily visualised. Previous models such as ESIM use subtractive operations on alignment vectors, edging on the intuition that these vectors represent contradiction. Our model is capable of visually demonstrating this phenomena. As such, our design choice enables a new way of deriving insight from neural NLI models.</p><p>Thirdly, the alignment factorization layer is expressive and powerful, combining ideas from standard machine learning literature <ref type="bibr" target="#b21">(Rendle, 2010)</ref> with modern neural NLI models. The factorization layer tries to decompose the alignment vector (constructed from the variations of a − b, a b and [a; b]), learning higher-order feature interactions between each compared alignment. In other words, it models the second-order (pairwise) interactions between each feature in every alignment vector using factorized parameters, allowing more expressive comparison to be made over traditional fully-connected layers (FC). Moreover, factorization-based models are also known to be able to model low-rank structure and reduce risks of overfitting. The effectiveness of the factorization alignment over alternative baselines such as feed-forward neural networks is confirmed by early experiments.</p><p>The major contributions of this work are summarized as follows:</p><p>• We introduce a Compare, Compress and Propagate (ComProp) architecture for NLI.</p><p>The key idea is to use the myriad of generated comparison vectors for augmentation of the base word representation instead of simply aggregating them for prediction. Subsequently, a standard compositional encoder can then be used to learn representations from the augmented word representations. We show that we are able to derive meaningful insight from visualizing these augmented features.</p><p>• For the first time, we adopt expressive factorization layers to model the relationships between soft-aligned sub-phrases of sentence pairs. Empirical experiments confirm the effectiveness of this new layer over standard fully connected layers.</p><p>• Overall, we propose a new neural model -CAFE (ComProp Alignment-Factorized Encoders) for NLI. Our model achieves stateof-the-art performance on SNLI, MultiNLI and the new SciTail dataset, outperforming existing state-of-the-art models such as ESIM. Ablation studies confirm the effectiveness of each proposed component in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Natural language inference (or textual entailment recognition) is a long standing problem in NLP research, typically carried out on smaller datasets using traditional methods <ref type="bibr" target="#b14">(Maccartney, 2009;</ref><ref type="bibr">Da- gan et al., 2006;</ref><ref type="bibr" target="#b15">MacCartney and Manning, 2008;</ref><ref type="bibr" target="#b11">Iftene and Balahur-Dobrescu, 2007</ref>). The relatively recent creation of 570K human annotated sentence pairs <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref> have spurred on many recent works that use neural networks for NLI. Many advanced neural architectures have been proposed for the NLI task, with most exploiting some variants of neural attention which learn to pay attention to important segments in a sentence ( <ref type="bibr" target="#b18">Parikh et al., 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2017b;</ref><ref type="bibr" target="#b27">Wang and Jiang, 2016b;</ref><ref type="bibr" target="#b22">Rocktäschel et al., 2015;</ref><ref type="bibr">Yu and Munkhdalai, 2017a)</ref>.</p><p>Amongst the myriad of neural architectures proposed for NLI, the ESIM ( <ref type="bibr" target="#b4">Chen et al., 2017b)</ref> model is one of the best performing models. The ESIM, primarily motivated by soft subphrase alignment in ( <ref type="bibr" target="#b18">Parikh et al., 2016)</ref>, learns alignments between BiLSTM encoded representations and aggregates them with another BiLSTM layer. The authors also propose the usage of subtractive composition, claiming that this helps model contradictions amongst alignments.</p><p>Compare-Aggregate models are also highly popular in NLI tasks. While this term was coined by ( <ref type="bibr" target="#b26">Wang and Jiang, 2016a</ref>), many prior NLI models follow this design ( <ref type="bibr" target="#b28">Wang et al., 2017;</ref><ref type="bibr" target="#b18">Parikh et al., 2016;</ref><ref type="bibr" target="#b10">Gong et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2017b</ref>).</p><p>The key idea is to aggregate matching features and pass them through a dense layer for prediction. ( <ref type="bibr" target="#b28">Wang et al., 2017)</ref> proposed BiMPM, which adopts multi-perspective cosine matching across sequence pairs. ( <ref type="bibr" target="#b26">Wang and Jiang, 2016a</ref>) proposed a one-way attention and convolutional aggregation layer. ( <ref type="bibr" target="#b10">Gong et al., 2017</ref>) learns representations with highway layers and adopts ResNet for learning features over an interaction matrix.</p><p>There are several other notable models for NLI. For instance, models that leverage directional selfattention ( <ref type="bibr" target="#b24">Shen et al., 2017)</ref> or Gumbel-Softmax ( <ref type="bibr" target="#b8">Choi et al., 2017)</ref>. DGEM is a graph based attention model which was proposed together with a new entailment challenge dataset, SciTail <ref type="bibr" target="#b12">(Khot et al., 2018)</ref>. Pretraining have been known to also be highly useful in the NLI task. For instance, contextualized vectors learned from machine translation ( <ref type="bibr" target="#b16">McCann et al., 2017</ref>) (CoVe) or language modeling ( <ref type="bibr" target="#b20">Peters et al., 2018</ref>) (ELMo) have showned to be able to improve performance when integrated with existing NLI models.</p><p>Our work compares and compresses alignment pairs using factorization layers which leverages the rich history of standard machine learning literature. Our factorization layers incorporate highly expressive factorization machines (FMs) <ref type="bibr" target="#b21">(Rendle, 2010)</ref> into neural NLI models. In standard machine learning tasks, FMs remain a very competitive choice for learning feature interactions ( <ref type="bibr">Xiao et al., 2017</ref>) for both standard classification and regression problems. Intuitively, FMs are adept at handling data sparsity (typically interactions) by using factorized parameters to approximate a feature matching matrix. This makes it suitable in our model architecture since feature interaction between subphrase alignment pairs is typically very sparse as well.</p><p>A recent work <ref type="bibr" target="#b1">(Beutel et al., 2018</ref>) reports an interesting empirical study pertaining to the ability of standard FC layers and their ability to model 'cross features' (or multiplicative features). Their overall finding suggests that while standard ReLU FC layers are able to approximate 2-way or 3-way features, they are extremely inefficient in doing so (requiring either very wide or deep layers). This further motivates the usage of FMs in this work and is well aligned with our empirical results, i.e., strong competitive performance with reasonably small parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Model</head><p>In this section, we provide a layer-by-layer description of our model architecture. Our model accepts two sentences as an input, i.e., P (premise) and H (hypothesis). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates a highlevel overview of our proposed model architecture.</p><p>- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Encoding Layer</head><p>This layer aims to learn a k-dimensional representation for each word. Following ( <ref type="bibr" target="#b10">Gong et al., 2017)</ref>, we learn feature-rich word representations by concatenating word embeddings, character embeddings and syntactic (part-of-speech tag) embeddings (provided in the datasets). Character representations are learned using a convolutional encoder with max pooling function and is commonly used in many relevant literature ( <ref type="bibr" target="#b28">Wang et al., 2017;</ref><ref type="bibr" target="#b5">Chen et al., 2017c</ref>).</p><p>Highway Encoder Subsequently, we pass each concatenated word vector into a two layer highway network ( <ref type="bibr" target="#b25">Srivastava et al., 2015</ref>) in order to learn a k-dimensional representation. Highway networks are gated projection layers which learn adaptively control how much information is being carried to the next layer. Our strategy is similar to ( <ref type="bibr" target="#b18">Parikh et al., 2016</ref>) which trains the projection layer in place of tuning the embedding matrix. The usage of highway layers over standard projection layers is empirically motivated. However, an intuition would be that the gates in this layer adapt to learn the relative importance of each word to the NLI task. Let H(.) and T (.) be single layered affine transforms with ReLU and sigmoid activation functions respectively. A single highway network layer is defined as:</p><formula xml:id="formula_0">y = H(x, W H ) · T (x, W T ) + C · x<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">C = (1−T (x, W T )) and W H , W T ∈ R r×d</formula><p>Notably, the dimensions of the affine transform might be different from the size of the input vector.</p><p>In this case, an additional nonlinear transform is used to project x to the same dimensionality. The output of this layer is ¯ P ∈ R k× P (premise) and ¯ H ∈ R k× H (hypothesis), with each word converted to a r-dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Soft-Attention Alignment Layer</head><p>This layer describes two soft-attention alignment techniques that are used in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-Attention Alignment Layer</head><p>This layer learns an alignment of sub-phrases between ¯ P and ¯ H. Let F (.) be a standard projection layer with ReLU activation function. The alignment matrix of two sequences is defined as follows:</p><formula xml:id="formula_2">e ij = F (¯ p i ) · F ( ¯ h j )<label>(2)</label></formula><p>where E ∈ R p× h and ¯ p i , ¯ h j are the i-th and j-th word in the premise and hypothesis respectively.</p><formula xml:id="formula_3">β i = p j=1 exp(e ij ) p k=1 exp(e ik ) ¯ p j (3) α j = h i=1 exp(e ij ) h k=1 exp(e kj ) ¯ h i<label>(4)</label></formula><p>where β i is the sub-phrase in ¯ P that is softly aligned to h i . Intuitively, β i is a weighted sum across {p j } p j=1 , selecting the most relevant parts of ¯ P to represent h i .</p><p>Intra-Attention Alignment Layer This layer learns a self-alignment of sentences and is applied to both ¯ P and ¯ H independently. For the sake of brevity, let ¯ S represent either ¯ P or ¯ H, the intraattention alignment is computed as:</p><formula xml:id="formula_4">s i = p j=1 exp(f ij ) p k=1 exp(f ik ) ¯ s j<label>(5)</label></formula><p>where f ij = G(¯ s i ) · G(¯ s j ) and G(.) is a nonlinear projection layer with ReLU activation function. The intra-attention layer models similarity of each word with respect to the entire sentence, capturing long distance dependencies and 'global' context of the entire sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Alignment Factorization Layer</head><p>This layer aims to learn a scalar valued feature for each comparison between aligned sub-phrases. Firstly, we introduce our factorization operation, which lives at the core of our neural model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factorization Operation</head><p>Given an input vector x, the factorization operation (Rendle, 2010) is defined as:</p><formula xml:id="formula_5">Z(x) = w 0 + n i=1 w i x i + n i=1 n j=i+1 v i , v j x i x j<label>(6)</label></formula><p>where Z(x) is a scalar valued output, .; . is the dot product between two vectors and w 0 is the global bias. Factorization machines model lowrank structure within the matching vector producing a scalar feature. The parameters of this layer are w 0 ∈ R, w ∈ R r and v ∈ R r×k . The first term n i=1 w i x i is simply a linear term. The second term n i=1 n j=i+1 v i , v j x i x j captures all pairwise interactions in x (the input vector) using the factorization of matrix v.</p><p>Inter-Alignment Factorization This operation compares the alignment between inter-attention aligned representations, i.e., (β i , h i ) and (α j , p j ). Let (a, b) represent an alignment pair, we apply the following operations:</p><formula xml:id="formula_6">y c = Z([a; b]) ; y s = Z(a − b) ; y m = Z(a b)<label>(7)</label></formula><p>where y c , y s , y m ∈ R, Z(.) is the factorization operation, <ref type="bibr">[.; .]</ref> is the concatenation operator and is the element-wise multiplication. The intuition of modeling subtraction is targeted at capturing contradiction. However, instead of simply concatenating the extra comparison vectors, we compress them using the factorization operation. Finally, for each alignment pair, we obtain three scalar-valued features which map precisely to a word in the sequence.</p><p>Intra-Alignment Factorization Next, for each sequence, we also apply alignment factorization on the intra-aligned sentences. Let (s, s ) represent an intra-aligned pair from either the premise or hypothesis, we compute the following operations:</p><formula xml:id="formula_7">v c = Z([s; s ]) ; v s = Z(s − s ) ; v m = Z(s s )<label>(8)</label></formula><p>where v c , v s , v m ∈ R and Z(.) is the factorization operation. Applying alignment factorization to intra-aligned representations produces another three scalar-valued features which are mapped to each word in the sequence. Note that each of the six factorization operations has its own parameters but shares them amongst all words in the sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Propagation and Augmentation</head><p>Finally, the six factorized features are then aggregated 1 via concatenation to form a final feature vector that is propagated to upper representation learning layers via augmentation of the word representation ¯ P or ¯ H.</p><formula xml:id="formula_8">u i = [s i ; f i intra ; f i inter ]<label>(9)</label></formula><p>where s i is i-th word in ¯ P or ¯ H, and f i intra and f i inter are the intra-aligned [v c ; v s ; v m ] and interaligned [y c ; y s ; y m ] features for the i-th word in the sequence respectively. Intuitively, f i intra augments each word with global knowledge of the sentence and f i inter augments each word with crosssentence knowledge via inter-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Sequential Encoder Layer</head><p>For each sentence, the augmented word representations u 1 , u 2 , . . . u are then passed into a sequential encoder layer. We adopt a standard vanilla LSTM encoder.</p><formula xml:id="formula_9">h i = LST M (u, i), ∀i ∈ [1, . . . ]<label>(10)</label></formula><p>where represents the maximum length of the sequence. Notably, the parameters of the LSTM are siamese in nature, sharing weights between both premise and hypothesis. We do not use a bidirectional LSTM encoder, as we found that it did not lead to any improvements on the held-out set. A logical explanation would be because our word representations are already augmented with global (intra-attention) information. As such, modeling in the reverse direction is unnecessary, resulting in some computational savings.</p><p>Pooling Layer Next, to learn an overall representation of each sentence, we apply a pooling function across all hidden outputs of the sequential encoder. The pooling function is a concatenation of temporal max and average (avg) pooling.</p><formula xml:id="formula_10">x = [max([h 1 , · · · h ]); avg([h 1 , · · · h ])] (11)</formula><p>where x is a final 2k-dimensional representation of the sentence (premise or hypothesis). We also experimented with sum and avg standalone poolings and found sum pooling to be relatively competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Prediction Layer</head><p>Finally, given a fixed dimensional representation of the premise x p and hypothesis x h , we pass their concatenation into a two-layer h-dimensional highway network. Since the highway network has been already defined earlier, we omit the technical details here. The final prediction layer of our model is computed as follows:</p><formula xml:id="formula_11">y out = H 2 (H 1 ([x p ; x h ; x p x h ; x p − x h ]))<label>(12)</label></formula><p>where H 1 (.), H 2 (.) are highway network layers with ReLU activation. The output is then passed into a final linear softmax layer.</p><formula xml:id="formula_12">y pred = sof tmax(W F · y out + b F )<label>(13)</label></formula><p>where W F ∈ R h×3 and b F ∈ R 3 . The network is then trained using standard multi-class cross entropy loss with L2 regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe our experimental setup and report our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Params Train Test Single Model (w/o Cross Sentence Attention) 300D Gumbel TreeLSTM ( <ref type="bibr" target="#b8">Choi et al., 2017)</ref> 2.9M 91.2 85.6 300D DISAN <ref type="bibr" target="#b24">(Shen et al., 2017)</ref> 2.4M 91.1 85.6 300D Residual Stacked Encoders ( <ref type="bibr" target="#b17">Nie and Bansal, 2017)</ref> 9.7M 89.8 85.7 600D Gumbel TreeLSTM ( <ref type="bibr" target="#b8">Choi et al., 2017)</ref> 10M 93.1 86.0 300D CAFE (w/o CA)</p><p>3.7M 87.3 85.9 Single Models 100D LSTM with attention <ref type="bibr" target="#b22">(Rocktäschel et al., 2015)</ref> 250K 85.3 83.5 300D mLSTM ( <ref type="bibr" target="#b27">Wang and Jiang, 2016b)</ref> 1.9M 92.0 86.1 450D LSTMN + deep att. fusion <ref type="figure" target="#fig_0">(Cheng et al., 2016)</ref> 3.4M 88.5 86.3 200D DecompAtt + Intra-Att ( <ref type="bibr" target="#b18">Parikh et al., 2016</ref>) 580K 90.5 86.8 300D NTI-SLSTM-LSTM ( <ref type="bibr">Yu and Munkhdalai, 2017b)</ref> 3.2M 88.5 87.3 300D re-read LSTM <ref type="figure" target="#fig_0">(Sha et al., 2016)</ref> 2.0M 90.7 87.5 BiMPM ( <ref type="bibr" target="#b28">Wang et al., 2017)</ref> 1.6M 90.9 87.5 448D DIIN ( <ref type="bibr" target="#b10">Gong et al., 2017)</ref> 4.4M 91.2 88.0 600D ESIM ( <ref type="bibr" target="#b4">Chen et al., 2017b)</ref> 4.3M 92.6 88.0 150D CAFE (SUM+2x200D MLP) 750K 88.2 87.7 200D CAFE (SUM+2x400D MLP)</p><p>1.4M 89.4 88.1 300D CAFE <ref type="table" target="#tab_2">(SUM+2x600D MLP)</ref> 3.5M 89.2 88.3 300D CAFE <ref type="figure">(AVGMAX+300D HN)</ref> 4.7M 89.8 88.5 Ensemble Models 600D ESIM + 300D Tree-LSTM ( <ref type="bibr" target="#b4">Chen et al., 2017b)</ref> 7.7M 93.5 88.6 BiMPM ( <ref type="bibr" target="#b28">Wang et al., 2017)</ref> 6.4M 93.2 88.8 448D DIIN ( <ref type="bibr" target="#b10">Gong et al., 2017)</ref> 17.0M 92.3 88.9 300D CAFE <ref type="table">(Ensemble)</ref> 17.5M 92.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">89.3 External Resource Models</head><p>BiAttentive Classification + CoVe + Char ( <ref type="bibr" target="#b16">McCann et al., 2017)</ref> 22M 88.5 88.1 KIM <ref type="figure" target="#fig_0">(Chen et al., 2017a)</ref> 4.3M 94.1 88.6 ESIM + ELMo ( <ref type="bibr" target="#b20">Peters et al., 2018)</ref> 8.0M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="91.6">88.7 200D CAFE (AVGMAX + 200D MLP) + ELMo</head><p>1.4M 89.5 89.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>To ascertain the effectiveness of our models, we use the SNLI (Bowman et al., 2015) and MultiNLI ( <ref type="bibr">Williams et al., 2017</ref>) benchmarks which are standard and highly competitive benchmarks for the NLI task. We also include the newly released SciTail dataset ( <ref type="bibr" target="#b12">Khot et al., 2018</ref>) which is a binary entailment classification task constructed from science questions. Notably, SciTail is known to be a difficult dataset for NLI, made evident by the low accuracy scores even though it is binary in nature.</p><p>SNLI The state-of-the-art competitors on this dataset are the BiMPM ( <ref type="bibr" target="#b28">Wang et al., 2017</ref>), ESIM (Chen et al., 2017b) and DIIN ( <ref type="bibr" target="#b10">Gong et al., 2017)</ref>. We compare against competitors across three settings. The first setting disallows cross sentence attention. In the second setting, cross sentence is allowed. The last (third) setting is a comparison between model ensembles while the first two settings only comprise single models. Note that we consider the 1st setting to be relatively less important (since our focus is not on the encoder itself) but still report the results for completeness.</p><p>MultiNLI We compare on two test sets (matched and mismatched) which represent indomain and out-domain performance. The main competitor on this dataset is the ESIM model, a powerful state-of-the-art SNLI baseline. We also compare with ESIM + Read <ref type="bibr" target="#b29">(Weissenborn, 2017)</ref>.</p><p>SciTail This dataset only has one official setting. We compare against the reported results of ESIM ( <ref type="bibr" target="#b4">Chen et al., 2017b</ref>) and DecompAtt ( <ref type="bibr" target="#b18">Parikh et al., 2016</ref>) in the original paper. We also compare with DGEM, the new model proposed in ( <ref type="bibr" target="#b12">Khot et al., 2018)</ref>.</p><p>Across all experiments and in the spirit of fair comparison, we only compare with works that <ref type="formula" target="#formula_0">(1)</ref> do not use extra training data and (2) do not use external resources (such as external knowledge bases, etc.). However, for the sake of completeness, we still report their scores 2 ( <ref type="bibr" target="#b16">McCann et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2017a;</ref><ref type="bibr" target="#b20">Peters et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement our model in <ref type="bibr">TensorFlow (Abadi et al., 2015)</ref> and train them on Nvidia P100 GPUs. We use the Adam optimizer ( <ref type="bibr" target="#b13">Kingma and Ba, 2014</ref>) with an initial learning rate of 0.0003. L2 regularization is set to 10 −6 . Dropout with a keep probability of 0.8 is applied after each fullyconnected, recurrent or highway layer. The batch size is tuned amongst {128, 256, 512}. The number of latent factors k for the factorization layer is tuned amongst {5, 10, 50, 100, 150}. The size of the hidden layers of the highway network layers are set to 300. All parameters are initialized with xavier initialization. Word embeddings are preloaded with 300d GloVe embeddings ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>) and fixed during training. Sequence lengths are padded to batch-wise maximum. The batch order is (randomly) sorted within buckets following (Parikh et al., 2016). <ref type="table" target="#tab_0">Table 1</ref> reports our results on the SNLI benchmark. On the cross sentence (single model setting), the performance of our proposed CAFE model is extremely competitive. We report the test accuracy of CAFE at different extents of parameterization, i.e., varying the size of the LSTM encoder, width of the pre-softmax hidden layers and final pooling layer. CAFE obtains 88.5% accuracy on the SNLI test set, an extremely competitive score on the extremely popular benchmark. Notably, competitive results can be also achieved with a much smaller parameterization. For example, CAFE also achieves 88.3% and 88.1% test accuracy with only 3.5M and 1.5M parameters respectively. This outperforms the state-of-theart ESIM and DIIN models with only a fraction of the parameter cost. At 88.1%, our model has about three times less parameters than ESIM/DIIN (i.e., 1.4M versus 4.3M/4.4M). Moreover, our lightweight adaptation achieves 87.7% with only 750K parameters, which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model (86.8%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>Finally, an ensemble of 5 CAFE models achieves 89.3% test accuracy, the best test scores on the SNLI benchmark to date 3 . Overall, we believe that the good performance of our CAFE can be attributed to (1) the effectiveness of the ComProp architecture (i.e., providing word representations with global and local knowledge for better representation learning) and (2) the expressiveness of alignment factorization layers that are used to decompose and compare word alignments. More details are given at the ablation study. Finally, we emphasize that CAFE is also relatively lightweight, efficient and fast to train given its performance. A single run on SNLI takes approximately 5 minutes per epoch with a batch size of 256. Overall, a single run takes ≈ 3 hours to get to convergence.   <ref type="table" target="#tab_2">Table 2</ref> reports our results on the MultiNLI and SciTail datasets. On MultiNLI, CAFE significantly outperforms ESIM, a strong state-of-the-art model on both settings. We also outperform the ESIM + Read model <ref type="bibr" target="#b29">(Weissenborn, 2017</ref>). An ensemble of CAFE models achieve competitive re-sult on the MultiNLI dataset. On SciTail, our proposed CAFE model achieves state-of-the-art performance. The performance gain over strong baselines such as DecompAtt and ESIM are ≈ 10% − 13% in terms of accuracy. CAFE also outperforms DGEM, which uses a graph-based attention for improved performance, by a significant margin of 5%. As such, empirical results demonstrate the effectiveness of our proposed CAFE model on the challenging SciTail dataset.   <ref type="table" target="#tab_4">Table 3</ref> reports ablation studies on the MultiNLI development sets. In (1), we replaced all FM functions with regular full-connected (FC) layers in order to observe the effect of FM versus FC. More specifically, we experimented with several FC configurations as follows: (a) 1-layer linear, (b) 1-layer ReLU (c) 2-layer ReLU. The 1-layer linear setting performs the best and is therefore reported in <ref type="table" target="#tab_4">Table 3</ref>. Using ReLU seems to be worse than nonlinear FC layers. Overall, the best combination (option a) still experienced a decline in performance in both development sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultiNLI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In (2-3), we explore the utility of using character and syntactic embeddings, which we found to have helped CAFE marginally. In (4), we remove the inter-attention alignment features, which naturally impact the model performance significantly. In (5-6), we explore the effectiveness of the highway layers (in prediction layers and encoding layers) by replacing them to FC layers. We observe that both highway layers have marginally helped the overall performance. Finally, in (7-9), we remove the alignment features based on their composition type. We observe that the Sub and Concat compositions were more important than the Mul composition. However, removing any of the three will result in some performance degradation. Finally, in (10), we replace the LSTM encoder with a BiLSTM, observing that adding bi-directionality did not improve performance for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Linguistic Error Analysis</head><p>We perform a linguistic error analysis using the supplementary annotations provided by the MultiNLI dataset. We compare against the model outputs of the ESIM model across 13 categories of linguistic phenenoma <ref type="bibr">(Williams et al., 2017)</ref>. Table 4 reports the result of our error analysis. We observe that our CAFE model generally outperforms ESIM on most categories. On the mismatched setting, CAFE outperforms ESIM in 12 out of 13 categories, losing only in one percentage point in Active/Passive category. On the matched setting, CAFE is outperformed by ESIM very marginally on coreference and paraphrase categories. Despite generally achieving much superior results, we noticed that CAFE performs poorly on conditionals 4 on the matched setting. Measuring the absolute ability of CAFE, we find that CAFE performs extremely well in handling linguistic patterns of paraphrase detection and active/passive. This is likely to be attributed by the alignment strategy that CAFE and ESIM both exploits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Interpreting and Visualizing with CAFE</head><p>Finally, we also observed that the propagated features are highly interpretable, giving insights to the inner workings of the CAFE model. <ref type="figure" target="#fig_1">Figure 2</ref> shows a visualization of the feature values from an example in the SNLI test set. The ground Legend is denoted by {inter,intra} followed by the operations mul, sub or cat (concat).</p><p>truth is contradiction. Based on the above example we make several observations. Firstly, inter mul features mostly capture identical words (or semantically similar words), i.e., inter mul features for 'river' spikes in both sentences. Secondly, inter sub spikes on conflicting words that might cause contradiction, e.g., 'sedan' and 'land rover' are not the same vehicle. Another interesting observation is that we notice the inter sub features for driven and stuck spiking. This also validates the observation of <ref type="bibr" target="#b4">(Chen et al., 2017b)</ref>, which shows what the sub vector in the ESIM model is looking out for contradictory information. However, our architecture allows the inspection of these vectors since they are compressed via factorization, leading to larger extents of explainability -a quality that neural models inherently lack. We also observed that intra-attention (e.g., intra cat) features seem to capture the more important words in the sentence ('river', 'sedan', 'land rover').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a new neural architecture, CAFE for NLI. CAFE achieves very competitive performance on three benchmark datasets. Extensive ablation studies confirm the effectiveness of FM layers over FC layers. Qualitatively, we show how different compositional operators (e.g., sub and mul) behave in NLI task and shed light on why subtractive composition helps in other models such as ESIM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High level overview of our proposed architecture (best viewed in color). Alignment vectors are compressed and then propagated to upper representation learning layers (RNN encoders). Intra-attention is omitted in this diagram due to the lack of space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of six Propgated Features (Best viewed in color). Legend is denoted by {inter,intra} followed by the operations mul, sub or cat (concat).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Performance comparison of all published models on the SNLI benchmark.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance comparison (accuracy) on MultiNLI 

and SciTail. Models with  †, # and are reported from (Weis-
senborn, 2017), (Khot et al., 2018) and (Williams et al., 2017) 
respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Ablation study on MultiNLI development sets. HW 

stands for Highway. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Linguistic Error Analysis on MultiNLI dataset.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Following (Parikh et al., 2016), we may also concatenate the intra-aligned vector to ui which we found to have speed up convergence.</note>

			<note place="foot" n="2"> Additionally, we added ELMo (Peters et al., 2018) to our CAFE model at the embedding layer. We report CAFE + ELMo under external resource models. This was done post review after EMNLP. Due to resource constraints, we did not train CAFE + ELMo ensembles but a single run (and single model) of CAFE + ELMo already achieves 89.0 score on SNLI.</note>

			<note place="foot" n="3"> As of 22nd May 2018, the deadline of the EMNLP submisssion.</note>

			<note place="foot" n="4"> This only accounts for 5% of samples.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matched</head><p>Mismatched </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<title level="m">aoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org</title>
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden</editor>
		<meeting><address><addrLine>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever; Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Latent cross: Making use of context in recurrent recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04289</idno>
		<title level="m">Natural language inference with external knowledge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P17-1152</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-1152" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent neural network-based sentence encoder with gated attention for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 2nd Workshop on Evaluating Vector Space Representations for NLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-08" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on</title>
		<meeting>the 2016 Conference on</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Natural Language Processing</title>
		<meeting><address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of task-specific tree structures with tree-lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02786</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment</title>
		<meeting>the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment<address><addrLine>Berlin, Heidelberg, MLCW&apos;05</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1709.04348</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hypothesis transformation and semantic variability rules used in recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. Association for Computational Linguistics</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, RTE &apos;07</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="125" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Stanford, CA, USA. AAI3364139</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling semantic containment and exclusion in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Computational Linguistics</title>
		<meeting>the 22Nd International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, COLING &apos;08</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shortcutstacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the 2nd Workshop on Evaluating Vector Space Representations for NLP<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-08" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2010 IEEE 10th International Conference on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Factorization machines</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reading and thinking: Re-read LSTM unit for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="2870" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR abs/1611.01747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="doi">10.24963/ijcai.2017/579</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/579" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentySixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia</title>
		<meeting>the TwentySixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia</meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reading twice for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<idno>CoRR abs/1706.02596</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
