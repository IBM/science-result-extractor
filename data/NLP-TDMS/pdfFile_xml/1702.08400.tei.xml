<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asymmetric Tri-training for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-05-13">13 May 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
						</author>
						<title level="a" type="main">Asymmetric Tri-training for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-13">13 May 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In un-supervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain. In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the development of deep neural networks including deep convolutional neural networks (CNN) ( <ref type="bibr" target="#b18">Krizhevsky et al., 2012)</ref>, the recognition abilities of images and languages have improved dramatically. Training deep-layered networks with a large number of labeled samples enables us to correctly categorize samples in diverse domains. In addition, the transfer learning of CNN is utilized in many studies. For object detection or segmentation, we can transfer the knowledge of a CNN trained with a large-scale dataset by fine-tuning it on a relatively small dataset ( <ref type="bibr" target="#b14">Girshick et al., 2014;</ref><ref type="bibr" target="#b23">Long et al., 2015a</ref>). Moreover, features from a CNN trained on ImageNet ( <ref type="bibr" target="#b9">Deng et al., 2009</ref>) are useful for multimodal learning tasks including image captioning ( <ref type="bibr" target="#b36">Vinyals et al., 2015)</ref> and visual question answering ( <ref type="bibr" target="#b0">Antol et al., 2015</ref>).</p><p>One of the problems of neural networks is that although they perform well on the samples generated from the same distribution as the training samples, they may find it difficult to correctly recognize samples from different distributions at the test time. One example is images collected from the Internet, which may come in abundance and be fully labeled. They have a distribution different from the images taken from a camera. Thus, a classifier that performs well on various domains is important for practical use. To realize this, it is necessary to learn domain-invariantly discriminative representations. However, acquiring such representations is not easy because it is often difficult to collect a large number of labeled samples and because samples from different domains have domain-specific characteristics.</p><p>In unsupervised domain adaptation, we try to train a classifier that works well on a target domain on the condition that we are provided labeled source samples and unlabeled target samples during training. Most of the previous deep domain adaptation methods have been proposed mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains. These methods aimed to obtain domain-invariant features by minimizing the divergence between domains as well as a category loss on the source domain <ref type="bibr" target="#b11">(Ganin &amp; Lempitsky, 2014;</ref><ref type="bibr" target="#b24">Long et al., 2015b;</ref>. However, as shown in <ref type="bibr" target="#b3">(Ben-David et al., 2010)</ref>, theoretically, if a classifier that works well on both the source and the target domains does not exist, we cannot expect a discriminative classifier for the target domain. That is, even if the distributions are matched on the nondiscriminative representations, the classifier may not work</p><formula xml:id="formula_0">!"#$$%&amp;%'()* !"#$$%&amp;%'(+* ,#-'"'./$01(2'/$#34"'$* !"#$$%&amp;%'()* !"#$$%&amp;%'(+* 56"#-'"'./7#(8'7/$#34"'$* !"#$$/9* !"#$$/9* !"#$$%&amp;%'(/7*</formula><p>:$'1.0;"#-'"'./7#(8'7/$#34"'$* <ref type="figure">Figure 1</ref>. Outline of our model. We assign pseudo-labels to unlabeled target samples based on the predictions from two classifiers trained on source samples.</p><p>well on the target domain. Since directly learning discriminative representations for the target domain, in the absence of target labels, is considered very difficult, we propose to assign pseudo-labels to target samples and train targetspecific networks as if they were true labels.</p><p>Co-training and tri-training ( <ref type="bibr" target="#b38">Zhou &amp; Li, 2005</ref>) leverage multiple classifiers to artificially label unlabeled samples and retrain the classifiers. However, the methods do not assume labeling samples from different domains. Since our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose asymmetric tri-training for unsupervised domain adaptation. By asymmetric, we mean that we assign different roles to three classifiers.</p><p>In this paper, we propose a novel tri-training method for unsupervised domain adaptation, where we assign pseudolabels to unlabeled samples and train neural networks utilizing the samples. As described in <ref type="figure">Fig. 1</ref>, two networks are used to label unlabeled target samples and the remaining network is trained by the pseudo-labeled target samples. Our method does not need any special implementations. We evaluate our method on the digit classification task, traffic sign classification task and sentiment analysis task using the Amazon Review dataset, and demonstrate state-of-the-art performance in nearly all experiments. In particular, in the adaptation scenario, MNIST→SVHN, our method outperformed other methods by more than 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>As many methods have been proposed to tackle various tasks in domain adaptation, we present details of the research most closely related to our paper.</p><p>A number of previous methods attempted to realize adaptation by utilizing the measurement of divergence between different domains <ref type="bibr" target="#b11">(Ganin &amp; Lempitsky, 2014;</ref><ref type="bibr" target="#b24">Long et al., 2015b;</ref><ref type="bibr" target="#b22">Li et al., 2016</ref>). The methods are based on the theory proposed in <ref type="bibr" target="#b3">(Ben-David et al., 2010)</ref>, which states that the expected loss for a target domain is bounded by three terms: (i) expected loss for the source domain; (ii) domain divergence between source and target; and (iii) the minimum value of a shared expected loss. The shared expected loss means the sum of the loss on the source and target domain. As the third term, which is usually considered to be very low, cannot be evaluated when labeled target samples are absent, most methods try to minimize the first term and the second term. With regards to training deep architectures, the maximum mean discrepancy (MMD) or a loss of domain classifier network is utilized to measure the divergence corresponding to the second term ( <ref type="bibr" target="#b15">Gretton et al., 2012;</ref><ref type="bibr" target="#b11">Ganin &amp; Lempitsky, 2014;</ref><ref type="bibr" target="#b24">Long et al., 2015b;</ref><ref type="bibr" target="#b6">Bousmalis et al., 2016)</ref>. However, the third term is very important in training CNN, which simultaneously extract representations and recognize them. The third term can easily be large when the representations are not discriminative for the target domain. Therefore, we focus on how to learn target-discriminative representations considering the third term. In ( <ref type="bibr" target="#b25">Long et al., 2016</ref>) the focus was on the point we have stated and a target-specific classifier was constructed using a residual network structure. Different from their method, we constructed a target-specific network by providing artificially labeled target samples.</p><p>Several transductive methods use similarity of features to provide labels for unlabeled samples ( <ref type="bibr" target="#b30">Rohrbach et al., 2013;</ref><ref type="bibr" target="#b17">Khamis &amp; Lampert, 2014</ref>). For unsupervised domain adaptation, in <ref type="bibr" target="#b32">(Sener et al., 2016</ref>), a method was proposed to learn labeling metrics by using the k-nearest neighbors between unlabeled target samples and labeled source samples. In contrast to this method, our method explicitly and simply backpropagates the category loss for target samples based on pseudo-labeled samples. Our approach does not require any special modules.</p><p>Many methods proposed to give pseudo-labels to unlabeled samples by utilizing the predictions of a classifier and retraining it including the pseudo-labeled samples, which is called self-training. The underlying assumption of selftraining is that one's own high-confidence predictions are correct ( <ref type="bibr" target="#b39">Zhu, 2005)</ref>. As the predictions are mostly correct, utilizing samples with high confidence will further improve the performance of the classifier. Co-training utilizes two classifiers, which have different views on one sample, to provide pseudo-labels <ref type="bibr" target="#b5">(Blum &amp; Mitchell, 1998;</ref><ref type="bibr" target="#b35">Tanha et al., 2011</ref>). Then, the unlabeled samples are added to training set if at least one classifier is confident about the predictions. The generalization ability of co-training is theoretically ensured ( <ref type="bibr" target="#b2">Balcan et al., 2004;</ref><ref type="bibr" target="#b8">Dasgupta et al., 2001</ref>) under some assumptions and applied to various tasks <ref type="bibr" target="#b37">(Wan, 2009;</ref><ref type="bibr" target="#b21">Levin et al., 2003</ref>). In (Chen et al., 2011), the idea of co-training was incorporated into domain adaptation. Tri-training can be regarded as the extension of co-training ( <ref type="bibr" target="#b38">Zhou &amp; Li, 2005</ref>). Similar to co-training, tritraining uses the output of three different classifiers to give pseudo-labels to unlabeled samples. Tri-training does not require partitioning features into different views; instead, tri-training initializes each classifier differently. However,</p><formula xml:id="formula_1">! " ! ! # ! ! $ ! %&amp;! $' ! ()* +, * +,</formula><p>(--.-/01234-/56!+4/, * +-.-!/41708+594+47-$52:4$-/56!+4/, <ref type="figure">Figure 2</ref>. The proposed method includes a shared feature extractor (F ), classifiers for labeled samples (F1, F2), which learn from labeled source samples, and newly labeled target samples. In addition, a target-specific classifier (Ft) learns from pseudo-labeled target samples. Our method first trains networks from only labeled source samples, then labels the target samples based on the output of F1, F2. We train all architectures using them as if they are correctly labeled samples.</p><formula xml:id="formula_2">"#$%&amp;! ;! &lt; " ! &lt; # ! &lt; $ ! =&gt;! %&amp;! #' !?&gt;! %&amp;! "' !?&gt;! %&amp;! #' !=&gt;! %&amp;! "' !=&gt;! '(! '(! =-.-@/41708+594+-A02-$52:4$-/56!+4! ?-.-%594+-A02-/01234-/56!+4! &lt;! B9C43$DE4-A02-35$4:02?-+0//! ()* +, &lt; " ! '&lt; #-.-%594+DF:-F4$G02H/! &lt; $ -.-*52:4$-/!43DID3-F4$G02H! &lt;-.-(J5247-F4$G02H!</formula><p>tri-training does not assume that the unlabeled samples follow the different distributions from the ones which labeled samples are generated from. Therefore, we develop a tritraining method suitable for domain adaptation by using three classifiers asymmetrically.</p><p>In <ref type="bibr" target="#b20">(Lee, 2013)</ref>, the effect of pseudo-labels in a neural network was investigated. They argued that the effect of training a classifier with pseudo-labels is equivalent to entropy regularization, thus leading to a low-density separation between classes. In addition, in our experiment, we observe that target samples are separated in hidden features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we provide details of the proposed model for domain adaptation. We aim to construct a targetspecific network by utilizing pseudo-labeled target samples. Simultaneously, we expect two labeling networks to acquire target-discriminative representations and gradually increase accuracy on the target domain.</p><p>We show our proposed network structure in <ref type="figure">Fig. 2</ref>. Here F denotes the network which outputs shared features among three networks, F 1 and F 2 classify features generated from F . Their predictions are utilized to give pseudo-labels. The classifier F t classifies features generated from F , which is a target-specific network. Here F 1 , F 2 learn from source and pseudo-labeled target samples and F t learns only from pseudo-labeled target samples. The shared network F learns from all gradients from F 1 , F 2 , F t . Without such a shared network, another option for the network architecture we can think of is training three networks separately, but this is inefficient in terms of training and implementation. Furthermore, by building a shared network F , F 1 and F 2 can also harness the target-discriminative representations learned by the feedback from F t .</p><p>The set of source samples is defined as</p><formula xml:id="formula_3">(x i , y i ) ms i=1 ∼ S, the unlabeled target set is (x i ) mt i=1</formula><p>∼ T , and the pseudolabeled target set is</p><formula xml:id="formula_4">(x i , ˆ y i ) nt i=1 ∼ T l .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Loss for Multiview Features Network</head><p>In the existing works (Chen et al., 2011) on co-training for domain adaptation, given features are divided into separate parts and considered to be different views.</p><p>As we aim to label target samples with high accuracy, we expect F 1 , F 2 to classify samples based on different viewpoints. Therefore, we make a constraint for the weight of F 1 , F 2 to make their inputs different to each other. We add the term |W 1 T W 2 | to the cost function, where W 1 , W 2 denote fully connected layers' weights of F 1 and F 2 which are first applied to the feature F (x i ). Each network will learn from different features with this constraint. The objective for learning F 1 , F 2 is defined as</p><formula xml:id="formula_5">E(θ F , θ F1 , θ F2 ) = 1 n n i=1 L y (F 1 • F (x i )), y i ) + L y (F 2 • (F (x i )), y i ) + λ|W 1 T W 2 | (1)</formula><p>where L y denotes the standard softmax cross-entropy loss function. We decided the trade-off parameter λ based on validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Procedure and Labeling Method</head><p>Pseudo-labeled target samples will provide targetdiscriminative information to the network. However, since they certainly contain false labels, we have to pick up reliable pseudo-labels. Our labeling and learning method is aimed at realizing this.</p><p>The entire procedure of training the network is shown in Algorithm 1. First, we train the entire network with source training set S. Here F 1 , F 2 are optimized by Eq. (1) and F t is trained on standard category loss. After training on S, to provide pseudo-labels, we use predictions of F 1 and F 2 , namely y 1 , y 2 obtained from x k . When C 1 , C 2 denote the class which has the maximum predicted probability for y 1 , y 2 , we assign a pseudo-label to x k if the following two conditions are satisfied. First, we require C 1 = C 2 to give pseudo-labels, which means two different classifiers agree with the prediction. The second requirement is that the maximizing probability of y 1 or y 2 exceeds the threshold parameter, which we set as 0.9 or 0.95 in the experiment.</p><p>We suppose that unless one of two classifiers is confident of the prediction, the prediction is not reliable. If the two requirements are satisfied,</p><formula xml:id="formula_6">x k , ˆ y k = C 1 = C 2</formula><p>is added to T l . To prevent the overfitting to pseudo-labels, we resample the candidate for labeling samples in each step. We set the Algorithm 1 iter denotes the iteration of training. The function Labeling means the method of labeling. We assign pseudo-labels to samples when the predictions of F 1 and F 2 agree and at least one of them is confident of their predictions.</p><formula xml:id="formula_7">Input: data S = (x i , t i ) m i=1 , T = (x j ) n j=1 T l = ∅ for j = 1 to iter do Train F, F 1 , F 2 , F t with mini-batch from training set S end for N t = N init T l = Labeling(F, F 1 , F 2 ,T , N t ) L = S ∪ T l for k steps do for j = 1 to iter do Train F, F 1 , F 2 with mini-batch from training set L Train F, F t with mini-batch from training set T l end for T l = ∅, N t = k/20 * n T l = Labeling(F, F 1 , F 2 ,T , N t ) L = S ∪ T l end for</formula><p>number of the initial candidates N init as 5,000. We gradually increase the number of the candidates N t = k/20 * n, where n denotes the number of all target samples and k denotes the number of steps, and we set the maximum number of pseudo-labeled candidates as 40,000. After the pseudo-labeled training set T l is composed, F, F 1 , F 2 are updated by the objective Eq. (1) on the labeled training set L = S ∪ T l . Then, F, F t are simply optimized by the category loss for T l .</p><p>Discriminative representations will be learned by constructing a target-specific network trained only on target samples. However, if only noisy pseudo-labeled samples are used for training, the network may not learn useful representations. Then, we use both source samples and pseudo-labeled samples for training F, F 1 , F 2 to ensure the accuracy. Also, as the learning proceeds, F will learn target-discriminative representations, resulting in an improvement in accuracy in F 1 , F 2 . This cycle will gradually enhance the accuracy in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Batch Normalization for Domain Adaptation</head><p>Batch normalization (BN) <ref type="bibr" target="#b16">(Ioffe &amp; Szegedy, 2015)</ref>, which whitens the output of the hidden layer in a CNN, is an effective technique to accelerate training speed and enhance the accuracy of the model. In addition, in domain adaptation, whitening the hidden layer's output is effective for improving the performance, which make the distribution in different domains similar <ref type="bibr" target="#b34">(Sun et al., 2016;</ref><ref type="bibr" target="#b22">Li et al., 2016)</ref>.</p><p>Input samples of F 1 , F 2 include both pseudo-labeled target samples and source samples. Introducing BN will be useful for matching the distribution and improves the performance. We add the BN layer in the last layer in F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head><p>In this section, we provide a theoretical analysis to our approach. First, we provide an insight into existing theory, then we introduce a simple expansion of the theory related to our method.</p><p>In <ref type="bibr" target="#b3">(Ben-David et al., 2010)</ref>, an equation was introduced showing that the upper bound of the expected error in the target domain depends on three terms, which include the divergence between different domains and the error of an ideal joint hypothesis. The divergence between source and target domain, H∆H-distance, is defined as follows:</p><formula xml:id="formula_8">d H∆H (S, T ) = 2 sup (h,h ′ )∈H 2 E x∼S [h(x) 񮽙 = h ′ (x)] − E x∼T [h(x) 񮽙 = h ′ (x)]</formula><p>This distance is frequently used to measure the adaptability between different domains.</p><p>The ideal joint hypothesis is defined as h * = arg min</p><formula xml:id="formula_9">h∈H R S (h * ) + R T (h * )</formula><p>, and its corresponding error</p><formula xml:id="formula_10">is C = R S (h * ) + R T (h * )</formula><p>, where R denotes the expected error on each hypothesis. The theorem is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. (Ben-David et al., 2010)</head><p>Let H be the hypothesis class. Given two different domains S, T , we have</p><formula xml:id="formula_11">∀h ∈ H, R T (h) ≤ R S (h) + 1 2 d H∆H (S, T ) + C</formula><p>This theorem means that the expected error on the target domain is upper bounded by three terms, the expected error on the source domain, the domain divergence measured by the disagreement of the hypothesis, and the error of the ideal joint hypothesis. In the existing work <ref type="bibr" target="#b11">(Ganin &amp; Lempitsky, 2014;</ref><ref type="bibr" target="#b24">Long et al., 2015b</ref>), C was disregarded because it was considered to be negligibly small. If we are provided with fixed features, we do not need to consider the term because the term is also fixed. However, if we assume that x s ∼ S, x t ∼ T are obtained from the last fully connected layer of deep models, we note that C is determined by the output of the layer, and further note the necessity of considering this term.</p><p>We consider the pseudo-labeled target samples set</p><formula xml:id="formula_12">T l = (x i , ˆ y i ) mt i=1</formula><p>given false labels at the ratio of ρ. The shared error of h * on S, T l is denoted as C ′ . Then, the following inequality holds:</p><formula xml:id="formula_13">∀h ∈ H, R T (h) ≤ R S (h) + 1 2 d H∆H (S, T ) + C ≤ R S (h) + 1 2 d H∆H (S, T ) + C ′ + ρ</formula><p>We show a simple derivation of the inequality in the Supplementary material. In Theorem 1, we cannot measure C in the absence of labeled target samples. We can approximately evaluate and minimize it by using pseudo-labels.</p><note type="other">Furthermore, when we consider the second term on the right-hand side, our method is expected to reduce this term. This term intuitively denotes the discrepancy between different domains in the disagreement of two classifiers. If we regard certain h and h ′ as F 1 and F 2 , respectively, E x∼Sx [h(x) 񮽙 = h ′ (x)] should be very low because training is based on the same labeled samples. Moreover, for the same reason, E</note><formula xml:id="formula_14">x∼Tx [h(x) 񮽙 = h ′ (x)</formula><p>] is expected to be low, although we use the training set T l instead of genuine labeled target samples. Thus, our method will consider both the second and the third term in Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment and Evaluation</head><p>We perform extensive evaluations of our method on image datasets and a sentiment analysis dataset. We evaluate the accuracy of target-specific networks in all experiments.</p><p>Visual Domain Adaptation For visual domain adaptation, we perform our evaluation on the digits datasets and traffic signs datasets.</p><p>Digits datasets include MNIST ( <ref type="bibr" target="#b19">LeCun et al., 1998</ref> We do not evaluate our method on Office ( <ref type="bibr" target="#b31">Saenko et al., 2010)</ref>, which is the most commonly used dataset for visual domain adaptation. As pointed out by <ref type="bibr" target="#b6">(Bousmalis et al., 2016)</ref>, some labels in that dataset are noisy and some images contain other classes' objects. Furthermore, many previous studies have evaluated the fine-tuning of pretrained networks using ImageNet. This protocol assumes the existence of another source domain. In our work, we want to evaluate the situation where we have access to only one source domain and one target domain.</p><p>Adaptation in Amazon Reviews To investigate the behavior on language datasets, we also evaluated our method on the Amazon Reviews dataset ( <ref type="bibr" target="#b4">Blitzer et al., 2006</ref>) with the same preprocessing as used by <ref type="bibr" target="#b7">(Chen et al., 2011;</ref><ref type="bibr" target="#b12">Ganin et al., 2016</ref>). The dataset contains reviews on four types of products: books, DVDs, electronics, and kitchen appliances. We evaluate our method on 12 domain adaptation scenarios. The results are shown in <ref type="table">Table 1</ref>.  <ref type="bibr" target="#b12">Ganin et al., 2016</ref>) in the Amazon Reviews experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Detail</head><p>In experiments on image datasets, we employ the architecture of CNN used in <ref type="bibr" target="#b11">(Ganin &amp; Lempitsky, 2014</ref>). For a fair comparison, we separate the network at the hidden layer from which ( <ref type="bibr" target="#b11">Ganin &amp; Lempitsky, 2014</ref>) constructed discriminator networks. Therefore, when considering one classifier, for example, F 1 • F , the architecture is identical to previous work. We also follow <ref type="bibr" target="#b11">(Ganin &amp; Lempitsky, 2014</ref>) in the other protocols. We set the threshold value for the labeling method as 0.95 in MNIST→SVHN. In other scenarios, we set it as 0.9. We use MomentumSGD for optimization and set the momentum as 0.9, while the learning rate is determined on validation splits and uses either [0.01, 0.05]. λ is set 0.01 in all scenarios. In our Supplementary material, we provide details of the network architecture and hyper-parameters.</p><p>For experiments on the Amazon Reviews dataset, we use a similar architecture to that used in ( <ref type="bibr" target="#b12">Ganin et al., 2016)</ref>: with sigmoid activated, one dense hidden layer with 50 hidden units, and softmax output. We extend the architecture to our method similarly in the architecture of CNN. λ is set as 0.001 based on the validation. Since the input is sparse, we use Adagrad (Duchi et al., 2011) for optimization. We repeat this evaluation 10 times and report mean accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Result</head><p>In <ref type="table" target="#tab_4">Tables 1 and 3</ref> MNIST→MNIST-M: last pooling layer MNIST→MNIST-M First, we evaluate the adaptation scenario between the hand-written digits dataset MNIST and its transformed dataset MNIST-M. MNIST-M is composed by merging the clip of the background from BSDS500 datasets ( <ref type="bibr" target="#b1">Arbelaez et al., 2011</ref>). A patch is randomly taken from the images in BSDS500, merged to MNIST digits. Even with this simple domain shift, the adaptation performance of CNN is much worse than the case where it was trained on target samples. From 59,001 target training samples, we randomly select 1,000 labeled target samples as a validation split and tuned hyper-parameters.</p><p>Our method outperforms the other existing method by about 7%. Visualization of features in the last pooling layer is shown in <ref type="figure" target="#fig_2">Fig. 3(a)(b)</ref>. We can observe that the red target samples are more dispersed when adaptation is achieved. We show the comparison of the accuracy between the actual labeling accuracy on target samples during training and the test accuracy in <ref type="figure" target="#fig_3">Fig. 4</ref>. The test accuracy is very low at first, but as the steps increase, the accuracy becomes closer to that of the labeling accuracy. In this adaptation, we can clearly see that the actual labeling accuracy gradually improves with the accuracy of the network.</p><p>SVHN↔MNIST We increase the gap between distributions in this experiment.  centered in images, thus adaptation from MNIST to SVHN is rather difficult. In both settings, we use 1,000 labeled target samples to find the optimal hyperparameters.</p><p>We evaluate our method on both adaptation scenarios and achieved state-of-the-art performance on both datasets. In particular, for the adaptation MNIST→SVHN, we outperformed other methods by more than 10%. In <ref type="figure" target="#fig_2">Fig. 3(c)(d)</ref>, we visualize the representations in MNIST→SVHN. Although the distributions seem to be separated between domains, the red SVHN samples become more discriminative using our method compared with non-adapted embedding. We also show the comparison between actual labeling method accuracy and testing accuracy in <ref type="figure" target="#fig_3">Fig. 4(b)(c)</ref>. In this <ref type="figure">figure,</ref> we can see that the labeling accuracy rapidly drops in the initial adaptation stage. On the other hand, testing accuracy continues to improve, and finally exceeds the labeling accuracy. There are two questions about this interesting phenomenon. The first question is why does the labeling method continue to decrease despite the increase in the test accuracy? Target samples given pseudo-labels always include mistakenly labeled samples whereas those given no labels are ignored in our method. Therefore, the error will be reinforced in the target samples that are included in training set. The second question is why does the test accuracy continue to increase despite the lower labeling accuracy? The assumed reasons are that the network already acquires target discriminative representations in this phase and they can improve the accuracy using source samples and correctly labeled target samples.</p><p>In <ref type="figure" target="#fig_3">Fig. 4(f)</ref>, we also show the comparison of accuracy of the three networks F 1 , F 2 , F t in SVHN→MNIST. The accuracy of three networks is nearly the same in every step. The same thing is observed in other scenarios. From this result, we can state that the target-discriminative representations are shared in all three networks. Our method also outperforms other methods in this experiment. In this experiment, the effect of BN is not clear compared with other scenarios. The domain gap is considered small in this scenario as the performance of the source-only classifier shows. In <ref type="figure" target="#fig_3">Fig. 4(d)</ref>, although the labeling accuracy is dropping, the accuracy of the learned network's prediction is improving as in MNIST↔SVHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYN DIGITS→SVHN</head><p>SYN SIGNS→GTSRB This setting is similar to the pre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient stop branch</head><p>Ft F1, F2 None MNIST→MNIST-M 56.4 95.4 94.0 MNIST→SVHN 47.7 47.5 52.8 SYN SIGNS→GTRB 96.5 93.1 96.2 <ref type="table">Table 2</ref>. Results of Gradient stop experiment. When stopping gradients from Ft, we do not use backward gradients from Ft to F , and F learns only from F1, F2. When stopping gradients from F1, F2, we do not use backward gradients from F1, F2 to F , and F learns from Ft. None denotes our proposed method, we backward all gradients from all branches to F . In these three adaptation scenarios, our method shows stable performance.</p><p>vious setting, adaptation from synthetic images to real images, but we have a larger number of classes, namely 43 classes instead of 10. We use the SYN SIGNS dataset <ref type="bibr" target="#b11">(Ganin &amp; Lempitsky, 2014</ref>) for the source dataset and the GTSRB dataset <ref type="bibr" target="#b33">(Stallkamp et al., 2011</ref>) for the target dataset, which consist of real traffic sign images. We select randomly 31,367 samples for target training samples and evaluate accuracy on the rest of the samples. A total of 3,000 labeled target samples are used for validation.</p><p>In this scenario, our method outperforms other methods. This result shows that our method is effective for the adaptation from synthesized images to real images, which have diverse classes. In <ref type="figure" target="#fig_3">Fig. 4(e)</ref>, the same tendency as in MNIST↔SVHN is observed in this adaptation scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Stop Experiment</head><p>We evaluate the effect of the target-specific network in our method. We stop the gradient from upper layer networks F 1 , F 2 , and F t to examine the effect of F t . <ref type="table">Table 2</ref> shows three scenarios including the case where we stop the gradient from F 1 , F 2 , and F t . In all scenarios, when we backward all gradients from F 1 , F 2 , F t , we obtain clear performance improvements.</p><p>In the experiment MNIST→MNIST-M, we can assume that only the backpropagation from F 1 , F 2 cannot construct discriminative representations for target samples and confirm the effect of F t . For the adaptation MNIST→SVHN, the best performance is realized when F receives all gradients from upper networks. Backwarding all gradients will ensure both target-specific discriminative representations in difficult adaptations. In SYN SIGNS→GTSRB, backwarding only from F t produces the worst performance because these domains are similar and noisy pseudo-labeled target samples worsen the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A-distance</head><p>From the theoretical results in <ref type="bibr" target="#b3">(Ben-David et al., 2010)</ref>, A-distance is usually used as a measure of domain discrepancy. The way of estimating empirical A-distance is simple, in which we train a classifier to classify a domain from each domains' feature. Then, the approximate distance is calculated   asˆdasˆ asˆd A = 2(1 − 2ǫ), where ǫ is the generalization error of the classifier. In <ref type="figure" target="#fig_3">Fig. 4(g)</ref>, we show the A-distance calculated from each CNN features. We used linear SVM to calculate the distance. From this graph, we can see that our method certainly reduces the A-distance compared with the CNN trained on only source samples. In addition, when comparing DANN and our method, although DANN reduces A-distance much more than our method, our method shows superior performance. This indicates that minimizing the domain discrepancy is not necessarily an appropriate way to achieve better performance.</p><p>Amazon Reviews Reviews are encoded in 5,000 dimensional vectors of bag-of-words unigrams and bigrams with binary labels. Negative labels are attached to the samples if they are ranked with 1-3 stars. Positive labels are attached if they are ranked with 4 or 5 stars. We have 2,000 labeled source samples and 2,000 unlabeled target samples for training, and between 3,000 and 6,000 samples for testing. We use 200 of labeled target samples for validation.</p><p>From the results in <ref type="table" target="#tab_4">Table 3</ref>, our method performs better than VFAE ( <ref type="bibr" target="#b26">Louizos et al., 2015</ref>) and DANN ( <ref type="bibr" target="#b12">Ganin et al., 2016</ref>) in nine settings out of twelve. Our method is effective in learning a shallow network on different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a novel asymmetric tri-training method for unsupervised domain adaptation, which is simply implemented. We aimed to learn discriminative representations by utilizing pseudo-labels assigned to unlabeled target samples. We utilized three classifiers, two networks assign pseudo-labels to unlabeled target samples and the remaining network learns from them.</p><p>We evaluated our method both on domain adaptation on a visual recognition task and a sentiment analysis task, outperforming other methods. In particular, our method outperformed the other methods by more than 10% in the MNIST→SVHN adaptation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem</head><p>We introduce the derivation of theorem of the main paper. The ideal joint hypothesis is defined as h * = arg min</p><formula xml:id="formula_15">h∈H R S (h * ) + R T (h * )</formula><p>, and its corresponding error</p><formula xml:id="formula_16">is C = R S (h * ) + R T (h * )</formula><p>, where R denotes the expected error on each hypothesis.</p><p>We consider the pseudo-labeled target samples set</p><formula xml:id="formula_17">T l = (x i , ˆ y i ) mt i=1</formula><p>given false labels at the ratio of ρ. The minimum shared error on S, T l is denoted as C ′ . Then, the following inequality holds:</p><formula xml:id="formula_18">∀h ∈ H, R T (h) ≤ R S (h) + 1 2 d H∆H (S X , T X ) + C ≤ R S (h) + 1 2 d H∆H (S X , T X ) + C ′ + ρ</formula><p>Proof. The probabiliy of false labels in the pseudo-labeled set T l is ρ. When we consider 0-1 loss function for l, the difference between the error based on the true labeled set and pseudo-labeled set is</p><formula xml:id="formula_19">|l(h(x i ), y i ) − l(h(x i ), ˆ y i )| = 1 y i 񮽙 = ˆ y i 0 y i = ˆ y i</formula><p>Then, the difference in the expected error is,</p><formula xml:id="formula_20">E[|l(h(x i ), y i ) − l(h(x i ), ˆ y i )|] ≤ |R T l (h) − R T (h)| ≤ ρ</formula><p>From the characteritic of the loss function, the triangle inequality will hold, then</p><formula xml:id="formula_21">R S (h) + R T (h) = R S (h) + R T (h) − R T l (h) + R T l (h) ≤ R S (h) + R T l (h) + |R T l (h) − R T (h)| ≤ R S (h) + R T l (h) + ρ</formula><p>From this result, the main inequality holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Architectures and training detail</head><p>Four types of architectures are used for our method, which is based on <ref type="bibr" target="#b11">(Ganin &amp; Lempitsky, 2014</ref>). The network topology is shown in Figs 6, 7 and 8. The other hyperparameters are decided on the validation splits. The learning rate is set to 0.05 in SVHN↔MNIST. In the other scenarios, it is set to 0.01. The batchsize for training F t , F is set as 128, the batchsize for training F 1 , F 2 , F is set as 64 in all scenarios.</p><p>In MNIST→MNIST-M, the dropout rate used in the experiment is 0.2 for training F t , 0.5 for training F 1 , F 2 . In MNIST→SVHN, we did not use dropout. We decreased learning rate to 0.001 after step 10. In SVHN→MNIST, the dropout rate used in the experiment is 0.5. In SYNDIGITS→SVHN, the dropout rate used in the experiment is 0.5. In SYNSIGNS→GTSRB, the dropout rate used in the experiment is 0.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary experiments on MNIST→MNIST-M</head><p>We observe the behavior of our model when increasing the number of steps up to one hundred. We show the result in <ref type="figure" target="#fig_5">Fig. 5</ref>. Our model's accuracy gets about 97%. In our main experiments, we set the number of steps thirty, but from this experiment, further improvements can be expected.  <ref type="figure">Figure 6</ref>. The architecture used for MNIST→MNIST-M. We added BN layer in the last convolution layer and FC layers in F1, F2. We also used dropout in our experiment.  <ref type="figure">Figure 8</ref>. The architecture used in the adaptation Synthetic Signs→GTSRB. We added a BN layer after the last convolution layer in F and also used dropout.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>), MNIST-M (Ganin &amp; Lempitsky, 2014), Street View House Num- bers (SVHN) (Netzer et al., 2011), and Synthetic Digits (SYN DIGITS) (Ganin &amp; Lempitsky, 2014). We further evaluate our method on traffic sign datasets including Synthetic Traffic Signs (SYN SIGNS) (Moiseev et al., 2013) and German Traffic Signs Recognition Benchmark (Stallkamp et al., 2011) (GTSRB). In total, five adaptation scenarios are evaluated in this experiment. As the datasets used for evaluation are varied in previous works, we extensively evaluate our method on the five scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We compare our method with five methods for unsupervised domain adaptation includ- ing state-of-the art methods in visual domain adapta- tion; Maximum Mean Discrepancy (MMD) (Long et al., 2015b), Domain Adversarial Neural Network (DANN) (Ganin &amp; Lempitsky, 2014), Deep Reconstruction Clas- sification Network (DRCN) (Ghifary et al., 2016), Do- main Separation Networks (DSN) (Bousmalis et al., 2016), and k-Nearest Neighbor based adaptation (kNN-Ad) (Sener et al., 2016). We cite the results of MMD from (Bousmalis et al., 2016). In addition, we compare our method with CNN trained only on source samples. We compare our method with Variational Fair AutoEncoder (VFAE) (Louizos et al., 2015) and DANN (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. We confirm the effect our method by visualization of the learned representations by using t-distributed stochastic neighbor embedding (t-SNE) (Maaten &amp; Hinton, 2008). Red points are target samples and blue points are source samples. The samples are all from testing samples. (a), (c) The case where we only use source samples for training. (b), (d) The case of adaptation by our method. In both scenarios, MNIST→SVHN and MNIST→MNIST-M, we can see that the target samples are more dispersed through adaptation.</figDesc><graphic url="image-38.png" coords="6,278.64,285.43,165.36,165.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) ∼ (e): Comparison of the actual accuracy of pseudo-labels and learned network accuracy during training. The blue curve is the pseudo-label accuracy and the red curve is the learned network accuracy. Note that the labeling accuracy is computed using (the number of correctly labeled samples)/(the number of labeled samples). The green curve is the number of labeled target samples in each step. (f): Comparison of the accuracy of three networks in our model. Three networks almost simultaneously improve accuracy. (g): Comparison of the A-distance of different methods. Our model slightly reduced the divergence of the domain compared with source-only trained CNN.</figDesc><graphic url="image-47.png" coords="7,428.64,185.42,106.90,79.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Source→Target</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The behavior of our model when increasing the number of steps up to 100. Our model achieves accuracy of about 97%.</figDesc><graphic url="image-126.png" coords="11,85.53,79.56,178.43,141.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-128.png" coords="12,55.44,76.93,486.00,161.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-129.png" coords="12,55.44,304.73,486.00,148.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-130.png" coords="12,55.44,530.24,486.00,136.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>, we show the main results of the experi- ments. When training only on source samples, the effect of the BN is not clear as in Tables 1. However, in all image recognition experiments, the effect of BN in our method is clear; at the same time, the effect of our method is also clear when we do not use BN in the network architecture. The ef- fect of the weight constraint is obvious in MNIST→SVHN.</figDesc><table>SOURCE 
MNIST 
SVHN 
MNIST 
SYN DIGITS SYN SIGNS 
METHOD 
TARGET MNIST-M 
MNIST 
SVHN 
SVHN 
GTSRB 

Source Only w/o BN 
59.1(56.6) 68.1(59.2) 37.2(30.5) 
84.1(86.7) 
79.2(79.0) 
Source Only with BN 
57.1 
70.1 
34.9 
85.5 
75.7 
MMD (Long et al., 2015b) 
76.9 
71.1 
-
88.0 
91.1 
DANN (Ganin &amp; Lempitsky, 2014) 
81.5 
71.1 
35.7 
90.3 
88.7 
DRCN (Ghifary et al., 2016) 
-
82.0 
40.1 
-
-
DSN (Bousmalis et al., 2016) 
83.2 
82.7 
-
91.2 
93.1 
kNN-Ad (Sener et al., 2016) 
86.7 
78.8 
40.3 
-
-

Ours w/o BN 
85.3 
79.8 
39.8 
93.1 
96.2 
Ours w/o weight constraint (λ = 0) 
94.2 
86.2 
49.7 
92.4 
94.0 
Ours 
94.0 
85.0 
52.8 
92.9 
96.2 

Table 1. Results of the visual domain adaptation experiment on digits and traffic signs dataset. In every setting, our method outperforms 
other method by a large margin. In source only results, we show the results reported in (Bousmalis et al., 2016) and (Ghifary et al., 2016) 
in parentheses. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Amazon Reviews experimental results. The accuracy 
(%) of the proposed method is shown with the result of VFAE 
(Louizos et al., 2015) and DANN (Ganin et al., 2016). 

</table></figure>

			<note place="foot" n="1"> The University of Tokyo, Tokyo, Japan. Correspondence to: Kuniaki Saito &lt;k-saito@mi.t.u-tokyo.ac.jp&gt;, Yoshitaka Ushiku &lt;ushiku@mi.t.u-tokyo.ac.jp&gt;, Tatsuya Harada &lt;harada@mi.t.u-tokyo.ac.jp&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aishwarya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiasen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Dhruv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqa</surname></persName>
		</author>
		<title level="m">Visual question answering. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cotraining and expansion: Towards bridging theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Jennifer Wortman. A theory of learning from different domains. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaughan</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fernando. Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pereira</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pac generalization bounds for co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li-Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evgeniya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>François</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reconstructionclassification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bastiaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mengjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coconut: Co-classification with output space regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on Challenges in Representation Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised improvement of visual detectors using co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mingsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mingsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00830</idno>
		<title level="m">The variational fair autoencoder</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluation of traffic sign recognition methods trained on synthetically generated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACIVS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: a multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baochen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ensemble based co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jafar</forename><surname>Tanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Van Someren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamideh</forename><surname>Afsarmanesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BNAIC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tri-training: Exploiting unlabeled data using three classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1529" to="1541" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
