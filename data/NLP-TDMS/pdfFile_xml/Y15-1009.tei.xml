<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T01:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Long Short-Term Memory Networks for Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
							<email>zhangshu@cn.fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Fujitsu Research and Development Center</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Zheng</surname></persName>
							<email>dqzheng@mtlab.hit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Hu</surname></persName>
							<email>xchu@mtlab.hit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
							<email>yangming@cn.fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Fujitsu Research and Development Center</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Long Short-Term Memory Networks for Relation Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Relation classification is an important semantic processing, which has achieved great attention in recent years. The main challenge is the fact that important information can appear at any position in the sentence. Therefore, we propose bidirec-tional long short-term memory networks (BLSTM) to model the sentence with complete, sequential information about all words. At the same time, we also use features derived from the lexical resources such as WordNet or NLP systems such as dependency parser and named entity rec-ognizers (NER). The experimental results on SemEval-2010 show that BLSTM-based method only with word embeddings as input features is sufficient to achieve state-of-the-art performance, and importing more features could further improve the performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The automatic classification of semantic relations is an important task, which could offer useful information for many applications, such as question answering, information extraction, the construction and completion of semantic or relational knowledge base.</p><p>In this work, we focus on the classification of semantic relations between pairs of nominals ( <ref type="bibr" target="#b2">Hendrickx et al., 2010)</ref>. Given a sentence S with annotated pairs of nominal e1 and e2, the task is to classify which of the following nine semantic relations holds between the nominals: Cause-Effect, Instrument-Agency, Product-Producer, ContentContainer, Entity-Origin, Entity-Destination, Component-Whole, Member-Collection, Message-Topic, or Other if it does not belongs to any of the nine annotated relations.</p><p>For example, News and commotion are connected in a Cause-Effect relation in the sentence "The news brought about a commotion in the office." In this instance, the relation between news and commotion could be inferred by the meaning of the two nominals and the context of "brought about" around them. Therefore, how to grasp and represent the lexical and context information are the key research points for semantic relation classification.</p><p>Supervised methods with carefully handcrafted features from lexical and semantic resources have achieved high performance <ref type="bibr" target="#b2">(Hendrickx et al., 2010;</ref><ref type="bibr" target="#b0">Rink and Harabagiu, 2010)</ref>. However, the selection of features and the effective integration of knowledge sources into relation classification seem to be difficult.</p><p>Recently, deep neural networks has been applied with the aim of reducing the number of handcrafted features, and getting effective features from lexical and sentence level <ref type="bibr" target="#b3">(Socher et al., 2012;</ref><ref type="bibr" target="#b1">Zeng et al., 2014;</ref><ref type="bibr" target="#b11">Yu et al., 2014)</ref>.</p><p>Different from previous work, we propose bidirectional long short-term memory networks (BLSTM) to solve the relation classification. For every word in a given sentence, BLSTM has complete, sequential information about all words before and after it. Long distance relationship may be solved in some extent in this networks. At the same time, we also use features derived from the lexical resources such as WordNet or NLP tools such as dependency parser and named entity recognizers (NER). The experimental results show that only using word embedding as input features is enough to achieve state-of-the-art results. Importing more features could further improve the performance of the relation classification. <ref type="table" target="#tab_3">SemEval-2010 task 8 focused on semantic rela- tion classification, it provides a standard testbed  to evaluate and compare the performance of dif- ferent approaches.</ref> SVM <ref type="bibr" target="#b0">(Rink and Harabagiu, 2010)</ref>: Using SVM classifier and a number of features derived from NLP tools and many external resources, it achieves the highest performance among the participating systems (10 teams, 28 runs).</p><p>Neural network has got great achievement in many applications, it has also been utilized in relation classification as shown in the followings:</p><p>MV-RNN (Socher et al., 2012): They propose a recursive neural network model to learn compositional vector representations for phrases and sentences of arbitrary syntactic type and length.</p><p>CNN ( <ref type="bibr" target="#b1">Zeng et al. (2014)</ref>: Sentence level features are learned using a convolutional model, and concatenated with lexical features to form the final extracted feature vector.</p><p>FCM ( <ref type="bibr" target="#b11">Yu et al., 2014</ref>): They decompose the sentence into substructures, and extract features for each substructure. Finally they combine these features with the embeddings of words in this substructure to form a substructure embedding.</p><p>CR-CNN (Santos et al., 2015): They propose network to learn a distributed vector representation for each relation class. A ranking loss function is proposed to reduce the impact of artificial classes.</p><p>DepNN ( <ref type="bibr" target="#b4">Liu et al., 2015)</ref>: Using a recursive neural network to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path.</p><p>From the above works, we can see that many different neural network models have been applied to solve relation classification recently. The main target is to learn the effective features in lexical and sentence level to represent the latent relation between the given nominals.</p><p>Our work has the same target, and we try to apply BLSTM to mine the sentence level features with its advantage of capturing long distance relationship in a sentence. We also study the influence of adding features obtained from NLP tools and resources on the final classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Long Short Term Memory</head><p>The Long Short Term Memory architecture was proposed and extended <ref type="bibr">(Hochreiter and Schmid- huber, 1997;</ref><ref type="bibr" target="#b14">Gers et al., 2002</ref>) with the motivation on an analysis of Recurrent Neural Nets ( <ref type="bibr" target="#b13">Hochreiter et al., 2001</ref>), which found that long time lags were inaccessible to existing architectures, because backpropagated error either blows up or decays exponentially.</p><p>A LSTM layer consists of a set of recurrently connected blocks, known as memory blocks. Each one contains one or more recurrently connected memory cells and three multiplicative units -the input, output and forget gates -that provide continuous analogues of write, read and reset operations for the cells. LSTM has achieved the best known results in handwriting recognition ( <ref type="bibr" target="#b6">Graves et al., 2009</ref>) and speech recognition ( <ref type="bibr" target="#b7">Graves et al., 2013</ref>).  <ref type="figure" target="#fig_0">Figure 1</ref> shows one cell of LSTM memory block. More precisely, the input xt to the cells is multiplied by the activation of the input gate, the output to the net is multiplied by that of the output gate, and the previous cell values are multiplied by the forget gate. The net can only interact with the cells via the gates.</p><p>The basic idea of bidirectional LSTM is to present each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. This means that for every point in a given sequence, the network has complete, sequential information about all points before and after it. The structure of BLSTM is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>We propose bidirectional long short-term memory networks (BLSTM) to solve the relation classification. It includes the following parts:</p><p>(1) Initial feature extraction: extract from the input sentence. (2) Features embedding: transform all initial features into real-valued vector representation. (3) BLSTM-based sentence level representation: get high level feature representation from step (2). (4) Constructing feature vector: get lexical level and sentence level features from step (2) and step (3), and concatenate them to form the final feature vector. (5) Classifying: feed final feature vector into a multilayer perceptron (MLP) and softmax layer to get the probability distribution of relation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Initial Feature Extraction</head><p>Besides word and position features, we utilize NLP tools and resources to get POS, NER, dependency parse and hypernyms features. We aim to grasp more features which may indicate the relationship of the pair of two nominals. All these features could be classified into two types: lexical features and relative position relationship features.</p><p>We extract word, POS, NER and hypernyms as lexical features. The WordNet hypernyms are adopted as <ref type="bibr">MVRNN (Socher et al., 2012)</ref>.</p><p>Three different relative position relationship features are extracted and shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>In this work we also utilize the relative word position proposed by <ref type="bibr" target="#b1">Zeng et al. (2014)</ref>. The position feature (PF) is derived from the relative distances of the current word to the target nominals e1 and e2. For instance, the word sat in the sentence shown in <ref type="figure" target="#fig_2">Figure 3</ref>, its relative distance to the target nominal cat (e1) and mat (e2) are 1 and -3.</p><p>We also chose the Stanford dependency parser to capture long distance relationships between two nominals in a sentence. Our dependency features are based on paths in the dependency tree. Here, we extract two types of features:</p><p>Relative dependency features:  Relative root feature: r_r (root node), r_c (child node of root), r_o (others)  Relative e1 feature: e1_e1 (e1 node), e1_c (child node of e1), e1_p (parent node of e1), e1_o (others)  Relative e2 feature: e2_e2 (e2 node), e2_c (child node of e2), e2_p (parent node of e2), e2_o (others) Dep features: the tag of the current word to its parent node on the dependency tree</p><p>The above features represent the relationship between the current word and the target node, including the root, e1, e2 and their parent node. <ref type="figure">Fig- ure 4</ref> gives an example of dependency parser results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Embedding</head><p>Word Embedding is to map each word into a real-valued vector to represent syntactic and semantic information about the words. The size of the word embedding w d is a hyperparameter, which is usually set 50 or 100. For other kinds of initial features, we also transform them into a vector representation r kj , where j means the jth type of feature, the dimension is d kj . The initial value of the vector is random generated with the method proposed by <ref type="bibr" target="#b10">Glorot and Bengio (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given an embedding matrix</head><p>Given a sentence x={w1, <ref type="bibr">w2</ref> The parameter m is the size of features. Its value is 6 in this paper, because we choose the following six kinds of features: POS, NER, hypernyms(WNSYN), position feature (PF), dependency feature (Dep), relative-dependency feature (Relative-Dep).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">BLSTM-based Sentence Level Representation</head><p>It is well known that humans can exploit longer context to mine the relationship of two nominals in a sentence. LSTM has shown its merit on capturing long distance relationship in different fields. With this motivation, we adopt BLSTM to get the sentence level representation.</p><p>The LSTM equations are given for a single memory block.</p><p>Input Gates: í µí± í µí±¡ = í µí¼(í µí± í µí±¥í µí± í µí±¥ í µí±¡ + í µí± ℎí µí± ℎ í µí±¡−1 + í µí± í µí±í µí± í µí± í µí±¡−1 + í µí± í µí± )</p><p>Forget Gates: í µí± í µí±¡ = í µí¼(í µí± í µí±¥í µí± í µí±¥ í µí±¡ + í µí± ℎí µí± ℎ í µí±¡−1 + í µí± í µí±í µí± í µí± í µí±¡−1 + í µí± í µí± )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cells: í µí± í µí±¡ = í µí± í µí±¡ í µí± í µí±¡−1 + í µí± í µí±¡ tanh(í µí± í µí±¥í µí± í µí±¥ í µí±¡ + í µí± ℎí µí± ℎ í µí±¡−1 + í µí± í µí± )</head><p>Output gates:</p><p>í µí± í µí±¡ = í µí¼(í µí± í µí±¥í µí± í µí±¥ í µí±¡ + í µí± ℎí µí± ℎ í µí±¡−1 + í µí± í µí±í µí± í µí± í µí±¡ + í µí± í µí± )</p><p>Cell Outputs: ℎ í µí±¡ = í µí± í µí±¡ tanh í µí± í µí±¡ where σ is the activation function, and i, f, o and c are respectively the input gate, forget gate, output gate and memory cell.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the network contained two sub-networks for the left and right sequence context. The outputs of these subnets for the ith word are integrated in the following way:</p><p>í µí°¹ í µí± = [í µí°¹_ℎ í µí± , í µí°¹_í µí± í µí± , í µí°µ_ℎ í µí± , í µí°µ_ℎ í µí± ] where F and B refer to forward and backward directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Constructing Feature Vector</head><p>Inspired by the work from <ref type="bibr" target="#b1">Zeng et al. (2014)</ref>, we extract and concatenate sentence level features and lexical level features to form the finally extracted feature vector.</p><p>Lexical level features are focused on the two target nominals e1and e2. We concatenate the vector got from feature embeddings and BLSTM layer to represent the two nominals as <ref type="bibr">[xe1, Fe1, xe2, Fe2]</ref>.</p><p>Sentence level features are focused on the context information, which are constructed from the output of BLSTM layer. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, the matrix got from BLSTM could be divided into A, B and C parts by e1 and e2. Max pooling operation is adopted to extract the vector from A and B parts, B and C parts respectively. The vector m1 and m2 is concatenated to form the sentence level representation. The motivation of constructing sentence level in this way is to strengthen the influence of the context between two entities, which are usually contained more information for indicating the relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Classifying</head><p>A multilayer perceptron (MLP) will be used for combining sentence level feature and lexical feature into the final extracted feature vector. Finally, the final extracted features are fed into a softmax classifier to predict the sematic relation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data and metrics</head><p>Experiments are conducted on the SemEval-2010 task 8 dataset <ref type="bibr" target="#b2">(Hendrickx et al., 2010)</ref>. It includes 8,000 training instances and 2,717 test instances. There are 9 relation types, and each type has two directions. If the instance could not refer to any of 9 relation types, there is a type Other.</p><p>We adopt the official evaluation metric to evaluate our systems, which is based on macro-averaged F1-score for the nine proper relations and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments setting</head><p>The dimension of feature embeddings used in the experiments are listed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Embedding We select two available trained word embeddings to see its influence to the classification performance. One is from <ref type="bibr" target="#b8">Turian et al. (2010)</ref>, the dimension of word embedding is 50. The other is from Jeffrey <ref type="bibr" target="#b9">Pennington et al. (2014)</ref>, the dimension of word embedding is 100.</p><p>As shown in the above, position feature (PF) contains two elements, and relative-dependency feature (Relative-Dep) contains three elements. Therefore, embedding dimension of PF is 2*5, that of RELATIVE-DEP is 3*10.</p><p>The BLSTM layer contains 400 units for each direction, and MLP layer contains 1000 units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Analysis</head><p>Firstly, we testify the performance of proposed BLSTM-based method with two feature set. One only uses word embedding as input, the other uses all features shown in section 4.1. We also list the results of CNN and CR-CNN methods as reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Feature   <ref type="table" target="#tab_3">In table 2</ref>, only using word embedding as input features, BLSTM-based method achieves F1 of 82.7, which is similar to the results of CNN with multiple features, and CR-CNN with only word embedding features. However, CR_CNN use word embeddings of size 400, our method use word embeddings of size 100. It proves that BLSTM-based method is effective to mine the relationship between two nominals. With more features, the performance achieves F1 of 84.3, which testifies general features gotten from NLP tools could improve the classification performance.</p><p>Secondly, we testify the influence of different features for the classification by removing one type of features from feature set in each time.</p><p>From <ref type="table" target="#tab_5">Table 3</ref>, we see that the performance has very slight change by removing position and NER features. It shows that BLSTM has better representation on sentence level relationship without position features. The information of position features is already contained in BLSTM networks. The whole features are considered from lexical and sentence level. The performances of removing PF or NER feature don't change obviously, maybe the information they contained is represented by other features.  Finally, we compare the results in different word embedding size. In <ref type="table" target="#tab_7">Table 4</ref> we give the result with using word embedding of size 50. It achieves a F1 of 83.6, about 0.7% less than that with using word embedding of size 100, which shows larger size of dimension of word embedding may contain more information, and it could improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Removed</head><p>We also compare the LSTM based method with only one direction such as forward or backward. The results shows BLSTM has a slight advantage over unidirectional LSTM.</p><p>Compared with proposed constructing sentence level feature vector in <ref type="figure" target="#fig_4">figure 5</ref>, we use Max pooling operation directly from A+B+C parts. The result shows F1 of 83.1, which is lower than our method with F1 of 83.6. It proves that our proposed method is effective.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose bidirectional long shortterm memory networks (BLSTM) to solve the relation classification. BLSTM is proposed to mine the sentence level representation. The experiment results show that only using word embedding as input features is enough to achieve state-of-the-art results. Importing more features could further improve the performance of the relation classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. LSTM memory block with one cell</figDesc><graphic url="image-1.png" coords="2,317.52,248.05,218.04,167.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Bidirectional LSTM</figDesc><graphic url="image-2.png" coords="2,306.12,640.20,218.16,109.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of relative position relationship features</figDesc><graphic url="image-3.png" coords="3,306.12,197.40,218.28,93.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, where V is the size of word vocabulary. Each word w has its embedding by using the matrix-vector product: w wrd w r W v  where w v is one-hot represenation, to get one column of the matrix W wrd .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Constructing sentence level feature vector</figDesc><graphic url="image-5.png" coords="4,317.52,159.48,209.88,93.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 . Comparison with previously published results</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 . Results of removing one kind of feature</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 . Results of removing one kind of feature</head><label>4</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">UTD: Classifying Semantic Relations by Combining Lexical and Semantic Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Workshop on Semantic Evaluation</title>
		<meeting>5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2010 Task 8: Multi-way Classification of Semantic Relations Between Pairs of Nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Diarmuid Ó Sé Aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Dependency-based Neural Network for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying Relations by Ranking with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cí Cero Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Novel Connectionist System for Improved Unconstrained Handwriting Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ferná Ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech Recognition with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Understanding the Difficulty of Training Deep Feedforward Neural Networks. International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factor-based Compositional Embedding Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<editor>Kremer, S. C.and Kolen, J. F.,</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
	<note>editors, A Field Guide to Dynamical Recurrent Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Precise Timing with LSTM Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
