<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<email>soegaard@hum.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<email>yoav.goldberg@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Copenhagen</orgName>
								<orgName type="institution" key="instit2">Ilan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep multi-task learning with low level tasks supervised at lower layers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="231" to="235"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In all previous work on deep multi-task learning we are aware of, all task super-visions are on the same (outermost) layer. We present a multi-task learning architecture with deep bi-directional RNNs, where different tasks supervision can happen at different layers. We present experiments in syntactic chunking and CCG supertag-ging, coupled with the additional task of POS-tagging. We show that it is consistently better to have POS supervision at the innermost rather than the outermost layer. We argue that this is because &quot;low-level&quot; tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks. Finally, we also show how this architecture can be used for domain adaptation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We experiment with a multi-task learning (MTL) architecture based on deep bi-directional recurrent neural networks (bi-RNNs) <ref type="bibr" target="#b10">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b6">Irsoy and Cardie, 2014</ref>). MTL can be seen as a way of regularizing model induction by sharing representations (hidden layers) with other inductions <ref type="bibr" target="#b0">(Caruana, 1993)</ref>. We use deep bi-RNNs with task supervision from multiple tasks, sharing one or more bi-RNNs layers among the tasks. Our main contribution is the novel insight that (what has historically been thought of as) low-level tasks are better modeled in the low layers of such an architecture. This is in contrast to previous work on deep MTL <ref type="bibr" target="#b2">(Collobert et al., 2011;</ref><ref type="bibr" target="#b7">Luong et al., 2015)</ref> , in which supervision for all tasks happen at the same (outermost) layer. Multiple-tasks supervision at the outermost layer has a strong tradition in neural net models in vision and elsewhere <ref type="bibr" target="#b0">(Caruana, 1993;</ref><ref type="bibr" target="#b15">Zhang and Zhang, 2014;</ref><ref type="bibr" target="#b14">Yim et al., 2015</ref>). However, in NLP it is natural to think of some levels of analysis as feeding into others, typically with low-level tasks feeding into highlevel ones; e.g., POS tags as features for syntactic chunking <ref type="bibr" target="#b9">(Sang and Buchholz, 2000</ref>) or parsing ( <ref type="bibr" target="#b8">Nivre et al., 2007)</ref>. Our architecture can be seen as a seamless way to combine multi-task and cascaded learning. We also show how the proposed architecture can be applied to domain adaptation, in a scenario in which we have high-level task supervision in the source domain, and lower-level task supervision in the target domain.</p><p>As a point of comparison, Collobert et al. (2011) improved deep convolutional neural network models of syntactic chunking by also having task supervision from POS tagging at the outermost level. In our work, we use recurrent instead of convolutional networks, but our main contribution is observing that we obtain better performance by having POS task supervision at a lower layer. While Collobert et al. (2011) also experiment with NER and SRL, they only obtain improvements from MTL with POS and syntactic chunking. We show that similar gains can be obtained for CCG supertagging.</p><p>Our contributions (i) We present a MTL architecture for sequence tagging with deep bi-RNNs; (ii) We show that having task supervision from all tasks at the outermost level is often suboptimal; (iii) we show that this architecture can be used for domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sequence tagging with deep bi-RNNs</head><p>Notation We use x 1:n to denote a sequence of n vectors x 1 , · · · , x n . F θ (·) is a function parameterized with parameters θ. We write F L (·) as a shortcut to F θ L -an instantiation of F with a spe-cific set of parameters θ L . We use • to denote a vector concatenation operation.</p><p>Deep bi-RNNs We use a specific flavor of Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b3">(Elman, 1990)</ref> called long short-term memory networks (LSTMs) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>). For brevity, we treat RNNs as a black-box abstraction, and LSTMs as an instance of the RNN interface. For further details on RNNs and LSTMs, see <ref type="bibr">(Gold- berg, 2015;</ref><ref type="bibr" target="#b1">Cho, 2015)</ref>. We view RNN as a parameterized function RN N θ (x 1:n ) mapping a sequence of n input vectors x 1:n , x i ∈ R d in to a an output vector h n ∈ R dout . The output vector h n is conditioned on all the input vectors x 1:n , and can be thought of as a summary of x 1:n . The RNN can be applied to all prefixes x 1:i , 1 ≤ i ≤ n of x 1:n , resulting in n output vectors h 1:n , where h 1:i summarizes x 1:i .</p><p>A deep RNN (or k-layer RNN) is composed of k RNN functions RN N 1 , · · · , RN N k that feed into each other: the output h 1:n of RN N becomes the input of RN N +1 . Stacking RNNs in this way was empirically shown to be effective.</p><p>A bidirectional RNN <ref type="bibr" target="#b10">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b6">Irsoy and Cardie, 2014</ref>) is composed of two RNNs, RN N F and RN N R , one reading the sequence in its regular order, and the other reading it in reverse. Concretely, given a sequence x 1:n and a desired index i, the function BIRN N θ (x 1:n , i) is defined as:</p><formula xml:id="formula_0">BIRN N θ (x 1:n , i) = v i = h F,i • h R,i h F,i = RN N F (x 1 , x 2 , · · · , x i ) h R,i = RN N R (x n , x n−1 , · · · , x i )</formula><p>The vector v i = BIRN N (x 1:n , i) is then a representation of the ith item in x 1:n , taking into account both the entire history x 1:i and the entire future x i:n .</p><p>Finally, in a deep bidirectional RNN, both RN N F and RN N R are k-layer RNNs, and</p><formula xml:id="formula_1">BIRN N (x 1:n , i) = v i = h F,i • h R,i</formula><p>. Greedy sequence tagging with deep bi-RNNs In a sequence tagging task, we are given an input w 1 , · · · , w n and need to predict an output</p><formula xml:id="formula_2">y 1 , · · · , y n , y i ∈ [1, · · · , |L|],</formula><p>where L is a label set of interest; i.e., in a POS tagging task, L is the part-of-speech tagset, and y i is the pos-tag for word w i .</p><p>If we take the inputs x 1:n to correspond to a sequence of sentence words w 1 , · · · , w n , we can think of v i = BIRN N (x 1:n , i) as inducing an infinite window around a focus word w i . We can then use v i as an input to a multiclass classification function f (v i ), to assign a tagˆytagˆ tagˆy i to each input location i. The tagger is greedy: the tagging decisions are independent of each other. However, as shown below and in other recent work using bi-RNNs for sequence tagging, we can still produce competitive tagging accuracies, because of the richness of the representation v i that takes the entire input sequence into account.</p><p>For a k-layer bi-RNN tagger we get:</p><formula xml:id="formula_3">tag(w 1:n , i) = ˆ y i = f (v k i ) v k i = BIRN N k (x 1:n , i) x 1:n = E(w 1 ), E(w 2 ), · · · , E(w n )</formula><p>where E as an embedding function mapping each word in the vocabulary into a d emb -dimensional vector, and v k i is the output of the kth BIRNN layer as defined above.</p><p>All the parameters (the embedding vectors for the different vocabulary items, the parameters of the different RNNs and the parameters of the classification function f ) are trained jointly in order to minimize the tagging loss over a sentence. The embedding vectors are often initialized using vectors that were pre-trained in a semi-supervised manner.</p><p>This sequence tagging architecture was introduced to NLP by <ref type="bibr" target="#b6">Irsoy and Cardie (2014)</ref>. A similar architecture (with an RNN instead of bi-RNN) was applied to CCG supertagging by <ref type="bibr" target="#b13">Xu et al (2015)</ref>.</p><p>MTL in deep bi-RNNs In a multi-task learning (MTL) setting, we have several prediction tasks over the same input space. For example, in sequence tagging, the input may be the words in the sentence, and the different tasks can be POS-tagging, named entity recognition, syntactic chunking, or CCG supertagging. Note that the different tasks do not have to be traditional NLP tasks, but also, say, two POS-annotated corpora with slightly different guidelines. Each task has its own output vocabulary (a task specific tagset), but all of them map the length n input sequence into a length n output sequence.</p><p>Intuitively, although NLP tasks such as POS tagging, syntactic chunking and CCG supertagging are different than each other, they also share lot of substructure, e.g., knowing that a word is a verb can help in determining its CCG supertag and the syntactic chunk it participate in. We would therefore like for these models to share parameters.</p><p>The common approach is to share parameters across most of the network. In the k-layers deep bi-RNN tagger described above this is naturally achieved by sharing the bi-RNN part of the network across tasks, but training a specialized classification tagger f t (v k i ) for each task t. This encourages the deep bi-RNN to learn a representation v k i that is useful for prediction of the different tasks, allowing them to share parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervising different tasks on different layers</head><p>Previous work in NLP on cascaded learning such as <ref type="bibr" target="#b11">Shen and Sarkar (2005)</ref> suggests there is sometimes a natural order among the different tasks: some tasks may benefit more from other tasks, than the other way around. This suggests having task supervision for low-level tasks at the lower bi-RNN layers. This also enables task-specific deep learning of the high-level tasks.</p><p>Instead of conditioning all tasks on the outermost bi-RNN layer, we associate an RNN level (t) with each task t, and let the task specific classifier feed from that layer, e.g., pos tag(w 1:n , i) = f pos (v ). This enables a hierarchy a task with cascaded predictions, as well as deep task-specific learning for high-level tasks. This means there will be layers shared by all tasks and layers that are specific to some tasks:</p><formula xml:id="formula_4">pos tag(w 1:n , i) = f pos (v (pos) i ) chunk tag(w 1:n , i) = f chunk (v (chunk) i ) ccg tag(w 1:n , i) = f ccg (v (ccg) i ) v i = BIRN N (x 1:n , i) x 1:n = E(w 1 ), E(w 2 ), · · · , E(w n )</formula><p>The Multi-task training protocol We assume T different training set, D 1 , · · · , D T , where each D t contains pairs of input-output sequences (w 1:n , y t 1:n ), w i ∈ V , y t i ∈ L t . The input vocabulary V is shared across tasks, but the output vocabularies (tagset) L t are task dependent.</p><p>At each step in the training process we choose a random task t, followed by a random training instance (w 1:n , y t 1:n ) ∈ D t . We use the tagger to predict the labelsˆylabelsˆ labelsˆy t i , suffer a loss with respect to the true labels y t i and update the model parameters. Notice that a task t is associated with a bi-RNN level (t). The update for a sample from task t affects the parameters of f t and BIRN N 1 , · · · , BIRN N (t) , but not the parameters of f t =t or BIRN N j&gt;&gt;(t) .</p><p>Implementation details Our implementation is based the CNN library 1 for dynamic neural networks. We use CNN's LSTM implementation as our RNN variant. The classifiers f t () take the form of a linear transformation followed by a softmax f t (v) = arg max i sof tmax(W (t) v+b t ) <ref type="bibr">[i]</ref>, where the weights matrix W (t) and bias vector b (t) are task-specific parameters. We use a cross-entropy loss summed over the entire sentence. The network is trained using back-propagation and SGD with batch-sizes of size 1, with the default learning rate. Development data is used to determine the number of iterations.</p><p>We initialize the embedding layer E with pretrained word embeddings. We use the Senna embeddings 2 in our domain adaptation experiments, but these embeddings may have been induced from data including the test data of our main experiments, so we use the Polyglot embeddings in these experiments. <ref type="bibr">3</ref> We use the same dimensionality for the hidden layers as in our pre-trained embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We experiment with POS-tagging, syntactic chunking and CCG supertagging. See examples of the different tasks below:</p><formula xml:id="formula_5">WORDS Vinken , 61 years old POS NNP , CD NNS JJ CHUNKS B-NP I-NP I-NP I-NP I-NP CCG N , N/N N (S[adj]\ NP)\ NP</formula><p>In-domain MTL In these experiments, POS, Chunking and CCG data are from the English Penn Treebank. We use sections 0-18 for training POS and CCG supertagging, 15-18 for training chunking, 19 for development, 20 for evaluating chunking, and 23 for evaluating CCG supertagging. These splits were motivated by the need for comparability with previous results. <ref type="bibr">4</ref>  <ref type="formula">(1)</ref>   We do MTL training for either (POS+chunking) or (POS+CCG), with POS being the lower-level task. We experiment three architectures: single task training for higher-level tasks (no POS layer), MTL with both tasks feeding off of the outer layer, and MTL where POS feeds off of the inner (1st) layer and the higher-level task on the outer <ref type="formula">(3rd</ref> Our CHUNKS results are competitive with stateof-the-art. <ref type="bibr" target="#b12">Suzuki and Isozaki (2008)</ref>, for example, reported an F 1 -score of 95.15% on the CHUNKS data. Our model also performs considerably better than the MTL model in <ref type="bibr">Collobert et al. (2011) (94.10%)</ref>. Note that our relative improvements are also bigger than those reported by <ref type="bibr" target="#b2">Collobert et al. (2011)</ref>. Our CCG super tagging results are also slighly better than a recently reported result in <ref type="bibr" target="#b13">Xu et al. (2015)</ref> (93.00%). Our results are significantly better (p &lt; 0.05) than our baseline, and POS supervision at the lower layer is consistently better than standard MTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAYERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DOMAINS CHUNKS POS BROADCAST (6) BC-NEWS (8) MAGAZINES</head><p>Additional tasks? We also experimented with NER (CoNLL 2003), super senses (SemCor), and the Streusle Corpus of texts annotated with MWE brackets and super sense tags. In none of these cases, MTL led to improvements. This suggests that MTL only works when tasks are sufficiently similar, e.g., all of syntactic nature. Collobert et al. (2011) also observed a drop in NER performance and insignificant improvements for SRL. We believe this is an important observation, since previous work on deep MTL often suggests that most tasks benefit from each other.</p><p>Domain adaptation We experiment with domain adaptation for syntactic chunking, based on OntoNotes 4.0. We use WSJ newswire as our source domain, and broadcast, broadcasted news, magazines, and weblogs as target domains. We assume main task (syntactic chunking) supervision for the source domain, and lower-level POS supervision for the target domains. The results in <ref type="table" target="#tab_1">Table 1</ref> indicate that the method is effective for domain adaptation when we have POS supervision for the target domain. We believe this result is worth exploring further, as the scenario in which we have target-domain training data for low-level tasks such as POS tagging, but not for the task we are interested in, is common. The method is effective only when the lower-level POS supervision is applied at the lower layer, supporting the importance of supervising different tasks at different layers.</p><p>Rademacher complexity is the ability of models to fit random noise. We use the procedure in <ref type="bibr" target="#b16">Zhu et al. (2009)</ref> to measure Rademacher complexity, i.e., computing the average fit to k random relabelings of the training data. The subtask in our set-up acts like a regularizer, increasing the inductive bias of our model, preventing it from learning random patterns in data. Rademacher complexity measures the decrease in ability to learn such patterns. We use the CHUNKS data in these experiments. A model that does not fit to the random data, will be right in 1/22 cases (with 22 labels). We report the Rademacher complexities relative to this. 1.298 1.034 0.990</p><p>Our deep single task model increases performance over this baseline by 30%. In contrast, we see that when we predict both POS and the target task at the top layer, Rademacher complexity is lower and close to a random baseline. Interestingly, regularization seems to be even more effective, when the subtask is predicted from a lower layer. <ref type="bibr">234</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>MTL and sharing of intermediate representations, allowing supervision signals of different tasks to benefit each other, is an appealing idea. However, in case we suspect the existence of a hierarchy between the different tasks, we show that it is worthwhile to incorporate this knowledge in the MTL architecture's design, by making lower level tasks affect the lower levels of the representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Domain adaptation results for chunking across four domains (averages over micro-F 1 s</head><label>1</label><figDesc></figDesc><table>for 
</table></figure>

			<note place="foot" n="1"> http://www.github.com/clab/cnn 2 http://ronan.collobert.com/senna/ 3 http://polyglot.readthedocs.org 4 In CCG supertagging, we follow common practice and only evaluate performance with respect to the 425 most frequent labels. For this reason, we also do not calculate any loss from not predicting the other labels during training (but we do suffer a loss for tokens tagged with a different label during evaluation).</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multitask learning: a knowledgebased source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural language understanding with distributed representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1511.07916</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding Structure in Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1510.00726</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opinion Mining with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<title level="m">Multi-task Sequence to Sequence Learning</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 shared task on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>the CoNLL Shared Task Session of EMNLP-CoNLL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2000 shared task chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
			<publisher>November</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voting between multiple data representations for text chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Meeting of the Canadian Society for Computational Intelligence</title>
		<meeting>the 18th Meeting of the Canadian Society for Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ccg supertagging with a recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In ACL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rotating Your Face Using Multi-task Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungin</forename><surname>Yoo Amd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkyu</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving Multiview Face Detection with Multi-Task Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human Rademacher complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
