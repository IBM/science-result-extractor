<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>April 3-7, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
							<email>esantus@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Schlechtweg</surname></persName>
							<email>dominik.schlechtweg@gmx.de</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Stuttgart</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection</title>
					</analytic>
					<monogr>
						<title level="j" type="main">the Association for Computational Linguistics</title>
						<meeting>the 15th Conference of the European Chapter <address><addrLine>Valencia, Spain</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="volume">1</biblScope>
							<biblScope unit="page" from="65" to="75"/>
							<date type="published">April 3-7, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The fundamental role of hypernymy in NLP has motivated the development of many methods for the automatic identification of this relation, most of which rely on word distribution. We investigate an extensive number of such unsupervised measures, using several distributional semantic models that differ by context type and feature weighting. We analyze the performance of the different methods based on their linguistic motivation. Comparison to the state-of-the-art supervised methods shows that while supervised methods generally outperform the unsupervised ones, the former are sensitive to the distribution of training instances, hurting their reliability. Being based on general linguistic hypotheses and independent from training data, unsupervised measures are more robust , and therefore are still useful artillery for hypernymy detection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last two decades, the NLP community has invested a consistent effort in developing automated methods to recognize hypernymy. Such effort is motivated by the role this semantic relation plays in a large number of tasks, such as taxonomy creation ( <ref type="bibr" target="#b38">Snow et al., 2006;</ref><ref type="bibr" target="#b24">Navigli et al., 2011</ref>) and recognizing textual entailment ( <ref type="bibr" target="#b9">Dagan et al., 2013)</ref>. The task has appeared to be, however, a challenging one, and the numerous approaches proposed to tackle it have often shown limitations.</p><p>Early corpus-based methods have exploited patterns that may indicate hypernymy (e.g. "animals such as dogs") <ref type="bibr" target="#b14">(Hearst, 1992;</ref><ref type="bibr" target="#b37">Snow et al., 2005</ref>), but the recall limitation of this approach, requiring both words to co-occur in a sentence, motivated the development of methods that rely on adaptations of the distributional hypothesis <ref type="bibr">(Har- ris, 1954)</ref>.</p><p>The first distributional approaches were unsupervised, assigning a score for each (x, y) wordpair, which is expected to be higher for hypernym pairs than for negative instances. Evaluation is performed using ranking metrics inherited from information retrieval, such as Average Precision (AP) and Mean Average Precision (MAP). Each measure exploits a certain linguistic hypothesis such as the distributional inclusion hypothesis <ref type="bibr" target="#b40">(Weeds and Weir, 2003;</ref><ref type="bibr" target="#b17">Kotlerman et al., 2010)</ref> and the distributional informativeness hypothesis ( <ref type="bibr" target="#b32">Santus et al., 2014;</ref><ref type="bibr" target="#b28">Rimell, 2014</ref>).</p><p>In the last couple of years, the focus of the research community shifted to supervised distributional methods, in which each (x, y) word-pair is represented by a combination of x and y's word vectors (e.g. concatenation or difference), and a classifier is trained on these resulting vectors to predict hypernymy ( <ref type="bibr" target="#b3">Baroni et al., 2012;</ref><ref type="bibr" target="#b30">Roller et al., 2014;</ref><ref type="bibr" target="#b41">Weeds et al., 2014</ref>). While the original methods were based on count-based vectors, in recent years they have been used with word embeddings ( <ref type="bibr" target="#b23">Mikolov et al., 2013;</ref><ref type="bibr" target="#b27">Pennington et al., 2014)</ref>, and have gained popularity thanks to their ease of use and their high performance on several common datasets. However, there have been doubts on whether they can actually learn to recognize hypernymy ( <ref type="bibr" target="#b21">Levy et al., 2015b)</ref>.</p><p>Additional recent hypernymy detection methods include a multimodal perspective ( <ref type="bibr" target="#b16">Kiela et al., 2015</ref>), a supervised method using unsupervised measure scores as features ( <ref type="bibr" target="#b34">Santus et al., 2016a)</ref>, and a neural method integrating path-based and distributional information <ref type="bibr" target="#b36">(Shwartz et al., 2016)</ref>.</p><p>In this paper we perform an extensive evaluation of various unsupervised distributional measures for hypernymy detection, using several distributional semantic models that differ by context type and feature weighting. Some measure vari-ants and context-types are tested for the first time. <ref type="bibr">1</ref> We demonstrate that since each of these measures captures a different aspect of the hypernymy relation, there is no single measure that consistently performs well in discriminating hypernymy from different semantic relations. We analyze the performance of the measures in different settings and suggest a principled way to select the suitable measure, context type and feature weighting according to the task setting, yielding consistent performance across datasets.</p><p>We also compare the unsupervised measures to the state-of-the-art supervised methods. We show that supervised methods outperform the unsupervised ones, while also being more efficient, computed on top of low-dimensional vectors. At the same time, however, our analysis reassesses previous findings suggesting that supervised methods do not actually learn the relation between the words, but only characteristics of a single word in the pair ( <ref type="bibr" target="#b21">Levy et al., 2015b</ref>). Moreover, since the features in embedding-based classifiers are latent, it is difficult to tell what the classifier has learned. We demonstrate that unsupervised methods, on the other hand, do account for the relation between words in a pair, and are easily interpretable, being based on general linguistic hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Distributional Semantic Spaces</head><p>We created multiple distributional semantic spaces that differ in their context type and feature weighting. As an underlying corpus we used a concatenation of the following two corpora: ukWaC <ref type="bibr">(Fer- raresi, 2007</ref>), a 2-billion word corpus constructed by crawling the .uk domain, and WaCkypedia EN ( <ref type="bibr" target="#b2">Baroni et al., 2009</ref>), a 2009 dump of the English Wikipedia. Both corpora include POS, lemma and dependency parse annotations. Our vocabulary (of target and context words) includes only nouns, verbs and adjectives that occurred at least 100 times in the corpus.</p><p>Context Type We use several context types:</p><p>• Window-based contexts: the contexts of a target word w i are the words surrounding it in a ksized window: w i−k , ..., w i−1 , w i+1 , ..., w i+k .</p><p>If the context-type is directional, words occurring before and after w i are marked differently, i.e.:   <ref type="figure">Figure 1</ref>: An example dependency tree of the sentence cute cats drink milk, with the target word cats. The dependencybased contexts are drink-v:nsubj and cute-a:amod −1 . The joint-dependency context is drink-v#milk-n. Differently from <ref type="bibr" target="#b6">Chersoni et al. (2016)</ref>, we exclude the dependency tags to mitigate the sparsity of contexts.</p><formula xml:id="formula_0">w i−k /l, ..., w i−1 /l, w i+1 /r, ..., w i+k /r.</formula><p>Out-of-vocabulary words are filtered out before applying the window. We experimented with window sizes 2 and 5, directional and indirectional (win2, win2d, win5, win5d).</p><p>• Dependency-based contexts: rather than adjacent words in a window, we consider neighbors in a dependency parse tree <ref type="bibr" target="#b25">(Padó and Lapata, 2007;</ref><ref type="bibr" target="#b0">Baroni and Lenci, 2010)</ref>. The contexts of a target word w i are its parent and daughter nodes in the dependency tree (dep). We also experimented with a joint dependency context inspired by <ref type="bibr" target="#b6">Chersoni et al. (2016)</ref>, in which the contexts of a target word are the parent-sister pairs in the dependency tree (joint). Feature Weighting Each distributional semantic space is spanned by a matrix M in which each row corresponds to a target word while each column corresponds to a context. The value of each cell M i,j represents the association between the target word w i and the context c j . We experimented with two feature weightings:</p><p>• Frequency -raw frequency (no weighting): M i,j is the number of co-occurrences of w i and c j in the corpus.</p><p>• Positive PMI (PPMI) -pointwise mutual information (PMI) <ref type="bibr" target="#b7">(Church and Hanks, 1990</ref>) is defined as the log ratio between the joint probability of w and c and the product of their marginal probabilities:</p><formula xml:id="formula_1">P M I(w, c) = logˆP logˆ logˆP (w,c) ˆ P (w) ˆ P (c)</formula><p>, wherê P (w), ˆ P (c), andˆPandˆ andˆP (w, c) are estimated by the relative frequencies of a word w, a context c and a word-context pair (w, c), respectively. To handle unseen pairs (w, c), yielding P M I(w, c) = log(0) = −∞, PPMI (Bullinaria and <ref type="bibr" target="#b5">Levy, 2007</ref>) assigns zero to negative PMI scores: P P M I(w, c) = max(P M I(w, c), 0).</p><p>In addition, one of the measures we used <ref type="bibr">(San- tus et al., 2014</ref>) required a third feature weighting:</p><p>• Positive LMI (PLMI) -positive local mutual information (PLMI) <ref type="bibr" target="#b10">(Evert, 2005;</ref><ref type="bibr">Ev- ert, 2008)</ref>. PPMI was found to have a bias towards rare events. PLMI simply balances PPMI by multiplying it by the co-occurrence frequency of w and c: P LM I(w, c) = f req(w, c) · P P M I(w, c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Hypernymy Detection Measures</head><p>We experiment with a large number of unsupervised measures proposed in the literature for distributional hypernymy detection, with some new variants. In the following section, v x and v y denote x and y's word vectors (rows in the matrix M ). We consider the scores as measuring to what extent y is a hypernym of x (x → y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Similarity Measures</head><p>Following the distributional hypothesis <ref type="bibr" target="#b13">(Harris, 1954)</ref>, similar words share many contexts, thus have a high similarity score. Although the hypernymy relation is asymmetric, similarity is one of its properties ( <ref type="bibr" target="#b32">Santus et al., 2014</ref>).</p><p>• Cosine Similarity ( <ref type="bibr" target="#b31">Salton and McGill, 1986)</ref> A symmetric similarity measure:</p><formula xml:id="formula_2">cos(x, y) = v x · v y v x · v y</formula><p>• Lin Similarity (Lin, 1998) A symmetric similarity measure that quantifies the ratio of shared contexts to the contexts of each word:</p><formula xml:id="formula_3">Lin(x, y) = Σ c∈ vx∩ vy [ v x [c] + v y [c]] Σ c∈ vx v x [c] + Σ c∈ vy v y [c]</formula><p>• APSyn (Santus et al., 2016b) A symmetric measure that computes the extent of intersection among the N most related contexts of two words, weighted according to the rank of the shared contexts (with N as a hyper-parameter):</p><formula xml:id="formula_4">AP Syn(x, y) = Σ c∈N ( vx)∩N ( vy) 1 rankx(c)+ranky(c) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inclusion Measures</head><p>According to the distributional inclusion hypothesis, the prominent contexts of a hyponym (x) are expected to be included in those of its hypernym (y).</p><p>• Weeds Precision ( <ref type="bibr" target="#b40">Weeds and Weir, 2003)</ref> A directional precision-based similarity measure. This measure quantifies the weighted inclusion of x's contexts by y's contexts:</p><formula xml:id="formula_5">W eedsP rec(x → y) = Σ c∈ vx∩ vy v x [c] Σ c∈ vx v x [c]</formula><p>• cosWeeds (Lenci and Benotto, 2012) Geometric mean of cosine similarity and Weeds precision:</p><formula xml:id="formula_6">cosW eeds(x → y) = cos(x, y) · W eedsP rec(x → y)</formula><p>• ClarkeDE (Clarke, 2009) Computes degree of inclusion, by quantifying weighted coverage of the hyponym's contexts by those of the hypernym:</p><formula xml:id="formula_7">CDE(x → y) = Σ c∈ vx∩ vy min( vx[c], vy[c]) Σ c∈ vx vx[c]</formula><p>• balAPinc ( <ref type="bibr" target="#b17">Kotlerman et al., 2010</ref>) Balanced average precision inclusion.</p><formula xml:id="formula_8">AP inc(x → y) = Ny r=1 [P (r) · rel(cr)] Ny</formula><p>is an adaptation of the average precision measure from information retrieval for the inclusion hypothesis. N y is the number of non-zero contexts of y and P (r) is the precision at rank r, defined as the ratio of shared contexts with y among the top r contexts of x. rel(c) is the relevance of a context c, set to 0 if c is not a context of y, and to 1 − ranky(c)</p><p>Ny+1 otherwise, where rank y (c) is the rank of the context c in y's sorted vector. Finally,</p><formula xml:id="formula_9">balAP inc(x → y) = Lin(x, y) · AP inc(x → y)</formula><p>is the geometric mean of APinc and Lin similarity.</p><p>• invCL (Lenci and Benotto, 2012) Measures both distributional inclusion of x in y and distributional non-inclusion of y in x:</p><formula xml:id="formula_10">invCL(x → y) = CDE(x → y) · (1 − CDE(y → x))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Informativeness Measures</head><p>According to the distributional informativeness hypothesis, hypernyms tend to be less informative than hyponyms, as they are likely to occur in more general contexts than their hyponyms.</p><p>• SLQS ( <ref type="bibr" target="#b32">Santus et al., 2014</ref>)</p><formula xml:id="formula_11">SLQS(x → y) = 1 − E x E y</formula><p>The informativeness of a word x is evaluated as the median entropy of its top N contexts:</p><formula xml:id="formula_12">E x = median N i=1 (H(c i ))</formula><p>, where H(c) is the entropy of context c.</p><p>• SLQS Sub A new variant of SLQS based on the assumption that if y is judged to be a hypernym of x to a certain extent, then x should be judged to be a hyponym of y to the same extent (which is not the case for regular SLQS). This is achieved by subtraction:</p><formula xml:id="formula_13">SLQS sub (x → y) = E y − E x</formula><p>It is weakly symmetric in the sense that</p><formula xml:id="formula_14">SLQS sub (x → y) = −SLQS sub (y → x).</formula><p>SLQS and SLQS Sub have 3 hyper-parameters: i) the number of contexts N ; ii) whether to use median or average entropy among the top N contexts; and iii) the feature weighting used to sort the contexts by relevance (i.e., PPMI or PLMI).</p><p>• SLQS Row Differently from SLQS, SLQS Row computes the entropy of the target rather than the average/median entropy of the contexts, as an alternative way to compute the generality of a word. <ref type="bibr">2</ref> In addition, parallel to SLQS we tested SLQS Row with subtraction, SLQS Row Sub.</p><p>• RCTC (Rimell, 2014) Ratio of change in topic coherence:</p><formula xml:id="formula_15">RCT C(x → y) = T C(tx)/T C(t x\y ) T C(ty)/T C(t y\x )</formula><p>where t x are the top N contexts of x, considered as x's topic, and t x\y are the top N contexts of x which are not contexts of y. T C(A) is the topic coherence of a set of words A, defined as the median pairwise PMI scores between words in A. N is a hyper-parameter. The measure is based on the assumptions that excluding y's contexts from x's increases the coherence of the topic, while excluding x's contexts from y's decreases the coherence of the topic. We include this measure under the informativeness inclusion, as it is based on a similar hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reversed Inclusion Measures</head><p>These measures are motivated by the fact that, even though-being more general-hypernyms are expected to occur in a larger set of contexts, sentences like "the vertebrate barks" or "the mammal arrested the thieves" are not common, since hyponyms are more specialized and are hence more appropriate in such contexts. On the other hand, hyponyms are likely to occur in broad contexts (e.g. eat, live), where hypernyms are also appropriate. In this sense, we can define the reversed inclusion hypothesis: "hypernym's contexts are likely to be included in the hyponym's contexts". The following variants are tested for the first time.</p><p>•</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reversed Weeds</head><p>RevW eeds(x → y) = W eeds(y → x)</p><formula xml:id="formula_16">• Reversed ClarkeDE RevCDE(x → y) = CDE(y → x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>We use four common semantic relation datasets: BLESS (Baroni and Lenci, 2011), EVALution ( <ref type="bibr" target="#b33">Santus et al., 2015)</ref>, <ref type="bibr">Lenci/Benotto (Benotto, 2015)</ref>, and Weeds ( <ref type="bibr" target="#b41">Weeds et al., 2014</ref>). The datasets were constructed either using knowledge resources (e.g. WordNet, Wikipedia), crowdsourcing or both. The semantic relations and the size of each dataset are detailed in <ref type="table">Table 1</ref>.</p><p>In our distributional semantic spaces, a target word is represented by the word and its POS tag. While BLESS and Lenci/Benotto contain this information, we needed to add POS tags to the other datasets. For each pair (x, y), we considered 3 pairs (x-p, y-p) for p ∈ {noun, adjective, verb}, and added the respective pair to the dataset only if the words were present in the corpus.   <ref type="table">Table 2</ref>: Best performing unsupervised measures on each dataset in terms of Average Precision (AP) at k = 100, for hypernym vs. all other relations and vs. each single relation. AP for k = all is also reported for completeness. We excluded the experiments of hypernym vs. random-(n, v, j) for brevity; most of the similarity and some of the inclusion measures achieve AP @100 = 1.0 in these experiments.</p><p>We split each dataset randomly to 90% test and 10% validation. The validation sets are used to tune the hyper-parameters of several measures: SLQS (Sub), APSyn and RCTC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparing Unsupervised Measures</head><p>In order to evaluate the unsupervised measures described in Section 3, we compute the measure scores for each (x, y) pair in each dataset. We first measure the method's ability to discriminate hypernymy from all other relations in the dataset, i.e. by considering hypernyms as positive instances, and other word pairs as negative instances. In addition, we measure the method's ability to discriminate hypernymy from every other relation in the dataset by considering one relation at a time. For a relation R we consider only (x, y) pairs that are annotated as either hypernyms (positive instances) or R (negative instances). We rank the pairs according to the measure score and compute average precision (AP) at k = 100 and k = all. 5 lated differently in each sense. We consider y as a hypernym of x if hypernymy holds in some of the words' senses. Therefore, when a pair is assigned both hypernymy and another relation, we only keep it as hypernymy. <ref type="bibr">5</ref> We tried several cut-offs and chose the one that seemed to be more informative in distinguishing between the unsupervised measures. <ref type="table">Table 2</ref> reports the best performing measure(s), with respect to AP @100, for each relation in each dataset. The first observation is that there is no single combination of measure, context type and feature weighting that performs best in discriminating hypernymy from all other relations. In order to better understand the results, we focus on the second type of evaluation, in which we discriminate hypernyms from each other relation.</p><p>The results show preference to the syntactic context-types (dep and joint), which might be explained by the fact that these contexts are richer (as they contain both proximity and syntactic information) and therefore more discriminative. In feature weighting there is no consistency, but interestingly, raw frequency appears to be successful in hypernymy detection, contrary to previously reported results for word similarity tasks, where PPMI was shown to outperform it ( <ref type="bibr" target="#b5">Bullinaria and Levy, 2007;</ref><ref type="bibr" target="#b20">Levy et al., 2015a</ref>).</p><p>The new SLQS variants are on top of the list in many settings. In particular they perform well in discriminating hypernyms from symmetric relations (antonymy, synonymy, coordination).</p><p>The measures based on the reversed inclusion hypothesis performed inconsistently, achieving perfect score in the discrimination of hypernyms from unrelated words, and performing well  in few other cases, always in combination with syntactic contexts.</p><p>Finally, the results show that there is no single combination of measure and parameters that performs consistently well for all datasets and classification tasks. In the following section we analyze the best combination of measure, context type and feature weighting to distinguish hypernymy from any other relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Best Measure Per Classification Task</head><p>We considered all relations that occurred in two datasets. For such relation, for each dataset, we ranked the measures by their AP@100 score, selecting those with score ≥ 0.8. 6 <ref type="table">Table 3</ref> displays the intersection of the datasets' best measures.</p><p>Hypernym vs. Meronym The inclusion hypothesis seems to be most effective in discriminating between hypernyms and meronyms under syntactic contexts. We conjecture that the windowbased contexts are less effective since they capture topical context words, that might be shared also among holonyms and their meronyms (e.g. car will occur with many of the neighbors of wheel). However, since meronyms and holonyms often have different functions, their functional contexts, which are expressed in the syntactic context-types, are less shared. This is where they mostly differ from hyponym-hypernym pairs, which are of the same function (e.g. cat is a type of animal). <ref type="table">Table 2</ref> shows that SLQS performs well in this task on BLESS. This is contrary to previous findings that suggested that SLQS is weak in discriminating between hypernyms and meronyms, as in many cases the holonym is more general than the meronym ( <ref type="bibr" target="#b36">Shwartz et al., 2016</ref>). 7 The <ref type="bibr">6</ref> We considered at least 10 measures, allowing scores slightly lower than 0.8 when others were unavailable. <ref type="bibr">7</ref> In the hypernymy dataset of <ref type="bibr" target="#b36">Shwartz et al. (2016)</ref>, surprising result could be explained by the nature of meronymy in this dataset: most holonyms in BLESS are rather specific words. BLESS was built starting from 200 basic level concepts (e.g. goldfish) used as the x words, to which y words in different relations were associated (e.g. eye, for meronymy; animal, for hypernymy). x words represent hyponyms in the hyponym-hypernym pairs, and should therefore not be too general. Indeed, SLQS assigns high scores to hyponym-hypernym pairs. At the same time, in the meronymy relation in BLESS, x is the holonym and y is the meronym. For consistency with EVALution, we switched those pairs in BLESS, placing the meronym in the x slot and the holonym in the y slot. As a consequence, after the switching, holonyms in BLESS are usually rather specific words (e.g., there are no holonyms like animal and vehicle, as these words were originally in the y slot). In most cases, they are not more general than their meronyms ((eye, goldfish)), yielding low SLQS scores which are easy to separate from hypernyms. We note that this is a weakness of the BLESS dataset, rather than a strength of the measure. For instance, on EVALution, SLQS performs worse (ranked only as high as 13th), as this dataset has no such restriction on the basic level concepts, and may contain pairs like (eye, animal).</p><p>Hypernym vs. Attribute Symmetric similarity measures computed on syntactic contexts succeed to discriminate between hypernyms and attributes. Since attributes are syntactically different from hypernyms (in attributes, y is an adjective), it is unsurprising that they occur in different syntactic contexts, yielding low similarity scores.   <ref type="table">Table 4</ref>: Best performance on the validation set (10%) of each dataset for the supervised and unsupervised measures, in terms of Average Precision (AP) at k = 100, for hypernym vs. each single relation.</p><p>Hypernym vs. Antonym In all our experiments, antonyms were the hardest to distinguish from hypernyms, yielding the lowest performance. We found that SLQS performed reasonably well in this setting. However, the measure variations, context types and feature weightings were not consistent across datasets. SLQS relies on the assumption that y is a more general word than x, which is not true for antonyms, making it the most suitable measure for this setting.</p><p>Hypernym vs. Synonym SLQS performs well also in discriminating between hypernyms and synonyms, in which y is also not more general than x. We observed that in the joint context type, the difference in SLQS scores between synonyms and hypernyms was the largest. This may stem from the restrictiveness of this context type. For instance, among the most salient contexts we would expect to find informative contexts like drinks milk for cat and less informative ones like drinks water for animal, whereas the nonrestrictive single dependency context drinks would probably be present for both.</p><p>Another measure that works well is invCL: interestingly, other inclusion-based measures assign high scores to (x, y) when y includes many of x's contexts, which might be true also for synonyms (e.g. elevator and lift share many contexts). invCL, on the other hand, reduces with the ratio of y's contexts included in x, yielding lower scores for synonyms.</p><p>Hypernym vs. Coordination We found no consistency among BLESS and Weeds. On Weeds, inclusion-based measures (ClarkeDE, invCL and Weeds) showed the best results. The best performing measures on BLESS, however, were variants of SLQS, that showed to perform well in cases where the negative relation is symmetric (antonym, synonym and coordination). The difference could be explained by the nature of the datasets: the BLESS test set contains 1,185 hypernymy pairs, with only 129 distinct ys, many of which are general words like animal and object. The Weeds test set, on the other hand, was intentionally constructed to contain an overall unique y in each pair, and therefore contains much more specific ys (e.g. (quirk, strangeness)). For this reason, generality-based measures perform well on BLESS, and struggle with Weeds, which is handled better using inclusion-based measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to State-of-the-art Supervised Methods</head><p>For comparison with the state-of-the-art, we evaluated several supervised hypernymy detection methods, based on the word embeddings of x and y: concatenation v x ⊕ v y ( <ref type="bibr" target="#b3">Baroni et al., 2012</ref>), difference v y − v x (Weeds et al., 2014), and ASYM ( <ref type="bibr" target="#b30">Roller et al., 2014</ref>). We downloaded several pretrained embeddings ( <ref type="bibr" target="#b23">Mikolov et al., 2013;</ref><ref type="bibr">Pen- nington et al., 2014;</ref><ref type="bibr" target="#b19">Levy and Goldberg, 2014)</ref>, and trained a logistic regression classifier to predict hypernymy. We used the 90% portion (originally the test set) as the train set, and the other 10% (originally the validation set) as a test set, reporting the best results among different vectors, method AP@100 original AP@100 switched ∆ supervised concat, word2vec, L1 0.995 0.575 -0.42 unsupervised cosWeeds, win2d, ppmi 0.818 0.882 +0.064 <ref type="table">Table 5</ref>: Average Precision (AP) at k = 100 of the best supervised and unsupervised methods for hypernym vs. random-n, on the original BLESS validation set and the validation set with the artificially added switched hypernym pairs. method and regularization factor. 8 <ref type="table">Table 4</ref> displays the performance of the best classifier on each dataset, in a hypernym vs. a single relation setting. We also re-evaluated the unsupervised measures, this time reporting the results on the validation set (10%) for comparison.</p><p>The overall performance of the embeddingbased classifiers is almost perfect, and in particular the best performance is achieved using the concatenation method ( <ref type="bibr" target="#b3">Baroni et al., 2012</ref>) with either GloVe ( <ref type="bibr" target="#b27">Pennington et al., 2014</ref>) or the dependency-based embeddings ( <ref type="bibr">Levy and Gold- berg, 2014</ref>). As expected, the unsupervised measures perform worse than the embedding-based classifiers, though generally not bad on their own.</p><p>These results may suggest that unsupervised methods should be preferred only when no training data is available, leaving all the other cases to supervised methods. This is, however, not completely true. As others previously noticed, supervised methods do not actually learn the relation between x and y, but rather separate properties of either x or y. <ref type="bibr" target="#b21">Levy et al. (2015b)</ref> named this the "lexical memorization" effect, i.e. memorizing that certain ys tend to appear in many positive pairs (prototypical hypernyms).</p><p>On that account, the Weeds dataset has been designed to avoid such memorization, with every word occurring once in each slot of the relation. While the performance of the supervised methods on this dataset is substantially lower than their performance on other datasets, it is yet well above the random baseline which we might expect from a method that can only memorize words it has seen during training. 9 This is an indication that supervised methods can abstract away from the words.</p><p>Indeed, when we repeated the experiment with a lexical split of each dataset, i.e., such that the train and test set consist of distinct vocabularies, we found that the supervised methods' performance did not decrease dramatically, in contrast to the <ref type="bibr">8</ref> In our preliminary experiments we also trained other classifiers used in the distributional hypernymy detection literature (SVM and SVM+RBF kernel), that performed similarly. We report the results for logistic regression, since we use the prediction probabilities to measure average precision. <ref type="bibr">9</ref> The dataset is balanced between its two classes.</p><p>findings of <ref type="bibr" target="#b21">Levy et al. (2015b)</ref>. The large performance gaps reported by <ref type="bibr" target="#b21">Levy et al. (2015b)</ref> might be attributed to the size of their training sets. Their lexical split discarded around half of the pairs in the dataset and split the rest of the pairs equally to train and test, resulting in a relatively small train set. We performed the split such that only around 30% of the pairs in each dataset were discarded, and split the train and test sets with a ratio of roughly 90/10%, obtaining large enough train sets.</p><p>Our experiment suggests that rather than memorizing the verbatim prototypical hypernyms, the supervised models might learn that certain regions in the vector space pertain to prototypical hypernyms. For example, device (from the BLESS train set) and appliance (from the BLESS test set) are two similar words, which are both prototypical hypernyms. Another interesting observation was recently made by <ref type="bibr" target="#b29">Roller and Erk (2016)</ref>: they showed that when dependency-based embeddings are used, supervised distributional methods trace x and y's separate occurrences in different slots of Hearst patterns <ref type="bibr" target="#b14">(Hearst, 1992)</ref>.</p><p>Whether supervised methods only memorize or also learn, it is more consensual that they lack the ability to capture the relation between x and y, and that they rather indicate how likely y (x) is to be a hypernym (hyponym) ( <ref type="bibr" target="#b21">Levy et al., 2015b;</ref><ref type="bibr">San- tus et al., 2016a;</ref><ref type="bibr" target="#b36">Shwartz et al., 2016;</ref><ref type="bibr" target="#b29">Roller and Erk, 2016)</ref>. While this information is valuable, it cannot be solely relied upon for classification.</p><p>To better understand the extent of this limitation, we conducted an experiment in a similar manner to the switched hypernym pairs in <ref type="bibr" target="#b34">Santus et al. (2016a)</ref>. We used BLESS, which is the only dataset with random pairs. For each hypernym pair (x 1 , y 1 ), we sampled a word y 2 that participates in another hypernym pair (x 2 , y 2 ), such that (x 1 , y 2 ) is not in the dataset, and added (x 1 , y 2 ) as a random pair. We added 139 new pairs to the validation set, such as (rifle, animal) and (salmon, weapon). We then used the best supervised and unsupervised methods for hypernym vs. randomn on BLESS to re-classify the revised validation set. <ref type="table">Table 5</ref> displays the experiment results.</p><p>The switched hypernym experiment paints a much less optimistic picture of the embeddings' actual performance, with a drop of 42 points in average precision. 121 out of the 139 switched hypernym pairs were falsely classified as hypernyms. Examining the y words of these pairs reveals general words that appear in many hypernym pairs (e.g. animal, object, vehicle). The unsupervised measure was not similarly affected by the switched pairs, and the performance even slightly increased. This result is not surprising, since most unsupervised measures aim to capture aspects of the relation between x and y, while not relying on information about one of the words in the pair. <ref type="bibr">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>The results in Section 5 suggest that a supervised method using the unsupervised measures as features could possibly be the best of both worlds. We would expect it to be more robust than embeddingbased methods on the one hand, while being more informative than any single unsupervised measure on the other hand.</p><p>Such a method was developed by <ref type="bibr" target="#b34">Santus et al. (2016a)</ref>, however using mostly features that describe a single word, e.g. frequency and entropy. It was shown to be competitive with the state-of-theart supervised methods. With that said, it was also shown to be sensitive to the distribution of training examples in a specific dataset, like the embeddingbased methods.</p><p>We conducted a similar experiment, with a much larger number of unsupervised features, namely the various measure scores, and encountered the same issue. While the performance was good, it dropped dramatically when the model was tested on a different test set.</p><p>We conjecture that the problem stems from the currently available datasets, which are all somewhat artificial and biased. Supervised methods which are strongly based on the relation between the words, e.g. those that rely on path-based information ( <ref type="bibr" target="#b36">Shwartz et al., 2016)</ref>, manage to overcome the bias. Distributional methods, on the other hand, are based on a weaker notion of the relation between words, hence are more prone to overfit the distribution of training instances in a specific dataset. In the future, we hope that new datasets will be available for the task, which would be drawn from corpora and will reflect more realistic distributions of words and semantic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We performed an extensive evaluation of unsupervised methods for discriminating hypernyms from other semantic relations. We found that there is no single combination of measure and parameters which is always preferred; however, we suggested a principled linguistic-based analysis of the most suitable measure for each task that yields consistent performance across different datasets.</p><p>We investigated several new variants of existing methods, and found that some variants of SLQS turned out to be superior on certain tasks. In addition, we have tested for the first time the joint context type ( <ref type="bibr" target="#b6">Chersoni et al., 2016)</ref>, which was found to be very discriminative, and might hopefully benefit other semantic tasks.</p><p>For comparison, we evaluated the state-ofthe-art supervised methods on the datasets, and they have shown to outperform the unsupervised ones, while also being efficient and easier to use. However, a deeper analysis of their performance demonstrated that, as previously suggested, these methods do not capture the relation between x and y, but rather indicate the "prior probability" of either word to be a hyponym or a hypernym. As a consequence, supervised methods are sensitive to the distribution of examples in a particular dataset, making them less reliable for real-world applications. Being motivated by linguistic hypotheses, and independent from training data, unsupervised measures were shown to be more robust. In this sense, unsupervised methods can still play a relevant role, especially if combined with supervised methods, in the decision whether the relation holds or not.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>See Fig- ure 1 for an illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label>4</label><figDesc></figDesc></figure>

			<note place="foot" n="1"> Our code and data are available at: https://github.com/vered1986/UnsupervisedHypernymy</note>

			<note place="foot" n="2"> In our preliminary experiments, we noticed that the entropies of the targets and those of the contexts are not highly correlated, yielding a Spearman&apos;s correlation of up to 0.448 for window based spaces, and up to 0.097 for the dependency-based ones (p &lt; 0.01).</note>

			<note place="foot" n="3"> We removed the entailment relation, which had too few instances, and conflated relations to coarse-grained relations (e.g. HasProperty and HasA into attribute). 4 Lenci/Benotto includes pairs to which more than one relation is assigned, e.g. when x or y are polysemous, and re-</note>

			<note place="foot">nearly 50% of the SLQS false positive pairs were meronymholonym pairs, in many of which the holonym is more general than the meronym by definition, e.g. (mauritius, africa).</note>

			<note place="foot" n="10"> Turney and Mohammad (2015) have also shown that unsupervised methods are more robust than supervised ones in a transfer-learning experiment, when the &quot;training data&quot; was used to tune their parameters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Ido Dagan, Alessandro Lenci, and Yuji Matsumoto for their help and advice. Vered Shwartz is partially supported by an Intel ICRI-CI grant, the Israel Science Foundation grant 880/12, and the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1). Enrico Santus is partially supported by HK PhD Fellowship Scheme under PF12-13656.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Proceedings of the gems 2011 workshop on geometrical models of natural language semantics. In How we BLESSed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Distributional models for semantic relations: A sudy on hyponymy and antonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Pisa</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior research methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph P</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representing verbs with rich contexts: an evaluation on verb similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuele</forename><surname>Chersoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Blache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1967" to="1972" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-theoretic semantics for natural language: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Geometrical Models of Natural Language Semantics</title>
		<meeting>the Workshop on Geometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sammons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Evert</forename></persName>
		</author>
		<title level="m">The statistics of word cooccurrences: word pairs and collocations. Dissertation</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Corpora and collocations. Corpus linguistics. An international handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">Evert</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="223" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Building a very large corpus of english obtained by web crawling: ukwac. Masters thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Bologna, Italy</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">The 15th International Conference on Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting image generality for lexical entailment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="296" to="304" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A graph-based algorithm for inducing lexical taxonomies from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1872" to="1877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributional lexical entailment by topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relations such as hypernymy: Identifying and exploiting hearst patterns in distributional vectors for lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2163" to="2172" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Introduction to modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>McGraw-Hill, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frances</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Proceedings of the 4th workshop on linked data in linguistics: Resources and applications. In EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models</title>
		<meeting>the 4th workshop on linked data in linguistics: Resources and applications. In EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nine features in a random forest to learn taxonomical semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin-Shing</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4557" to="4564" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised measure of word similarity: How to outperform cooccurrence and vector cosine in vsms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin-Shing</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4260" to="4261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2389" to="2398" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Experiments with three approaches to recognizing lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="437" to="476" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COL-ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
