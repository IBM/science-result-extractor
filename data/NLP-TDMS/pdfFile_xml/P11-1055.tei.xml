<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T03:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 19-24, 2011. 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
							<email>raphaelh@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
							<email>clzhang@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
							<email>xiaoling@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
							<email>weld@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 49th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Portland, Oregon</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="541" to="550"/>
							<date type="published">June 19-24, 2011. 2011</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web&apos;s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint-for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information-extraction (IE), the process of generating relational data from natural-language text, continues to gain attention. Many researchers dream of creating a large repository of high-quality extracted tuples, arguing that such a knowledge base could benefit many important tasks such as question answering and summarization. Most approaches to IE use supervised learning of relation-specific examples, which can achieve high precision and recall. Unfortunately, however, fully supervised methods are limited by the availability of training data and are unlikely to scale to the thousands of relations found on the Web.</p><p>A more promising approach, often called "weak" or "distant" supervision, creates its own training data by heuristically matching the contents of a database to corresponding text <ref type="bibr">(Craven and Kum- lien, 1999</ref>). For example, suppose that r(e 1 , e 2 ) = Founded(Jobs, Apple) is a ground tuple in the database and s ="Steve Jobs founded Apple, Inc." is a sentence containing synonyms for both e 1 = Jobs and e 2 = Apple, then s may be a natural language expression of the fact that r(e 1 , e 2 ) holds and could be a useful training example.</p><p>While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles ( <ref type="bibr" target="#b10">Hoffmann et al., 2010)</ref>),  observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles).</p><p>To fix this problem they cast weak supervision as a form of multi-instance learning, assuming only that at least one of the sentences containing e 1 and e 2 are expressing r(e 1 , e 2 ), and their method yields a substantial improvement in extraction performance.</p><p>However, <ref type="bibr">Riedel et al.'</ref>s model (like that of previous systems <ref type="bibr" target="#b13">(Mintz et al., 2009)</ref>) assumes that relations do not overlap -there cannot exist two facts r(e 1 , e 2 ) and q(e 1 , e 2 ) that are both true for any pair of entities, e 1 and e 2 . Unfortunately, this assumption is often violated; 541 for example both Founded(Jobs, Apple) and CEO-of(Jobs, Apple) are clearly true. Indeed, 18.3% of the weak supervision facts in Freebase that match sentences in the NY Times 2007 corpus have overlapping relations. This paper presents MULTIR, a novel model of weak supervision that makes the following contributions:</p><p>• MULTIR introduces a probabilistic, graphical model of multi-instance learning which handles overlapping relations.</p><p>• MULTIR also produces accurate sentence-level predictions, decoding individual sentences as well as making corpus-level extractions.</p><p>• MULTIR is computationally tractable. Inference reduces to weighted set cover, for which it uses a greedy approximation with worst case running time O(|R| · |S|) where R is the set of possible relations and S is largest set of sentences for any entity pair. In practice, MULTIR runs very quickly.</p><p>• We present experiments showing that MULTIR outperforms a reimplementation of 's approach on both aggregate (corpus as a whole) and sentential extractions. Additional experiments characterize aspects of MULTIR's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Weak Supervision from a Database</head><p>Given a corpus of text, we seek to extract facts about entities, such as the company Apple or the city Boston. A ground fact (or relation instance), is an expression r(e) where r is a relation name, for example Founded or CEO-of, and e = e 1 , . . . , e n is a list of entities. An entity mention is a contiguous sequence of textual tokens denoting an entity. In this paper we assume that there is an oracle which can identify all entity mentions in a corpus, but the oracle doesn't normalize or disambiguate these mentions. We use e i ∈ E to denote both an entity and its name (i.e., the tokens in its mention).</p><p>A relation mention is a sequence of text (including one or more entity mentions) which states that some ground fact r(e) is true. For example, "Steve Ballmer, CEO of Microsoft, spoke recently at CES." contains three entity mentions as well as a relation mention for CEO-of(Steve Ballmer, Microsoft). In this paper we restrict our attention to binary relations. Furthermore, we assume that both entity mentions appear as noun phrases in a single sentence.</p><p>The task of aggregate extraction takes two inputs, Σ, a set of sentences comprising the corpus, and an extraction model; as output it should produce a set of ground facts, I, such that each fact r(e) ∈ I is expressed somewhere in the corpus.</p><p>Sentential extraction takes the same input and likewise produces I, but in addition it also produces a function, Γ : I → P(Σ), which identifies, for each r(e) ∈ I, the set of sentences in Σ that contain a mention describing r(e). In general, the corpuslevel extraction problem is easier, since it need only make aggregate predictions, perhaps using corpuswide statistics. In contrast, sentence-level extraction must justify each extraction with every sentence which expresses the fact.</p><p>The knowledge-based weakly supervised learning problem takes as input (1) Σ, a training corpus, (2) E, a set of entities mentioned in that corpus, (3) R, a set of relation names, and (4), ∆, a set of ground facts of relations in R. As output the learner produces an extraction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modeling Overlapping Relations</head><p>We define an undirected graphical model that allows joint reasoning about aggregate (corpus-level) and sentence-level extraction decisions. <ref type="figure" target="#fig_0">Figure 1(a)</ref> shows the model in plate form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Random Variables</head><p>There exists a connected component for each pair of entities e = (e 1 , e 2 ) ∈ E × E that models all of the extraction decisions for this pair. There is one Boolean output variable Y r for each relation name r ∈ R, which represents whether the ground fact r(e) is true. Including this set of binary random variables enables our model to extract overlapping relations.</p><p>Let S (e 1 ,e 2 ) ⊂ Σ be the set of sentences which contain mentions of both of the entities. For each sentence x i ∈ S (e 1 ,e 2 ) there exists a latent variable Z i which ranges over the relation names r ∈ R and, 542  importantly, also the distinct value none. Z i should be assigned a value r ∈ R only when x i expresses the ground fact r(e), thereby modeling sentencelevel extraction. <ref type="figure" target="#fig_0">Figure 1</ref>(b) shows an example instantiation of the model with four relation names and three sentences.</p><formula xml:id="formula_0">E × E í µí± R S í µí± í</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Joint, Conditional Extraction Model</head><p>We use a conditional probability model that defines a joint distribution over all of the extraction random variables defined above. The model is undirected and includes repeated factors for making sentence level predictions as well as globals factors for aggregating these choices.</p><p>For each entity pair e = (e 1 , e 2 ), define x to be a vector concatenating the individual sentences x i ∈ S (e 1 ,e 2 ) , Y to be vector of binary Y r random variables, one for each r ∈ R, and Z to be the vector of Z i variables, one for each sentence x i . Our conditional extraction model is defined as follows:</p><formula xml:id="formula_1">p(Y = y, Z = z|x; θ) def = 1 Z x r Φ join (y r , z) i Φ extract (z i , x i )</formula><p>where the parameter vector θ is used, below, to define the factor Φ extract .</p><p>The factors Φ join are deterministic OR operators</p><formula xml:id="formula_2">Φ join (y r , z) def = 1 if y r = true ∧ ∃i : z i = r 0 otherwise</formula><p>which are included to ensure that the ground fact r(e) is predicted at the aggregate level for the assignment Y r = y r only if at least one of the sentence level assignments Z i = z i signals a mention of r(e). The extraction factors Φ extract are given by</p><formula xml:id="formula_3">Φ extract (z i , x i ) def = exp   j θ j φ j (z i , x i )  </formula><p>where the features φ j are sensitive to the relation name assigned to extraction variable z i , if any, and cues from the sentence x i . We will make use of the <ref type="bibr" target="#b13">Mintz et al. (2009)</ref> sentence-level features in the expeiments, as described in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>This model was designed to provide a joint approach where extraction decisions are almost entirely driven by sentence-level reasoning. However, defining the Y r random variables and tying them to the sentencelevel variables, Z i , provides a direct method for modeling weak supervision. We can simply train the model so that the Y variables match the facts in the database, treating the Z i as hidden variables that can take any value, as long as they produce the correct aggregate predictions. This approach is related to the multi-instance learning approach of , in that both models include sentence-level and aggregate random variables. However, their sentence level variables are binary and they only have a single aggregate variable that takes values r ∈ R ∪ {none}, thereby ruling out overlapping relations. Additionally, their aggregate decisions make use of Mintzstyle aggregate features ( <ref type="bibr" target="#b13">Mintz et al., 2009)</ref>, that collect evidence from multiple sentences, while we use 543</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs:</head><p>(1) Σ, a set of sentences, (2) E, a set of entities mentioned in the sentences, (3) R, a set of relation names, and (4) ∆, a database of atomic facts of the form r(e 1 , e 2 ) for r ∈ R and e i ∈ E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definitions:</head><p>We define the training set {(x i , y i )|i = 1 . . . n}, where i is an index corresponding to a particular entity pair (e j , e k ) in ∆, x i contains all of the sentences in Σ with mentions of this pair, and only the deterministic OR nodes. Perhaps surprising, we are still able to improve performance at both the sentential and aggregate extraction tasks.</p><formula xml:id="formula_4">y i = relVector(e j , e k ). Computation: initialize parameter vector Θ ← 0 for t = 1...T do for i = 1...n do (y , z ) ← arg max y,z p(y, z|x i ; θ) if y = y i then z * ← arg max z p(z|x i , y i ; θ) Θ ← Θ + φ(x i , z * ) − φ(x i , z ) end if end for end for Return Θ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning</head><p>We now present a multi-instance learning algorithm for our weak-supervision model that treats the sentence-level extraction random variables Z i as latent, and uses facts from a database (e.g., Freebase) as supervision for the aggregate-level variables Y r .</p><p>As input we have (1) Σ, a set of sentences, (2) E, a set of entities mentioned in the sentences, (3) R, a set of relation names, and (4) ∆, a database of atomic facts of the form r(e 1 , e 2 ) for r ∈ R and e i ∈ E. Since we are using weak learning, the Y r variables in Y are not directly observed, but can be approximated from the database ∆. We use a procedure, relVector(e 1 , e 2 ) to return a bit vector whose j th bit is one if r j (e 1 , e 2 ) ∈ ∆. The vector does not have a bit for the special none relation; if there is no relation between the two entities, all bits are zero.</p><p>Finally, we can now define the training set to be pairs {(x i , y i )|i = 1 . . . n}, where i is an index corresponding to a particular entity pair (e j , e k ), x i contains all of the sentences with mentions of this pair, and y i = relVector(e j , e k ).</p><p>Given this form of supervision, we would like to find the setting for θ with the highest likelihood:</p><formula xml:id="formula_5">O(θ) = i p(y i |x i ; θ) = i z p(y i , z|x i ; θ)</formula><p>However, this objective would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to data sets of the size we consider. Instead, we make two approximations, described below, leading to a Perceptron-style additive <ref type="bibr" target="#b6">(Collins, 2002</ref>) parameter update scheme which has been modified to reason about hidden variables, similar in style to the approaches of ( <ref type="bibr" target="#b11">Liang et al., 2006;</ref><ref type="bibr" target="#b23">Zettlemoyer and Collins, 2007)</ref>, but adapted for our specific model. This approximate algorithm is computationally efficient and, as we will see, works well in practice.</p><p>Our first modification is to do online learning instead of optimizing the full objective. Define the feature sums φ(x, z) = j φ(x j , z j ) which range over the sentences, as indexed by j. Now, we can define an update based on the gradient of the local log likelihood for example i:</p><formula xml:id="formula_6">∂ log O i (θ) ∂θ j = E p(z|x i ,y i ;θ) [φ j (x i , z)] −E p(y,z|x i ;θ) [φ j (x i , z)]</formula><p>where the deterministic OR Φ join factors ensure that the first expectation assigns positive probability only to assignments that produce the labeled facts y i but that the second considers all valid sets of extractions. Of course, these expectations themselves, especially the second one, would be difficult to compute exactly. Our second modification is to do a Viterbi approximation, by replacing the expectations with maximizations. Specifically, we compute the most likely sentence extractions for the label facts arg max z p(z|x i , y i ; θ) and the most likely extraction for the input, without regard to the labels, arg max y,z p(y, z|x i ; θ). We then compute the features for these assignments and do a simple additive update. The final algorithm is detailed in <ref type="figure" target="#fig_1">Figure 2</ref>. 544</p><p>To support learning, as described above, we need to compute assignments arg max z p(z|x, y; θ) and arg max y,z p(y, z|x; θ). In this section, we describe algorithms for both cases that use the deterministic OR nodes to simplify the required computations.</p><p>Predicting the most likely joint extraction arg max y,z p(y, z|x; θ) can be done efficiently given the structure of our model. In particular, we note that the factors Φ join represent deterministic dependencies between Z and Y, which when satisfied do not affect the probability of the solution. It is thus sufficient to independently compute an assignment for each sentence-level extraction variable Z i , ignoring the deterministic dependencies. The optimal setting for the aggregate variables Y is then simply the assignment that is consistent with these extractions. The time complexity is O(|R| · |S|).</p><p>Predicting sentence level extractions given weak supervision facts, arg max z p(z|x, y; θ), is more challenging. We start by computing extraction scores Φ extract (x i , z i ) for each possible extraction assignment Z i = z i at each sentence x i ∈ S, and storing the values in a dynamic programming table. Next, we must find the most likely assignment z that respects our output variables y. It turns out that this problem is a variant of the weighted, edge-cover problem, for which there exist polynomial time optimal solutions.</p><p>Let G = (E, V = V S ∪ V y ) be a complete weighted bipartite graph with one node v S i ∈ V S for each sentence x i ∈ S and one node v y r ∈ V y for each relation r ∈ R where y r = 1. The edge weights are given by c((</p><formula xml:id="formula_7">v S i , v y r )) def = Φ extract (x i , z i ).</formula><p>Our goal is to select a subset of the edges which maximizes the sum of their weights, subject to each node v S i ∈ V S being incident to exactly one edge, and each node v y r ∈ V y being incident to at least one edge.</p><p>Exact Solution An exact solution can be obtained by first computing the maximum weighted bipartite matching, and adding edges to nodes which are not incident to an edge. This can be computed in time O(|V|(|E| + |V| log |V|)), which we can rewrite as O((|R| + |S|)(|R||S| + (|R| + |S|) log(|R| + |S|))).</p><p>Approximate Solution An approximate solution can be obtained by iterating over the nodes in V y , í µí±£ bornIn y í µí±£ locatedIn y í µí±£ 1 S í µí±£ 2 S í µí±£ 3 S í µí±(í µí± 1 = bornIn|í µí°±) í µí±(í µí± 3 = locatedIn|í µí°±) í µí±(í µí± 1 … <ref type="figure">Figure 3</ref>: Inference of arg max z p(Z = z|x, y) requires solving a weighted, edge-cover problem.</p><p>and each time adding the highest weight incident edge whose addition doesn't violate a constraint. The running time is O(|R||S|). This greedy search guarantees each fact is extracted at least once and allows any additional extractions that increase the overall probability of the assignment. Given the computational advantage, we use it in all of the experimental evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>We follow the approach of  for generating weak supervision data, computing features, and evaluating aggregate extraction. We also introduce new metrics for measuring sentential extraction performance, both relation-independent and relation-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Generation</head><p>We used the same data sets as  for weak supervision. The data was first tagged with the Stanford NER system ( <ref type="bibr" target="#b9">Finkel et al., 2005</ref>) and then entity mentions were found by collecting each continuous phrase where words were tagged identically (i.e., as a person, location, or organization). Finally, these phrases were matched to the names of Freebase entities. Given the set of matches, define Σ to be set of NY Times sentences with two matched phrases, E to be the set of Freebase entities which were mentioned in one or more sentences, ∆ to be the set of Freebase facts whose arguments, e 1 and e 2 were mentioned in a sentence in Σ, and R to be set of relations names used in the facts of ∆. These sets define the weak supervision data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Features and Initialization</head><p>We use the set of sentence-level features described by , which were originally developed by <ref type="bibr" target="#b13">Mintz et al. (2009)</ref>. These include indicators for various lexical, part of speech, named entity, and dependency tree path properties of entity mentions in specific sentences, as computed with the Malt dependency parser ( <ref type="bibr" target="#b15">Nivre and Nilsson, 2004</ref>) and OpenNLP POS tagger 1 . However, unlike the previous work, we did not make use of any features that explicitly aggregate these properties across multiple mention instances.</p><p>The MULTIR algorithm has a single parameter T , the number of training iterations, that must be specified manually. We used T = 50 iterations, which performed best in development experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation Metrics</head><p>Evaluation is challenging, since only a small percentage (approximately 3%) of sentences match facts in Freebase, and the number of matches is highly unbalanced across relations, as we will see in more detail later. We use the following metrics.</p><p>Aggregate Extraction Let ∆ e be the set of extracted relations for any of the systems; we compute aggregate precision and recall by comparing ∆ e with ∆. This metric is easily computed but underestimates extraction accuracy because Freebase is incomplete and some true relations in ∆ e will be marked wrong.</p><p>Sentential Extraction Let S e be the sentences where some system extracted a relation and S F be the sentences that match the arguments of a fact in ∆. We manually compute sentential extraction accuracy by sampling a set of 1000 sentences from S e ∪ S F and manually labeling the correct extraction decision, either a relation r ∈ R or none. We then report precision and recall for each system on this set of sampled sentences. These results provide a good approximation to the true precision but can overestimate the actual recall, since we did not manually check the much larger set of sentences where no approach predicted extractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Precision / Recall Curves</head><p>To compute precision / recall curves for the tasks, we ranked the MULTIR extractions as follows. For sentence-level evaluations, we ordered according to the extraction factor score Φ extract (z i , x i ). For aggregate comparisons, we set the score for an extraction Y r = true to be the max of the extraction factor scores for the sentences where r was extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>To evaluate our algorithm, we first compare it to an existing approach for using multi-instance learning with weak supervision ( ), using the same data and features. We report both aggregate extraction and sentential extraction results. We then investigate relation-specific performance of our system. Finally, we report running time comparisons. <ref type="figure" target="#fig_2">Figure 4</ref> shows approximate precision / recall curves for three systems computed with aggregate metrics (Section 6.3) that test how closely the extractions match the facts in Freebase. The systems include the original results reported by  as well as our new model (MULTIR). We also compare with SOLOR, a reimplementation of their algorithm, which we built in Factorie ( <ref type="bibr" target="#b12">McCallum et al., 2009</ref>), and will use later to evaluate sentential extraction. MULTIR achieves competitive or higher precision over all ranges of recall, with the exception of the very low recall range of approximately 0-1%. It also significantly extends the highest recall achieved, from 20% to 25%, with little loss in precision. To investigate the low precision in the 0-1% recall range, we manually checked the ten highest con-546 fidence extractions produced by MULTIR that were marked wrong. We found that all ten were true facts that were simply missing from Freebase. A manual evaluation, as we perform next for sentential extraction, would remove this dip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Aggregate Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Sentential Extraction</head><p>Although their model includes variables to model sentential extraction,  did not report sentence level performance. To generate the precision / recall curve we used the joint model assignment score for each of the sentences that contributed to the aggregate extraction decision. <ref type="figure" target="#fig_2">Figure 4</ref> shows approximate precision / recall curves for MULTIR and SOLOR computed against manually generated sentence labels, as defined in Section 6.3. MULTIR achieves significantly higher recall with a consistently high level of precision. At the highest recall point, MULTIR reaches 72.4% precision and 51.9% recall, for an F1 score of 60.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Relation-Specific Performance</head><p>Since the data contains an unbalanced number of instances of each relation, we also report precision and recall for each of the ten most frequent relations. Let S M r be the sentences where MULTIR extracted an instance of relation r ∈ R, and let S F r be the sentences that match the arguments of a fact about relation r in ∆. For each r, we sample 100 sentences from both S M r and S F r and manually check accuracy. To estimate precisioñ P r we compute the ratio of true relation mentions in S M r , and to estimate recall˜Rcall˜ call˜R r we take the ratio of true relation mentions in S F r which are returned by our system. <ref type="table">Table 1</ref> presents this approximate precision and recall for MULTIR on each of the relations, along with statistics we computed to measure the quality of the weak supervision. Precision is high for the majority of relations but recall is consistently lower. We also see that the Freebase matches are highly skewed in quantity and can be low quality for some relations, with very few of them actually corresponding to true extractions. The approach generally performs best on the relations with a sufficiently large number of true matches, in many cases even achieving precision that outperforms the accuracy of the heuristic matches, at reasonable recall levels. <ref type="table">Table 1</ref> also highlights some of the effects of learning with overlapping relations. For example, in the data, almost all of the matches for the administrative divisions relation overlap with the contains relation, because they both model relationships for a pair of locations. Since, in general, sentences are much more likely to describe a contains relation, this overlap leads to a situation were almost none of the administrate division matches are true ones, and we cannot accurately learn an extractor. However, we can still learn to accurately extract the contains relation, despite the distracting matches. Similarly, the place of birth and place of death relations tend to overlap, since it is often the case that people are born and die in the same city. In both cases, the precision outperforms the labeling accuracy and the recall is relatively high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Overlapping Relations</head><p>To measure the impact of modeling overlapping relations, we also evaluated a simple, restricted baseline. Instead of labeling each entity pair with the set of all true Freebase facts, we created a dataset where each true relation was used to create a different training example. Training MULTIR on this data simulates effects of conflicting supervision that can come from not modeling overlaps. On average across relations, precision increases 12 points but recall drops 26 points, for an overall reduction in F1 score from 60.5% to 40.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Running Time</head><p>One final advantage of our model is the modest running time. Our implementation of the   <ref type="table">Table 1</ref>: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy (% true) of matches between sentences and facts in Freebase.  approach required approximately 6 hours to train on NY Times 05-06 and 4 hours to test on the NY Times 07, each without preprocessing. Although they do sampling for inference, the global aggregation variables require reasoning about an exponentially large (in the number of sentences) sample space. In contrast, our approach required approximately one minute to train and less than one second to test, on the same data. This advantage comes from the decomposition that is possible with the deterministic OR aggregation variables. For test, we simply consider each sentence in isolation and during training our approximation to the weighted assignment problem is linear in the number of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Discussion</head><p>The sentential extraction results demonstrates the advantages of learning a model that is primarily driven by sentence-level features. Although previous approaches have used more sophisticated features for aggregating the evidence from individual sentences, we demonstrate that aggregating strong sentence-level evidence with a simple deterministic OR that models overlapping relations is more effective, and also enables training of a sentence extractor that runs with no aggregate information.</p><p>While the Riedel et al. approach does include a model of which sentences express relations, it makes significant use of aggregate features that are primarily designed to do entity-level relation predictions and has a less detailed model of extractions at the individual sentence level. Perhaps surprisingly, our model is able to do better at both the sentential and aggregate levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Supervised-learning approaches to IE were introduced in <ref type="bibr" target="#b18">(Soderland et al., 1995)</ref> and are too numerous to summarize here. While they offer high precision and recall, these methods are unlikely to scale to the thousands of relations found in text on the Web. Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE ( <ref type="bibr" target="#b17">Shinyama and Sekine, 2006</ref>), TEXTRUNNER ( <ref type="bibr" target="#b1">Banko et al., 2007;</ref><ref type="bibr" target="#b0">Banko and Etzioni, 2008)</ref> and WOE ( <ref type="bibr" target="#b21">Wu and Weld, 2010)</ref>) can scale to millions of documents, but don't output canonicalized relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Weak Supervision</head><p>Weak supervision (also known as distant-or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus. <ref type="bibr" target="#b7">Craven and Kumlien (1999)</ref> introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor. <ref type="bibr">Bellare and Mc- Callum (2007)</ref> used a database of BibTex records to train a CRF extractor on 12 bibliographic relations. The KYLIN system aplied weak supervision to learn relations from Wikipedia, treating infoboxes as the associated database ( <ref type="bibr" target="#b19">Wu and Weld, 2007)</ref>; <ref type="bibr" target="#b20">Wu et al. (2008)</ref> extended the system to use smoothing over an automatically generated infobox taxonomy. <ref type="bibr" target="#b13">Mintz et al. (2009)</ref> used Freebase facts to train 100 relational extractors on Wikipedia. <ref type="bibr" target="#b10">Hoffmann et al. (2010)</ref> describe a system similar to KYLIN, but which dynamically generates lexicons in order to handle sparse data, learning over 5000 Infobox relations with an average F1 score of 61%.  perform weak supervision, while using selectional preference constraints to a jointly reason about entity types.</p><p>The NELL system <ref type="bibr">(Carlson et al., 2010</ref>) can also be viewed as performing weak supervision. Its initial knowledge consists of a selectional preference constraint and 20 ground fact seeds. NELL then matches entity pairs from the seeds to a Web corpus, but instead of learning a probabilistic model, it bootstraps a set of extraction patterns using semisupervised methods for multitask learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Multi-Instance Learning</head><p>Multi-instance learning was introduced in order to combat the problem of ambiguously-labeled training data when predicting the activity of different drugs ( <ref type="bibr" target="#b8">Dietterich et al., 1997</ref>). <ref type="bibr" target="#b3">Bunescu and Mooney (2007)</ref> connect weak supervision with multi-instance learning and extend their relational extraction kernel to this context. , combine weak supervision and multi-instance learning in a more sophisticated manner, training a graphical model, which assumes only that at least one of the matches between the arguments of a Freebase fact and sentences in the corpus is a true relational mention. Our model may be seen as an extension of theirs, since both models include sentence-level and aggregate random variables. However, Riedel et al. have only a single aggregate variable that takes values r ∈ R ∪ {none}, thereby ruling out overlapping relations. We have discussed the comparison in more detail throughout the paper, including in the model formulation section and experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We argue that weak supervision is promising method for scaling information extraction to the level where it can handle the myriad, different relations on the Web. By using the contents of a database to heuristically label a training corpus, we may be able to automatically learn a nearly unbounded number of relational extractors. Since the processs of matching database tuples to sentences is inherently heuristic, researchers have proposed multi-instance learning algorithms as a means for coping with the resulting noisy data. Unfortunately, previous approaches assume that all relations are disjoint -for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple), because two relations are not allowed to have the same arguments.</p><p>This paper presents a novel approach for multiinstance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show improvements for both sentential and aggregate (corpus level) extraction, and demonstrate that the approach is computationally efficient.</p><p>Our early progress suggests many interesting directions. By joining two or more Freebase tables, we can generate many more matches and learn more relations. We also wish to refine our model in order to improve precision. For example, we would like to add type reasoning about entities and selectional preference constraints for relations. Finally, we are also interested in applying the overall learning approaches to other tasks that could be modeled with weak supervision, such as coreference and named entity classification.</p><p>The source code of our system, its output, and all data annotations are available at http://cs.uw.edu/homes/raphaelh/mr.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Network structure depicted as plate model and (b) an example network instantiation for the pair of entities Steve Jobs, Apple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The MULTIR Learning Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Aggregate extraction precision / recall curves for Riedel et al. (2010), a reimplementation of that approach (SOLOR), and our algorithm (MULTIR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sentential extraction precision / recall curves for MULTIR and SOLOR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Relation</head><label></label><figDesc></figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Sebastian Riedel and Limin Yao for sharing their data and providing valuable advice. This material is based upon work supported by a WRF / TJ Cable Professorship, a gift from Google and by the Air Force Research Laboratory (AFRL) under prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL). 549</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The tradeoffs between open and traditional relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 46th Annual Meeting of the Association for Computational Linguistics (ACL-08)</title>
		<meeting>46th Annual Meeting of the Association for Computational Linguistics (ACL-08)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07)</title>
		<meeting>the 20th International Joint Conference on Artificial Intelligence (IJCAI-07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning extractors from unlabeled text using relevant databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Workshop on Information Integration on the Web</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to extract relations from the web using minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07)</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-10)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology</title>
		<meeting>the Seventh International Conference on Intelligent Systems for Molecular Biology</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning 5000 relational extractors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An end-to-end discriminative approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factorie: Probabilistic programming via imperatively defined factor graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Conference (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th</title>
		<meeting>the 47th</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Daniel Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL-2009)</title>
		<imprint>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Memory-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Natural Language Learning (CoNLL-04)</title>
		<meeting>the Conference on Natural Language Learning (CoNLL-04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth European Conference on Machine Learning (ECML-2010)</title>
		<meeting>the Sixteenth European Conference on Machine Learning (ECML-2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Preemptive information extraction using unrestricted relation discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Shinyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computation Linguistics (HLT-NAACL-06)</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computation Linguistics (HLT-NAACL-06)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Crystal: Inducing a conceptual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Aseltine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">G</forename><surname>Lehnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-1995)</title>
		<meeting>the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-1995)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1314" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autonomously semantifying wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Information and Knowledge Management (CIKM-2007)</title>
		<meeting>the International Conference on Information and Knowledge Management (CIKM-2007)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatically refining the wikipedia infobox ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on World Wide Web (WWW-2008)</title>
		<meeting>the 17th International Conference on World Wide Web (WWW-2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open information extraction using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Meeting of the Association for Computational Linguistics (ACL-2010)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collective cross-document relation extraction without labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010)</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
