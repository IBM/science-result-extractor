<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T01:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Semantic Role Labeling with Self-Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mobile Internet Group</orgName>
								<orgName type="institution">Tencent Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mobile Internet Group</orgName>
								<orgName type="institution">Tencent Technology Co</orgName>
								<address>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
							<email>ydchen@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Semantic Role Labeling with Self-Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic Role Labeling (SRL) is believed to be a crucial step towards natural language understanding and has been widely studied. Recent years, end-to-end SRL with recurrent neu-ral networks (RNN) has gained increasing attention. However , it remains a major challenge for RNNs to handle structural information and long range dependencies. In this paper , we present a simple and effective architecture for SRL which aims to address these problems. Our model is based on self-attention which can directly capture the relationships between two tokens regardless of their distance. Our single model achieves F1 = 83.4 on the CoNLL-2005 shared task dataset and F1 = 82.7 on the CoNLL-2012 shared task dataset, which outperforms the previous state-of-the-art results by 1.8 and 1.0 F1 score respectively. Besides, our model is computationally efficient, and the parsing speed is 50K tokens per second on a single Titan X GPU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Semantic Role Labeling is a shallow semantic parsing task, whose goal is to determine essentially "who did what to whom", "when" and "where". Semantic roles indicate the basic event properties and relations among relevant entities in the sentence and provide an intermediate level of semantic representation thus benefiting many NLP applications, such as Information Extraction ( <ref type="bibr">Bastianelli et al. 2013</ref>), Question Answering ( <ref type="bibr" target="#b35">Surdeanu et al. 2003;</ref><ref type="bibr" target="#b22">Moschitti, Morarescu, and Harabagiu 2003;</ref><ref type="bibr" target="#b8">Dan and Lapata 2007)</ref>, Machine Translation ( <ref type="bibr" target="#b17">Knight and Luk 1994;</ref><ref type="bibr" target="#b39">Ueffing, Haffari, and Sarkar 2007;</ref><ref type="bibr" target="#b41">Wu and Fung 2009)</ref> and Multi-document Abstractive Summarization (Genest and Lapalme 2011).</p><p>Semantic roles are closely related to syntax. Therefore, traditional SRL approaches rely heavily on the syntactic structure of a sentence, which brings intrinsic complexity and restrains these systems to be domain specific. Recently, end-to-end models for SRL without syntactic inputs achieved promising results on this task ( <ref type="bibr" target="#b43">Zhou and Xu 2015;</ref><ref type="bibr" target="#b21">Marcheggiani, Frolov, and Titov 2017;</ref><ref type="bibr" target="#b15">He et al. 2017</ref>). As the pioneering work, <ref type="bibr" target="#b43">Zhou and Xu (2015)</ref> introduced a stacked long short-term memory network (LSTM) and achieved the state-of-the-art results. <ref type="bibr" target="#b15">He et al., (2017)</ref> reported further improvements by using deep highway bidirectional LSTMs with constrained decoding. These successes involving end-to-end models reveal the potential ability of LSTMs for handling the underlying syntactic structure of the sentences.</p><p>Despite recent successes, these RNN-based models have limitations. RNNs treat each sentence as a sequence of words and recursively compose each word with its previous hidden state. The recurrent connections make RNNs applicable for sequential prediction tasks with arbitrary length, however, there still remain several challenges in practice. The first one is related to memory compression problem <ref type="bibr" target="#b6">(Cheng, Dong, and Lapata 2016)</ref>. As the entire history is encoded into a single fixed-size vector, the model requires larger memory capacity to store information for longer sentences. The unbalanced way of dealing with sequential information leads the network performing poorly on long sentences while wasting memory on shorter ones. The second one is concerned with the inherent structure of sentences. RNNs lack a way to tackle the tree-structure of the inputs. The sequential way to process the inputs remains the network depth-in-time, and the number of nonlinearities depends on the time-steps.</p><p>To address these problems above, we present a deep attentional neural network (DEEPATT) for the task of SRL 1 . Our models rely on the self-attention mechanism which directly draws the global dependencies of the inputs. In contrast to RNNs, a major advantage of self-attention is that it conducts direct connections between two arbitrary tokens in a sentence. Therefore, distant elements can interact with each other by shorter paths (O(1) v.s. O(n)), which allows unimpeded information flow through the network. Self-attention also provides a more flexible way to select, represent and synthesize the information of the inputs and is complementary to RNN based models. Along with self-attention, DEEP-ATT comes with three variants which uses recurrent (RNN), convolutional (CNN) and feed-forward (FFN) neural network to further enhance the representations.</p><p>Although DEEPATT is fairly simple, it gives remarkable empirical results. Our single model outperforms the previ-ous state-of-the-art systems on the CoNLL-2005 shared task dataset and the CoNLL-2012 shared task dataset by 1.8 and 1.0 F 1 score respectively. It is also worth mentioning that on the out-of-domain dataset, we achieve an improvement upon the previous end-to-end approach ( <ref type="bibr" target="#b15">He et al. 2017</ref>) by 2.0 F 1 score. The feed-forward variant of DEEPATT allows significantly more parallelization, and the parsing speed is 50K tokens per second on a single Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Role Labeling</head><p>Given a sentence, the goal of SRL is to identify and classify the arguments of each target verb into semantic roles. For example, for the sentence "Marry borrowed a book from John last week." and the target verb borrowed, SRL yields the following outputs:</p><formula xml:id="formula_0">[ ARG0 Marry ] [ V borrowed ] [ ARG1 a book ] [ ARG2 from John ] [ AM-TMP last week ].</formula><p>Here ARG0 represents the borrower, ARG1 represents the thing borrowed, ARG2 represents the entity borrowed from, AM-TMP is an adjunct indicating the timing of the action and V represents the verb.</p><p>Generally, semantic role labeling consists of two steps: identifying and classifying arguments. The former step involves assigning either a semantic argument or nonargument for a given predicate, while the latter includes labeling a specific semantic role for the identified argument. It is also common to prune obvious non-candidates before the first step and to apply post-processing procedure to fix inconsistent predictions after the second step. Finally, a dynamic programming algorithm is often applied to find the global optimum solution for this typical sequence labeling problem at the inference stage.</p><p>In this paper, we treat SRL as a BIO tagging problem. Our approach is extremely simple. As illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, the original utterances and the corresponding predicate masks are first projected into real-value vectors, namely embeddings, which are fed to the next layer. After that, we design a deep attentional neural network which takes the embeddings as the inputs to capture the nested structures of the sentence and the latent dependency relationships among the labels. On the inference stage, only the topmost outputs of attention sub-layer are taken to a logistic regression layer to make the final decision 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Attentional Neural Network for SRL</head><p>In this section, we will describe DEEPATT in detail. The main component of our deep network consists of N identical layers. Each layer contains a nonlinear sub-layer followed by an attentional sub-layer. The topmost layer is the softmax classification layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><p>Self-attention or intra-attention, is a special case of attention mechanism that only requires a single sequence to </p><formula xml:id="formula_1">Softmax P (B ARG0 ) P (I ARG0 ) P (B V ) P (B ARG1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word &amp; Predicate</head><p>Sub-Layer compute its representation. Self-attention has been successfully applied to many tasks, including reading comprehension, abstractive summarization, textual entailment, learning task-independent sentence representations, machine translation and language understanding <ref type="bibr" target="#b6">(Cheng, Dong, and Lapata 2016;</ref><ref type="bibr">Parikh et al. 2016;</ref><ref type="bibr" target="#b19">Lin et al. 2017;</ref><ref type="bibr" target="#b28">Paulus, Xiong, and Socher 2017;</ref><ref type="bibr" target="#b40">Vaswani et al. 2017;</ref><ref type="bibr" target="#b33">Shen et al. 2017)</ref>.</p><p>In this paper, we adopt the multi-head attention formulation by <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref>. <ref type="figure">Figure 2</ref> depicts the computation graph of multi-head attention mechanism. The center of the graph is the scaled dot-product attention, which is a variant of dot-product (multiplicative) attention <ref type="bibr" target="#b20">(Luong, Pham, and Manning 2015)</ref>. Compared with the standard additive attention mechanism (Bahdanau, Cho, and Bengio 2014) which is implemented using a one layer feed-forward neural network, the dot-product attention utilizes matrix production which allows faster computation. Given a matrix of n query vectors Q ∈ R n×d , keys K ∈ R n×d and values V ∈ R n×d , the scaled dot-product attention computes the attention scores based on the following equation:</p><formula xml:id="formula_2">Attention(Q, K, V) = softmax( QK T √ d )V (1)</formula><p>where d is the number of hidden units of our network.</p><p>The multi-head attention mechanism first maps the matrix of input vectors X ∈ R t×d to queries, keys and values matrices by using different linear projections. Then h parallel heads are employed to focus on different part of channels</p><formula xml:id="formula_3">X Q K V Linear Linear Linear Split Split Split Matmul Softmax Matmul Concat Linear Y h parallel heads</formula><p>Figure 2: The computation graph of multi-head self-attention mechanism. All heads can be computed in parallel using highly optimized matrix multiplication codes.</p><p>of the value vectors. Formally, for the i-th head, we denote the learned linear maps by</p><formula xml:id="formula_4">W Q i ∈ R n×d/h , W K i ∈ R n×d/h and W V i ∈ R n×d/h</formula><p>, which correspond to queries, keys and values respectively. Then the scaled dot-product attention is used to compute the relevance between queries and keys, and to output mixed representations. The mathematical formulation is shown below:</p><formula xml:id="formula_5">M i = Attention(QW Q i , KW K i , VW V i )<label>(2)</label></formula><p>Finally, all the vectors produced by parallel heads are concatenated together to form a single vector. Again, a linear map is used to mix different channels from different heads:</p><formula xml:id="formula_6">M = Concat(M 1 , . . . , M h ) (3) Y = MW (4)</formula><p>where M ∈ R n×d and W ∈ R d×d . The self-attention mechanism has many appealing aspects compared with RNNs or CNNs. Firstly, the distance between any input and output positions is 1, whereas in RNNs it can be n. Unlike CNNs, self-attention is not limited to fixed window sizes. Secondly, the attention mechanism uses weighted sum to produce output vectors. As a result, the gradient propagations are much easier than RNNs or CNNs. Finally, the dot-product attention is highly parallel. In contrast, RNNs are hard to parallelize owing to its recursive computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nonlinear Sub-Layers</head><p>The successes of neural networks root in its highly flexible nonlinear transformations. Since attention mechanism uses weighted sum to generate output vectors, its representational power is limited. To further increase the expressive power of our attentional network, we employ a nonlinear sub-layer to transform the inputs from the bottom layers. In this paper, we explore three kinds of nonlinear sub-layers, namely recurrent, convolutional and feed-forward sub-layers.</p><p>Recurrent Sub-Layer We use bidirectional LSTMs to build our recurrent sub-layer. Given a sequence of input vectors {x t }, two LSTMs process the inputs in opposite directions. To maintain the same dimension between inputs and outputs, we use the sum operation to combine two representations:</p><formula xml:id="formula_7">− → h t = LSTM(x t , − → h t−1 ) (5) ← − h t = LSTM(x t , ← − h t+1 )<label>(6)</label></formula><formula xml:id="formula_8">y t = − → h t + ← − h t<label>(7)</label></formula><p>Convolutional Sub-Layer For convolutional sub-layer, we use the Gated Linear Unit (GLU) proposed by <ref type="bibr" target="#b9">Dauphin et al. (2016)</ref>. Compared with the standard convolutional neural network, GLU is much easier to learn and achieves impressive results on both language modeling and machine translation task ( <ref type="bibr" target="#b9">Dauphin et al. 2016;</ref><ref type="bibr" target="#b11">Gehring et al. 2017</ref>). Given two filters W ∈ R k×d×d and V ∈ R k×d×d , the output activations of GLU are computed as follows:</p><formula xml:id="formula_9">GLU(X) = (X * W) σ(X * V)<label>(8)</label></formula><p>The filter width k is set to 3 in all our experiments. </p><formula xml:id="formula_10">FFN(X) = ReLU(XW 1 )W 2<label>(9)</label></formula><p>where W 1 ∈ R d×h f and W 2 ∈ R h f ×d are trainable matrices. Unless otherwise noted, we set h f = 800 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Topology</head><p>Previous works pointed out that deep topology is essential to achieve good performance (Zhou and Xu 2015; <ref type="bibr" target="#b15">He et al. 2017)</ref>. In this work, we use the residual connections proposed by <ref type="bibr" target="#b14">He et al. (2016)</ref> to ease the training of our deep attentional neural network. Specifically, the output Y of each sub-layer is computed by the following equation:</p><formula xml:id="formula_11">Y = X + Sub-Layer(X)<label>(10)</label></formula><p>We then apply layer normalization (Ba, Kiros, and Hinton 2016) after the residual connection to stabilize the activations of deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Position Encoding</head><p>The attention mechanism itself cannot distinguish between different positions. So it is crucial to encode positions of each input words. There are various ways to encode positions, and the simplest one is to use an additional position embedding. In this work, we try the timing signal approach proposed by <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref>, which is formulated as follows:</p><formula xml:id="formula_12">timing(t, 2i) = sin(t/10000 2i/d )<label>(11)</label></formula><formula xml:id="formula_13">timing(t, 2i + 1) = cos(t/10000 2i/d )<label>(12)</label></formula><p>The timing signals are simply added to the input embeddings. Unlike the position embedding approach, this approach does not introduce additional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pipeline</head><p>The first step of using neural networks to process symbolic data is to represent them by distributed vectors, also called embeddings ( <ref type="bibr" target="#b4">Bengio et al. 2003</ref>). We take the very original utterances and the corresponding predicate masks m as the input features. m t is set to 1 if the corresponding word is a predicate, or 0 if not. Formally, in SRL task, we have a word vocabulary V and mask vocabulary C = {0, 1}. Given a word sequence {x 1 , x 2 , . . . , x T } and a mask sequence {m 1 , m 2 , ..., m T }, each word x t ∈ V and its corresponding predicate mask m t ∈ C are projected into real-valued vectors e(x t ) and e(m t ) through the corresponding lookup table layer, respectively. The two embeddings are then concatenated together as the output feature maps of the lookup table layers. Formally speaking, we have x t = [e(w t ), e(m t )].</p><p>We then build our deep attentional neural network to learn the sequential and structural information of a given sentence based on the feature maps from the lookup table layer. Finally, we take the outputs of the topmost attention sub-layer as inputs to make the final predictions.</p><p>Since there are dependencies between semantic labels, most previous neural network models introduced a transition model for measuring the probability of jumping between the labels. Different from these works, we perform SRL as a typical classification problem. Latent dependency information is embedded in the topmost attention sub-layer learned by our deep models. This approach is simpler and easier to implement compared to previous works.</p><p>Formally, given an input sequence x = {x 1 , x 2 , . . . , x n }, the log-likelihood of the corresponding correct label sequence y = {y 1 , y 2 , . . . , y n } is log p(y|x; θ) = n t=1 log p(y t |x; θ).</p><p>Our model predict the corresponding label y t based on the representation h t produced by the topmost attention sublayer of DEEPATT: p(y t |x; θ) = p(y t |h t ; θ)</p><p>= softmax(W o h t ) T δ yt ,</p><p>Where W o is the softmax matrix and δ yt is Kronecker delta with a dimension for each output symbol, so softmax(W o h t ) T δ yt is exactly the y t 'th element of the distribution defined by the softmax. Our training objective is to maximize the log probabilities of the correct output labels given the input sequence over the entire training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We report our empirical studies of DEEPATT on the two commonly used datasets from the CoNLL-2005 shared task and the CoNLL-2012 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>The CoNLL-2005 dataset takes section 2-21 of the Wall Street Journal (WSJ) corpus as training set, and section 24 as development set. The test set consists of section 23 of the WSJ corpus as well as 3 sections from the Brown corpus (Carreras and M` arquez 2005). The CoNLL-2012 dataset is extracted from the OntoNotes v5.0 corpus. The description and separation of training, development and test set can be found in <ref type="bibr">Pardhan et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Setup</head><p>Initialization We initialize the weights of all sub-layers as random orthogonal matrices. For other parameters, we initialize them by sampling each element from a Gaussian distribution with mean 0 and variance 1 √ d</p><p>. The embedding layer can be initialized randomly or using pre-trained word embeddings. We will discuss the impact of pre-training in the analysis subsection. <ref type="bibr">3</ref> Settings and Regularization The settings of our models are described as follows. The dimension of word embeddings and predicate mask embeddings is set to 100 and the number of hidden layers is set to 10. We set the number of hidden units d to 200. The number of heads h is set to 8. We apply dropout ( <ref type="bibr" target="#b34">Srivastava et al. 2014</ref>) to prevent the networks from over-fitting. Dropout layers are added before residual connections with a keep probability of 0.8. Dropout is also applied before the attention softmax layer and the feed-froward ReLU hidden layer, and the keep probabilities are set to 0.9. We also employ label smoothing technique ( <ref type="bibr" target="#b36">Szegedy et al. 2016</ref>) with a smoothing value of 0.1 during training.</p><p>Learning Parameter optimization is performed using stochastic gradient descent. We adopt Adadelta (Zeiler 2012) ( = 10 6 and ρ = 0.95) as the optimizer. To avoid exploding gradients problem, we clip the norm of gradients with a predefined threshold 1.0 ( <ref type="bibr" target="#b27">Pascanu et al. 2013</ref>). Each SGD contains a mini-batch of approximately 4096 tokens for the CoNLL-2005 dataset and 8192 tokens for the CoNLL-2012 dataset. The learning rate is initialized to 1.0. After training 400k steps, we halve the learning rate every 100K steps. We train all models for 600K steps. For DEEP-ATT with FFN sub-layers, the whole training stage takes about two days to finish on a single Titan X GPU, which is 2.5 times faster than the previous approach ( <ref type="bibr" target="#b15">He et al. 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In <ref type="table" target="#tab_1">Table 1</ref>    on the out-of-domain dataset, which outperforms the previous state-of-the-art system by 2.0 F 1 score. On the CoNLL-2012 dataset, the single model of FFN variant also outperforms the previous state-of-the-art by 1.0 F 1 score. When ensembling 5 models with FFN nonlinear sub-layers, our approach achieves an F 1 score of 84.6 and 83.9 on the two datasets respectively, which has an absolute improvement of 1.4 and 0.5 over the previous state-of-the-art. These results are consistent with our intuition that the self-attention layers is helpful to capture structural information and long distance dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>In this subsection, we discuss the main factors that influence our results. We analyze the experimental results on the development set of CoNLL-2005 dataset.</p><p>Model Depth Previous works (Zhou and Xu 2015; <ref type="bibr" target="#b15">He et al. 2017)</ref> show that model depth is the key to the success of end-to-end SRL approach. Our observations also coincide with previous works. Rows 1-5 of  model only achieves 79.9 F 1 score. Increasing depth consistently improves the performance on the development set, and our best model consists of 10 layers. For DEEPATT with 12 layers, we observe a slightly performance drop of 0.1 F 1 .</p><p>Model Width We also conduct experiments with different model widths. We increase the number of hidden units from 200 to 400 and 400 to 600 as listed in rows 1, 6 and 7 of <ref type="table" target="#tab_3">Table 3</ref>, and the corresponding hidden size h f of FFN sublayers is increased to 1600 and 2400 respectively. IncreasDecoding F 1 Speed Argmax Decoding 83.1 50K Constrained Decoding 83.0 17K <ref type="table">Table 4</ref>: Comparison between argmax decoding and constrained decoding on top of our model. ing model widths improves the F 1 slightly, and the model with 600 hidden units achieves an F 1 of 83.4. However, the training and parsing speed are slower as a result of larger parameter counts.</p><p>Word Embedding Previous works found that the performance can be improved by pre-training the word embeddings on large unlabeled data ( <ref type="bibr" target="#b7">Collobert et al. 2011</ref>; Zhou and Xu 2015). We use the GloVe (Pennington, Socher, and Manning 2014) embeddings pre-trained on Wikipedia and Gigaword. The embeddings are used to initialize our networks, but are not fixed during training. Rows 1 and 8 of <ref type="table" target="#tab_3">Table 3</ref> show the effects of additional pre-trained embeddings. When using pre-trained GloVe embeddings, the F 1 score increases from 79.6 to 83.1.</p><p>Position Encoding From rows 1, 9 and 10 of <ref type="table" target="#tab_3">Table 3</ref> we can see that the position encoding plays an important role in the success of DEEPATT. Without position encoding, the DEEPATT with FFN sub-layers only achieves 20.0 F 1 score on the CoNLL-2005 development set. When using position embedding approach, the F 1 score boosts to 79.4. The timing approach is surprisingly effective, which outperforms the position embedding approach by 3.7 F 1 score.</p><p>Nonlinear Sub-Layers DEEPATT requires nonlinear sublayers to enhance its expressive power. Row 11 of <ref type="table" target="#tab_3">Table 3</ref> shows the performance of DEEPATT without nonlinear sublayers. We can see that the performance of 10 layered DEEP-ATT without nonlinear sub-layers only matches the 4 layered DEEPATT with FFN sub-layers, which indicates that the nonlinear sub-layers are the essential components of our attentional networks. <ref type="table">Table 4</ref> show the effects of constrained decoding ( <ref type="bibr" target="#b15">He et al. 2017</ref>) on top of DEEPATT with FFN sub-layers. We observe a slightly performance drop when using constrained decoding. Moreover, adding constrained decoding slow down the decoding speed significantly. For DEEPATT, it is powerful enough to capture the relationships among labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constrained Decoding</head><p>Detailed Scores We list the detailed performance on frequent labels in <ref type="table" target="#tab_6">Table 5</ref>. The results of the previous stateof-the-art ( <ref type="bibr" target="#b15">He et al. 2017</ref>) are also shown for comparison. Compared with <ref type="bibr" target="#b15">He et al. (2017)</ref>, our model shows improvement on all labels except AM-PNC, where He's model performs better. <ref type="table" target="#tab_7">Table 6</ref> shows the results of identifying and classifying semantic roles. Our model improves the previous state-of-the-art on both identifying correct spans as well    Labeling Confusion <ref type="table" target="#tab_8">Table 7</ref> shows a confusion matrix of our model for the most frequent labels. We only consider predicted arguments that match gold span boundaries. Compared with the previous work ( <ref type="bibr" target="#b15">He et al. 2017)</ref>, our model still confuses ARG2 with AM-DIR, AM-LOC and AM-MNR, but to a lesser extent. This indicates that our model has some advantages on such difficult adjunct distinction <ref type="bibr" target="#b16">(Kingsbury, Palmer, and Marcus 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>SRL Gildea and Jurafsky (2002) developed the first automatic semantic role labeling system based on FrameNet.</p><p>Since then the task has received a tremendous amount of attention. The focus of traditional approaches is devising appropriate feature templates to describe the latent structure of utterances. <ref type="bibr" target="#b30">Pradhan et al. (2005)</ref>; <ref type="bibr" target="#b35">Surdeanu et al. (2003);</ref><ref type="bibr" target="#b24">Palmer, Gildea, and Xue (2010)</ref> explored the syntactic features for capturing the overall sentence structure. Combination of different syntactic parsers was also proposed to avoid prediction risk which was introduced by <ref type="bibr" target="#b35">Surdeanu et al. (2003)</ref>; <ref type="bibr" target="#b18">Koomen et al. (2005)</ref>; <ref type="bibr" target="#b31">Pradhan et al. (2013)</ref>. Beyond these traditional methods above, <ref type="bibr" target="#b7">Collobert et al. (2011)</ref> proposed a convolutional neural network for SRL to reduce the feature engineering. The pioneering work on building an end-to-end system was proposed by <ref type="bibr" target="#b43">Zhou and Xu (2015)</ref>, who applied an 8 layered LSTM model which outperformed the previous state-of-the-art system. <ref type="bibr" target="#b15">He et al.(2017)</ref> improved further with highway LSTMs and constrained decoding. They used simplified input and output layers compared with <ref type="bibr" target="#b43">Zhou and Xu (2015)</ref>. Marcheggiani, Frolov, Titov (2017) also proposed a bidirectional LSTM based model. Without using any syntactic information, their approach achieved the state-of-the-art result on the CoNLL-2009 dataset.</p><p>Our method differs from them significantly. We choose self-attention as the key component in our architecture instead of LSTMs. Like <ref type="bibr" target="#b15">He et al. (2017)</ref>, our system take the very original utterances and predicate masks as the inputs without context windows. At the inference stage, we apply argmax decoding approach on top of a simple logistic regression while Zhou and Xu (2015) chose a CRF approach and <ref type="bibr" target="#b15">He et al. (2017)</ref> chose constrained decoding. This approach is much simpler and faster than the previous approaches.</p><p>Self-Attention Self-attention have been successfully used in several tasks. <ref type="bibr" target="#b6">Cheng, Dong, and Lapata (2016)</ref> used LSTMs and self-attention to facilitate the task of machine reading. <ref type="bibr">Parikh et al. (2016)</ref> utilized self-attention to the task of natural language inference. <ref type="bibr" target="#b19">Lin et al. (2017)</ref> proposed self-attentive sentence embedding and applied them to author profiling, sentiment analysis and textual entailment. <ref type="bibr" target="#b28">Paulus, Xiong, and Socher (2017)</ref> combined reinforcement learning and self-attention to capture the long distance dependencies nature of abstractive summarization. <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref> applied self-attention to neural machine translation and achieved the state-of-the-art results. Very recently, <ref type="bibr" target="#b33">Shen et al. (2017)</ref> applied self-attention to language understanding task and achieved the state-of-the-art on various datasets. Our work follows this line to apply self-attention for learning long distance dependencies. Our experiments also show the effectiveness of self-attention mechanism on the sequence labeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a deep attentional neural network for the task of semantic role labeling. We trained our SRL models with a depth of 10 and evaluated them on the CoNLL-2005 shared task dataset and the CoNLL-2012 shared task dataset. Our experimental results indicate that our models substantially improve SRL performances, leading to the new state-of-theart.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of our deep attentional neural network. Original utterances and corresponding predicate masks are taken as the only inputs for our deep model. For example, love is the predicate and marked as 1, while other words are marked as 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Feed-forward Sub-Layer The feed-forward sub-layer is quite simple. It consists of two linear layers with hidden ReLU nonlinearity (Nair and Hinton 2010) in the middle. Formally, we have the following equation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>and 2, we give the comparisons of DEEPATT with previous approaches. On the CoNLL-2005 dataset, the sin-</figDesc><table>Model 

Development 
WSJ Test 
Brown Test 
Combined 

P 
R 
F1 
Comp. 
P 
R 
F1 
Comp. 
P 
R 
F1 
Comp. 
F1 
He et al. (Ensemble) (2017) 
83.1 82.4 82.7 
64.1 
85.0 84.3 84.6 
66.5 
74.9 72.4 73.6 
46.5 
83.2 
He et al. (Single) (2017) 
81.6 81.6 81.6 
62.3 
83.1 83.0 83.1 
64.3 
72.8 71.4 72.1 
44.8 
81.6 
Zhou and Xu (2015) 
79.7 79.4 79.6 
-
82.9 82.8 82.8 
-
70.7 68.2 69.4 
-
81.1 
FitzGerald et al. (Struct., Ensemble) (2015) 81.2 76.7 78.9 
55.1 
82.5 78.2 80.3 
57.3 
74.5 70.0 72.2 
41.3 
-
Täckström et al. (Struct.) (2015) 
81.2 76.2 78.6 
54.4 
82.3 77.6 79.9 
56.0 
74.3 68.6 71.3 
39.8 
-
Toutanova et al. (Ensemble) (2008) 
-
-
78.6 
58.7 
81.9 78.8 80.3 
60.1 
-
-
68.8 
40.8 
-
Punyakanok et al. (Ensemble) (2008) 
80.1 74.8 77.4 
50.7 
82.3 76.8 79.4 
53.8 
73.4 62.9 67.8 
32.3 
77.9 
DEEPATT (RNN) 
81.2 82.3 81.8 
62.4 
83.5 84.0 83.7 
65.2 
72.5 73.4 72.9 
44.7 
82.3 
DEEPATT (CNN) 
82.1 82.8 82.4 
63.6 
83.6 83.9 83.8 
65.4 
72.8 72.7 72.7 
45.9 
82.3 
DEEPATT (FFN) 
82.6 83.6 83.1 
65.2 
84.5 85.2 84.8 
66.4 
73.5 74.6 74.1 
48.4 
83.4 
DEEPATT (FFN, Ensemble) 
84.3 84.9 84.6 
67.3 
85.9 86.3 86.1 
69.0 
74.6 75.0 74.8 
48.6 
84.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison with previous methods on the CoNLL-2005 dataset. We report the results in terms of precision (P), 
recall (R), F 1 and percentage of completely correct predicates (Comp.). Our single and ensemble model lead to substantial 
improvements over the previous state-of-the-art results. 

Model 
Development 
Test 

P 
R 
F1 
Comp. 
P 
R 
F1 
Comp. 
He et al. (Ensemble) (2017) 
83.5 83.2 83.4 
67.5 
83.5 83.3 83.4 
68.5 
He et al. (Single) (2017) 
81.7 81.4 81.5 
64.6 
81.8 81.6 81.7 
66.0 
Zhou and Xu (2015) 
-
-
81.1 
-
-
-
81.3 
-
FitzGerald et al. (Struct., Ensemble) (2015) 81.0 78.5 79.7 
60.9 
81.2 79.0 80.1 
62.6 
Täckström et al. (Struct., Ensemble) (2015) 80.5 77.8 79.1 
60.1 
80.6 78.2 79.4 
61.8 
Pradhan et al.(Revised) (2013) 
-
-
-
-
78.5 76.6 77.5 
55.8 
DEEPATT (RNN) 
81.0 82.3 81.6 
64.6 
80.9 82.2 81.5 
65.7 
DEEPATT (CNN) 
80.1 82.5 81.3 
65.0 
79.8 82.6 81.2 
66.1 
DEEPATT (FFN) 
82.2 83.6 82.9 
66.7 
81.9 83.6 82.7 
67.5 
DEEPATT (FFN, Ensemble) 
83.6 84.7 84.1 
68.7 
83.3 84.5 83.9 
69.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results on the CoNLL-2012 dataset. The metrics are the same as above. Again, our model achieves the 
state-of-the-art performance. 

gle model of DEEPATT with RNN, CNN and FFN nonlinear 
sub-layers achieves an F 1 score of 82.3, 82.3 and 83.4 re-
spectively. The FFN variant outperforms previous best per-
formance by 1.8 F 1 score. Remarkably, we get 74.1 F 1 score 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 show</head><label>3</label><figDesc></figDesc><table>the effects of 
different number of layers. For DEEPATT with 4 layers, our 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Detailed results on the CoNLL-2005 development 
set. PE denotes the way to encoding word positions. GloVe 
refers to the GloVe embedding pre-trained on 6B tokens. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Detailed scores on the development set of CoNLL-
2005 dataset. We also list the previous state-of-the-art 
model (He et al. 2017) for comparison. 

Model 
Constituents Semantic Roles 
He et al. (2017) 
91.87 
87.10 
DEEPATT 
91.92 
88.88 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison with the previous work on identifying 
and classifying semantic roles. We list the percentage of cor-
rectly identified spans as well as the percentage of correctly 
classified semantic roles given the gold spans. 

as correctly classifying them into semantic roles. However, 
the majority of improvements come from classifying seman-
tic roles. This indicates that finding the right constituents re-
mains a bottleneck of our model. 

pred./gold 
A0 
A1 
A2 
A3 
ADV 
DIR 
LOC MNR 
PNC 
TMP 
A0 
-
48 
12 
7 
0 
0 
0 
0 
7 
0 
A1 
76 
-
35 
0 
7 
16 
19 
2 
30 
0 
A2 
10 
37 
-
42 
15 
33 
28 
35 
38 
0 
A3 
0 
0 
5 
-
0 
0 
0 
2 
7 
0 
ADV 
0 
0 
0 
0 
-
0 
9 
26 
15 
58 
DIR 
0 
0 
2 
0 
0 
-
9 
2 
0 
0 
LOC 
10 
10 
10 
7 
11 
16 
-
15 
0 
8 
MNR 
0 
0 
22 
28 
26 
0 
4 
-
0 
33 
PNC 
0 
0 
10 
7 
0 
0 
4 
2 
-
0 
TMP 
3 
2 
2 
7 
38 
33 
23 
13 
0 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Confusion matrix for labeling errors. Each cell 
shows the percentage of predicted labels for each gold la-
bel. 

</table></figure>

			<note place="foot" n="1"> Our source code is available at https://github.com/ XMUNLP/Tagger</note>

			<note place="foot" n="2"> In case of BIO violations, we simply treat the argument of the B tags as the argument of the whole span.</note>

			<note place="foot" n="3"> To be strictly comparable to previous work, we use the same vocabularies and pre-trained embeddings as He et al.(2017).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was done while the first author's internship at Tencent Technology. This work is supported by the Natural Science Foundation of China <ref type="bibr">(Grant No. 61573294, 61303082, 61672440)</ref>, the Ph.D. Programs Foundation of Ministry of Education of <ref type="bibr">China (Grant No. 20130121110040)</ref>, the Foundation of the State Language Commission of China (Grant No. WT135-10) and the Natural Science Foundation of Fujian Province (Grant No. 2016J05161). We also thank the anonymous reviews for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bastianelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Textual inference and meaning representation in human robot interaction</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</title>
		<meeting>the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</meeting>
		<imprint>
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Long shortterm memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marchine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using semantic roles to improve question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08083</idno>
		<title level="m">Language modeling with gated convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic role labeling with neural network factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Framework for abstractive summarization using text-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Genest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapalme</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Monolingual Text-To-Text Generation</title>
		<meeting>the Workshop on Monolingual Text-To-Text Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="64" to="73" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurafsky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="288" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adding semantic annotation to the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedins of the human language technology conference</title>
		<meeting>eedins of the human language technology conference</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a large-scale knowledge base for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="773" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized inference with multiple semantic role labeling systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koomen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference on Computational Natural Language Learning</title>
		<meeting>the 9th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<title level="m">Effective approaches to attention-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Open domain information extraction via automatic semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference&apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technology Series</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<idno type="arXiv">arXiv:1606.01933</idno>
		<title level="m">A decomposable attention model for natural language inference</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<title level="m">How to construct deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<title level="m">A deep reinforced model for abstractive summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic role chunking combining complementary syntactic views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference on Computational Natural Language Learning</title>
		<meeting>the 9th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using predicate-argument structures for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarseth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient inference and structured learning for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A global joint model for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="161" to="191" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transductive learning for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic roles for smt: a hybrid two-pass model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies</title>
		<meeting>Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
