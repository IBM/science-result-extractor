<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Neural Abstractive Document Summarization with Structural Regularization *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31 -November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
							<email>wangyuanzhuo@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Neural Abstractive Document Summarization with Structural Regularization *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4078" to="4087"/>
							<date type="published">October 31 -November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4078</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent neural sequence-to-sequence models have shown significant progress on short text summarization. However, for document sum-marization, they fail to capture the long-term structure of both documents and multi-sentence summaries, resulting in information loss and repetitions. In this paper, we propose to leverage the structural information of both documents and multi-sentence summaries to improve the document summariza-tion performance. Specifically, we import both structural-compression and structural-coverage regularization into the summariza-tion process in order to capture the information compression and information coverage properties, which are the two most important structural properties of document sum-marization. Experimental results demonstrate that the structural regularization improves the document summarization performance significantly , which enables our model to generate more informative and concise summaries, and thus significantly outperforms state-of-the-art neural abstractive methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document summarization is the task of generating a fluent and condensed summary for a document while retaining the gist information. Recent success of neural sequence-to-sequence (seq2seq) architecture on text generation tasks like machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) and image caption ( <ref type="bibr" target="#b32">Vinyals et al., 2015)</ref>, has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization <ref type="bibr">(Rush et al., 2015;</ref><ref type="bibr" target="#b29">Takase et al., 2016;</ref><ref type="bibr" target="#b9">Chopra et al., 2016;</ref><ref type="bibr" target="#b3">Cao et al., 2017;</ref>, which builds onesentence summaries from one or two-sentence in- * This work was done while the first author was doing internship at Baidu Inc.   <ref type="table">Table 1</ref> on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and the Point-gen-cov model, lose much salient information of the input document and focus on the same set of sentences repeatedly. The Hierarchical-baseline model fails to detect several specific sentences that are salient and relevant for each summary sentence and focuses on the same set of sentences repeatedly. On the contrary, our method with structural regularizations focuses on different sets of source sentences when generating different summary sentences and discovers more salient information from the document.</p><p>put. However, the extension of sentence abstractive methods to document summarization task is not straightforward.</p><p>As long-distance dependencies are difficult to be captured in the recurrent framework <ref type="bibr" target="#b2">(Bengio et al., 1994)</ref>, the seq2seq models are not yet able to achieve convincing performance in encoding and decoding for a long sequence of multiple sentences ( <ref type="bibr" target="#b5">Chen et al., 2017;</ref><ref type="bibr">Koehn and Knowles, Original Text (truncated)</ref>: the family of conjoined twin sisters who died 19 days after they were born have been left mortified <ref type="bibr">(2)</ref> after they arrived at their gravesite to find cemetery staff had cleared the baby section of all mementos and tossed them in the rubbish <ref type="bibr">(3)</ref> . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus <ref type="bibr">(1)</ref> . they died in hospital less than a month after they were born and their parents , simon howie and renee young , laid them to rest at pinegrove memorial park in sydney 's west <ref type="bibr">(2)</ref> . scroll down for video . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus <ref type="bibr">(1)</ref> . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare <ref type="bibr">(3)</ref> . ' we took renee's aunts to see the girls for the first time and we found everything had been stripped away , ' mr howie told daily mail australia . ' we were devastated and mortified . we 've had a little shrine set up and we 've been adding to it since the funeral . ' it 's heartbreaking to know we 've set this up and it has been treated like rubbish . ' faith and hope were buried in a pink coffin and their family and friends released doves and pink and white balloons at their funeral . their family and friends had built up a small memorial with pink and white statues , flowers , pebbles and toys over the past 11 months . when they arrived on thursday , everything had been removed apart from a bunch of flowers . the twins were buried at pinegrove memorial park in western sydney after they died after just 19 days <ref type="bibr">(2)</ref> . their family and friends had built a small shrine at their gravesite , which they have added to since the funeral . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when parents simon howie and renee young arrived on thursday they found the site completely bare <ref type="bibr">(3)</ref> . Gold Reference: faith and hope howie were born with one body and two faces on may 8. they tragically died in hospital just 19 days after they were born . parents simon howie and renee young visit their grave at pinegrove in western sydney fortnightly . they arrived on thursday to find the grave bare of all the girls ' mementos . staff had cleared entire baby section and thrown belongings in rubbish . Seq2seq-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . <ref type="bibr">Point-cov (See et al., 2017)</ref>: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . they died in hospital less than a month after they were born and their parents , simon howie and renee young , laid them to rest at pinegrove memorial park in sydney 's west. Hierarchical-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition . they died in hospital less than a month after they died in hospital less than a month after they were born and laid them to rest at pinegrove memorial park in sydney 's west . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when they were born on thursday they found the site completely bare . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when they found the site completely bare . Our Method: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition <ref type="bibr">(1)</ref> . they died in hospital less than a month after they were born and their parents laid them to rest at pinegrove memorial park in sydney 's west <ref type="bibr">(2)</ref> . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare <ref type="bibr">(3)</ref> . <ref type="table">Table 1</ref>: Comparison of the generated summaries of four abstractive summarization models and the gold reference summary on a news article. The summaries generated by the seq2seq models, both the Seq2seq-baseline model and the Point-cov model, lose some salient information. The Seq2seq-baseline model even contains serious information repetitions. The Hierarchicalbaseline model not only contains serious repetitions, but also makes non-grammatical or non-coherent sentences. On the contrary, the summary generated by our model contains more salient information and is more concise. Our model also shows the ability to generate a summary sentence by compressing several source sentences, such as shortening a long sentence. 2017). In document summarization, it is also difficult for the seq2seq models to discover important information from too much input content of a document ( <ref type="bibr">Tan et al., 2017a,b)</ref>. The summary generated by the seq2seq models usually loses salient information of the original document or even contains repetitions (see <ref type="table">Table 1</ref>).</p><p>In fact, both document and summary naturally have document-sentence hierarchical structure, instead of being a flat sequence of words. It is widely aware that the hierarchical structure is necessary and useful for neural document modeling. Hierarchical neural models have already been successfully used in document-level language modeling ( <ref type="bibr" target="#b23">Lin et al., 2015</ref>) and document classification ( <ref type="bibr" target="#b33">Yang et al., 2016)</ref>. However, few work makes use of the hierarchical structure of document and multi-sentence summary in document summarization. The basic hierarchical encoderdecoder model ( ) is also not yet able to capture the structural properties of both document and summary (see <ref type="figure" target="#fig_0">Figure 1 1</ref> ), resulting in 1 To simulate the sentence-level attention mechanism on the gold reference summary, we compute the words-matching similarities (based on TF-IDF cosine similarity) between a reference-summary sentence and the corresponding source document sentences and normalize them into attention distributions. The sentence-level attention distributions of the Seq2seq-baseline model and the Point-gen-cov model are computed by summing the attention weights of all words in each sentence and then normalized across sentences. more serious repetitions and even nonsensical sentences (see <ref type="table">Table 1</ref>).</p><p>In document summarization, information compression and information coverage are the two most important structural properties. Based on the hierarchical structure of document and summary, they can be realized at the sentencelevel as: (1) Structural-compression: each summary sentence is generated by compressing several specific source sentences; (2) Structuralcoverage: different summary sentences usually focus on different sets of source sentences to cover more salient information of the original document. <ref type="figure" target="#fig_0">Figure 1</ref>(a) intuitively shows the two properties in human-written gold reference summaries. We import both structural-compression and structural-coverage regularizations into the document summarization process based on a hierarchical encoder-decoder with hybrid sentenceword attention model. Typically, we design an effective learning and inference algorithm to explicitly model the structural-compression and structural-coverage properties of document summarization process, so as to generate more informative and concise summaries (see <ref type="table">Table 1</ref>).</p><p>We conduct our experiments on benchmark datasets and the results demonstrate that properly modeling the structural-compression and structural-coverage properties based on the hier-</p><formula xml:id="formula_0">sentence-level encoder word-level encoder hierarchical decoder Sentence-level Attention Word-level Attention c t s GRU c t−1 s x t−1 ' d t s c t ,k w GRU c t ,k−1 w e t ,k−1 d t ,k w γ t,k Structural Compression Structural Coverage α t h 1 h 2 h 3 h 1,1 h 1,2 h 1,3 h 2,1 h 2,2 h 2,3 h 3,1 h 3,2 h 3,3 h t−1 ' h t ,k−1 ' α 1...t</formula><p>Figure 2: Our hierarchical encoder-decoder model with structural regularization for abstractive document summarization.</p><p>archical structure of document and summary, improves document summarization performance significantly. Our model is able to generate more informative and concise summaries by enhancing sentences compression and coverage, and significantly outperforms state-of-the-art seq2seq-based abstractive methods, especially on summarizing long documents with long summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hierarchical Encoder-Decoder Model</head><p>In this section, we introduce our baseline hierarchical encoder-decoder model which consists of two parts: a hierarchical encoder and a hierarchical decoder, as shown in <ref type="figure">Figure 2</ref>. Similar to ( , both the encoder and decoder consists of two levels: a sentence level and a word level.</p><p>The main distinction is that we design a hybrid sentence-word attention mechanism on the hierarchical decoder to help organize summary content and realize summary sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical Encoder</head><p>The goal of the encoder is to map the input document to a hidden vector representation. We consider a source document X as a sequence of sentences: X = {s i }, and each sentence s i as a sequence of words: s i = {w ij }. The wordlevel encoder encodes the words of a sentence into a sentence representation, and the sentencelevel encoder encodes the sentences of a document into the document representation. In this work, both the word-level encoder and sentencelevel encoder use the bidirectional Gated Recurrent Unit (BiGRU) ( <ref type="bibr" target="#b10">Chung et al., 2014</ref>). The word-level encoder sequentially updates its hidden state upon each received word, as</p><formula xml:id="formula_1">h i,j = BiGRU (h i,j−1 , e i,j )</formula><p>where h i,j and e i,j denote the hidden state and the embedding of word w i,j , respectively. The concatenation of the forward and backward final hidden states in the word-level encoder is indicated as the vector representation r i of sentence s i , which is used as input to the sentencelevel encoder. The sentence-level encoder updates its hidden state after receiving each sentence representation, as h i = BiGRU (h i−1 , r i ) where h i denotes the hidden state of sentence s i . The concatenation of the forward and backward final states in the sentence-level encoder is used as the vector representation of document d.</p><p>In the hierarchical encoder architecture, long dependency problem will be largely reduced at both the sentence level and the word level, so it can better capture the structural information of the input document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Decoder with Hybrid</head><p>Sentence-Word Attention </p><formula xml:id="formula_2">′ t,k = GRU (h ′ t,k−1 , e t,k−1 )</formula><p>where h ′ t,k denotes the hidden state of word w ′ t,k in sentence s ′ t and e t,k−1 denotes the embedding of previously generated word</p><formula xml:id="formula_3">w ′ t,k−1 in sentence s ′ t .</formula><p>In this work, we design a hybrid sentenceword attention mechanism based on the hierarchical encoder-decoder architecture, which contains both sentence-level attention and word-level attention, to better exploit both the sentence-level information and word-level information from the input document and the output summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Sentence-level Attention</head><p>The sentence-level attention mechanism is designed on the sentence-level encoder and decoder, which is used to help our model to detect important and relevant source sentences in each sentence generation step. α i t indicates how much the t-th summary sentence attends to the i-th source sentence, which is computed by</p><formula xml:id="formula_4">α i t = e f (h i ,h ′ t ) / ∑ l e f (h l ,h ′ t )</formula><p>where f is the function modeling the relation between h i and h ′ t . We use the function f (a,</p><formula xml:id="formula_5">b) = v T tanh(W a a + W b b),</formula><p>where v, W a , W b are all learnable parameters. Then the sentence level context vector c s t when generating the tth sentence s ′ t can be computed as: c s t = ∑ i α i t h i , which is incorporated into the sentence-level decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Word-level Attention with</head><p>Sentence-level Normalization The word-level attention is designed on the wordlevel encoder and decoder, which is used to help our model to realize the summary sentence by locating relevant words in the selected source sentences in each word generation step. Let β i,j t,k denotes how much the j-th word in source sentence s i contributes to generating the k-th word in summary sentence s ′ t , which is computed by</p><formula xml:id="formula_6">β i,j t,k = e f (h i,j ,h ′ t,k ) / ∑ l e f (h i,l ,h ′ t,k ) .</formula><p>Since the word-level attention above is within each source sentence, we normalize it by sentencelevel attentions to get word attention over all source words, as γ i t,k = β i t,k α i t . Then the wordlevel context vector when generating word w ′ t,k can be computed as:</p><formula xml:id="formula_7">c w t,k = ∑ i ∑ j γ i,j t,k h i,j</formula><p>, which is also incorporated into the word-level decoding process.</p><p>At each word generation step, the vocabulary distribution is calculated from the context vector c w t,k and the decoder state h ′ t,k by:</p><formula xml:id="formula_8">P vocab (w ′ t,k ) = sof tmax(Wv(Wc[h ′ t,k , c w t,k ] + bc) + bv)<label>(1)</label></formula><p>where W v , W c , b c and b v are learned parameters. We also incorporate the copy mechanism (See et al., 2017) based on the normalized wordlevel attention to help generate out-of-vocabulary (OOV) words during the sentence realization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structural Regularization</head><p>Although the above hierarchical encoder-decoder model is designed based on the documentsentence hierarchical structure, it can't capture the basic structural properties of document summarization (see <ref type="figure" target="#fig_0">Figure 1</ref>(d) and <ref type="table">Table 1</ref>). However, the hierarchical architecture makes it possible for importing structural regularization to capture the sentence-level characteristics of document summarization process. In this work, we propose to model the structural-compression and structural-coverage properties based on the hierarchical encoder-decoder model by adding structural regularization during both the model learning phase and inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structural Compression</head><p>Compression is a basic property of document summarization, which has been widely explored in traditional document summarization research, such as sentence compression-based methods which shorten sentences by removing non-salient parts ( <ref type="bibr" target="#b17">Li et al., 2013;</ref><ref type="bibr" target="#b12">Durrett et al., 2016</ref>) and sentence fusion-based methods which merge information from several different source sentences ( <ref type="bibr" target="#b1">Barzilay and McKeown, 2005;</ref><ref type="bibr" target="#b8">Cheung and Penn, 2014</ref>). As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, each summary sentence in the human-written reference summary is also created by compressing several specific source sentences.</p><p>In this paper, we propose to model the structural-compression property of document summarization based on sentence-level attention distributions by:</p><formula xml:id="formula_9">strCom(α t ) = 1 − 1 logN N ∑ i=1 α i t logα i t<label>(2)</label></formula><p>where α t denotes the sentence-level attention distribution when generating the tth summary sentence and N denotes the length of distribution α t . The right part in the above formula is actually the entropy of the distribution α t . As the attention distribution becomes sparser, the entropy of the distribution becomes lower, so the value of strCom(α t ) defined above will become larger. Sparse sentence-level attentions help the model compress and generalize several specific source sentences which are salient and relevant in the sentence generation process. Note that, 0 ≤ strCom(α t ) ≤ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structural Coverage</head><p>A good summary should have the ability to cover most of the important information of an input document. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the humanwritten reference summary covers the information of many source sentences. Coverage has been used as a measure in many traditional document summarization research, such as the submodularbased methods which optimize the information coverage of the summary with similarity-based coverage metrics ( <ref type="bibr" target="#b22">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b4">Chali et al., 2017)</ref>. In this work, we simply model the structuralcoverage property of summary based on the hierarchical architecture by encouraging different summary sentences to focus on different sets of source sentences so that the summary can cover more salient sentences of the input document. We measure the structural-coverage of summary based on the sentence-level attention distributions:</p><formula xml:id="formula_10">strCov(α t ) = 1 − ∑ i min(α i t , t−1 ∑ t ′ =0 α i t ′ ) (3)</formula><p>which is used to encourage different summary sentences to focus on different sets of source sentences during the summary generation process. As the sentence-level attention distributions of different summary sentences become more diversified, the summary will cover more source sentences, which is effective to improve the informativeness and conciseness of summaries. Note that, 0≤strCov(α t ) ≤ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Learning</head><p>Experimental results reveal that the properties of structural-compression and structural-coverage are hard to be captured by both the seq2seq models and the hierarchical encoder-decoder baseline model, which largely restricts their performance (Section 4). In this work, we model them explicitly by regulating the sentence-level attention distributions based on the hierarchical encoderdecoder framework. The loss function L of the model is the mix of negative log-likelihood of generating summaries over training set T , the structural-compression loss and the structuralcoverage loss:</p><formula xml:id="formula_11">L = ∑ (X,Y )∈T {−logP (Y |X; θ) + λ1 ∑ t strCom(αt) structural−compression loss + λ2 ∑ t strCov(αt) structural−coverage loss }<label>(4)</label></formula><p>where λ 1 and λ 2 are hyper-parameters tuned on the validation set. We use Adagrad (Duchi et al.,</p><p>2011) with learning rate 0.1 and an initial accumulator value 0.1 to optimize the model parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical Decoding Algorithm</head><p>The traditional beam search algorithm that widely used for text generation can only help generate fluent sentence, and is not easy to extend to the sentence level. The reason is that the K-best sentences generated by a word decoder will mostly be similar to each other ( <ref type="bibr" target="#b30">Tan et al., 2017a</ref>). We propose a hierarchical beam search algorithm with structural-compression and structural-coverage regularization. The hierarchical decoding algorithm has two levels: K-best word-level beam search and N -best sentence-level beam search. At the word-level, the vanilla beam search algorithm is used to maximize the accumulated scorê P (s ′ t ) of generating current summary sentence s ′ t . At the sentencelevel, N -best beam search is realized by maximizing the accumulated score score t of all the sentences generated, including the sentences generation score, structural-compression score and structural-coverage score, which are defined as:</p><formula xml:id="formula_12">scoret = t ∑ t ′ =0 { ˆ P (s ′ t ′ )+ζ1strCom(α t ′ )+ζ2strCov(α t ′ )}<label>(5)</label></formula><p>where ζ 1 and ζ 2 are factors introduced to control the influence of structural regularization during the decoding process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We conduct our experiments on the CNN/Daily Mail dataset ( <ref type="bibr" target="#b15">Hermann et al., 2015)</ref>, which has been widely used for exploration on summarizing documents with multi-sentence summaries <ref type="bibr">(Nal- lapati et al., 2016;</ref><ref type="bibr" target="#b28">See et al., 2017;</ref><ref type="bibr" target="#b30">Tan et al., 2017a;</ref><ref type="bibr" target="#b26">Paulus et al., 2017</ref>  use 512-dimensional hidden states. The dimension of word embeddings is 128, which is learned from scratch during training. We use a vocabulary of 50k words for both the encoder and decoder. We trained our model on a single Tesla K40m GPU with a batch size of 16 and an epoch is set containing 10,000 randomly sampled documents. Convergence is reached within 300 epochs. After tuning on the validation set, parameters λ 1 , λ 2 , ζ 1 and ζ 2 , are set as -0.5, -1.0, 1.2 and 1.4, respectively. At the test time, we use the hierarchical decoding algorithm with sentence-level beam size 4 and word-level beam size 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>ROUGE Evaluation. We evaluate our models with the widely used ROUGE <ref type="bibr" target="#b21">(Lin, 2004</ref>) toolkit. We compare our system's results with the results of state-of-the-art neural summarization approaches reported in recent papers, including both abstractive models and extractive models. The extractive models include SummaRuNNer ( <ref type="bibr">Nallap- ati et al., 2017)</ref> and SummaRuNNer-abs which is similar to SummaRuNNer but is trained directly on the abstractive summaries. The abstractive models include: 1) Seq2seq-baseline, which uses the basic seq2seq encoder-decoder architecture with attention mechanism, and incorporates with copy mechanism ( <ref type="bibr" target="#b28">See et al., 2017</ref>) to alleviate the OOV problem. 2) ABS-temp-attn ( , which uses Temporal Attention on the seq2seq architecture to overcome the repetition problem. 3) Point-cov ( <ref type="bibr" target="#b28">See et al., 2017)</ref>, which is an extension of the Seq2seq-baseline model by importing word-coverage mechanism to reduce repetitions in summary. 4) Graph-attention ( <ref type="bibr" target="#b30">Tan et al., 2017a</ref>), which length Method Rouge-1 Rouge-2 R.-L &lt; 100</p><p>Our M.  uses a graph-ranking based attention mechanism based on a hierarchical architecture to identify important sentences. 5) Hierachical-baseline, which just uses the basic hierarchical encoder-decoder with hybrid attention model proposed in this paper.</p><p>Results in <ref type="table" target="#tab_3">Table 2</ref> show that our model significantly outperforms all the neural abstractive baselines and extractive baselines. An interesting observation is that the performance of the Hierarchical-baseline model are lower than the Seq2seq-baseline model, which demonstrates the difficulty for a traditional model to identify the structural properties of document summarization process. Our model outperforms the Hierarchical-baseline model by more than 4 ROUGE points, which demonstrates that the structural regularization improves the document summarization performance significantly.</p><p>To verify the superiority of our model on generating long summaries, we also compare our method with the best seq2seq model <ref type="bibr">Point-cov (See et al., 2017)</ref> by evaluating them on a test set w.r.t. different length of reference summaries. The results are shown in <ref type="table" target="#tab_5">Table 3</ref>, which demonstrate that our model is better at generating long summary than the seq2seq model. As the summary becomes longer, our system will obtain larger advantages over the baseline (from +0.22 Rouge-1, +0.08 Rouge-2 and +0.39 Rouge-L for summary less than 100 words, rising to +5.00 Rouge-1, +0.54 Rouge-2 and +4.88 Rouge-L for summaries more than 150 words).</p><p>Human Evaluation. In addition to the ROUGE evaluation, we also conducted human evaluation on 50 random samples from CNN/DailyMail test set and compared the summaries generated by our method with the outputs of <ref type="bibr">Seq2seq-baseline and Point-cov (See et al., 2017)</ref>. Three data annotators were asked to compare the generated summaries   with the human summaries, and assess each summary from four independent perspectives: (1) Informative: How informative the summary is? <ref type="formula" target="#formula_9">(2)</ref> Concise: How concise the summary is? (3) Coherent: How coherent (between sentences) the summary is? (4) Fluent: How fluent, grammatical the sentences of a summary are? Each property is assessed with a score from 1(worst) to 5(best). The average results are presented in <ref type="table" target="#tab_7">Table 4</ref>.</p><p>The results show that our model consistently outperforms the Seq2seq-baseline model and the previous state-of-the-art method Point-cov. As shown in <ref type="table">Table 1</ref>, the summary generated by Seq2Seq-Baseline usually contains repetition of sentences or phrases, which seriously affects its informativeness, conciseness as well as coherence.</p><p>The Point-cov model effectively alleviates the information repetition problem, however, it usually loses some salient information and mainly copies original sentences directly from the input document. The summaries generated by our method obviously contains more salient information and are more concise through sentences compression, which shows the effectiveness of the structural regularization in our model. The results also show that the sentence-level modeling of document and summary in our model makes the generated summaries achieve better inter-sentence coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Validation</head><p>To verify the effectiveness of each component in our model, we conduct several ablation experiments. Based on the Hierarchical-baseline model, several different structural regularizations are added one by one: +strCom indi- cates adding structural-compression regularization during model learning, +strCov indicates adding structural-coverage regularization during model learning, +hierD indicates using the hierarchical decoding algorithm with both structuralcompression and structural-coverage regularizations during inference. Results on the test set are shown in <ref type="table" target="#tab_8">Table 5</ref>. Our method much outperforms all the compared systems, which verifies the effectiveness of each component of our model. Note that, both the structural-compression and structural-coverage regularization significantly affect the summarization performance. The higher structuralcompression and structural-coverage scores will lead to higher ROUGE scores. Therefore, we can conclude that the structural-compression and structural-coverage regularization based on our hierarchical model have significant contributions to the increase of ROUGE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Structural Properties Analysis</head><p>We further compare the ability of different models in capturing the structural-compression and structural-coverage properties of document summarization. <ref type="figure" target="#fig_1">Figure 3</ref> shows the comparison results of 4000 document-summary pairs with 14771 reference-summary sentences sampled from CNN/Daily Mail dataset. <ref type="figure" target="#fig_1">Figure 3(a)</ref> shows that most samples (over 95%) fall into the righttop area in human-made summaries, which indicates high structural-compression and structural- coverage scores. However, <ref type="figure" target="#fig_1">Figure 3 (b)</ref> and <ref type="formula">(c)</ref> show that in both the Seq2seq-baseline model and the Hierarchical-baseline model, most samples fall into the left-bottom area (low structuralcompression and structural-coverage), and only about 13% and 7% samples fall into the righttop area, respectively. <ref type="figure" target="#fig_1">Figure 3 (d)</ref> shows that our system with structural regularization achieves similar behaviors to human-made summaries (over 80% samples fall into the right-top area). The results demonstrate that the structural-compression and structural-coverage properties are common in document summarization, but both the seq2seq models and the basic hierarchical encoder-decoder models are not yet able to capture them properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of Structural Regularization</head><p>The structural regularization based on our hierarchical encoder-decoder with hybrid attention model improves the quality of summaries from two aspects: (1) The summary covers more salient information and contains very few repetitions, which can be seen both qualitatively <ref type="table" target="#tab_0">(Table 1</ref> and <ref type="figure" target="#fig_0">Figure 1</ref>) and quantitatively <ref type="table" target="#tab_0">(Table 5 and</ref>  <ref type="figure" target="#fig_2">Figure  4)</ref>. <ref type="formula" target="#formula_9">(2)</ref> The model has the ability to shorten a long sentence to generate a more concise one or compress several different sentences to generate a more informative one by merging the information from them. <ref type="table">Table 6</ref> shows several examples of abstractive summaries produced by sentence compression in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Recently some work explored the seq2seq models on document summarization, which exhibit some undesirable behaviors, such as inaccurately reproducing factual details, OOVs and repetitions. To alleviate these issues, copying mechanism ( <ref type="bibr" target="#b13">Gu et al., 2016;</ref>) has been incorporated into the encoderdecoder architecture to help generate information correctly. Distraction-based attention model Original Text: luke lazarus , a 23-year-old former private school boy , was jailed for at least three years on march 27 for raping an 18-year-old virgin in an alleyway outside his father 's soho nightclub in kings cross , inner sydney in may 2013 .(...) Summary: luke lazarus was jailed for at least three years on march 27 for raping an 18-year-old virgin in an alleyway outside his father 's soho nightclub in may 2013 . Original Text: (...) amy wilkinson , 28 , claimed housing benefit and council tax benefit even though she was living in a home owned by her mother and her partner , who was also working .wilkinson , who was a british airways cabin crew attendant , was ordered to pay back a total of 17,604 that she claimed over two years when she appeared at south and east cheshire magistrates court last week . (...) Summary: amy wilkinson , 28 , claimed housing benefit and council tax benefit even though she was living in a home owned by her mother and her partner . she was ordered to pay back a total of 17,604 that she claimed over two years when she appeared at south and east cheshire magistrates court last week . Original Text: (...) a grand jury charged durst with possession of a firearm by a felon , and possession of both a firearm and an illegal drug : 5 ounces of marijuana , said assistant district attorney chris bowman , spokesman for the district attorney . millionaire real estate heir robert durst was indicted wednesday on the two weapons charges that have kept him in new orleans even though his lawyers say he wants to go to los angeles as soon as possible to face a murder charge there . his arrest related to those charges has kept durst from being extradited to los angeles , where he 's charged in the december 2000 death of longtime friend susan berman .(...) Summary: durst entered his plea during an arraignment in a new orleans court on weapons charges that accused him of possessing both a firearm and an illegal drug , marijuana . the weapons arrest has kept durst in new orleans even though he is charged in the december 2000 death of a longtime friend . <ref type="table">Table 6</ref>: Examples of sentences compression or fusion by our model. The link-through denotes deleting the non-salient part of the original text. The italic denotes novel words or sentences generated by sentences fusion or compression.</p><p>( <ref type="bibr" target="#b6">Chen et al., 2016</ref>) and word-level coverage mechanism ( <ref type="bibr" target="#b28">See et al., 2017</ref>) have also been investigated to alleviate the repetition problem. Reinforcement learning has also been studied to improve the document summarization performance from global sequence level ( <ref type="bibr" target="#b26">Paulus et al., 2017</ref>).</p><p>Hierarchical Encoder-Decoder architecture is first proposed by  to train an auto-encoder to reconstruct multi-sentence paragraphs. In summarization field, hierarchical encoder has first been used to alleviate the long dependency problem for long inputs <ref type="bibr">(Cheng and La- pata, 2016;</ref>. <ref type="bibr" target="#b31">Tan et al. (2017b)</ref> also propose to use a hierarchical encoder to encode multiple summaries produced by several extractive summarization methods, and then decode them into a headline. However, these models don't model the decoding process hierarchically. <ref type="bibr" target="#b30">Tan et al. (2017a)</ref> first use the hierarchical encoder-decoder architecture on generating multisentences summaries. They mainly focus on incorporating sentence ranking into abstractive document summarization to help detect important sentences. Different from that, our work mainly tends to verify the necessity of leveraging document structure in document summarization and studies how to properly capture the structural properties of document summarization based on the hierarchical architecture to improve the performance of document summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we analyze and verify the necessity of leveraging document structure in document summarization, and explore the effectiveness of capturing structural properties of document summarization by importing both structuralcompression and structural-coverage regularization based on the proposed hierarchical encoderdecoder with hybrid attention model. Experimental results demonstrate that the structural regularization enables our model to generate more informative and concise summaries by enhancing sentences compression and coverage. Our model achieves considerable improvement over state-ofthe-art seq2seq-based abstractive methods, especially on long document with long summary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of sentence-level attention distributions for the summaries in Table 1 on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and the Point-gen-cov model, lose much salient information of the input document and focus on the same set of sentences repeatedly. The Hierarchical-baseline model fails to detect several specific sentences that are salient and relevant for each summary sentence and focuses on the same set of sentences repeatedly. On the contrary, our method with structural regularizations focuses on different sets of source sentences when generating different summary sentences and discovers more salient information from the document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparisons of structural-compression and structural-coverage analysis results on random samples from CNN/Daily Mail datasets, which demonstrate that both the Seq2seq-baseline model and the Hierarchical-baseline model are not yet able to capture them properly, but our model with structural regularizations achieves similar behavior with the gold reference summary.</figDesc><graphic url="image-10.png" coords="7,340.03,142.94,75.12,75.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The structural regularization reduces undesirable repetitions while summaries from the Seq2seq-baseline and the Hierarchical-baseline contains many n-gram repetitions.</figDesc><graphic url="image-13.png" coords="8,93.83,62.81,174.60,65.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>( a) Gold Reference I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 O 1 O 2 O 3 O 4 O 5 (b) Seq2Seq-baseline O 1 O 2 O 3 (d) Hierarchical-baseline O 1 O 2 O 3 O 4 (c) Point-gen-cov O 1 O 2 (e) Our Method O 1 O 2 O 3 I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20</head><label>(</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Rouge F1 scores on the test set. All our ROUGE</head><label>2</label><figDesc></figDesc><table>scores have a 95% confidence interval of at most ±0.25 
as reported by the official ROUGE script. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison results w.r.t different length of refer-

ence summary. &lt; 100 indicates the reference summary has 
less than 100 words (occupy 94.47% of test set). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 : Human evaluation results. * indicates the difference between Our Model and other models are statistic significant (p &lt; 0.05) by two-tailed t-test.</head><label>4</label><figDesc></figDesc><table>Method 
R-1 
R-2 
R-L 
strCom strCov 
Hierarchical-b. 34.95 14.79 32.68 0.22 
0.31 
+strCom 
37.03 16.21 34.44 0.64 
0.71 
+strCov 
39.52 17.12 36.44 0.65 
0.87 
+hierD 
40.30 18.02 37.36 0.68 
0.93 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results of adding different components of our 

method in terms of ROUGE-1, ROUGE-2, ROUGE-L, str-
Com (Equation 1) and strCov (Equation 2) scores. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by National Key Research and Development Program of China under grants 2016YFB1000902 and 2017YFC0820404, and National Natural Science Foundation of China under grants 61572469, 91646120, 61772501 and 61572473. We thank the anonymous reviewers for their helpful comments about this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentence fusion for multidocument news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="328" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04434</idno>
		<title level="m">Faithful to the original: Fact aware neural abstractive summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards abstractive multi-document summarization using submodular function-based framework, sentence compression and merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Tanvee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir Tafseer</forename><surname>Nayeem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="418" to="424" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05436</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distraction-based neural networks for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08462</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07252</idno>
		<title level="m">Neural summarization by extracting sentences and words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised sentence enhancement for automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><forename type="middle">Chi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="775" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning-based single-document summarization with compression and anaphoricity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08887</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06393</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08148</idno>
		<title level="m">Pointing the unknown words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03872</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on</title>
		<meeting>the 2013 Conference on</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="490" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01057</idno>
		<title level="m">A hierarchical neural autoencoder for paragraphs and documents</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hierarchical recurrent neural network for document modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04368</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural headline generation on abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graphbased attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From neural sentence summarization to headline generation: A coarse-to-fine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07073</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
