<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
							<email>lige@sei.pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
							<email>chenyunchuan11@mails.ucas.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
							<email>zhijin@sei.pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">Institute of Software</orgName>
								<orgName type="laboratory">Key Laboratory of High Confidence Software Technologies (Peking University)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task 8, and achieve an F 1-score of 86.1%, outperforming previous state-of-the-art recorded results. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classifying relations between two entities in a given context is an important task in natural language processing (NLP). Take the following sentence as an example: "Jewelry and other smaller <ref type="bibr">[valuables]</ref> e 1 were locked in a <ref type="bibr">[safe]</ref> e 2 or a closet with a deadbolt." The marked entities valuables and safe are of relation Content-Container(e 1 , e 2 ). Relation classification plays a key role in various NLP applications, and has become a hot research topic in recent years.</p><p>Nowadays, neural network-based approaches have made significant improvement in relation classification, compared with traditional methods based on either human-designed features <ref type="bibr" target="#b9">(Kambhatla, 2004;</ref><ref type="bibr" target="#b6">Hendrickx et al., 2009</ref>) or kernels ( <ref type="bibr" target="#b0">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b13">Plank and Moschitti, 2013)</ref>. For example, <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> and <ref type="bibr" target="#b16">Xu et al. (2015a)</ref> utilize convolutional neural networks (CNNs) for relation classification. <ref type="bibr" target="#b17">Xu et al. (2015b)</ref> apply long short term memory (LSTM)-based recurrent neural networks (RNNs) along the shortest dependency path. <ref type="bibr" target="#b12">Nguyen and Grishman (2015)</ref> build ensembles of gated recurrent unit (GRU)-based RNNs and CNNs.</p><p>We have noticed that these neural models are typically designed in shallow architectures, e.g., one layer of CNN or RNN, whereas evidence in the deep learning community suggests that deep architectures are more capable of information integration and abstraction ( <ref type="bibr" target="#b4">Graves et al., 2013;</ref><ref type="bibr">Hermans and Schrauwen, 2013;</ref><ref type="bibr" target="#b8">Irsoy and Cardie, 2014</ref>). A natural question is then whether such deep architectures are beneficial to the relation classification task.</p><p>In this paper, we propose the deep recurrent neural networks (DRNNs) to classify relations. The deep RNNs can explore the representation space in different levels of abstraction and granularity. By visualizing how RNN units are related to the ultimate classification, we demonstrate that different layers indeed learn different representations: low-level layers enable sufficient information mix, while highlevel layers are more capable of precisely locating the information relevant to the target relation between * Equal contribution. <ref type="bibr">†</ref>   <ref type="bibr">[valuables]</ref> [safe] two entities. Following our previous work ( <ref type="bibr" target="#b17">Xu et al., 2015b</ref>), we leverage the shortest dependency path (SDP, <ref type="figure" target="#fig_0">Figure 1</ref>) as the backbone of our RNNs.</p><formula xml:id="formula_0">e 1 e 2 e 1 e 2 (a) (b)</formula><p>We further observe that the relationship between two entities are directed. Two sub-paths, separated by entities' common ancestor, can be mapped to subject-predicate and object-predicate components of a relation. By changing the order of these two sub-paths, we obtain a new data sample with the inversed relationship ( <ref type="figure" target="#fig_0">Figure 1b)</ref>. Such data augmentation technique can provide additional data samples without using external data resources.</p><p>We evaluated our proposed method on the SemEval-2010 relation classification task. Even if we do not apply data augmentation, the DRNNs model has achieved a high performance of 84.2% F 1 -score with a depth of 3, but the performance decreases when the depth is too large. This is because the deep RNN is a large model, which necessitates more data samples for training. Applying data augmentation can alleviate the problem of data sparseness and sustain a deeper RNN to improve the performance to 86.1%. The results show that both our deep networks and the data augmentation strategy have contributed to the relation classification task, and that they are coupled well together for further performance improvement.</p><p>The rest of this paper is organized as follows. Section 2 reviews related work; Section 3 describes our DRNNs model in detail. Section 4 presents in-depth experimental results. Finally, we have conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional methods for relation classification mainly fall into two groups: feature-based or kernel-based. The former approaches extract different types of features and feed them into a classifier, e.g., a maximum entropy model <ref type="bibr" target="#b9">(Kambhatla, 2004</ref>). Various features, including lexical, syntactic, as well as semantic ones, are shown to be useful to relation classification ( <ref type="bibr" target="#b6">Hendrickx et al., 2009)</ref>. By contrast, kernel-based methods do not have explicit feature representations, but require predefined similarity measure of two data samples. <ref type="bibr" target="#b0">Bunescu and Mooney (2005)</ref> design a kernel along the shortest dependency path (SDP) between two entities by observing that the relation strongly relies on SDPs. <ref type="bibr" target="#b13">Plank and Moschitti (2013)</ref> combine structural information and semantic information in a tree kernel.</p><p>Neural networks have now become a prevailing technique in this task. Socher et al. (2011) design a recursive neural network along the constituency parse tree. <ref type="bibr" target="#b5">Hashimoto et al. (2013)</ref>, also on the basis of recursive networks, emphasize more on important phrases; <ref type="bibr" target="#b3">Ebrahimi and Dou (2015)</ref> restrict recursive networks to SDP. In our previous study ( <ref type="bibr" target="#b17">Xu et al., 2015b</ref>), we introduce SDP-based recurrent neural network to classify relations. <ref type="bibr" target="#b19">Zeng et al. (2014)</ref>, on the other hand, apply CNNs to relation classification. Along this line, dos <ref type="bibr" target="#b2">Santos et al. (2015)</ref> replace the common softmax loss function with a ranking loss in their CNN model. <ref type="bibr" target="#b16">Xu et al. (2015a)</ref> design a negative sampling method for SDP-based CNNs.</p><p>Besides, representative hybrid models of CNNs and recursive/recurrent networks include <ref type="bibr" target="#b10">Liu et al. (2015)</ref> and <ref type="bibr" target="#b12">Nguyen and Grishman (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Methodology</head><p>In this section, we describe our methodology in detail. Subsection 3.1 provides an overall picture of our DRNNs model. Subsections 3.2 and 3.3 describe deep recurrent neural networks. The proposed data augmentation technique is introduced in Subsection 3.4. Finally, we present our training objective in Subsection 3.5. <ref type="figure">Figure 2</ref> depicts the overall architecture of the DRNNs model. Given a sentence and its dependency parse tree, 1 we follow our previous work ( <ref type="bibr" target="#b17">Xu et al., 2015b</ref>) and build DRNNs on the shortest dependency path (SDP), which serves as a backbone. In particular, an RNN picks up information along each sub-path, separated by the common ancestor of marked entities. Also, we take advantage of four information channels, namely, word embeddings, POS embeddings, grammatical relation embeddings, and WordNet embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Different from <ref type="bibr" target="#b17">Xu et al. (2015b)</ref>, we design deep RNNs with up to four hidden layers so as to capture information in different levels of abstraction. For each RNN layer, max pooling gathers information from different recurrent nodes. Notice that the four channels (with eight sub-paths) are processed in a similar way. Then all pooling layers are concatenated and fed into a hidden layer for information integration. Finally, we have a softmax output layer for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recurrent Neural Networks on Shortest Dependency Path</head><p>In this subsection, we introduce a single layer of RNN based on SDP, serving as a building block of our deep architecture.</p><p>Compared with a raw word sequence or a whole parse tree, the shortest dependency path (SDP) between two entities has two main advantages. First, it reduces irrelevant information; second, grammatical relations between words focus on the action and agents in a sentence and are naturally suitable for relation classification. Existing studies have demonstrated the effectiveness of SDP <ref type="figure">(</ref> Focused on the SDP, an RNN keeps a hidden state vector h, changing with the input word at each step accordingly. Concretely, the hidden state h t , for the t-th word in the sub-path, depends on its previous state h t−1 and the current word's embedding x t . For the simplicity and without loss of generality, we use vanilla recurrent networks with perceptron-like interaction, that is, the input is linearly transformed by a weight matrix and non-linearly squashed by an activation function, i.e.,</p><formula xml:id="formula_1">h t = f (W in x t + W rec h t−1 + b h ) (1)</formula><p>where W in and W rec are weight matrices for the input and recurrent connections, respectively. b h is a bias term, and f is a non-linear activation function (ReLU in our experiment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep Recurrent Neural Networks</head><p>Although an RNN, as described above, is suitable for picking information along a sequence (a subpath in our task) by its iterative nature, the machine learning community suggests that deep architectures may be more capable of information integration, and can capture different levels of abstraction. A single-layer RNN can be viewed that it is deep along time steps. When unfolded, however, the RNN has only one hidden layer to capture the current input, as well as to retain the information in its previous step. In this sense, single-layer RNNs are actually shallow in information processing <ref type="bibr">(Hermans and Schrauwen, 2013;</ref><ref type="bibr" target="#b8">Irsoy and Cardie, 2014</ref>). ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word/GR/POS/WordNet embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hidden layer</head><p>Figure 2: The overall architecture of DRNNs. Two recurrent neural networks pick up information along the shortest dependency path, separated by its common ancestor. We use four information channels, namely words, part-of-speech tags, grammatical relations (GR), and WordNet hypernyms.</p><p>In the relation classification task, words along SDPs provide information from different perspectives. On the one hand, the marked entities themselves are informative. On the other hand, the entities' common ancestor (typically verbs) tells how the two entities are related to each other. Such heterogeneous information might necessitate more complex machinery than a single RNN layer.</p><p>Following such intuition, we investigate deep RNNs by stacking multiple hidden layers on the top of one another, that is, every layer treats its previous layer as input, and computes its activation similar to Equation 1. Formally, we have</p><formula xml:id="formula_2">h (i) t = f (W (i−1) in h (i−1) t + W (i) rec h (i) t−1 + W (i−1) cross h (i−1) t−1 + b (i) )<label>(2)</label></formula><p>where the subscripts refer to time steps, and superscripts indicate the layer number. To enhance information propagation, we add a "cross" connection for hidden layers (i ≥ 2) from the lower layer in the previous time step, given by W</p><formula xml:id="formula_3">(i−1) cross h (i−1) t−1 in Equation 2.</formula><p>(See also and arrows in <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Augmentation</head><p>Neural networks, especially deep ones, are likely to be prone to overfitting. The SemEval-2010 relation classification dataset, we use, comprises only several thousand samples, which may not fully sustain the training of deep RNNs.</p><p>To mitigate this problem, we propose a data augmentation technique for relation classification by making use of the directionality of relationships.</p><p>The two sub-paths <ref type="figure" target="#fig_0">Figure 1</ref>, for example, can be mapped to the subject-predicate and object-predicate components in the relation Content-Container(e 1 , e 2 ). If we change the order of these two subpaths, we obtain</p><formula xml:id="formula_4">[valuables] e 1 → jewelry → locked locked ← in ← closet ← [safe] e 2 in</formula><formula xml:id="formula_5">[safe] e 1 → closet → in → locked locked ← jewelry ← [valuables] e 2</formula><p>Then the relationship becomes Container-Content(e 1 , e 2 ), which is exactly the inverse of Content-Container(e 1 , e 2 ). In this way, we can augment the dataset without using additional resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Objective</head><p>For each recurrent layer and embedding layer (over each sub-path for each channel), we apply a max pooling layer to gather information. In total, we have 40 pools, which are concatenated and fed to a hidden layer for information integration.</p><p>Finally, a softmax layer outputs the estimated probability that two sub-paths (s left and s right ) are of relation r. For a single data sample i, we apply the standard cross-entropy loss, denoted as J(s left i , s right i</p><p>, r i ). With the data augmentation technique, our overall training objective is</p><formula xml:id="formula_6">J = m i=1 J(s left i , s right i , r i ) + J(s right i , s left i , r −1 i ) + λ ω i=1 W i F</formula><p>where r −1 refers to the inverse of relation r. m is the number of data samples in the original training set. ω is the number of weight matrices in DRNNs. λ is a regularization coefficient, and · F denotes Frobenius norm of a matrix. For decoding (predicting the relation of an unseen sample), the data augmentation technique provides new opportunities, because we can use the probability of r(e 1 , e 2 ), r −1 (e 2 , e 1 ), or both. Section 4.3 provides detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our experiments in detail. Subsection 4.1 introduces the dataset; Subsection 4.2 describes hyperparameter settings. We discuss the details of data augmentation in Subsection 4.3 and the rationale for using RNNs in Subsection 4.4. Subsection 4.5 compares our DRNNs model with other methods in the literature. In Subsection 4.6, we have quantitative and qualitative analysis of how the depth affects our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluated our DRNNs model on the SemEval-2010 Task 8 dataset, which is an established benchmark for relation classification <ref type="bibr" target="#b6">(Hendrickx et al., 2009</ref>). The dataset contains 8000 sentences for training, and 2717 for testing. We split 800 samples out of the training set for validation.</p><p>There are 9 directed relations and an undirected default relation Other; thus, we have 19 different labels in total. However, the Other class is not taken into consideration when we compute the official measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameter Settings</head><p>This subsection presents hyperparameters of our proposed model. We basically followed the settings in our previous work ( <ref type="bibr" target="#b17">Xu et al., 2015b</ref>). Word embeddings were 200-dimensional, pretrained ourselves using word2vec ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>) on the Wikipedia corpus; embeddings in other channels were 50-dimensional initialized randomly. The hidden layers in each channel had the same number of units as their embeddings (either 200 or 50); the penultimate hidden layer was 100-dimensional. An 2 penalty of 10 −5 was also applied as in <ref type="bibr" target="#b17">Xu et al. (2015b)</ref>, but we chose the dropout rate by validation with a granularity of 5% for our model variants (with different depths).</p><p>We also chose the depth of DRNNs by validation from the set {1, 2, · · · , 6}. The 3-layer and 4-layer DRNNs yield the highest performance with and without data augmentation, respectively. Section 4.6 provides both quantitative and qualitative analysis regarding the effect of depth.</p><p>We applied mini-batched stochastic gradient descent for optimization, where gradients were computed by standard back-propagation.   <ref type="table">Table 2</ref>: Comparing CNNs and RNNs (also using F 1 -score as the measurement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variant of Data augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Augmentation Details</head><p>As mentioned in Section 4.1, the SemEval-2010 Task 8 dataset contains an undirected class Other in addition to 9 directed relations (18 classes). For data augmentation, it is natural that the inversed</p><p>Other relation is also in the Other class itself. However, if we augment all the relations, we observe a performance degradation of 0.7% <ref type="table">(Table 1)</ref>. We deem the Other class contains mainly noise, and is inimical to our model. Then we conducted another experiment where we only augmented the Other class. The result verifies our conjecture as we obtained an even larger degradation of 1.1% in this setting.</p><p>The pilot experiments suggest that we should take into consideration unfavorable noise when performing data augmentation. In this experiment, if we reverse the directed relations only and leave the Other class intact, the performance is improved by a large margin of 1.9%. This shows that our proposed data augmentation technique does help to mitigate the problem of data sparseness, if we carefully rule out the impact of noise.</p><p>During validation and testing, we shall decode the target label of an unseen data sample (with two entities e 1 and e 2 ). Through data augmentation, we are equipped with the probability of r −1 (e 2 , e 1 ) in addition to r(e 1 , e 2 ). In our experiment, we tried several settings and chose to use r −1 (e 2 , e 1 ) only, because it yields the highest the validation result. We think this is probably because the Other class brings more noise to r than r −1 , as the Other class is not augmented (and hence asymmetric).</p><p>We would like to point out that our data augmentation method is a general technique for relation classification, which is not ad hoc to a specific dataset; that the methodology for dealing with noise is also potentially applicable to other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RNNs vs. CNNs</head><p>As both RNNs and CNNs are prevailing neural models for NLP, we are curious whether deep architectures are also beneficial to CNNs. We tried a CNN with a sliding window of size 3 based on SDPs, similar to <ref type="bibr" target="#b16">Xu et al. (2015a)</ref>; other settings were as our DRNNs.</p><p>The results are shown in <ref type="table">Table 2</ref>. We observe that a single layer of CNN is also effective, yielding an F 1 -score slightly worse than our RNN. But the deep architecture hurts the performance of CNNs in this task. One plausible explanation is that, when convolution is performed, the beginning and end of a sentence are typically padded with a special symbol or simply zero. However, the shortest dependency path between two entities is usually not very long (∼4 on average). Hence, sentence boundaries may play a large role in convolution, which makes CNNs vulnerable.</p><p>On the contrary, RNNs can deal with sentence boundaries smoothly, and the performance continues to increase with up to 4 hidden layers. (Details are deferred to Subsection 4.6.) <ref type="table" target="#tab_3">Table 3</ref> compares our DRNNs model with previous state-of-the-art methods. <ref type="bibr">2</ref> The first entry in the table presents the highest performance achieved by traditional feature-based methods. <ref type="bibr" target="#b6">Hendrickx et al. (2009)</ref> feed a variety of handcrafted features to the SVM classifier and achieve an F 1 -score of 82.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Overall Performance</head><p>Recent performance improvements on this dataset are mostly achieved with the help of neural networks. In an early study, <ref type="bibr" target="#b15">Socher et al. (2012)</ref> build a recursive network on constituency trees, but   achieve a performance worse than <ref type="bibr" target="#b6">Hendrickx et al. (2009)</ref>. They extend their recursive network with matrix-vector interaction and elevate the F 1 -score to 82.4%. <ref type="bibr" target="#b3">Ebrahimi and Dou (2015)</ref> restrict the recursive network to SDP, which is slightly better than a sentence-wide network. In our previous study ( <ref type="bibr" target="#b17">Xu et al., 2015b</ref>), we introduce recurrent neural networks based on SDP and improve the F 1 -score to 83.7%.</p><p>In the school of convolution, <ref type="bibr" target="#b19">Zeng et al. (2014)</ref> construct a CNN on the word sequence; they also integrate word position embeddings, which benefit the CNN architecture. dos <ref type="bibr" target="#b2">Santos et al. (2015)</ref> propose a similar CNN model, named CR-CNN, by replacing the common softmax cost function with a ranking-based cost function. By diminishing the impact of the Other class, they achieve an F 1 -score of 84.1%. <ref type="bibr" target="#b16">Xu et al. (2015a)</ref> design an SDP-based CNN with negative sampling, improving the performance to 85.6%.</p><p>Hybrid models of CNNs and RNNs do not appear to be very useful, achieving up to an F 1 -score of 84.1% ( <ref type="bibr" target="#b10">Liu et al., 2015;</ref><ref type="bibr" target="#b12">Nguyen and Grishman, 2015)</ref>. <ref type="bibr" target="#b18">Yu et al. (2014)</ref> propose a Feature-rich Compositional Embedding Model (FCM), which combines unlexicalized linguistic contexts and word embeddings. They do not use neural networks (at least in the usual sense) and achieve an F 1 -score of 83.0%.</p><p>Our DRNNs model, along with data augmentation, achieves an F 1 -score of 86.1%. Even if we do not apply data augmentation, the DRNNs model yields 84.2% F 1 -score, which is also the highest score achieved without special treatment to the noisy Other class. The above results show the effectiveness of DRNNs, especially trained with a large (augmented) dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of DRNNs' Depth</head><p>In this subsection, we analyze the effect of depth in our DRNNs model. We have tested the depth from the set {1, 2, · · · , 6}, and plot the results in <ref type="figure" target="#fig_5">Figure 3</ref>. Initially, the performance increases if the depth is larger in both settings with and without augmentation. However, if we do not augment data, the performance peaks when the depth is 3. Provided with augmented training samples, the F 1 -score continues to increase with up to 4 layers, and ends up with an F 1 -score of 86.1%.</p><p>We next investigate how RNN units in different layers are related to the ultimate task of interest. This is accomplished by tracing back information from pooling layers. Noticing that the pooling layer takes maximum value in each dimension, we can compute how much a hidden layer's units are gathered by pooling for further processing. In this way, we are able to demonstrate the information flow in RNN hidden units. We plot three examples in <ref type="figure">Figure 4</ref>. Here, rectangles refer to RNN hidden layers, unfolded along time. (Rounded rectangles are word embeddings.) The intensity of color reflects the ratio of the pooling proportion.</p><p>• Sample 1: "Until 1864 <ref type="bibr">[vessels]</ref> e 1 in the service of certain UK public offices defaced the Red Ensign with the [badge] e 2 of their office" with label Instrument-Agency(e 2 , e 1 ). Its two sub-paths of SDP are</p><formula xml:id="formula_7">[vessels] e 1 → until → defaced defaced ← with ← [badge] e 2</formula><p>From <ref type="figure">Figure 4a</ref>, we see that entities like vessels and badge are darker than the verb phrase defaced with on the embedding layer. When information is propagating horizontally and vertically, these entities are getting lighter, while the verb phrase becomes darker gradually. Intuitively, we think that, considering the relation Instrument-Agency(e 2 , e 1 ), it is less informative with only two entities vessels and badge. When adding the semantic of verb phrase defaced with, we are more aware of the target relation. 3</p><p>• Sample 2: "Most of the [verses] e 1 of the plantation songs had some reference to [freedom] e 2 " with label Message-Topic(e 1 , e 2 ). Its two sub-paths of SDP are</p><formula xml:id="formula_8">[verses] e 1 → of → most → had had ← reference ← to ← [freedom] e 2</formula><p>Similar to Sample 1, we see from <ref type="figure">Figure 4b</ref> that the color of the "pivot" verb had is getting darker vertically, and becomes the darkest in the fourth RNN layer, indicating the highest pooling portion. This is probably because had links two ends of the relation, Message and Topic.</p><p>•  <ref type="table" target="#tab_3">motifs   of  use  evident  evident  in  ewer   20C 22C 24C 26C  30C  28C   verses  of  most  had  had  reference  to  freedom  vessels  until  defaced  defaced  with   30C 32C 34C 36C  40C  38C   badge   10C 18C 26C 34C  50C  42C</ref> MaTIIInstrument-AgencyMe2,Ie1T</p><p>McTIIComponent-WholeMe1,Ie2T MbTIIMessage-TopicMe1,Ie2T</p><p>Figure 4: Visualization of information propagation along multiple RNN layers.</p><p>We summarize our findings as follows. <ref type="formula">(1)</ref> Pooled information usually peaks at one or a few words in the embedding layer. This makes sense because there is no information flow in this layer. (2) Information scatters over a wider range in hidden layers, showing that the recurrent propagation does mix information. (3) For a higher-level layer, the network pays more attention to those words that are more relevant to the relation, but whether entities or their common ancestor is more relevant is not consistent among different data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed deep recurrent neural networks, named DRNNs, to improve the performance of relation classification. The DRNNs model, consisting of several RNN layers, explores the representation space of different abstraction levels. By visualizing DRNNs' units, we demonstrated that high-level layers are more capable of integrating information relevant to target relations. In addition, we have designed a data augmentation strategy by leveraging the directionality of relations. When evaluated on the SemEval dataset, our DRNNs model results in substantial performance boost. The performance generally improves when the depth increases; with a depth of 4, our model reaches the highest F 1 -measure of 86.1%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The dependency parse tree corresponding to the sentence "Jewelry and other smaller [valuables] e 1 were locked in a [safe] e 2 or a closet with a deadbolt." Red arrows indicate the shortest dependency path between e 1 and e 2 . (b) The augmented data sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Ebrahimi and Dou, 2015; Liu et al., 2015; Xu et al., 2015b; Xu et al., 2015a); details are not repeated here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis of the depth. 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Sample 3: "A more spare, less robust use of classical [motifs] e 1 is evident in a [ewer] e 2 of 1784-85" with label Component-Whole(e 1 , e 2 ). Its two sub-paths of SDP are [motifs] e 1 → of → use → evident evident ← in ← [ewer] e 2 Different from Figures 4a and 4b, higher layers pay more attention to entities rather than their ancestor. In this example, motifs and ewer appear to be more relevant to the relation Component-Whole than their common ancestor evident. The pooling proportion of entities (mo- tifs, ewer) is increasing, while other words' proportion is decreasing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Comparison of previous relation classification systems.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Parsed by the Stanford parser (de Marneffe et al., 2006).</note>

			<note place="foot" n="2"> This paper was preprinted on arXiv on 14 Jan 2016.</note>

			<note place="foot" n="3"> Using vanilla RNN with a depth of 1, we obtained a slightly better accuracy in this paper than Xu et al. (2015b).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all reviewers for their constructive comments. This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201, the National Natural Science Foundation of China under Grant Nos. 61232015, 91318301, 61421091, and 61502014.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
	<note>Bunescu and Mooney2005</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Marneffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
	<note>Cıcero Nogueira dos Santos, Bing Xiang,</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chain based rnn for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1244" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple customization of recursive neural networks for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1372" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hendrickx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training and analysing deep recurrent neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
	<note>Irsoy and Cardie2014</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Interactive Poster and Demonstration Sessions</title>
		<meeting>the ACL Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dependencybased neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Combining neural networks and log-linear models to improve relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grishman2015] Thien Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05926</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
	<note>Plank and Moschitti2013</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
