<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Goodman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">University of Sheffield</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
							<email>a.vlachos@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">University of Sheffield</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">University of Sheffield</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1" to="11"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenomena , resolve anaphora, and identify word senses to eliminate ambiguous interpretations. Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a dependency parse, utilizes a graph to represent relationships between concepts (Ba-narescu et al., 2013). As with dependency parsing, transition-based approaches are a common approach to this problem. However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding. Imitation learning algorithms have been shown to help these systems recover from such errors. To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions: noise reduction and targeted exploration. The former mitigates the noise in the feature representation, a result of the complexity of the task. The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space. We achieve state-of-the art results, and improve upon standard transition-based parsing by 4.7 F 1 points.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Meaning representation languages and systems have been devised for specific domains, such as ATIS for air-travel bookings ( <ref type="bibr" target="#b8">Dahl et al., 1994)</ref> and database queries ( <ref type="bibr" target="#b34">Zelle and Mooney, 1996;</ref><ref type="bibr"></ref>  is that it is domain-independent and useful in a variety of applications ( <ref type="bibr" target="#b1">Banarescu et al., 2013</ref>). The first AMR parser by <ref type="bibr" target="#b10">Flanigan et al. (2014)</ref> used graph-based inference to find a highestscoring maximum spanning connected acyclic graph. Later work by <ref type="bibr" target="#b32">Wang et al. (2015b)</ref> was inspired by the similarity between the dependency parse of a sentence and its semantic AMR graph ( <ref type="figure" target="#fig_0">Figure 1</ref>). <ref type="bibr" target="#b32">Wang et al. (2015b)</ref> start from the dependency parse and learn a transition-based parser that converts it incrementally into an AMR graph using greedy decoding. An advantage of this approach is that the initial stage of dependency parsing is well-studied and trained using larger corpora than that for which AMR annotations exist.</p><p>Greedy decoding, where the parser builds the parse while maintaining only the best hypothesis at each step, has a well-documented disadvantage: error propagation ( <ref type="bibr" target="#b20">McDonald and Nivre, 2007)</ref>. When the parser encounters states during parsing that are unlike those found during training, it is more likely to make mistakes, leading to states which are increasingly more foreign and causing errors to accumulate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>One way to ameliorate this problem is to employ imitation learning algorithms for structured prediction. Algorithms such as SEARN <ref type="bibr" target="#b9">(Daumé III et al., 2009</ref>), DAGGER <ref type="bibr" target="#b26">(Ross et al., 2011)</ref>, and LOLS ( <ref type="bibr" target="#b3">Chang et al., 2015</ref>) address the problem of error propagation by iteratively adjusting the training data to increasingly expose the model to training instances it is likely to encounter during test. Such algorithms have been shown to improve performance in a variety of tasks including information extraction( <ref type="bibr" target="#b30">Vlachos and Craven, 2011)</ref>, dependency parsing ( <ref type="bibr" target="#b13">Goldberg and Nivre, 2013)</ref>, and feature selection ( <ref type="bibr" target="#b15">He et al., 2013)</ref>. In this work we build on the transition-based parsing approach of <ref type="bibr" target="#b32">Wang et al. (2015b)</ref> and explore the applicability of different imitation algorithms to AMR parsing, which has a more complex output space than those considered previously.</p><p>The complexity of AMR parsing affects transition-based methods that rely on features to represent structure, since these often cannot capture the information necessary to predict the correct transition according to the gold standard. In other words, the features defined are not sufficient to "explain" why different actions should preferred by the model. Such instances become noise during training, resulting in lower accuracy. To address this issue, we show that the α-bound <ref type="bibr" target="#b17">Khardon and Wachman (2007)</ref>, which drops consistently misclassified training instances, provides a simple and effective way of reducing noise and raising performance in perceptron-style classification training, and does so reliably across a range of parameter settings. This noise reduction is essential for imitation learning to gain traction in this task, and we gain 1.8 points of F 1 -Score using the DAGGER imitation learning algorithm.</p><p>DAGGER relies on an externally specified expert (oracle) to define the correct action in each state; this defines a simple 0-1 loss function for each action. Other imitation learning algorithms (such as LOLS, SEARN) and the variant of DAGGER proposed by <ref type="bibr" target="#b29">Vlachos and Clark (2014)</ref> (henceforth V-DAGGER) can leverage a task level loss function that does not decompose over the actions taken to construct the AMR graph. However these require extra computations to roll-out to an end-state AMR graph for each possible action not taken. The large action-space of our transition system makes these algorithms computationally infeasible, and roll-outs to an end-state for many of the possible actions will provide little additional information. Hence we modify the algorithms to target this exploration to actions where the classifier being trained is uncertain of the correct response, or disagrees with the expert. This provides a further gain of 2.7 F 1 points. This paper extends imitation learning to structured prediction tasks more complex than previously attempted. In the process, we review and compare recently proposed algorithms and show how their components can be recombined and adjusted to construct a variant appropriate to the task in hand. Hence we invest some effort reviewing these algorithms and their common elements.</p><p>Overall, we obtain a final F-Score of 0.70 on the newswire corpus of LDC2013E117 ( <ref type="bibr" target="#b18">Knight et al., 2014)</ref>. This is identical to the score obtained by <ref type="bibr" target="#b31">Wang et al. (2015a)</ref>, the highest so far published. Our gain of 4.5 F 1 points from imitation learning over standard transition-based parsing is orthogonal to that of <ref type="bibr" target="#b31">Wang et al. (2015a)</ref> from additional trained analysers, including co-reference and semantic role labellers, incorporated in the feature set. We further test on five other corpora of AMR graphs, including weblog domains, and show a consistent improvement in all cases with the application of imitation learning using DAGGER and the targeted V-DAGGER we propose here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transition-based AMR parsing</head><p>AMR parsing is an example of the wider family of structured prediction problems, in which we seek a mapping from an input x ∈ X to a structured output y ∈ Y. Here x is the dependency tree, and y the AMR graph; both are graphs and we notationally replace x with s 1 and y with s T , with s 1...T ∈ S. s i are the intermediate graph configurations (states) that the system transitions through.</p><p>A transition-based parser starts with an input s 1 , and selects an action a 1 ∈ A, using a classifier. a i converts s i into s i+1 , i.e. s i+1 = a i (s i ). We term the set of states and actions s 1 , a 1 , . . . a T −1 , s T a trajectory of length T . The classifierˆπclassifierˆ classifierˆπ is trained to predict a i from s i , withˆπwithˆ withˆπ(s) = arg max a∈A w a · Φ(s), assuming a linear classifier and a feature function Φ(s).</p><p>We require an expert, π * , that can indicate what actions should be taken on each s i to reach the target (gold) end state. In problems like POStagging these are directly inferable from gold, as the number of actions (T ) equals the number of Action Name Param. Pre-conditions Outcome of action NextEdge lr β non-empty Set label of edge (σ0, β0) to lr. Pop β0. NextNode lc β empty Set concept of node σ0 to lc. Pop σ0, and initialise β. Swap β non-empty Make β0 parent of σ0 (reverse edge) and its sub-graph. Pop β0 and insert β0 as σ1. ReplaceHead β non-empty Pop σ0 and delete it from the graph. Parents of σ0 become parents of β0. Other children of σ0 become children of β0. Insert β0 at the head of σ and re-initialise β. Reattach κ β non-empty Pop β0 and delete edge (σ0, β0). Attach β0 as a child of κ. If κ has already been popped from σ then re-insert it as σ1. DeleteNode β empty; leaf σ0 Pop σ0 and delete it from the graph. Insert lc Insert a new node δ with AMR concept lc as the parent of σ0, and insert δ into σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>InsertBelow</head><p>Insert a new node δ with AMR concept lc as a child of σ0. </p><formula xml:id="formula_0">← π(s current ) s current ← a next (s current ) 4 s T ← s current</formula><p>tokens with a 1:1 correspondence between them.</p><p>In dependency parsing and AMR parsing this is not straightforward and dedicated transition systems are devised.</p><p>Given a labeled training dataset D, algorithm 1 is first used to generate a trajectory for each of the inputs (d ∈ D) with π = π * , the expert from which we wish to generalise. The data produced from all expert trajectories (i.e. s i,d , a i,d for all i ∈ 1 . . . T and all d ∈ 1 . . . D), are used to train the classifierˆπclassifierˆ classifierˆπ, the learned classifier, using standard supervised learning techniques. Algorithm 1 is reused to applyˆπapplyˆ applyˆπ to unseen data. Our transition system (defining A, S), and feature sets are based on <ref type="bibr" target="#b32">Wang et al. (2015b)</ref>, and are not the main focus of this paper. We introduce the key concepts here, with more details in the supplemental material.</p><p>We initialise the state with the stack of the nodes in the dependency tree, root node at the bottom. This stack is termed σ. A second stack, β is initialised with all children of the top node in σ. The state at any time is described by σ, β, and the current graph (which starts as the dependency tree with one node per token). At any stage before termination some of the nodes will be labelled with words from the sentence, and others with AMR concepts. Each action manipulates the top nodes in each stack, σ 0 and β 0 . We reach a terminal state when σ is empty. The objective function to maximise is the Smatch score , which calculates an F 1 -Score between the predicted and gold-target AMR graphs. <ref type="table" target="#tab_0">Table 1</ref> summarises the actions in A. NextNode and NextEdge form the core action set, labelling nodes and edges respectively without changing the graph structure. Swap, Reattach and ReplaceHead change graph structure, keeping it a tree. We permit a Reattach action to use parameter κ equal to any node within six edges from σ 0 , excluding any that would disconnect the graph or create a cycle.</p><p>The Insert/InsertBelow actions insert a new node as a parent/child of σ 0 . These actions are not used in <ref type="bibr" target="#b32">Wang et al. (2015b)</ref>, but Insert is very similar to the Infer action of <ref type="bibr" target="#b31">Wang et al. (2015a)</ref>. We do not use the Reentrance action of <ref type="bibr" target="#b32">Wang et al. (2015b)</ref>, as we found it not to add any benefit. This means that the output AMR is always a tree.</p><p>Our transition system has two characteristics which provide a particular challenge: given a sentence, the trajectory length T is theoretically unbounded; and |A| can be of the order 10 3 to 10 4 . Commonly used transition-based systems have a fixed trajectory length T , which often arises naturally from the nature of the problem. In PoStagging each token requires a single action, and in syntactic parsing the total size of the graph is limited to the number of tokens in the input. The lack of a bound in T here is due to Insert actions that can grow the the graph, potentially ad infinitum, and actions like Reattach, which can move a sub-graph repeatedly back-and-forth. The action space size is due to the size of the AMR vocabulary, which for relations (edge-labels) is restricted to about 100 possible values, but for concepts (node-labels) is almost as broad as an En-</p><formula xml:id="formula_1">Algorithm 2: Generic Imitation Learning Data: data D, expert π * , Loss function F (s) Result: learned classifier C, trained policyˆπpolicyˆ policyˆπ 1 Initialise C 0 ; for n = 1 to N do 2 Initialise E n = φ; 3 π Rollin = RollInP olicy(π * , C 0...n−1 , n); 4 π Rollout = RollOutP olicy(π * , C 0...n−1 , n); 5 for d ∈ D do 6</formula><p>Predict trajectoryˆstrajectoryˆ trajectoryˆs 1:T with π Rollin ;</p><formula xml:id="formula_2">7 forˆsforˆ forˆs t ∈ ˆ s 1:T do 8 foreach a j t ∈ Explore(ˆ s t , π * , π Rollin ) do 9 Φ j t = Φ(d, a j t , ˆ s 1:t ); 10 PredictˆsPredictˆ Predictˆs t+1:T with π Rollout ; 11 L j t = F (ˆ s T ); 12 foreach j do 13 ActionCost j t = L j t −min k L k t 14 Add (Φ t , ActionCost t ) to E n ; 15ˆπ 15ˆ 15ˆπ n , C n = T rain(C 1...n−1 , E 1 . . . E n );</formula><p>glish dictionary. The large action space and unbounded T also make beam search difficult to apply since it relies on a fixed length T with commensurability of actions at the same index on different search trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Imitation Learning for Structured Prediction</head><p>Imitation learning originated in robotics, training a robot to follow the actions of a human expert <ref type="bibr" target="#b27">(Schaal, 1999;</ref><ref type="bibr" target="#b28">Silver et al., 2008</ref>). The robot moves from state to state via actions, generating a trajectory in the same manner as the transitionbased parser of Algorithm 1.</p><p>In the imitation learning literature, the learning of a policyˆπpolicyˆ policyˆπ from just the expert generated trajectories is termed "exact imitation".As discussed, it is prone to error propagation, which arises because the implicit assumption of i.i.d. inputs (s i ) during training does not hold. The states in any trajectory are dependent on previous states, and on the policy used. A number of imitation learning algorithms have been proposed to mitigate error propagation, and share a common structure shown in Algorithm 2. <ref type="table" target="#tab_2">Table 2</ref> highlights some key differences between them.</p><p>The general algorithm firstly applies a policy π RollIn (usually the expert, π * , to start) to the data instances to generate a set of 'RollIn' trajectories in line 6 (we adopt the terminology of 'RollIn' and 'RollOut' trajectories from <ref type="bibr" target="#b3">Chang et al. (2015)</ref>). Secondly a number of 'what if' scenarios are considered, in which a different action a j t is taken from a given s t instead of the actual a t in the RollIn trajectory (line 8). Each of these exploratory actions generates a RollOut trajectory (line 10) to a terminal state, for which a loss (L) is calculated using a loss function, F (s j T ), defined on the terminal states. For a number of different exploratory actions taken from a state s t on a RollIn trajectory, the action cost (or relative loss) of each is calculated (line 13). Finally the generated s t , a j t , ActionCost j t data are used to train a classifier, using any cost-sensitive classification (CSC) method (line 15). New π RollIn and π RollOut are generated, and the process repeated over a number of iterations. In general the starting expert policy is progressively removed in each iteration, so that the training data moves closer and closer to the distribution encountered by just the trained classifier. This is required to reduce error propagation. For a general imitation learning algorithm we need to specify:</p><p>• the policy to generate the RollIn trajectory (the RollInPolicy) • the policy to generate RollOut trajectories, including rules for interpolation of learned and expert policies (the RollOutPolicy) • which one-step deviations to explore with a RollOut (the Explore function) • how RollOut data are used in the classification learning algorithm to generatê π i . (within the Train function) Exact Imitation can be considered a single iteration of this algorithm, with π RollIn equal to the expert policy, and a 0-1 binary loss for F (0 loss for π * (s t ), the expert action, and a loss of 1 for any other action); all one-step deviations from the expert trajectory are considered without explicit RollOut to a terminal state.</p><p>In SEARN <ref type="bibr" target="#b9">(Daumé III et al., 2009</ref>), one of the first imitation learning algorithms in this framework, the π RollIn and π RollOut policies are identical within each iteration, and are a stochastic blend of the expert and all classifiers trained in previous iterations. The Explore function considers every possible one-step deviation from the RollIn trajectories, with a full RollOut to a terminal state. The   T rain function uses only the training data from the most recent iteration (E n ) to train C n . LOLS extends this work to provide a deterministic learned policy ( <ref type="bibr" target="#b3">Chang et al., 2015)</ref>, withˆπ withˆ withˆπ n = C n . At each iterationˆπiterationˆ iterationˆπ n is trained on all previously gathered data E 1...n ; π RollIn uses the latest classifierˆπclassifierˆ classifierˆπ n−1 , and each RollOut uses the same policy for all actions in the trajectory; either π * with probability β, orˆπorˆ orˆπ n−1 otherwise. Both LOLS and SEARN use an exhaustive search of alternative actions as an Explore function. <ref type="bibr" target="#b3">Chang et al. (2015)</ref> consider Structured Contextual Bandits (SCB) as a partial information case, the SCB modification of LOLS permits only one cost function call per RollIn (received from the external environment), so exhaustive RollOut exploration at each step is not possible. SCB-LOLS Explore picks a single step t ∈ {1 . . . T } at random at which to make a random single-step deviation.</p><p>Another strand of work uses only the expert policy when calculating the action cost. <ref type="bibr" target="#b24">Ross and Bagnell (2010)</ref> introduce SMILE, and later DAG-GER ( <ref type="bibr" target="#b26">Ross et al., 2011</ref>). These do not RollOut as such, but as in exact imitation consider all one-step deviations from the RollIn trajectory and obtain a 0/1 action cost for each by asking the expert what it would do in that state. At the nth iteration the training trajectories are generated from an interpolation of π * andˆπandˆ andˆπ n−1 , with the latter progressively increasing in importance; π * is used with probability (1-δ) n−1 for some decay rate δ. ˆ π n is trained using all E 1...n . Ross et al. (2011) discuss and reject calculating an action cost by completing a RollOut from each one-step deviation to a terminal state. Three reasons given are:</p><p>1 to calculate an action cost in their AGGREVATE algorithm. These RollOuts use the expert policy only, and allow a cost-sensitive classifier to be trained that can learn that some mistakes are more serious than others. As with DAGGER, the trained policy cannot become better than the expert.</p><p>V-DAGGER is the variant proposed by Vlachos and Clark (2014) in a semantic parsing task. It is the same as DAGGER, but with RollOuts using the same policy as RollIn. For both V-DAGGER and SEARN, the stochasticity of the RollOut means that a number of independent samples are taken for each one-step deviation to reduce the variance of the action cost, and noise in the training data. This noise reduction comes at the expense of the time needed to compute additional RollOuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adapting imitation learning to AMR</head><p>Algorithms with full RollOuts have particular value in the absence of an optimal (or nearoptimal) expert able to pick the best action from any state. If we have a suitable loss function, then the benefit of RollOuts may become worth the computation expended on them. For AMR parsing we have both a loss function in Smatch, and the ability to generate arbitrary RollOuts.</p><p>We therefore use a heuristic expert. This reduces the computational cost at the expense of not always predicting the best action. An expert needs an alignment between gold AMR nodes and tokens in the parse-tree or sentence to determine the actions to convert to one from the other. These alignments are not provided in the gold AMR, and our expert uses the AMR node to token alignments of JAMR ( <ref type="bibr" target="#b10">Flanigan et al., 2014</ref>). These alignments are not trained, but generated using regex and string matching rules. However, trajectories are in the range 50-200 actions for most training sentences, which combined with the size of |A| makes an exhaustive search of all one-step deviations expensive. Compare this to unlabeled shift-reduce parsers with 4 actions, or POS tagging with |A| ∼ 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Targeted exploration</head><p>To reduce this cost we note that exploring RollOuts for all possible alternative actions can be uninformative when the learned and expert policies agree on an action and none of the other actions score highly with the learned policy. Extending this insight we modify the Explore function in Algorithm 2 to only consider the expert action, plus all actions scored by the current learned policy that are within a threshold τ of the score for the best rated action. In the first iteration, when there is no current learned policy, we pick a number of actions (usually 10) at random for exploration. Both SCB-LOLS and AGGREVATE use partial exploration, but select the step t ∈ 1 . . . T , and the action a t at random. Here we optimise computational resources by directing the search to areas for which the trained policy is least sure of the optimal action, or disagrees with the expert.</p><p>Using imitation learning to address error propagation of transition-based parsing provides theoretical benefit from ensuring the distribution of s t , a t in the training data is consistent with the distribution on unseen test data. Using RollOuts that mix expert and learned policies additionally permits the learned policy to exceed the performance of a poor expert. Incorporating targeted exploration strategies in the Explore function makes this computationally feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Noise Reduction</head><p>Different samples for a RollOut trajectory using V-DAGGER or SEARN can give very different terminal states s T (the final AMR graph) from the same starting s t and a t due to the step-level stochasticity. The resultant high variance in the reward signal hinders effective learning. <ref type="bibr" target="#b9">Daumé III et al. (2009)</ref> have a similar problem, and note that an approximate cost function outperforms single Monte Carlo sampling, "likely due to the noise induced following a single sample".</p><p>To control noise we use the α-bound discussed by <ref type="bibr" target="#b17">Khardon and Wachman (2007)</ref>. This excludes a training example (i.e. an individual tuple s i , a i ) from future training once it has been misclassified α times in training. We find that this simple idea avoids the need for multiple RollOut samples.</p><p>An attraction of LOLS is that it randomly selects either expert or learned policy for each RollOut, and then applies this consistently to the whole trajectory. Using LOLS should reduce noise without increasing the sample size. Unfortunately the unbounded T of our transition system leads to problems if we drop the expert from the RollIn or RollOut policy mix too quickly, with many trajectories never terminating. UltimatelyˆπUltimatelyˆ Ultimatelyˆπ learns to stop doing this, but even with targeted exploration training time is prohibitive and our LOLS experiments failed to provide results. We find that V-DAGGER with an α-bound works as a good compromise, keeping the expert involved in RollIn, and speeding up learning overall.</p><p>Another approach we try is a form of focused costing ( <ref type="bibr" target="#b30">Vlachos and Craven, 2011</ref>). Instead of using the learned policy for β% of steps in the RollOut, we use it for the first b steps, and then revert to the expert. This has several potential advantages: the heuristic expert is faster than scoring all possible actions; it focuses the impact of the exploratory step on immediate actions/effects so that mistakesˆπmistakesˆ mistakesˆπ makes on a distant part of the graph do not affect the action cost; it reduces noise for the same reason. We increase b in each iteration so that the expert is asymptotically removed from RollOuts, a function otherwise supported by the decay parameter, δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transition System adaptations</head><p>Applying imitation learning to a transition system with unbounded T can and does cause problems in early iterations, with RollIn or RollOut trajectories failing to complete while the learned policy, ˆ π, is still relatively poor. To ensure every trajectory completes we add action constraints to the system. These avoid the most pathological scenarios, such as disallowing a Reattach of a previously Reattached sub-graph. These constraints are only needed in the first few iterations untiî π learns, via the action costs, to avoid these scenarios. They are listed in the Supplemental Material. As a final failsafe we insert a hard-stop on any trajectory once T &gt; 300.</p><p>To address the size of |A|, we only consider a subset of AMR concepts when labelling a node. <ref type="bibr" target="#b32">Wang et al. (2015b)</ref> use all concepts that occur in the training data in the same sentence as the lemma of the node, leading to hundreds or thousands of possible actions from some states. We use the smaller set of concepts that were assigned by the expert to the lemma of the current node any-  <ref type="table">Table 3</ref>: DAGGER with α-bound. All figures are F-Scores on the validation set. 5 iterations of classifier training take place after each DAgger iteration. A decay rate (δ) for π * of 0.3 was used.</p><p>where in the training data. We obtain these assignments from an initial application of the expert to the full training data. We add actions to use the actual word or lemma of the current node to increase generalisation, plus an action to append '-01' to 'verbify' an unseen word. This is similar to the work of <ref type="bibr" target="#b33">Werling et al. (2015)</ref> in word to AMR concept mapping, and is useful since 38% of the test AMR concepts do not exist in the training data <ref type="bibr" target="#b10">(Flanigan et al., 2014)</ref>.</p><p>Full details of the heuristics of the expert policy, features used and pre-processing are in Supplemental Material. All code is available at https://github.com/hopshackle/ dagger-AMR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Na¨ıveNa¨ıve Smatch as Loss Function</head><p>Smatch ) uses heuristics to control the combinatorial explosion of possible mappings between the input and output graphs, but is still too computationally expensive to be calculated for every RollOut during training. We retain Smatch for reporting all final results, but use 'Na¨ıveNa¨ıve Smatch' as an approximation during training. This skips the combinatorial mapping of nodes between predicted and target AMR graphs. Instead, for each graph we compile a list of:</p><p>• Node labels, e.g. name • Node-Edge-Node label concatenations, e.g. leave-01:ARG0:room • Node-Edge label concatenations, e.g. leave-01:ARG0, ARG0:room The loss is the number of entries that appear in only one of the lists. We do not convert to an F 1 score, as retaining the absolute number of mistakes is proportional to the size of the graph.</p><p>The flexibility of the transition system means multiple different actions from a given state s i can lead, via different RollOut trajectories, to the same target s T . This can result in many actions having the best action cost, reducing the signal in the training data and giving poor learning. To encourage short trajectories we break these ties with a penalty of T /5 to Na¨ıveNa¨ıve Smatch. Multiple routes of the same length still exist, and are preferred equally. Note that the ordering of the stack of dependency tree nodes in the transition system means we start at leaf nodes and move up the tree. This prevents sub-components of the output AMR graph being produced in an arbitrary order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The main dataset used is the newswire (proxy) section of LDC2014T12 ( <ref type="bibr" target="#b18">Knight et al., 2014</ref>). The data from years 1995-2006 form the training data, with 2007 as the validation set and 2008 as the test set. The data split is the same as that used by <ref type="bibr" target="#b10">Flanigan et al. (2014)</ref> and <ref type="bibr" target="#b32">Wang et al. (2015b)</ref>. <ref type="bibr">1</ref> We first assess the impact of noise reduction using the alpha bound, and report these experiments without Rollouts (i.e. using DAGGER) to isolate the effect of noise reduction. <ref type="table">Table 3</ref> summarises results using exact imitation and DAGGER with the α-bound set to discard a training instance after one misclassification. This is the most extreme setting, and the one that gave best results. We try AROW ( <ref type="bibr" target="#b7">Crammer et al., 2013)</ref>, PassiveAggressive (PA) <ref type="bibr" target="#b6">(Crammer et al., 2006</ref>), and perceptron <ref type="bibr" target="#b5">(Collins, 2002</ref>) classifiers, with averaging in all cases. We see a benefit from the α-bound for exact imitation only with AROW, which is more noise-sensitive than PA or the simple perceptron. With DAGGER there is a benefit for all classifiers. In all cases the α-bound and DAGGER are synergistic; without the α-bound imitation learning works less well, if at all. α=1 was the optimal setting, with lesser benefit observed for larger values.</p><p>We now turn our attention to targeted exploration and focused costing, for which we use V-DAGGER as explained in section 4. For all V-1 Formally <ref type="bibr" target="#b10">Flanigan et al. (2014;</ref><ref type="bibr" target="#b32">Wang et al. (2015b)</ref> use the pre-release version of this dataset (LDC2013E117). <ref type="bibr">Wer- ling et al. (2015)</ref> conducted comparative tests on the two versions, and found only a very minor changes of 0.1 to 0.2 points of F-score when using the final release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>Authors Algorithmic Approach R P F <ref type="bibr" target="#b10">Flanigan et al. (2014)</ref> Concept identification with semi-markov model followed by optimisation of constrained graph that contains all of these. 0.52 0.66 0.58</p><p>Werling et al. <ref type="formula">(2015)</ref> As <ref type="bibr" target="#b10">Flanigan et al. (2014)</ref>, with enhanced concept identification 0.59 0.66 0.62 <ref type="bibr" target="#b32">Wang et al. (2015b)</ref> Single stage using transition-based parsing algorithm 0.62 0.64 0.63 <ref type="bibr" target="#b22">Pust et al. (2015)</ref> Single stage System-Based Machine Translation --0.66 <ref type="bibr" target="#b21">Peng et al. (2015)</ref> Hyperedge replacement grammar 0.57 0.59 0.58 <ref type="bibr" target="#b0">Artzi et al. (2015)</ref> Combinatory Categorial Grammar induction 0.66 0.67 0.66 <ref type="bibr" target="#b31">Wang et al. (2015a)</ref> Extensions to action space and features in <ref type="bibr" target="#b32">Wang et al. (2015b)</ref> 0.69 0.71 0.70 This work</p><p>Imitation Learning with transition-based parsing 0.68 0.73 0.70 <ref type="table" target="#tab_5">Table 4</ref>: Comparison of previous work on the AMR task. R, P and F are Recall, Precision and F-Score.</p><p>DAGGER experiments we use AROW with regularisation parameter C=1000, and δ=0.3. <ref type="figure" target="#fig_2">Figure 2</ref> shows results by iteration of reducing the number of RollOuts explored. Only the expert action, plus actions that score close to the bestscoring action (defined by the threshold) are used for RollOuts. Using the action cost information from RollOuts does surpass simple DAGGER, and unsurprisingly more exploration is better. <ref type="figure">Figure 3</ref> shows the same data, but by total computational time spent 2 . This adjusts the picture, as small amounts of exploration give a faster benefit, albeit not always reaching the same peak performance. As a baseline, three iterations of V-DAGGER without targeted exploration (threshold = ∞) takes 9600 minutes on the same hardware to give an F-Score of 0.652 on the validation set. <ref type="figure">Figure 4</ref> shows the improvement using focused costing. The 'n/m' setting sets b, the number of initial actions taken byˆπbyˆ byˆπ in a RollOut to n, and then increases this by m at each iteration. We gain an increase of 2.9 points from 0.682 to 0.711. In all the settings tried, focused costing improves the results, and requires progressive removal of the expert to achieve the best score.</p><p>We use the classifier from the Focused Costing 5/5 run to achieve an F-Score on the held-out test set of 0.70, equal to the best published result so far ( <ref type="bibr" target="#b31">Wang et al., 2015a</ref>). Our gain of 4.7 points from imitation learning over standard transition-based parsing is orthogonal to that of <ref type="bibr" target="#b31">Wang et al. (2015a)</ref> using exact imitation with additional trained analysers; they experience a gain of 2 points from using a Charniak parser <ref type="bibr" target="#b4">(Charniak and Johnson, 2005</ref>) trained on the full OntoNotes corpus instead of the Stanford parser used here and in <ref type="bibr" target="#b32">Wang et al. (2015b)</ref>, and a further gain of 2 points from a semantic role labeller.  Using DAGGER with this system we obtained an F-Score of 0.60 in the Semeval 2016 task on AMR parsing, one standard deviation above the mean of all entries. ( <ref type="bibr" target="#b14">Goodman et al., 2016)</ref> Finally we test on all components of the LDC2014T12 corpus as shown in <ref type="table" target="#tab_6">Table 5</ref>, which include both newswire and weblog data, as well as the freely available AMRs for The Little Prince, (lpp) 3 . For each we use exact imitation, DAG-GER, and V-DAGGER on the train/validation/splits specified in the corpus. In all cases, imitation learning without RollOuts (DAGGER) improves on exact imitation, and incorporating RollOuts (V-DAGGER) provides an additional benefit. <ref type="bibr" target="#b23">Rao et al. (2015)</ref> use SEARN on the same datasets, but with a very different transition system. We show their results for comparison.</p><p>Our expert achieves a Smatch F-Score of 0.94 on the training data. This explains why DAG-GER, which assumes a good expert, is effective. Introducing RollOuts provides additional theoretical benefits from a non-decomposable loss function that can take into account longer-term impacts of an action. This provides much more information than the 0/1 binary action cost in DAGGER, and we can use Na¨ıveNa¨ıve Smatch as an approximation to our actual objective function during training. This informational benefit comes at the cost of increased noise and computational expense, which we control with targeted exploration and focused costing. We gain 2.7 points in F-Score, at the cost of 80-100x more computation. In problems with a less good expert, the gain from exploration could be much greater. Similarly, if designing an expert for a task is time-consuming, then it may be a better investment to rely on exploration with a poor expert to achieve the same result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Other strategies have been used to mitigate the error propagation problem in transition-based parsing. A common approach is to use beam search through state-space for each action choice to find a better approximation of the long-term score of the action, e.g. <ref type="bibr" target="#b35">Zhang and Clark (2008)</ref>. <ref type="bibr" target="#b11">Goldberg and Elhadad (2010)</ref> remove the determinism of the sequence of actions to create easy-first parsers, which postpone uncertain, error-prone decisions until more information is available. This contrasts with working inflexibly left-to-right along a sentence, or bottom-to-top up a tree. <ref type="bibr" target="#b12">Goldberg and Nivre (2012)</ref> introduce dynamic experts that are complete in that they will respond from any state, not just those on the perfect trajectory assuming no earlier mistakes; any expert used with an imitation learning algorithm needs to be complete in this sense. Their algorithm takes exploratory steps off the expert trajectory to augment the training data collected in a fashion very similar to DAGGER. <ref type="bibr" target="#b16">Honnibal et al. (2013)</ref> use a non-monotonic parser that allows actions that are inconsistent with previous actions. When such an action is taken it amends the results of previous actions to ensure post-hoc consistency. Our parser is nonmonotonic, and we have the same problem encountered by <ref type="bibr" target="#b16">Honnibal et al. (2013)</ref> with many different actions from a state s i able to reach the target s T , following different "paths up the mountain". This leads to poor learning. To resolve this with fixed T they break ties with a monotonic parser, so that actions that do not require later correction are scored higher in the training data. In our variable T environment, adding a penalty to the size of T is sufficient (section 4.4). <ref type="bibr" target="#b29">Vlachos and Clark (2014)</ref> use V-DAGGER to give a benefit of 4.8 points of F-Score in a domain-specific semantic parsing problem similar to AMR. Their expert is sub-optimal, with no information on alignment between words in the input sentence, and nodes in the target graph. The parser learns to link words in the input to one of the 35 node types, with the 'expert' policy aligning completely at random. This is infeasible with AMR parsing due to the much larger vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Imitation learning provides a total benefit of 4.5 points with our AMR transition-based parser over exact imitation. This is a more complex task than many previous applications of imitation learning, and we found that noise reduction was an essential pre-requisite. Using a simple 0/1 binary action cost using a heuristic expert provided a benefit of 1.8, with the remaining 2.7 points coming from RollOuts with targeted exploration, focused costing and a non-decomposable loss function that was a better approximation to our objective.</p><p>We have considered imitation learning algorithms as a toolbox that can be tailored to fit the characteristics of the task. An unbounded T meant that the LOLS RollIn was not ideal, but this could be modified to slow the loss of influence of the expert policy. We anticipate the approaches that we have found useful in the case of AMR to reduce the impact of noise, efficiently support large action spaces with targeted exploration, and cope with unbounded trajectories in the transition system will be of relevance to other structured prediction tasks. 9</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependency (left) and AMR graph (right) for: "The center will bolster NATO's defenses against cyber-attacks.'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithmˆπ</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Targeted exploration with V-DAGGER by iteration. Figure 3: Targeted exploration with V-DAGGER by time. Figure 4: Focused costing with V-DAGGER. All runs use threshold of 0.10.</figDesc><graphic url="image-1.png" coords="9,75.97,62.81,141.73,105.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Action Space for the transition-based graph parsing algorithm 

Algorithm 1: Greedy transition-based parsing 
Data: policy π, start state s 1 
Result: terminal state s T 

1 s current ← s 1 ; 
2 while s current not terminal do 

3 

a next </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of selected aspects of Imitation Learning algorithms. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>6</head><label>6</label><figDesc></figDesc><table>Exact Imitation 

Imitation Learning 
Experiment 
No α α=1 α-Gain No α α=1 IL Gain (α) IL Gain (No α) Total Gain 
AROW, C=10 
65.5 
66.8 
1.3 
65.5 
67.4 
0.6 
0.0 
1.9 
AROW, C=100 
66.4 
66.6 
0.2 
66.4 
67.7 
1.1 
0.0 
1.3 
AROW, C=1000 
66.4 
67.0 
0.6 
66.5 
68.2 
1.2 
0.1 
1.8 
PA, C=100 
66.7 
66.5 
-0.2 
67.2 
68.7 
2.2 
0.5 
2.0 
Perceptron 
65.5 
65.3 
-0.2 
66.6 
68.6 
3.3 
1.1 
3.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 lists previous AMR work on the same dataset.</head><label>4</label><figDesc></figDesc><table>Validation F-Score 
Test F-Score 
Dataset 
EI 
D 
V-D 
V-D Rao et al 
proxy 
0.670 0.686 0.704 0.70 
0.61 
dfa 
0.495 0.532 0.546 0.50 
0.44 
bolt 
0.456 0.468 0.524 0.52 
0.46 
xinhua 
0.598 0.623 0.683 0.62 
0.52 
lpp 
0.540 0.546 0.564 0.55 
0.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of Exact Imitation (EI), DAGGER (D), 
V-DAGGER (V-D) on all components of the LDC2014T12 
corpus. 

</table></figure>

			<note place="foot" n="2"> experiments were run on 8-core Google Cloud n1-highmem-8 machines.</note>

			<note place="foot" n="3"> http://amr.isi.edu/download.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Andreas Vlachos is supported by the EPSRC grant Diligent (EP/M005429/1) and Jason Naradowsky by a Google Focused Research award. We would also like to thank our anonymous reviewers for many comments that helped improve this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad-coverage ccg semantic parsing with amr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1699" to="1710" />
		</imprint>
	</monogr>
	<note>September. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smatch: an evaluation metric for semantic feature structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="748" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to search better than your teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2058" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive regularization of weight vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="155" to="187" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Expanding the scope of the atis task: The atis-3 corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Deborah A Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hunicke-Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Rudnicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discriminative graph-based parser for the abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Elhadad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="742" to="750" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="959" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ucl+sheffield at semeval-2016 task 8: Imitation learning for amr parsing with an alphabound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic feature selection for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A non-monotonic arc-eager transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Noise tolerant variants of the perceptron algorithm. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Khardon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Wachman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="227" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Abstract meaning representation (amr) annotation release 1.0. Linguistic Data Consortium Catalog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Baranescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2014" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Characterizing the errors of data-driven dependency parsing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A synchronous hyperedge replacement grammar based approach for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Using syntaxbased machine translation to parse english into abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06665</idno>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Parser for abstract meaning representation using learning to search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07586</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient reductions for imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="661" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reinforcement and imitation learning via interactive noregret learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Ross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5979</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is imitation learning the route to humanoid robots?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="233" to="242" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High performance outdoor navigation from overhead data using imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Stentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics: Science and Systems IV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new corpus and imitation learning framework for contextdependent semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="547" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Searchbased structured prediction applied to biomedical event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Boosting transition-based amr parsing with refined actions and auxiliary analyzers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="857" to="862" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A transition-based algorithm for amr parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>North American Association for Computational Linguistics</publisher>
			<pubPlace>Denver, Colorado</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust subgraph generation improves abstract meaning representation parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
