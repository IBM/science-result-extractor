<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ClearTK-TimeML: A minimalist approach to TempEval 2013</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-06-14">SemEval 2013. June 14-15, 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
							<email>steven.bethard@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Language and Education Research</orgName>
								<orgName type="institution">University of Colorado Boulder Boulder</orgName>
								<address>
									<postCode>80309-0594</postCode>
									<region>Colorado</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ClearTK-TimeML: A minimalist approach to TempEval 2013</title>
					</analytic>
					<monogr>
						<title level="m">Second Joint Conference on Lexical and Computational Semantics (*SEM)</title>
						<meeting> <address><addrLine>Atlanta, Georgia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">2</biblScope>
							<biblScope unit="page" from="10" to="14"/>
							<date type="published" when="2013-06-14">SemEval 2013. June 14-15, 2013</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ClearTK-TimeML submission to Temp-Eval 2013 competed in all English tasks: identifying events, identifying times, and identifying temporal relations. The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic annotation pipeline, and where temporal relations are only predicted for a small set of syntactic constructions and relation types. ClearTK-TimeML ranked 1 st for temporal relation F1, time extent strict F1 and event tense accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The TempEval shared tasks <ref type="bibr">(Verhagen et al., 2007;</ref><ref type="bibr" target="#b4">Verhagen et al., 2010;</ref><ref type="bibr" target="#b3">UzZaman et al., 2013</ref>) have been one of the key venues for researchers to compare methods for temporal information extraction. In TempEval 2013, systems are asked to identify events, times and temporal relations in unstructured text.</p><p>This paper describes the ClearTK-TimeML system submitted to TempEval 2013. This system is based off of the ClearTK framework for machine learning <ref type="bibr" target="#b3">(Ogren et al., 2008)</ref>  <ref type="bibr">1</ref> , and decomposes TempEval 2013 into a series of sub-tasks, each of which is formulated as a machine-learning classification problem. The goals of the ClearTK-TimeML approach were:</p><p>• To use a small set of simple features that can be derived from either tokens, part-of-speech tags or syntactic constituency parses.</p><p>• To restrict temporal relation classification to a subset of constructions and relation types for which the models are most confident.</p><p>1 http://cleartk.googlecode.com/ Thus, each classifier in the ClearTK-TimeML pipeline uses only the features shared by successful models in previous work <ref type="bibr" target="#b0">(Bethard and Martin, 2006;</ref><ref type="bibr" target="#b1">Bethard and Martin, 2007;</ref><ref type="bibr" target="#b2">Llorens et al., 2010;</ref><ref type="bibr" target="#b3">UzZaman and Allen, 2010</ref>) that can be derived from a simple morpho-syntactic annotation pipeline 2 . And each of the temporal relation classifiers is restricted to a particular syntactic construction and to a particular set of temporal relation labels. The following sections describe the models, classifiers and datasets behind the ClearTK-TimeML approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Time models</head><p>Time extent identification was modeled as a BIO token-chunking task, where each token in the text is classified as being at the B(eginning) of, I(nside) of, or O(utside) of a time expression. The following features were used to characterize tokens:</p><p>• The token's text • The token's stem • The token's part-of-speech • The unicode character categories for each character of the token, with repeats merged (e.g. Dec28 would be 'LuLlNd') • The temporal type of each alphanumeric sub-token, derived from a 58-word gazetteer of time words • All of the above features for the preceding 3 and following 3 tokens</p><p>Time type identification was modeled as a multiclass classification task, where each time is classified 2 OpenNLP sentence segmenter, ClearTK PennTreebankTokenizer, Apache Lucene Snowball stemmer, OpenNLP partof-speech tagger, and OpenNLP constituency parser as DATE, TIME, DURATION or SET. The following features were used to characterize times:</p><p>• The text of all tokens in the time expression • The text of the last token in the time expression • The unicode character categories for each character of the token, with repeats merged • The temporal type of each alphanumeric sub-token, derived from a 58-word gazetteer of time words</p><p>Time value identification was not modeled by the system. Instead, the TimeN time normalization system (Llorens et al., 2012) was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Event models</head><p>Event extent identification, like time extent identification, was modeled as BIO token chunking. The following features were used to characterize tokens:</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and following 3 tokens</head><p>Event aspect identification was modeled as a multiclass classification task, where each event is classified as PROGRESSIVE, PERFECTIVE, PERFECTIVE-PROGRESSIVE or NONE. The following features were used to characterize events:</p><p>• The part-of-speech tags of all tokens in the event • The text of any verbs in the preceding 3 tokens Event class identification was modeled as a multiclass classification task, where each event is classified as OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE, I-STATE or I-ACTION. The following features were used to characterize events:</p><p>• The stems of all tokens in the event • The part-of-speech tags of all tokens in the event Event modality identification was modeled as a multi-class classification task, where each event is classified as one of WOULD, COULD, CAN, etc. The following features were used to characterize events:</p><p>• The text of any prepositions, adverbs or modal verbs in the preceding 3 tokens Event polarity identification was modeled as a binary classification task, where each event is classified as POS or NEG. The following features were used to characterize events:</p><p>• The text of any adverbs in the preceding 3 tokens</p><p>Event tense identification was modeled as a multiclass classification task, where each event is classified as FUTURE, INFINITIVE, PAST, PASTPART, PRESENT, PRESPART or NONE. The following features were used to characterize events:</p><p>• The last two characters of the event • The part-of-speech tags of all tokens in the event • The text of any prepositions, verbs or modal verbs in the preceding 3 tokens</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Temporal relation models</head><p>Three different models, described below, were trained for temporal relation identification. All models followed a multi-class classification approach, pairing an event and a time or an event and an event, and trying to predict a temporal relation type (BEFORE, AFTER, INCLUDES, etc.) or NORELATION if there was no temporal relation between the pair. While the training and evaluation data allowed for 14 possible relation types, each of the temporal relation models was restricted to a subset of relations, with all other relations mapped to the NORELATION type. The subset of relations for each model was selected by inspecting the confusion matrix of the model's errors on the training data, and removing relations that were frequently confused and whose removal improved performance on the training data.</p><p>Event to document creation time relations were classified by considering (event, time) pairs where each event in the text was paired with the document creation time. The classifier was restricted to the relations BEFORE, AFTER and INCLUDES. The following features were used to characterize such relations:</p><p>• The event's aspect (as classified above)</p><p>• The event's class (as classified above)</p><p>• The event's modality (as classified above)</p><p>• The event's polarity (as classified above)</p><p>• The event's tense (as classified above)</p><p>• The text of the event, only if the event was identified as having class ASPECTUAL Event to same sentence time relations were classified by considering (event, time) pairs where the syntactic path from event to time matched a regular expression of syntactic categories and up/down movements through the tree: ˆ <ref type="table">((NP|PP|ADVP)</ref> The following features were used to characterize such relations:</p><p>• The aspect (as classified above) for each event • The class (as classified above) for each event • The tense (as classified above) for each event • The text of the first child of the grandparent of the event in the constituency tree, for each event • The path through the syntactic constituency tree from one event to the other • The tokens appearing between the two events</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Classifiers</head><p>The above models described the translation from TempEval tasks to classification problems and classifier features. For BIO token-chunking problems, Mallet 3 conditional random fields and LIBLINEAR 4 support vector machines and logistic regression were applied. For the other problems, LIBLINEAR, Mallet MaxEnt and OpenNLP MaxEnt 5 were applied. All classifiers have hyper-parameters that must be tuned during training -LIBLINEAR has the classifier type and the cost parameter, Mallet CRF has the iteration count and the Gaussian prior variance, etc. <ref type="bibr">6</ref> The best classifier for each training data set was selected via a grid search over classifiers and parameter settings. The grid of parameters was manually selected to provide several reasonable values for each classifier parameter. Each (classifier, parameters) point on the grid was evaluated with a 2-fold cross validation on the training data, and the best performing (classifier, parameters) was selected as the final model to run on the TempEval 2013 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Data sets</head><p>The classifiers were trained using the following sources of training data:</p><p>TB The TimeBank event, time and relation annotations, as provided by the TempEval organizers. AQ The AQUAINT event, time and relation annotations, as provided by the TempEval organizers. SLV The "Silver" event, time and relation annotations, from the TempEval organizers' system. BMK The verb-clause temporal relation annotations of ( ). These relations are added on top of the original relations. PM The temporal relations inferred via closure on the TimeBank and AQUAINT data by Philippe Muller <ref type="bibr">7</ref> . These relations replace the original ones, except in files where no relations were inferred (because of temporal inconsistencies). <ref type="table">Table 1</ref> shows the performance of the ClearTKTimeML models across the different tasks when trained on different sets of training data. The "Data" column of each row indicates both the training data sources (as in Section 6), and whether the events and times were predicted by the models ("system") or taken from the annotators ("human"). Performance is reported in terms of strict precision (P), Recall (R) and F1 for event extents, time extents and temporal relations, and in terms of Accuracy (A) on the correctly identified extents for event and time attributes. <ref type="table">Event  Time  Relation  annotation  events  extent  class tense aspect  extent  value type  type  sources  &amp; times F1  P  R  A  A  A  F1  P  R  A  A  F1  P  R  TB+BMK  system</ref>  Training on the AQUAINT (AQ) data in addition to the TimeBank (TB) hurt times and relations. Adding the AQUAINT data caused a -2.7 drop in extent precision, a -8.0 drop in extent recall, a -1.8 drop in value accuracy and a -0.4 drop in type accuracy, and a -3.6 to -4.3 drop in relation recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Training on the "Silver" (SLV) data in addition to TB+AQ data gave mixed results. There were big gains for time extent precision (+8.4), time value accuracy (+3.7), event extent recall (+2.5) and event class accuracy (+2.3), but a big drop for event tense accuracy (-6.6). Relation recall improved (+2.7 with system events and times, +6.0 with manual) but precision varied (-4.4 with system, +1.6 with manual).</p><p>Adding verb-clause relations (BMK) and closureinferred relations (PM) increased recall but lowered precision. With system-annotated events and times, the change was +2.2/-0.4 (recall/precision) for verb-clause relations, and +0.7/-1.2 for closureinferred relations. With manually-annotated events and times, the change was +2.2/-0.3 for verb-clause relations, and (the one exception where recall improved) +1.5/+1.9 for closure-inferred relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>Overall, the ClearTK-TimeML ranked 1 st in relation F1, time extent strict F1 and event tense accuracy.</p><p>Analysis across the different ClearTK-TimeML runs showed that including annotations from the AQUAINT corpus hurt model performance across a variety of tasks. A manual inspection of the AQUAINT corpus revealed many annotation errors, suggesting that the drop may be the result of attempting to learn from inconsistent training data. The AQUAINT corpus may thus have to be partially reannotated to be useful as a training corpus.</p><p>Analysis also showed that adding more relation annotations increased recall, typically at the cost of precision, even though the added annotations were highly accurate: ) reported agreement of 90%, and temporal closure relations were 100% deterministic from the already-annotated relations. One would expect that adding such highquality relations would only improve performance. But not all temporal relations were annotated by the TempEval 2013 annotators, so the system could be marked wrong for a finding a true temporal relation that was not noticed by the annotators. Further analysis is necessary to investigate this hypothesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The token's text • The token's stem • The token's part-of-speech • The syntactic category of the token's parent in the constituency tree • The text of the first sibling of the token in the constituency tree • The text of the preceding 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>↑)* ((VP|SBAR|S)↑)* (S|SBAR|VP|NP) (↓(VP|SBAR|S))* (↓(NP|PP|ADVP))*$. The classifier relations were re- stricted to INCLUDES and IS-INCLUDED. The follow- ing features were used to characterize such relations: • The event's class (as classified above) • The event's tense (as classified above) • The text of any prepositions or verbs in the 5 tokens following the event • The time's type (as classified above) • The text of all tokens in the time expression • The text of any prepositions or verbs in the 5 tokens preceding the time expression Event to same sentence event relations were clas- sified by considering (event, event) pairs where the syntactic path from one event to the other matchedˆ(matchedˆ((VP↑|ADJP↑|NP↑)? (VP|ADJP|S|SBAR) (↓(S|SBAR|PP))* ((↓VP|↓ADJP)*|(↓NP)*)$. The classi- fier relations were restricted to BEFORE and AFTER.</figDesc></figure>

			<note place="foot" n="3"> http://mallet.cs.umass.edu/ 4 http://www.csie.ntu.edu.tw/ ˜ cjlin/liblinear/ 5 http://opennlp.apache.org/</note>

			<note place="foot" n="6"> For BIO token-chunking tasks, LIBLINEAR also had a parameter for how many previous classifications to use as features. 7 https://groups.google.com/d/topic/tempeval/ LJNQKwYHgL8</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to Philippe Muller for providing the closureinferred relations. The project described was supported in part by Grant Number R01LM010090 from the National Library Of Medicine. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library Of Medicine or the National Institutes of Health.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identification of event mentions and their semantic class</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin2006] Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">146154</biblScope>
		</imprint>
	</monogr>
	<note>Acceptance rate 31%)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding temporal structure in text: Machine learning of syntactic temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin2007] Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<editor>Steven Bethard, James H. Martin, and Sara Klingenstein</editor>
		<meeting>the 4th International Workshop on Semantic Evaluations<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page">441</biblScope>
		</imprint>
	</monogr>
	<note>CU-TMP: temporal relation classification using syntactic and semantic features</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TIMEN: an open temporal expression normalisation resource</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Llorens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<editor>Llorens et al.2012] Hector Llorens, Leon Derczynski, Robert Gaizauskas, and Estela Saquete</editor>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Uppsala, Sweden; Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page">284291</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 5th International Workshop on Semantic Evaluation. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TRIPS and TRIOS system for TempEval-2: extracting temporal information from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ogren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013)</title>
		<editor>Naushad UzZaman, Hector Llorens, James F. Allen, Leon Derczynski, Marc Verhagen, and James Pustejovsky</editor>
		<meeting>the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantcis (*SEM 2013)<address><addrLine>Uppsala, Sweden; Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky; Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
	<note>Proceedings of the 4th International Workshop on Semantic Evaluations</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 13: TempEval-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="volume">5762</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
