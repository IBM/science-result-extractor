<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THE VARIATIONAL FAIR AUTOENCODER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
							<email>c.louizos@uva.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
							<email>yujiali@cs.toronto.edum.welling@uva.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">×</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Canadian Institute for Advanced Research (CIFAR)</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">‡</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
							<email>zemel@cs.toronto.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Canadian Institute for Advanced Research (CIFAR)</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">THE VARIATIONAL FAIR AUTOENCODER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the problem of learning representations that are invariant to certain nuisance or sensitive factors of variation in the data while retaining as much of the remaining information as possible. Our model is based on a variational autoencoding architecture (Kingma &amp; Welling, 2014; Rezende et al., 2014) with priors that encourage independence between sensitive and latent factors of variation. Any subsequent processing, such as classification, can then be performed on this purged latent representation. To remove any remaining dependencies we incorporate an additional penalty term based on the &quot;Maximum Mean Discrepancy&quot; (MMD) (Gretton et al., 2006) measure. We discuss how these architectures can be efficiently trained on data and show in experiments that this method is more effective than previous work in removing unwanted sources of variation while maintaining informative latent representations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In "Representation Learning" one tries to find representations of the data that are informative for a particular task while removing the factors of variation that are uninformative and are typically detrimental for the task under consideration. Uninformative dimensions are often called "noise" or "nuisance variables" while informative dimensions are usually called latent or hidden factors of variation. Many machine learning algorithms can be understood in this way: principal component analysis, nonlinear dimensional reduction and latent Dirichlet allocation are all models that extract informative factors (dimensions, causes, topics) of the data which can often be used to visualize the data. On the other hand, linear discriminant analysis and deep (convolutional) neural nets learn representations that are good for classification.</p><p>In this paper we consider the case where we wish to learn latent representations where (almost) all of the information about certain known factors of variation are purged from the representation while still retaining as much information about the data as possible. In other words, we want a latent representation z that is maximally informative about an observed random variable y (e.g., class label) while minimally informative about a sensitive or nuisance variable s. By treating s as a sensitive variable, i.e. s is correlated with our objective, we are dealing with "fair representations", a problem previously considered by <ref type="bibr" target="#b16">Zemel et al. (2013)</ref>. If we instead treat s as a nuisance variable we are dealing with "domain adaptation", in other words by removing the domain s from our representations we will obtain improved performance.</p><p>In this paper we introduce a novel model based on deep variational autoencoders (VAE) <ref type="bibr" target="#b13">Rezende et al., 2014</ref>). These models can naturally encourage separation between latent variables z and sensitive variables s by using factorized priors p(s)p(z). However, some dependencies may still remain when mapping data-cases to their hidden representation using the variational posterior q(z|x, s), which we stamp out using a "Maximum Mean Discrepancy" ( <ref type="bibr" target="#b5">Gretton et al., 2006</ref>) term that penalizes differences between all order moments of the marginal posterior distributions q(z|s = k) and q(z|s = k ) (for a discrete RV s). In experiments we show that this combined approach is highly successful in learning representations that are devoid of unwanted information while retaining as much information as possible from what remains. Factoring out undesired variations from the data can be easily formulated as a general probabilistic model which admits two distinct (independent) "sources"; an observed variable s, which denotes the variations that we want to remove, and a continuous latent variable z which models all the remaining information. This generative process can be formally defined as:</p><formula xml:id="formula_0">z ∼ p(z); x ∼ p θ (x|z, s)</formula><p>where p θ (x|z, s) is an appropriate probability distribution for the data we are modelling. With this formulation we explicitly encode a notion of 'invariance' in our model, since the latent representation is marginally independent of the factors of variation s. Therefore the problem of finding an invariant representation for a data point x and variation s can be cast as performing inference on this graphical model and obtaining the posterior distribution of z, p(z|x, s).</p><p>For our model we will employ a variational autoencoder architecture <ref type="bibr" target="#b13">Rezende et al., 2014</ref>); namely we will parametrize the generative model (decoder) p θ (x|z, s) and the variational posterior (encoder) q φ (z|x, s) as (deep) neural networks which accept as inputs z, s and x, s respectively and produce the parameters of each distribution after a series of non-linear transformations. Both the model (θ) and variational (φ) parameters will be jointly optimized with the SGVB ) algorithm according to a lower bound on the log-likelihood. This parametrization will allow us to capture most of the salient information of x in our embedding z. Furthermore the distributed representation of a neural network would allow us to better resolve the dependencies between x and s thus yielding a better disentangling between the independent factors z and s. By choosing a Gaussian posterior q φ (z|x, s) and standard isotropic Gaussian prior p(z) = N (0, I) we can obtain the following lower bound:</p><formula xml:id="formula_1">N n=1 log p(x n |s n ) ≥ N n=1 E q φ (zn|xn,sn) [log p θ (x n |z n , s n )] − KL(q φ (z n |x n , s n )||p(z)) (1) = F(φ, θ; x n , s n ) with q φ (z n |x n , s n ) = N (z n |µ n = f φ (x n , s n ), σ n = e f φ (xn,sn) ) and p θ (x n |z n , s n ) = f θ (z n , s n )</formula><p>with f θ (z n , s n ) being an appropriate probability distribution for the data we are modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SEMI-SUPERVISED MODEL</head><p>Factoring out variations in an unsupervised way can however be harmful in cases where we want to use this invariant representation for a subsequent prediction task. In particular if we have a situation where the nuisance variable s and the actual label y are correlated, then training an unsupervised model could yield random or degenerate representations with respect to y. Therefore it is more appropriate to try to "inject" the information about the label during the feature extraction phase. This can be quite simply achieved by introducing a second "layer" of latent variables to our generative model where we try to correlate z with the prediction task. Assuming that the invariant features are now called z 1 we enrich the generative story by similarly providing two distinct (independent) sources for z 1 ; a discrete (in case of classification)variable y which denotes the label of the data point x and a continuous latent variable z 2 which encodes the variation on z 1 that is not explained by y (x dependent noise). The process now can be formally defined as:</p><formula xml:id="formula_2">y, z 2 ∼ Cat(y)p(z 2 ); z 1 ∼ p θ (z 1 |z 2 , y);</formula><p>x ∼ p θ (x|z 1 , s) Similarly to the unsupervised case we use a variational auto-encoder and jointly optimize the variational and model parameters. The lower bound now becomes:</p><formula xml:id="formula_3">N n=1 log p(x n |s n ) ≥ N n=1</formula><p>E q φ (z1 n ,z2 n ,yn|xn,sn) [log p(z 2 ) + log p(y n ) + log p θ (z 1n |z 2n , y n )+ + log p θ (x n |z 1n , s n ) − log q φ (z 1n , z 2n , y n |x n , s n )] (2) where we assume that the posterior q φ (z 1n , z 2n , y n |x n , s n ) is factorized as q φ (z 1n , z 2n , y n |x n , s n ) = q φ (z 1n |x n , s n )q φ (y n |z 1n )q φ (z 2n |z 1n , y n ), and where:</p><formula xml:id="formula_4">q φ (z 1n |x n , s n ) = N (z 1n |µ n = f φ (x n , s n ), σ n = e f φ (xn,sn) ) q φ (y n |z 1n ) = Cat(y n |π n = softmax(f φ (z 1n ))) q φ (z 2n |z 1n , y n ) = N (z 2n |µ n = f φ (z 1n , y n ), σ n = e f φ (z1 n ,yn) ) p θ (z 1n |z 2n , y n ) = N (z 1n |µ n = f θ (z 2n , y n ), σ n = e f θ (z2 n ,yn) ) p θ (x n |z 1n , s n ) = f θ (z 1n</formula><p>, s n ) with f θ (z 1n , s n ) again being an appropriate probability distribution for the data we are modelling. The model proposed here can be seen as an extension to the 'stacked M1+M2' model originally proposed from , where we have additionally introduced the nuisance variable s during the feature extraction. Thus following  we can also handle the 'semisupervised' case, i.e., missing labels. In situations where the label is observed the lower bound takes the following form (exploiting the fact that we can compute some Kullback-Leibler divergences explicitly in our case):</p><formula xml:id="formula_5">N n=1 L s (φ, θ; x n , s n , y n ) = Ns n=1 E q φ (z1 n |xn,sn) [−KL(q φ (z 2n |z 1n , y n )||p(z 2 )) + log p θ (x n |z 1n , s n )]+ + E q φ (z1 n |xn,sn)q φ (z2 n |z1 n ,yn) [log p θ (z 1 |z 2n , y n ) − log q φ (z 1n |x n s n )]</formula><p>(3) and in the case that it is not observed we use q(y n |z 1n ) to 'impute' our data:</p><formula xml:id="formula_6">M m=1 L u (φ, θ; x m , s m ) = M m=1 E q φ (z1 m |xm,sm) [−KL(q(y m |z 1m )||p(y m )) + log p θ (x m |z 1m , s m )]+ + E q φ (z1 m ,ym|xm,sm) [−KL(q φ (z 2m |z 1m , y m )||p(z 2 ))]+ + E q φ (z1 m ,ym,z2 m |xm,sm) [log p θ (z 1m |z 2m , y m ) − log q φ (z 1m |x m , s m )]<label>(4)</label></formula><p>therefore the final objective function is:</p><formula xml:id="formula_7">F VAE (φ, θ; x n , x m , s n , s m , y n ) = N n=1 L s (φ, θ; x n , s n , y n ) + M m=1 L u (φ, θ; x m , s m )+ + α N n=1 E q(z1 n |xn,sn) [− log q φ (y n |z 1n )]<label>(5)</label></formula><p>where the last term is introduced so as to ensure that the predictive posterior q φ (y|z 1 ) learns from both labeled and unlabeled data. This semi-supervised model will be called "VAE" in our experiments.</p><p>However, there is a subtle difference between the approach of  and our model. Instead of training separately each layer of stochastic variables we optimize the model jointly. The potential advantages of this approach are two fold: as we previously mentioned if the label y and the nuisance information s are correlated then training a (conditional) feature extractor separately poses the danger of creating a degenerate representation with respect to the label y. Furthermore the label information will also better guide the feature extraction towards the more salient parts of the data, thus maintaining most of the (predictive) information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">FURTHER INVARIANCE VIA MAXIMUM MEAN DISCREPANCY</head><p>Despite the fact that we have a model that encourages statistical independence between s and z 1 a-priori we might still have some dependence in the (approximate) marginal posterior q φ (z 1 |s). In particular, this can happen if the label y is correlated with the sensitive variable s, which can allow information about s to "leak" into the posterior. Thus instead we could maximize a "penalized" lower bound where we impose some sort of regularization on the marginal q φ (z 1 |s). In the following we will describe one way to achieve this regularization through the Maximum Mean Discrepancy (MMD) ( <ref type="bibr" target="#b5">Gretton et al., 2006</ref>) measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">MAXIMUM MEAN DISCREPANCY</head><p>Consider the problem of determining whether two datasets {X} ∼ P 0 and {X } ∼ P 1 are drawn from the same distribution, i.e., P 0 = P 1 . A simple test is to consider the distance between empirical statistics ψ(·) of the two datasets:</p><formula xml:id="formula_8">1 N 0 N0 i=1 ψ(x i ) − 1 N 1 N1 i=1 ψ(x i ) 2 .<label>(6)</label></formula><p>Expanding the square yields an estimator composed only of inner products on which the kernel trick can be applied. The resulting estimator is known as Maximum Mean Discrepancy (MMD) ( <ref type="bibr" target="#b5">Gretton et al., 2006</ref>):</p><formula xml:id="formula_9">MMD (X, X ) = 1 N 2 0 N0 n=1 N0 m=1 k(x n , x m ) + 1 N 2 1 N1 n=1 N1 m=1 k(x n , x m ) − 2 N 0 N 1 N0 n=1 N1 m=1 k(x n , x m ).<label>(7)</label></formula><p>Asymptotically, for a universal kernel such as the Gaussian kernel k(x, x ) = e −γx−x 2 , MMD (X, X ) is 0 if and only if P 0 = P 1 . Equivalently, minimizing MMD can be viewed as matching all of the moments of P 0 and P 1 . Therefore, we can use it as an extra "regularizer" and force the model to try to match the moments between the marginal posterior distributions of our latent variables, i.e., q φ (z 1 |s = 0) and q φ (z 1 |s = 1) (in the case of binary nuisance information s 1 ). By adding the MMD penalty into the lower bound of our aforementioned VAE architecture we obtain our proposed model, the "Variational Fair Autoencoder" (VFAE):</p><formula xml:id="formula_10">F VFAE (φ, θ; x n , x m , s n , s m , y n ) = F VAE (φ, θ; x n , x m , s n , s m , y n ) − β MMD (Z 1s=0 , Z 1s=1 ) (8)</formula><p>where:</p><formula xml:id="formula_11">MMD (Z 1s=0 , Z 1s=1 ) = E ˜ p(x|s=0) [E q(z1|x,s=0) [ψ(z 1 )]] − E ˜ p(x|s=1) [E q(z1|x,s=1) [ψ(z 1 )]] 2 (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">FAST MMD VIA RANDOM FOURIER FEATURES</head><p>A naive implementation of MMD in minibatch stochastic gradient descent would require computing the M ×M Gram matrix for each minibatch during training, where M is the minibatch size. Instead, we can use random kitchen sinks <ref type="bibr" target="#b12">(Rahimi &amp; Recht, 2009</ref>) to compute a feature expansion such that computing the estimator (6) approximates the full MMD (7). To compute this, we draw a random</p><formula xml:id="formula_12">K × D matrix W,</formula><p>where K is the dimensionality of x, D is the number of random features and each entry of W is drawn from a standard isotropic Gaussian. The feature expansion is then given as:</p><formula xml:id="formula_13">ψ W (x) = 2 D cos 2 γ xW + b .<label>(10)</label></formula><p>where b is a D-dimensional uniform random vector with entries in <ref type="bibr">[0, 2π]</ref>. <ref type="bibr" target="#b17">Zhao &amp; Meng (2015)</ref> have successfully applied the idea of using random kitchen sinks to approximate MMD. This estimator is fairly accurate, and is typically much faster than the full MMD penalty. We use D = 500 in our experiments.</p><p>1 In case that we have more than two states for the nuisance information s, we minimize the MMD penalty between each marginal q(z|s = k) and q(z), i.e., K k=1 MMD(Z1 s=k , Z1) for all possible states K of s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We performed experiments on the three datasets that correspond to a "fair" classification scenario and were previously used by <ref type="bibr" target="#b16">Zemel et al. (2013)</ref>. In these datasets the "nuisance" or sensitive variable s is significantly correlated with the label y thus making the proper removal of s challenging. Furthermore, we also experimented with the Amazon reviews dataset to make a connection with the "domain-adaptation" literature. Finally, we also experimented with a more general task on the extended Yale B dataset; that of learning invariant representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATASETS</head><p>For the fairness task we experimented with three datasets that were previously used by <ref type="bibr" target="#b16">Zemel et al. (2013)</ref>. The German dataset is the smallest one with 1000 data points and the objective is to predict whether a person has a good or bad credit rating. </p><formula xml:id="formula_14">(x n |z 1n , s n ) = Bern(x n |π n = σ(f θ (z 1n , s n ))), where σ(·) is the sigmoid function 3 .</formula><p>For the domain adaptation task we used the Amazon reviews dataset (with similar preprocessing) that was also employed by <ref type="bibr" target="#b2">Chen et al. (2012)</ref> and <ref type="bibr" target="#b4">Ganin et al. (2015)</ref>. It is composed from text reviews about particular products, where each product belongs to one out of four different domains: "books", "dvd", "electronics" and "kitchen". As a result we performed twelve domain adaptation tasks. The labels y correspond to the sentiment of each review, i.e. either positive or negative.</p><p>Since each feature vector x is composed from counts of unigrams and bigrams we used a Poisson distribution for p θ (x n |z 1n , s n ) = Poisson(x n |λ n = e f θ (z1 n ,sn) ). It is also worthwhile to mention that we can fully exploit the semi-supervised nature of our model in this dataset, and thus for training we only use the source domain labels and consider the labels of the target domain as "missing".</p><p>For the general task of learning invariant representations we used the Extended Yale B dataset, which was also employed in a similar fashion by <ref type="bibr" target="#b10">Li et al. (2014)</ref>. It is composed from face images of 38 people under different lighting conditions (directions of the light source). Similarly to <ref type="bibr" target="#b10">Li et al. (2014)</ref>, we created 5 states for the nuisance variable s: light source in upper right, lower right, lower left, upper left and the front. The labels y correspond to the identity of the person. Following <ref type="bibr" target="#b10">Li et al. (2014)</ref>, we used the same training, test set and no validation set. For the p(x n |z 1n , s n ) distribution we used a Gaussian with means constrained in the 0-1 range (since we have intensity images) by a sigmoid, i.e. p θ (x n |z 1n , s n ) = N (x n |µ n = σ(f θ (z 1n , s n )), σ n = e f θ (z1 n ,sn) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EXPERIMENTAL SETUP</head><p>For the Adult dataset both encoders, for z 1 and z 2 , and both decoders, for z 1 and x, had one hidden layer of 100 units. For the Health dataset we had one hidden layer of 300 units for the z 1 encoder and x decoder and one hidden layer of 150 units for the z 2 encoder and z 1 decoder. For the much smaller German dataset we used 60 hidden units for both encoders and decoders. Finally, for the Amazon reviews and Extended Yale B datasets we had one hidden layer with 500, 400 units respectively for the z 1 encoder, x decoder, and 300, 100 units respectively for the z 2 encoder and z 1 decoder. of the MMD, β, was tuned according to a validation set. The scaling of the supervised cost was low (α = 1) for the Adult, Health and German datasets due to the correlation of s with y. On the Amazon reviews and Extended Yale B datasets however the scaling of the supervised cost was higher: α = 100 · Nbatch source+Nbatch target Nbatch source for the Amazon reviews dataset (empirically determined after observing the classification loss on the first few iterations on the first source-target pair) and α = 200 for the Extended Yale B dataset. Similarly, the scaling of the MMD penalty was β = 100 · N batch for the Amazon reviews dataset and β = 200 · N batch for the Extended Yale B.</p><p>Our evaluation is geared towards two fronts; removing information about s and classification accuracy for y. To measure the information about s in our new representation we simply train a classifier to predict s from z 1 . We utilize both Logistic Regression (LR) which is a simple linear classifier, and Random Forest (RF) which is a powerful non-linear classifier. Since on the datasets that we experimented with the nuisance variable s is binary we can easily find the random chance accuracy for s and measure the discriminatory information of s in z 1 . Furthermore, we also used the discrimination metric from <ref type="bibr" target="#b16">Zemel et al. (2013)</ref> as well a more "informed" version of the discrimination metric that instead of the predictions, takes into account the probabilities of the correct class. They are provided in the appendix A. Finally, for the classification performance on y we used the predictive posterior q φ (y|z 1 ) for the VAE/VFAE and a simple Logistic Regression for the original representations x. It should be noted that for the VFAE and VAE models we use a sample from q φ (z 1 |x, s) to make predictions, instead of using the mean. We found that the extra noise helps with invariance.</p><p>We implemented the Learning Fair Representations ( <ref type="bibr" target="#b16">Zemel et al., 2013</ref>) method (LFR) as a baseline using K = 50 dimensions for the latent space. To measure the accuracy on y in the results below we similarly used the LFR model predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">FAIR CLASSIFICATION</head><p>The results for all three datasets can be seen in <ref type="figure" target="#fig_1">Figure 3</ref>. Since we are dealing with the "fair" classification scenario here, low accuracy and discrimination against s is more important than the accuracy on y (as long as we do not produce degenerate representations).</p><p>On the Adult dataset, the highest accuracy on the label y and the lowest discrimination against s is obtained by our LFR baseline. Despite the fact that LFR appears to give the best tradeoff between accuracy and discrimination, it appears to retain information about s in its representation, which is discovered from the random forest classifier. In that sense, the VFAE method appears to do the best job in actually removing the sensitive information and maintaining most of the predictive information. Furthermore, the introduction of the MMD penalty in the VFAE model seems to provide a significant benefit with respect to our discrimination metrics, as both were reduced considerably compared to the regular VAE.</p><p>On the German dataset, all methods appear to be invariant with respect to the sensitive information s. However this is not the case for the discrimination metric, since LFR does appear to retain information compared to the VAE and VFAE. The MMD penalty in VFAE did seem improve the discrimination scores over the original VAE, while the accuracy on the labels y remained similar.</p><p>As for the Health dataset; this dataset is extremely imbalanced, with only 15% of the patients being admitted to a hospital. Therefore, each of the classifiers seems to predict the majority class as the label y for every point. For the invariance against s however, the results were more interesting. On the one hand, the VAE model on this dataset did maintain some sensitive information, which could be identified both linearly and non-linearly. On the other hand, VFAE and the LFR methods were able to retain less information in their latent representation, since only Random Forest was able to achieve higher than random chance accuracy. This further justifies our choice for including the MMD penalty in the lower bound of the VAE. .</p><p>In order to further assess the nature of our new representations, we visualized two dimensional <ref type="bibr">Barnes-Hut SNE (van der Maaten, 2013</ref>) embeddings of the z 1 representations, obtained from the model trained on the Adult dataset, in <ref type="figure" target="#fig_2">Figure 4</ref>. As we can see, the nuisance/sensitive variables s can be identified both on the original representation x and on a latent representation z 1 that does not have the MMD penalty and the independence properties between z 1 and s in the prior. By   Blue colour corresponds to males whereas red colour corresponds to females.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">DOMAIN ADAPTATION</head><p>As for the domain adaptation scenario and the Amazon reviews dataset, the results of our VFAE model can be seen in <ref type="table">Table 1</ref>. Our model was successful in factoring out the domain information, since the accuracy, measured both linearly (LR) and non-linearly (RF), was towards random chance (which for this dataset is 0.5). We should also mention that, on this dataset at least, completely removing information about the domain does not guarantee a better performance on y. The same effect was also observed by <ref type="bibr" target="#b4">Ganin et al. (2015)</ref> and <ref type="bibr" target="#b2">Chen et al. (2012)</ref>. As far as the accuracy on y is concerned, we compared against a recent neural network based state of the art method for domain adaptation, Domain Adversarial Neural Network (DANN) ( <ref type="bibr" target="#b4">Ganin et al., 2015</ref>). As we can observe in table 1, our accuracy on the labels y is higher on 9 out of the 12 domain adaptation tasks whereas on the remaining 3 it is quite similar to the DANN architecture. <ref type="table">Table 1</ref>: Results on the Amazon reviews dataset. The DANN column is taken directly from <ref type="bibr" target="#b4">Ganin et al. (2015)</ref> (the column that uses the original representation as input).</p><p>Source - <ref type="table">Target  S  Y  RF  LR  VFAE</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LEARNING INVARIANT REPRESENTATIONS</head><p>Regarding the more general task of learning invariant representations; our results on the Extended Yale B dataset also demonstrate our model's ability to learn such representations. As expected, on the original representation x the lighting conditions, s, are well identifiable with almost perfect accuracy from both RF and LR. This can also be seen in the two dimensional embeddings of the original space x in <ref type="figure" target="#fig_4">Figure 5a</ref>: the images are mostly clustered according to the lighting conditions. As soon as we utilize our VFAE model we simultaneously decrease the accuracy on s, from 96% to about 50%, and increase our accuracy on y, from 78% to about 85%. This effect can also be seen in <ref type="figure" target="#fig_4">Figure 5b</ref>: the images are now mostly clustered according to the person ID (the label y).</p><p>It is clear that in this scenario the information about s is purely "nuisance" with respect to the labels y. Therefore, by using our VFAE model we are able to obtain improved generalization and classification performance by effectively removing s from our representations.  returns 1 or 0, while p(z k = 1|x i , s = 1) returns values between values 0 and 1, then the penalty could still be satisfied, but information could still leak through. We addressed both of these issues in this paper.</p><p>Domain adaptation can also be cast as learning representations that are "invariant" with respect to a discrete variable s, the domain. Most similar to our work are neural network approaches which try to match the feature distributions between the domains. This was performed in an unsupervised way with mSDA <ref type="bibr" target="#b2">(Chen et al., 2012</ref>) by training denoising autoencoders jointly on all domains, thus implicitly obtaining a representation general enough to explain both the domain and the data. This is in contrast to our approach where we instead try to learn representations that explicitly remove domain information during the learning process. For the latter we find more similarities with "domain-regularized" supervised approaches that simultaneously try to predict the label for a data point and remove domain specific information. This is done with either MMD <ref type="bibr" target="#b11">(Long &amp; Wang, 2015;</ref><ref type="bibr" target="#b14">Tzeng et al., 2014</ref>) or adversarial ( <ref type="bibr" target="#b4">Ganin et al., 2015)</ref> penalties at the hidden layers of the network. In our model however the main "domain-regularizer" stems from the independence properties of the prior over the domain and latent representations. We also employ MMD on our model but from a different perspective since we consider a slightly more difficult case where the domain s and label y are correlated; we need to ensure that we remain as "invariant" as possible since q φ (y|z 1 ) might 'leak' information about s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduce the Variational Fair Autoencoder (VFAE), an extension of the semi-supervised variational autoencoder in order to learn representations that are explicitly invariant with respect to some known aspect of a dataset while retaining as much remaining information as possible. We further use a Maximum Mean Discrepancy regularizer in order to further promote invariance in the posterior distribution over latent variables. We apply this model to tasks involving developing fair classifiers that are invariant to sensitive demographic information and show that it produces a better tradeoff with respect to accuracy and invariance. As a second application, we consider the task of domain adaptation, where the goal is to improve classification by training a classifier that is invariant to the domain. We find that our model is competitive with recently proposed adversarial approaches. Finally, we also consider the more general task of learning invariant representations. We can observe that our model provides a clear improvement against a neural network that incorporates a Maximum Mean Discrepancy penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DISCRIMINATION METRICS</head><p>The Discrimination metric ( <ref type="bibr" target="#b16">Zemel et al., 2013</ref>) and the Discrimination metric that takes into account the probabilities of the correct class are mathematically formalized as:</p><formula xml:id="formula_15">Discrimination = N n=1 I[y s=0 n ] N s=0 − N n=1 I[y s=1 n ] N s=1 Discrimination prob. = N n=1 p(y s=0 n ) N s=0 − N n=1 p(y s=1 n ) N s=1</formula><p>where I[y s=0 n ] = 1 for the predictions y n that were done on the datapoints with nuisance variable s = 0, N s=0 denotes the total amount of datapoints that had nuisance variable s = 0 and p(y s=0 n ) denotes the probability of the prediction y n for the datapoints with s = 0. For the predictions and their respective probabilities we used a Logistic Regression classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROXY A-DISTANCE (PAD) FOR AMAZON REVIEWS DATASET</head><p>Similarly to <ref type="bibr" target="#b4">Ganin et al. (2015)</ref>, we also calculated the Proxy A-distance (PAD) <ref type="bibr" target="#b0">(Ben-David et al., 2007;</ref> scores for the raw data x and for the z 1 representations of VFAE. Briefly, Proxy Adistance is an approximation to the H-divergence measure of domain distinguishability proposed in <ref type="bibr" target="#b6">Kifer et al. (2004)</ref> and <ref type="bibr" target="#b0">Ben-David et al. (2007;</ref>. To compute it we first need to train a learning algorithm on the task of discriminating examples from the source and target domain. Afterwards we can use the test error of that algorithm in the following formula:</p><formula xml:id="formula_16">PAD() = 2(1 − 2)</formula><p>It is clear that low PAD scores correspond to low discrimination of the source and target domain examples from the classifier. To obtain for our model we used Logistic Regression. The resulting plot can be seen in <ref type="figure" target="#fig_6">Figure 6</ref>, where we have also added the plot from DANN ( <ref type="bibr" target="#b4">Ganin et al., 2015)</ref>, where they used a linear Support Vector Machine for the classifier, as a reference. It can be seen that our VFAE model can factor out the information about s better, since the PAD scores on our new representation are, overall, lower than the ones obtained from the DANN architecture.  </p><formula xml:id="formula_17">B →D B →E B →K D →B D →E D →K E →B E →D E →K K →B K →D K →E</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: Unsupervised model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Fair classification results. Columns correspond to each evaluation scenario (in order): Random/RF/LR accuracy on s, Discrimination/Discrimination prob. against s and Random/Model accuracy on y. Note that the objective of a "fair" encoding is to have low accuracy on S (where LR is a linear classifier and RF is nonlinear), low discrimination against S and high accuracy on Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: t-SNE (van der Maaten, 2013) visualizations from the Adult dataset on: (a): original x , (b): latent z 1 without s and MMD, (c): latent z 1 with s and without MMD, (d): latent z 1 with s and MMD. Blue colour corresponds to males whereas red colour corresponds to females.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: t-SNE (van der Maaten, 2013) visualizations of the Extended Yale B training set. (a): original x , (b): latent z 1 from VFAE. Each example is plotted with the person ID and the image. Zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Proxy A-distances (PAD) for the Amazon reviews dataset: left from our VFAE model, right from the DANN model (taken from Ganin et al. (2015))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on the Extended Yale B dataset. We also included the best result from Li et al. 
(2014) under the NN + MMD row. 

Method 
S 
Y 
RF 
LR 
Original x 
0.952 0.961 0.78 
NN + MMD -
-
0.82 
VFAE 
0.435 0.565 0.846 

4 RELATED WORK 

Most related to our "fair" representations view is the work from Zemel et al. (2013). They proposed a 
neural network based semi-supervised clustering model for learning fair representations. The idea is 
to learn a localised representation that maps each datapoint to a cluster in such a way that each cluster 
gets assigned roughly equal proportions of data from each group in s. Although their approach was 
successfully applied on several datasets, the restriction to clustering means that it cannot leverage the 
representational power of a distributed representation. Furthermore, this penalty does not account 
for higher order moments in the latent distribution. For example, if p(z k = 1|x i , s = 0) always </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhixiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lempitsky</forename></persName>
		</author>
		<title level="m">Adversarial Training of Neural Networks. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting change in data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth international conference on Very large data bases</title>
		<meeting>the Thirtieth international conference on Very large data bases</meeting>
		<imprint>
			<publisher>VLDB Endowment</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5244</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Learning unbiased features. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.3474" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3474</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
<note type="report_type">Barnes-Hut-SNE. ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fastmmd: Ensemble of circular discrepancy for efficient two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
