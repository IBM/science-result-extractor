<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Quality Estimation of Grammatical Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31 -November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
							<email>shamil@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NUS Graduate School for Integrative Sciences and Engineering</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Quality Estimation of Grammatical Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2528" to="2539"/>
							<date type="published">October 31 -November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2528</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Grammatical error correction (GEC) systems deployed in language learning environments are expected to accurately correct errors in learners&apos; writing. However, in practice, they often produce spurious corrections and fail to correct many errors, thereby misleading learners. This necessitates the estimation of the quality of output sentences produced by GEC systems so that instructors can selectively intervene and re-correct the sentences which are poorly corrected by the system and ensure that learners get accurate feedback. We propose the first neural approach to automatic quality estimation of GEC output sentences that does not employ any hand-crafted features. Our system is trained in a supervised manner on learner sentences and corresponding GEC system outputs with quality score labels computed using human-annotated references. Our neural quality estimation models for GEC show significant improvements over a strong feature-based baseline. We also show that a state-of-the-art GEC system can be improved when quality scores are used as features for re-ranking the N-best candidates.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of automatically correcting various kinds of errors in written text, termed as grammatical error correction (GEC), is primarily aimed at assisting language learning and providing corrective feedback to second-language learners. GEC systems are expected to give precise corrections and have the ability to correct most learner mistakes. In reality, however, this is not the case. State-of-the-art GEC systems <ref type="bibr">Chollampatt and Ng, 2018</ref>) have a precision below 70% and a recall around 40% when evaluated on benchmark datasets. This level of performance is impressive since GEC is a difficult task given the diversity and complexity of language errors. However, in real-world use cases such as language learning, erroneous feedback from automatic GEC systems can potentially mislead language learners. To prevent this, the instructor can intervene and re-correct the system's corrections when necessary, before they are provided as feedback to learners. Having quality estimates for the system's output sentences can help instructors to decide whether to check and fix the system's corrections (for higher quality corrections) or to ignore the system's corrections altogether and recorrect the original learner-written sentences (for lower quality ones) instead. This can significantly make the process of post-editing easier and faster. Such quality estimates can also directly help end users -the language learners -to decide on the extent to which the system's corrections can be trusted and seek assistance from instructors and other sources to get better corrective feedback if needed. In this paper, we propose a neural approach to automatic quality estimation of GEC output.</p><p>Quality of language output applications can refer to several aspects such as fluency, grammaticality, adequacy, and post-editing effort. While reference-based metrics such as MaxMatch or M 2 (Dahlmeier and Ng, 2012) and GLEU ( <ref type="bibr" target="#b13">Napoles et al., 2016a</ref><ref type="bibr" target="#b12">Napoles et al., , 2015</ref>) are used to evaluate GEC systems with human-annotated references, a few reference-less GEC metrics have been proposed to evaluate fluency, grammaticality, and adequacy ( <ref type="bibr">Napoles et al., 2016b;</ref><ref type="bibr" target="#b0">Asano et al., 2017;</ref><ref type="bibr">Choshen and Abend, 2018b</ref>). However, there has been no work in GEC addressing the estimation of post-editing effort. Also, to our knowledge, this is the first supervised approach to quality estimation (QE) for GEC system outputs, similar to the supervised QE task in machine translation (MT) ( <ref type="bibr" target="#b23">Specia et al., 2009)</ref>. Our neural models for GEC QE are based on variants of the predictor-estimator architecture ( <ref type="bibr" target="#b5">Kim et al., 2017a)</ref>, where knowledge from a pre-trained network for a word-prediction task is transferred to another network that estimates the quality score. Apart from re-implementing the recurrent predictor-estimator models, we propose convolutional variants that are faster to train and run. We release our source code 1 publicly.</p><p>In summary, the contributions of this paper are: (1) we propose the first supervised approach to QE of GEC system outputs, (2) we present neural QE models that outperform a strong feature-based baseline for estimating post-editing effort and an automatic GEC evaluation metric, (3) we propose new convolutional neural architectures for QE that can be potentially utilized for QE tasks in other language applications, and (4) we show that the performance of a state-of-the-art GEC system can be improved by adding QE scores as features in re-ranking the N-best candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The task of quality estimation became popular in machine translation (MT) through the studies by <ref type="bibr">Blatz et al. (2004)</ref> and <ref type="bibr" target="#b23">Specia et al. (2009)</ref>. Much of the later work in QE of MT was through the shared tasks in Workshop on Machine Translation (WMT) campaigns <ref type="bibr">(Bojar et al., 2016b</ref>) from 2012 onwards <ref type="bibr">(Callison-Burch et al., 2012)</ref>. Supervised methods of quality assessment have been applied to other natural language processing tasks such as text simplification <ref type="bibr">( ˇ Stajner et al., 2016)</ref>, language generation <ref type="bibr">(Dušek et al., 2017)</ref>, and in assisting interpreters ( <ref type="bibr" target="#b26">Stewart et al., 2018)</ref>.</p><p>In the context of GEC, <ref type="bibr" target="#b3">Heilman et al. (2014)</ref> attempted to predict grammaticality of learner sentences using regression with a variety of linguistic features such as the number of misspellings, language model scores, etc. They use a dataset of learner sentences manually annotated with subjective scores of grammaticality. However, their method was to assess learner writing and not for system evaluation. To evaluate GEC systems, <ref type="bibr" target="#b12">Napoles et al. (2015)</ref> developed reference-less metrics known as grammaticality-based metrics or GBMs. GBM scores are based on the number of errors detected using third-party tools or determined by a grammaticality prediction model <ref type="bibr" target="#b3">(Heilman et al., 2014</ref>). Their method ignores the source sentence completely and judges the system 1 https://github.com/nusnlp/neuqe outputs independently for grammaticality. <ref type="bibr" target="#b0">Asano et al. (2017)</ref> improved their method to account for fluency as well as faithfulness to the source sentence. <ref type="bibr">Choshen and Abend (2018b)</ref> provide another measurement for meaning preservation using a semantic annotation scheme. Contrary to prior work in GEC reference-less evaluation, our work is aimed at estimating post-editing effort in terms of translation error rate ( <ref type="bibr" target="#b18">Snover et al., 2006</ref>) and an automatic evaluation metric, <ref type="bibr">Max- Match (Dahlmeier and Ng, 2012)</ref>, in a supervised approach. We propose variants of the predictorestimator architecture <ref type="bibr" target="#b7">(Kim and Lee, 2016b;</ref><ref type="bibr" target="#b5">Kim et al., 2017a)</ref> and compare them to a competitive feature-based baseline, QuEst ( <ref type="bibr" target="#b22">Specia et al., 2013</ref><ref type="bibr" target="#b20">Specia et al., , 2015</ref>) that has been successfully used for a number of language pairs in MT QE and for other applications ( <ref type="bibr" target="#b26">Stewart et al., 2018)</ref>. It has also been the baseline for WMT QE tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Quality Estimation of GEC</head><p>Quality estimation (QE) of GEC can be defined as the task of estimating a quality scorê q given a source sentence S and its corresponding GEC system-corrected hypothesis, H. We formulate the GEC QE task as a supervised regression task to predict the quality scores, following the MT QE approach ( <ref type="bibr" target="#b23">Specia et al., 2009</ref>). The score is estimated using a trained regression model f with parameters θ, such thatˆqthatˆ thatˆq = f (S, H, θ). The model f is trained and evaluated by utilizing a set of learner-written sentences and their corresponding corrected hypotheses produced by a "blackbox" GEC system, i.e., neither the GEC system's model scores nor internal states will be known to the QE system. The gold-standard quality scores are obtained by comparing the system-corrected sentences and human-corrected references. We are primarily interested in estimating the post-editing effort for correcting the output sentences. Similar to MT QE, we assess GEC post-editing effort scores using human-targeted translation error rate or HTER ( <ref type="bibr" target="#b18">Snover et al., 2006</ref>). HTER is the minimum number of edit operations (insertions, deletions, substitutions, or shifts of word sequences) needed to transform the hypothesis sentence to the reference sentence, normalized by the length of the reference. A low HTER score indicates less post-editing effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HTER =</head><p>number of edits number of reference tokens In MT, the reference translations for HTER are targeted, i.e., they are created by post-editing system translated sentences. However, in GEC, highquality datasets annotated by experts with minimal edits are available <ref type="bibr">(Dahlmeier et al., 2013;</ref><ref type="bibr" target="#b30">Yannakoudakis et al., 2011</ref>) and GEC systems are typically trained to make minimal changes to input sentences. Hence, the actual human annotated references can be substituted for post-edited references of output sentences 2 . We also experiment with estimating an automatic GEC evaluation metric, MaxMatch or M 2 ( <ref type="bibr">Dahlmeier and Ng, 2012</ref>) as the quality score. M 2 is the most widely used GEC evaluation metric that computes the F 0.5 -score of phrase-level edits made by a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Quality Estimation Model</head><p>Our neural quality estimation (NQE) model uses the predictor-estimator architecture <ref type="bibr" target="#b5">(Kim et al., 2017a</ref>) to model the regression function f . Recurrent variants of the model have achieved the first and second places for WMT 2017 and 2016 sentence-level QE tasks, respectively <ref type="bibr" target="#b6">(Kim and Lee, 2016a;</ref><ref type="bibr" target="#b8">Kim et al., 2017b</ref>). The key idea behind the model is to employ a preliminary predictor neural network that is trained for the "word prediction" task, i.e., to predict the probabilities of the words in the target sentence given the source sentence and the remaining target context (the words in the target sentence other than the predicted word). The predictor networks are trained using large parallel texts (potentially erroneous learner sentences and their corresponding humancorrected sentences). The knowledge from the predictor network is transferred to the estimator network that is trained to estimate the quality scorêscorê q given the source sentence S and its corresponding system hypothesis H. Specifically, a pretrained predictor network takes as input S and H (in place of the target sentence) and predicts probability scores for words in H. The intuition is that hypothesis words that are likely to match the reference sentence will be assigned higher probabilities. The hidden representations from the predictor network, called quality vectors, having information about the quality of the hypothesis words, become the input to the estimator network that estimates the quality score. The estimator networks are trained using learner sentences and their corresponding GEC system-corrected hypotheses. The gold quality score is obtained by comparing a hypothesis and the corresponding human-corrected reference. We use high-quality datasets annotated minimally to train the estimator networks. Apart from re-implementing the recurrent neural network (RNN)-based predictor-estimator model, we build fully convolutional neural network (CNN)-based variants for both the predictor and the estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predictor Network</head><p>The inputs to the predictor network are the source sentence S with source tokens s 1 , ..., s m and its corresponding target sentence T with target tokens t 1 , ..., t n . The predictor networks are trained to predict each target token t j given the source sentence S and the remaining target tokens excluding the predicted target token, denoted by T −j . The output of the predictor network is a softmax probability score normalized across the target vocabulary, V t :</p><formula xml:id="formula_0">p(t|S, T −j ) = exp(o j,t ) t ∈Vt exp(o j,t )</formula><p>where o j,t is the node corresponding to the word t in the predictor's output vectors o j ∈ R |Vt| , when t j is predicted (see <ref type="figure" target="#fig_0">Figure 1</ref>). Predictor networks estimate the output probability by an architecture that extends the encoder-decoder neural network for sequence-to-sequence translation ( <ref type="bibr">Bahdanau et al., 2015)</ref>. Traditional encoder-decoder models use a bidirectional RNN on the source sentence and a forward RNN on the target side to capture the target context preceding the predicted target word. The predictor network additionally employs another backward RNN in the decoder to capture the target context following the predicted word <ref type="bibr" target="#b7">(Kim and Lee, 2016b</ref>). In the case of QE, the entire target sentence is available as input, unlike the case of sequence-to-sequence translation. Predictor networks are originally based on RNNs with gated recurrent units or GRUs ( <ref type="bibr">Cho et al., 2014</ref>) and a soft-attention mechanism similar to that in ( <ref type="bibr">Bahdanau et al., 2015</ref>). We use separate attention mechanisms for the forward and backward RNNs of the decoder in our implementation of the predictor network. Due to the recent success of multilayer convolutional encoder-decoder neural networks for MT <ref type="bibr">(Gehring et al., 2017)</ref> and subsequently for GEC <ref type="bibr">(Chollampatt and Ng, 2018</ref>) that enables better capturing of the local context, we create a multilayer convolutional variant of the predictor network. Using CNNs also helps in efficient parallelization and improves training and inference speed as shown in <ref type="bibr">(Gehring et al., 2017)</ref>. We use a similar architecture, which is explained in detail in <ref type="bibr">(Chollampatt and Ng, 2018)</ref>. In addition, analogous to the backward RNN in the decoder, we use a secondary CNN mechanism in each decoder layer for the convolutional predictor to capture the target words following the predicted target word. The first CNN uses k − 1 pre-paddings (paddings at the beginning), where k is the convolutional kernel width. This ensures that the decoder state corresponding to the previous target word does not include the target word to be predicted in its computation. For the same reason, the secondary CNN uses k − 1 post-paddings (paddings at the end). The convolutional predictor uses separate multistep attention mechanisms <ref type="bibr">(Gehring et al., 2017)</ref> for both the CNNs in each decoder layer. Additionally, for the prediction of the target word t j , the nearby target embeddings t j−1 and t j+1 are also used with a maxout non-linearity ( <ref type="bibr">Bahdanau et al., 2015)</ref> as done in the RNN-based predictor. The predictor network is trained to minimize the negative log-likelihood loss of the target words, similar to the neural language modeling objective ( <ref type="bibr">Bengio et al., 2003</ref>). The overall architecture of a predictor model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Quality Vectors: While training and testing the estimator, the internal hidden representations from the predictor for every hypothesis word, termed as quality vectors, are used as inputs to the estimator network. Specifically, we use the "pre-prediction" quality vectors in ( <ref type="bibr" target="#b5">Kim et al., 2017a</ref>), which performed the best for our GEC QE task. The quality vector q j ∈ R h corresponding to the hypothesis word t j is given by q j = h j • w t j where h j ∈ R h is the final hidden vector after the maxout layer <ref type="figure" target="#fig_0">(Figure 1</ref>), w t j is the column vector corresponding to the target word t j in the final linear transformation matrix W ∈ R h×|Vt| <ref type="figure" target="#fig_0">(Figure 1</ref>) that projects h j to the size of the target vocabulary V t , and • represents element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimator Network</head><p>The estimator network takes the quality vectors ( §4.1) as input and quality scores as labels during training. Our re-implementation of the estimator network in <ref type="bibr" target="#b8">Kim et al. (2017b)</ref> uses a bidirectional recurrent network with GRU cells to aggregate the quality vectors. The concatenated final states of the forward and the backward RNNs (with GRU cells) are used as the aggregated summary vector, which is projected to a scalar value using an affine transformation and clipped to the range between 0 and 1 using a sigmoid function.</p><p>We also propose a variant of the estimator networks using CNNs that achieves faster training and inference, and performs competitively. CNNs help to aggregate local quality statistics around quality vectors, thereby identifying sequences of words that have a higher or lower quality. In our proposed convolutional estimator model ( <ref type="figure" target="#fig_1">Figure  2</ref>), the quality vectors q 1 , ..., q n are transformed to q 1 , ..., q n where q j ∈ R h and h is the size of the hidden layer of the estimator. The transformed quality vectors are fed to a convolutional neural network with kernel width k q and h filters, followed by the rectified linear units or ReLU (Nair and Hinton, 2010) operation. Sufficient paddings are added on the left and right to retrieve back the same number of output vectors as the input vectors. The input vectors are added to the output vectors as residual connections. The resulting vector after these operations over a single convolutional window around q j , denoted by u j ∈ R h , is given by</p><formula xml:id="formula_1">u j = ReLU Conv(q j− kq 2 , · · · , q j+ kq 2 ) +q j u 1 , .</formula><p>.., u j , ..., u n are aggregated into a summary vector u ∈ R h using a weighted pooling based on attention weights α j for each u j :</p><formula xml:id="formula_2">α j = exp(v e u j ) n k=1 exp(v e u k ) u = n j=1 α j u j</formula><p>where v e ∈ R h is a trainable parameter. The summary vector u is then fed through another affine transformation with weights W u ∈ R h ×h and biases b u ∈ R h followed by ReLU resulting in the output vector u . The quality scorê q is computed by projecting u to a scalar value using an affine transformation with weights W q ∈ R h ×1 and bias b q ∈ R followed by a sigmoid operation σ to limit the score to between 0 and 1.</p><formula xml:id="formula_3">ˆ q = σ(u W q + b q )</formula><p>The network is trained using mean square error (MSE) as the loss function. We use the pre-trained predictor model only to generate input vectors to the estimator. The predictor parameters are not updated while training the estimator. We apply dropout ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>) to both the RNNbased and CNN-based estimator networks on the inputs to each layer during training. For the CNNbased predictor and estimator, learning is stabilized using strategies in <ref type="bibr">(Gehring et al., 2017</ref>) such as the initialization and weight normalization of CNNs and controlling the variance of activations after residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GEC System</head><p>The data to train and evaluate the NQE models require GEC system-generated hypotheses. For this, we train a single multilayer convolutional neural network GEC model initialized with pretrained embeddings following <ref type="bibr">(Chollampatt and Ng, 2018</ref>) on Lang-8 corpus only, following the same pre-processing method with 5,000 sentence pairs set aside for validation. The remaining data consists of 2.15M sentence pairs (25.47M source tokens and 28.94M target tokens). For training the model, we use only the annotated sentence pairs after sub-word segmentation (1.28M sentence pairs with 18.50M source sub-words and 21.88 target sub-words). During decoding, a beam width of 12 is used and the top candidate is chosen without any re-scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets</head><p>For training the QE models, we use sentences from the NUS Corpus of Learner English or NUCLE ( <ref type="bibr">Dahlmeier et al., 2013)</ref> and sentences from the training scripts of the Cambridge Learner Corpus-First Certificate Examination 3 (FCE) (Yannakoudakis et al., 2011) and their corresponding hypotheses generated by the GEC system described in §5.1. We use sentences from the FCE development set and CoNLL-2013 ( <ref type="bibr" target="#b16">Ng et al., 2013</ref>) test set and their GEC system-generated hypotheses as our development data. We separately test on two datasets, the FCE test set and the CoNLL-2014 test set ( <ref type="bibr" target="#b15">Ng et al., 2014</ref>). The statistics of the datasets are given in <ref type="table">Table 1</ref>. The gold-standard scores are obtained by computing HTER using TERp ( <ref type="bibr" target="#b19">Snover et al., 2009</ref>) and sentence-level M 2 using the MaxMatch scorer with the GEC hypotheses and the reference sentences src. words hyp. <ref type="table" target="#tab_0">words   Train  86,293 1,614,120 1,620,399  Development  3,635  63,782  63,890  FCE (test)  2,769  41,457  41,531  CoNLL-2014  1,312  30,144  30,109   Table 1</ref>: Statistics of the datasets used for QE.</p><p>sentences. When multiple references are available, the gold-standard score is chosen to be the best score (lowest HTER or highest M 2 ) among the scores computed against each reference separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">NQE Models and Training</head><p>We build variants of the NQE models denoted by NQE XY , where X indicates the predictor architecture and Y indicates the estimator architecture. X and Y can be recurrent (R) or convolutional (C), of which NQE RR is our replication of ( <ref type="bibr" target="#b5">Kim et al., 2017a</ref>). The predictor models are trained and validated using parallel data from Lang-8 with 2.15M and 5000 sentence pairs, respectively (described in §5.1). For the predictor models, we use a source and target vocabulary size of 30,000 words, with 500-dimensional source and target embeddings, and 700-dimensional hidden layer. For our convolutional predictor model, a kernel width of 3 and 7 encoder and decoder layers are used. We train the predictor network with ADADELTA optimizer (Zeiler, 2012) with a batch size of 64. We clip gradients by their 2 -norm with a threshold of 5.0 ( <ref type="bibr" target="#b17">Pascanu et al., 2013</ref>). The estimator networks, both recurrent and convolutional variants, use a hidden layer dimension of 100. They are trained with the Adam optimizer ( <ref type="bibr" target="#b9">Kingma and Ba, 2015</ref>) with an initial learning rate of 0.0005 and batch size of 32. We use dropout with a probability of 0.5 during training. Our final model is NQE ALL , which averages the estimated scores of all variants, namely, NQE RR , NQE CR , NQE CC , and NQE RC .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baselines</head><p>We use two non-neural baselines for comparison to our neural QE models. AVERAGE: The average score of training sentences is used as the estimated score for all test sentences. QUEST: We use the standard 17 sentence-level features in QuEst++ ( <ref type="bibr" target="#b20">Specia et al., 2015</ref>) which has been used as the baseline for WMT QE tasks from 2012 to 2017. The features are based on word-level statistics from the source-hypothesis sentence pairs, and statistics and features from language and lexical translation models trained using a parallel corpus. The descriptions of the 17 features can be found in ( <ref type="bibr">Bojar et al., 2017)</ref>. We use the Lang-8 corpus to train the language and translation models for QuEst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Evaluation</head><p>We evaluate primarily using the Pearson's correlation coefficient (PCC) metric following the recommendations in <ref type="bibr" target="#b1">(Graham, 2015)</ref> and the recent WMT shared tasks ( <ref type="bibr">Bojar et al., 2016a</ref><ref type="bibr">Bojar et al., , 2017</ref>. It is shown in <ref type="bibr" target="#b1">(Graham, 2015</ref>) that aggregates of gold score distributions are easier to predict and metrics such as mean absolute error (MAE) and root mean square error (RMSE) over-estimate systems that predict the aggregates accurately despite these systems performing poorly on tail ends of the distribution (higher quality and lower quality samples). PCC does not suffer from this weakness. We use William's Test <ref type="bibr" target="#b29">(Williams, 1959)</ref> following <ref type="bibr" target="#b1">(Graham, 2015)</ref> to assess the significance of the improvements. However, we also report the root mean square error (RMSE) which reflects the estimator's loss and shows the deviation from the AVERAGE baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Estimating Post-Editing Effort</head><p>We compare the QE models in terms of their performance in estimating the post-editing effort (HTER). The results, including those of the significance tests, are shown in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Estimating M 2 Score</head><p>We use NQE models to estimate the MaxMatch (M 2 ) GEC evaluation metric at the sentence-level, which computes F 0.5 based on phrase-level edits. Results are shown in <ref type="table" target="#tab_2">Table 3</ref>. All models significantly outperform the baseline QuEst on FCE (p &lt; 0.01) test set. NQE ALL is significantly better than all other systems except NQE CC on FCE (p &lt; 0.01). The PCC on CoNLL-2014 turns out to be much lower for all systems with the NQE models not significantly better than the baseline. Estimating M 2 appears to be more difficult compared to estimating post-editing effort with HTER scores. This could be because M 2 is a phraselevel measure with phrase-boundaries determined by matching with gold annotations, unlike HTER which is a token-level evaluation measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Improving GEC Performance</head><p>We use the estimated sentence-level M 2 scores as features to improve the performance of downstream GEC by using them as an additional feature during re-scoring the N-best candidates from a high-performing GEC baseline. Our GEC baseline is built on the multilayer convolutional architecture initialized with pre-trained embeddings and re-scoring <ref type="bibr">(Chollampatt and Ng, 2018)</ref>. We use the same hyper-parameter settings. The baseline GEC system consists of an ensemble of 3 sets of 4 models each. The first set consists of the 4 models released by <ref type="bibr">Chollampatt and Ng (2018)</ref>. The second set of 4 models is trained using a label-smoothed cross entropy loss function ( <ref type="bibr" target="#b27">Szegedy et al., 2016</ref>) which has been found to be effective in neural machine translation <ref type="bibr" target="#b28">(Vaswani et al., 2017;</ref><ref type="bibr">Edunov et al., 2018</ref>). We use a smoothing parameter of 0.1 following <ref type="bibr" target="#b28">Vaswani et al. (2017)</ref>. The third set of 4 models consists of high-recall models that make use of three techniques proposed by : <ref type="formula">(1)</ref> pre-training decoder parameters (2) source word dropout, and (3) edit-weighted negative log-likelihood. The parameters of the decoder are initialized using the parameters from a pretrained neural language model (NLM) of the same architecture as our decoder except for the attention mechanism. We train this NLM using 100 million sentences (1.42 billion words) from the Common Crawl corpora released by <ref type="bibr">Buck et al. (2014)</ref> for one epoch. We use the reported hyper-parameters  for the other two techniques. The 12-best candidates produced by this ensemble are then re-scored using edit operations and language model features following <ref type="bibr">Chol- lampatt and Ng (2018)</ref>.</p><p>The performance of the baseline system (Base GEC) in terms of document-level F 0.5 computed by M 2 scorer on the FCE and CoNLL-2014 test sets is reported in <ref type="table" target="#tab_4">Table 4</ref>. When we use the development set used by Chollampatt and Ng (2018) consisting of 5.4k sentences from NU-CLE to train the re-scorer, Base GEC system achieves a competitive performance compared to the top GEC systems with the best-published results on CoNLL-2014 test set: G&amp;J (Grundkiewicz and Junczys-Dowmunt, 2018), JGGH (Junczys- , and C&amp;N (Chollampatt and Ng, 2018). When we make use of the spelling error correction system (+SpellCheck) proposed by <ref type="bibr">Chollampatt and Ng (2017)</ref>, which is also used by G&amp;J and C&amp;N, our baseline achieves the highest reported F 0.5 on the CoNLL-2014 test set (56.43) when trained on public corpora alone. To the Base GEC system, we add the sentencelevel M 2 scores estimated by the final NQE model (NQE ALL ) as a feature in the re-scorer. Since our NQE models use NUCLE during training, we use our development set consisting of 3.6k sentences from FCE and CoNLL-2013 to re-train the rescorer instead of sentences from NUCLE so that the feature weights will not be biased. We observe a slight drop in performance upon retraining, potentially due to the fewer number of sentences and error annotations in this new development set. The added feature scores are also in the logarithmic scale, similar to LM and the encoder-decoder model score. When the estimated M 2 score is added, we find a significant improvement of 1.18 F 0.5 on the FCE test set and a significant improvement of 0.25 F 0.5 score on the CoNLL-2014 test set (p &lt; 0.001). Significance testing is done using sign test by bootstrap re-sampling <ref type="bibr" target="#b10">(Koehn, 2004</ref>) with 100 samples. The smaller margin of improvement on CoNLL-2014 is expected due to the low PCC values <ref type="table" target="#tab_2">(Table 3)</ref>. When we add spelling error correction, the results reach 48.70 F 0.5 score on FCE and 56.52 F 0.5 score on CoNLL-2014. However, the results obtained by training the rescorer with our development set (FCE+CoNLL) and adding the NQE models should not be directly compared to the top systems <ref type="bibr">(G&amp;J, JGGH, and FCE CoNLL-2014</ref> Best published results G&amp;J <ref type="formula">(2018)</ref>   C&amp;N) as they do not make use of the FCE data.</p><p>We also re-score using oracle sentence-level M 2 scores instead of the NQE estimated scores. We find that GEC performance can reach up to 80.74 F 0.5 for CoNLL-2014 and 76.70 F 0.5 on FCE. This shows that improving automatic QE can substantially improve downstream GEC simply via rescoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Analysis</head><p>Our results show that the NQE models perform better than feature-based baselines for QE of GEC. The crucial component of the NQE model that enables it to make better score estimates is the predictor network whose internal representations (quality vectors) are used as input to the estimator. The sum of nodes of a quality vector corresponds to the output node of the predictor network for a particular target word, and a softmax operation across all vocabulary words results in the predicted probability value. In <ref type="figure" target="#fig_3">Figure 3</ref>, we analyze the probability outputs by our convolutional predictor network for four GEC hypotheses for a source sentence 'We are all looking forward for you answer .'. Hypothesis 1 is the source sentence itself and the predictor has rightly identified the location of error by giving a low probability   score to the erroneous preposition 'for' (0.038). In Hypothesis 2, which also matches the actual human-annotated reference, the phrase 'for you' is replaced with 'to your'. The correct preposition 'to' gets a higher probability score. In Hypothesis 3, where a less suitable word 'the' is used, a lower probability score (0.003) is assigned compared to the word 'your' (0.322) in Hypothesis 2, despite Hypothesis 3 being grammatically correct. This indicates that the predictor rightly considers the faithfulness to the source sentence as well. When we analyze Hypothesis 4, which is grammatically correct and also faithful to the source, the probabilities of all words are much higher. Note that this hypothesis does not match the human annotated reference (Hypothesis 2). It is impractical to have human-annotated references that cover all possible corrections for all source sentences. This issue of reference-coverage has been noted previously in GEC literature <ref type="bibr">(Bryant and Ng, 2015;</ref><ref type="bibr">Napoles et al., 2016b;</ref><ref type="bibr">Choshen and Abend, 2018a</ref>). This example shows that QE systems can potentially address this issue, similar to the reference-less evaluation measures for GEC.</p><p>We study if the estimator networks are able to count edits, which is the basis of estimating HTER. To do this, we take an example sentence of 14 tokens: 'It will be incredible if we have a chance to watch the show .' as the source and the hypothesis as well as the reference. We substitute tokens one by one with an arbitrary token 'X', thereby increasing HTER linearly. <ref type="figure">Figure 4</ref> shows the performance of the NQE models compared to true HTER scores (straight line). We find that with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We propose the first supervised approach to quality estimation (QE) for GEC system outputs. We propose several neural QE model variants that perform significantly better than feature-based baselines in estimating the post-editing effort of GEC output sentences. We also show that the QE variants perform reasonably well on a more difficult task of estimating quality in terms of a GEC evaluation metric, M 2 , by showing that the estimated scores are useful in improving GEC performance via N-best re-scoring. In future, the general framework of QE for GEC can be used to train on subjective human rankings of hypotheses as well, so that the system can learn the intuitions underlying human judgments of quality instead of estimating a pre-defined measure such as HTER or M 2 .</p><p>Christopher </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of a predictor model. For our proposed CNN-based variant multiple layers of CNNs are stacked (only one is shown in the figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed convolutional estimator neural network with attention-based pooling and residual connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Hypothesis 2 :</head><label>2</label><figDesc>Grammatically correct and matches the human-annotated reference. Hypothesis 3: Grammatically correct, but not faithful to the source sentence. Hypothesis 4: Grammatically correct and faithful to the source sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Probabilities predicted by the convolutional predictor for different GEC hypotheses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>On the FCE 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of the NQE models in estimating post-editing effort (HTER). The matrices on the right 
show the results of the William's significance tests. A dark green cell indicates the system labeled on the 
row significantly outperforms the system labeled on the column (p-values shown in the color bar). 

FCE 
CoNLL-2014 

PCC↑ RMSE↓ PCC↑ RMSE↓ 

AVERAGE 
-
0.4529 
-
0.4302 
QUEST 
0.2506 0.4585 0.2182 0.4129 
NQE RR 
0.3594 0.4235 0.2100 0.3970 
NQE CR 
0.4066 0.4153 0.1992 0.4104 
NQE CC 
0.4129 0.4123 0.2017 0.4038 
NQE RC 
0.4028 0.4158 0.2104 0.4014 
NQE ALL 0.4186 0.4106 0.2210 0.3999 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of the NQE models for sentence-
level M 2 estimation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance (in terms of F 0.5 in %) when 
NQE-estimated sentence-level M 2 scores are used 
as features in re-scoring.  *  indicates statistically 
significant improvement compared to Base GEC 
(p &lt; 0.001). 

</table></figure>

			<note place="foot" n="2"> Since we use the original annotations instead of human post-edits as reference corrections, our HTER scores are the same as TER scores. Nevertheless, we use the term HTER for consistency with the QE task in MT.</note>

			<note place="foot" n="3"> The file IDs of training, development, and testing scripts of FCE are obtained from the FCE dataset for error detection at https://www.ilexir.co.uk/datasets/index.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving evaluation of machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Near human-level performance in grammatical error correction with hybrid machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin Junczys-Dowmunt</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting grammaticality on an ordinal scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mulholland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approaching neural grammatical error correction as a low-resource machine translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubha</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predictorestimator: Neural quality estimation based on target word prediction for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hun-Young</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyeok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Hoon</forename><surname>Na</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian and Low-Resource Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural network based translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A recurrent neural networks approach for estimating the quality of machine translation output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predictor-estimator using multilevel task learning with stack propagation for neural quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Hyeok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Hoon</forename><surname>Na</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Learning Representations</title>
		<meeting>the Third International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ground truth for grammatical error correction metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02592</idno>
		<title level="m">GLEU without tuning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2016b. There&apos;s no comparison</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The CoNLL-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The CoNLL-2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Machine Translation</title>
		<meeting>the Fourth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-level translation quality prediction with QuEst++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">System Demonstrations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">QuEst -a translation quality estimation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">G C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating the sentence-level quality of machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of European Association for Machine Translation</title>
		<meeting>the 13th Conference of European Association for Machine Translation</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Quality estimation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Sanjaštajnersanjaˇsanjaštajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Popovi´cpopovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Béchara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2016 Workshop and Shared Task on Quality Assessment for Text Simplification</title>
		<meeting>the LREC 2016 Workshop and Shared Task on Quality Assessment for Text Simplification</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic estimation of simultaneous interpreter performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Vogler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boydgraber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Regression Analysis. Wiley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">James</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">ADADELTA: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
