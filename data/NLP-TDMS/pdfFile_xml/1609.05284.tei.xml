<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReasoNet: Learning to Stop Reading in Machine Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
							<email>yeshen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98053</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
							<email>pshuang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98053</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98053</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98053</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReasoNet: Learning to Stop Reading in Machine Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3097983.3098177</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Reading Comprehension</term>
					<term>Deep Reinforcement Learning</term>
					<term>ReasoNet</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to eeectively exploit and then reason over the relation among queries, documents, and answers. Diierent from previous approaches using a xed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Teaching machines to read, process, and comprehend natural language documents is a coveted goal for artiicial intelligence <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref>. Genuine reading comprehension is extremely challenging, since eeective comprehension involves thorough understanding of documents and sophisticated inference. Toward solving this machine reading comprehension problem, in recent years, several works have collected various datasets, in the form of question, passage, and answer, to test machine on answering a question based on the provided passage <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Some large-scale cloze-style datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> have gained signiicant attention along with powerful deep learning models.</p><p>Recent approaches on cloze-style datasets can be separated into two categories: single-turn and multi-turn reasoning. Single turn reasoning models utilize attention mechanisms <ref type="bibr" target="#b0">[1]</ref> to emphasize speciic parts of the document which are relevant to the query.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciic permission and/or a fee. Request permissions from permissions@acm.org. KDD'17, <ref type="bibr">August 13-17, 2017</ref><ref type="bibr">, Halifax, NS, Canada © 2017</ref> These attention models subsequently calculate the relevance between a query and the corresponding weighted representations of document subunits (e.g. sentences or words) to score target candidates <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. However, considering the sophistication of the problem, after a single-turn comprehension, readers often revisit some speciic passage or the question to grasp a better understanding of the problem. With this motivation, recent advances in reading comprehension have made use of multiple turns to infer the relation between query, document and answer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. By repeatedly processing the document and the question after digesting intermediate information, multi-turn reasoning can generally produce a better answer and these existing works have demonstrated its superior performance consistently.</p><p>Existing multi-turn models have a pre-deened number of hops or iterations in their inference without regard to the complexity of each individual query or document. However, when human read a document with a question in mind, we often decide whether we want to stop reading if we believe the observed information is adequate already to answer the question, or continue reading after digesting intermediate information until we can answer the question with conndence. This behavior generally varies from document to document or question to question because it is related to the sophistication of the document or the diiculty of the question. Meanwhile, the analysis in <ref type="bibr" target="#b2">[3]</ref> also illustrates the huge variations in the diiculty level with respect to questions in the CNN/Daily Mail datasets <ref type="bibr" target="#b6">[7]</ref>. For a signiicant part of the datasets, this analysis shows that the problem cannot be solved without appropriate reasoning on both its query and document.</p><p>With this motivation, we propose a novel neural network architecture called Reasoning Network (ReasoNet). which tries to mimic the inference process of human readers. With a question in mind, ReasoNets read a document repeatedly, each time focusing on diierent parts of the document until a satisfying answer is found or formed. This reminds us of a Chinese proverb: "The meaning of a book will become clear if you read it hundreds of times. ". Moreover, unlike previous approaches using xed number of hops or iterations, ReasoNets introduce a termination state in the inference. This state can decide whether to continue the inference to the next turn after digesting intermediate information, or to terminate the whole inference when it concludes that existing information is sufcient to yield an answer. The number of turns in the inference is dynamically modeled by both the document and the query, and can be learned automatically according to the diiculty of the problem.</p><p>One of the signiicant challenges ReasoNets face is how to design an eecient training method, since the termination state is discrete and not connected to the nal output. This prohibits canonical back-propagation method being directly applied to train ReasoNets.</p><p>Motivated by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>, we tackle this challenge by proposing a reinforcement learning approach, which utilizes an instance-dependent reward baseline, to successfully train ReasoNets. Finally, by accounting for a dynamic termination state during inference and applying proposed deep reinforcement learning optimization method, ReasoNets achieve the state-of-the-art results in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, and the proposed structured Graph Reachability dataset, when the paper is rst publicly available on arXiv. 1 At the time of the paper submission, we apply ReasoNet to the competitive Stanford Question Answering Dataset(SQuAD), ReasoNets outperform all existing published approaches and rank at second place on the test set leaderboard. <ref type="bibr" target="#b1">2</ref> This paper is organized as follows. In Section 2, we review and compare recent work on machine reading comprehension tasks. In Section 3, we introduce our proposed ReasoNet model architecture and training objectives. Section 4 presents the experimental setting and results on unstructured and structured machine reading comprehension tasks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Recently, with large-scale datasets available and the impressive advance of various statistical models, machine reading comprehension tasks have attracted much attention. Here we mainly focus on the related work in cloze-style datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Based on how they perform the inference, we can classify their models into two categories: single-turn and multi-turn reasoning.</p><p>Single-turn reasoning: Single turn reasoning models utilize an attention mechanism to emphasize some sections of a document which are relevant to a query. This can be thought of as treating some parts unimportant while focusing on other important ones to nd the most probable answer. <ref type="bibr">Hermann et al. [7]</ref> propose the attentive reader and the impatient reader models using neural networks with an attention over passages to predict candidates. Hill et al. <ref type="bibr" target="#b7">[8]</ref> use attention over window-based memory, which encodes a window of words around entity candidates, by leveraging an endto-end memory network <ref type="bibr" target="#b21">[22]</ref>. Meanwhile, given the same entity candidate can appear multiple times in a passage, Kadlec et al. <ref type="bibr" target="#b8">[9]</ref> propose the attention-sum reader to sum up all the attention scores for the same entity. This score captures the relevance between a query and a candidate. Chen et al. <ref type="bibr" target="#b2">[3]</ref> propose using a bilinear term similarity function to calculate attention scores with pretrained word embeddings. <ref type="bibr">Trischler et al. [25]</ref> propose the EpiReader which uses two neural network structures: one extracts candidates using the attention-sum reader; the other reranks candidates based on a bilinear term similarity score calculated from query and passage representations.</p><p>Multi-turn reasoning: For complex passages and complex queries, human readers often revisit the given document in order to perform deeper inference after reading a document. Several recent studies try to simulate this revisit by combining the information in the query with the new information digested from previous iterations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. Hill et al. <ref type="bibr" target="#b7">[8]</ref> use multiple hops memory network to augment the query with new information from the previous hop. Gated Attention reader <ref type="bibr" target="#b5">[6]</ref> is an extension of the attention-sum reader with multiple iterations by pushing the query encoding into an attention-based gate in each iteration. Iterative Alternative (IA) reader <ref type="bibr" target="#b20">[21]</ref> produces a new query glimpse and document glimpse in each iteration and utilizes them alternatively in the next iteration. Cui et al. <ref type="bibr" target="#b4">[5]</ref> further propose to extend the query-speciic attention to both query-to-document attention and document-to-query attention, which is built from the intermediate results in the query-speciic attention. By reading documents and enriching the query in an iterative fashion, multi-turn reasoning has demonstrated their superior performance consistently.</p><p>Our proposed approach explores the idea of using both attentionsum to aggregate candidate attention scores and multiple turns to attain a better reasoning capability. Unlike previous approaches using a xed number of hops or iterations, motivated by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, we propose a termination module in the inference. The termination module can decide whether to continue to infer the next turn after digesting intermediate information, or to terminate the whole inference process when it concludes existing information is suucient to yield an answer. The number of turns in the inference is dynamically modeled by both a document and a query, and is generally related to the complexity of the document and the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REASONING NETWORKS</head><p>ReasoNets are devised to mimic the inference process of human readers. ReasoNets read a document repeatedly with attention on diierent parts each time until a satisfying answer is found. As shown in <ref type="figure">Figure 1</ref>, a ReasoNet is composed of the following components:</p><p>Memory: The external memory is denoted as M. It is a list of word vectors, M = {m i } i=1..D , where m i is a xed dimensional vector. For example, in the Graph Reachability, m i is the vector representation of each word in the graph description encoded by a bidirectional-RNN. Please refer to Section 4 for the detailed setup in each experiment.</p><p>Attention: The attention vector x t is generated based on the current internal state s t and the external memory M: x t = f at t (s t , M; θ x ). Please refer to Section 4 for the detailed setup in each experiment.</p><p>Internal State: The internal state is denoted as s which is a vector representation of the question state. Typically, the initial state s 1 is the last-word vector representation of query by an RNN. The t-th time step of the internal state is represented by s t . The sequence of internal states are modeled by an RNN: s t +1 = RNN(s t , x t ; θ s ), where x t is the attention vector mentioned above.</p><p>Termination Gate: The termination gate generates a random variable according to the current internal state; t t ∼ p(·| f t (s t ; θ t ))). t t is a binary random variable. If t t is true, the ReasoNet stops, and the answer module executes at time step t; otherwise the ReasoNet generates an attention vector x t +1 , and feeds the vector into the state network to update the next internal state s t +1 .</p><p>Answer:</p><p>The action of answer module is triggered when the termination gate variable is true: a t ∼ p(·| f a (s t ; θ a )).</p><p>In Algorithm 1, we describe the stochastic inference process of a ReasoNet. The process can be considered as solving a Partially Observable Markov Decision Process (POMDP) <ref type="bibr" target="#b9">[10]</ref> in the reinforcement learning (RL) literature. The state sequence s 1:T is hidden Step t = 1; Maximum</p><p>Step T max Output : Termination Step T , Answer a T 1 Sample t t from the distribution p(·| f t (s t ; θ t )); 2 if t t is false, go to Step 3; otherwise Step 6; 3 Generate attention vector x t = f at t (s t , M; θ x ); 4 Update internal state s t +1 = RNN(s t , x t ; θ s ); 5 Set t = t + 1; if t &lt; T max go to Step 1; otherwise Step 6; 6 Generate answer a t ∼ p(·| f a (s t ; θ a )); 7 Return T = t and a T = a t ; and dynamic, controlled by an RNN sequence model. The ReasoNet performs an answer action a T at the T -th step, which implies that the termination gate variables t 1:T = (t 1 = 0, t 2 = 0, ..., t T −1 = 0, t T = 1). The ReasoNet learns a stochastic policy π ((t t , a t )|s t ; θ ) with parameters θ to get a distribution of termination actions, to continue reading or to stop, and of answer actions if the model decides to stop at the current step. The termination step T varies from instance to instance.</p><p>The learnable parameters θ of the ReasoNet are the embedding matrices θ W , attention network θ x , the state RNN network θ s , the answer action network θ a , and the termination gate network θ t . The parameters θ = {θ W , θ x , θ s , θ a , θ t } are trained by maximizing the total expect reward. The expected reward for an instance is deened as:</p><formula xml:id="formula_0">(θ ) = E π (t 1:T ,a T ;θ ) T t =1 r t</formula><p>The reward can only be received at the nal termination step when an answer action a T is performed. We deene r T = 1 if t T = 1 and the answer is correct, and r T = 0 otherwise. The rewards on intermediate steps are zeros, {r t = 0} t =1...T −1 . can be maximized by directly applying gradient based optimization methods. The gradient of is given by:</p><formula xml:id="formula_1">θ (θ ) = E π (t 1:T ,a T ;θ ) [ θ logπ (t 1:T , a T ; θ )r T ]</formula><p>Motivated by the REINFORCE algorithm <ref type="bibr" target="#b30">[31]</ref>, we compute θ (θ ):</p><formula xml:id="formula_2">E π (t 1:T ,a T ;θ ) [ θ logπ (t 1:T , a T ; θ )r T ] = (t 1:T ,a T )∈A † π (t 1:T , a T ; θ ) [ θ logπ (t 1:T , a T ; θ )(r T − b T )]</formula><p>where A † is all the possible episodes, T , t 1:T , a T and r T are the termination step, termination action, answer action, and reward, respectively, for the (t 1:T , a T ) episode. b T is called the reward baseline in the RL literature to lower the variance <ref type="bibr" target="#b22">[23]</ref>. It is common to <ref type="bibr" target="#b23">[24]</ref>, and can be updated via an online moving average approach : b T = λb T + (1 − λ)r T . However, we empirically nd that the above approach leads to slow convergence in training ReasoNets. Intuitively, the average baselines {b T ;T = 1..T max } are global variables independent of instances. It is hard for these baselines to capture the dynamic termination behavior of ReasoNets. Since ReasoNets may stop at diierent time steps for diierent instances, the adoption of a global variable without considering the dynamic variance in each instance is inappropriate. To resolve this weakness in traditional methods and account for the dynamic characteristic of ReasoNets, we propose an instance-dependent baseline method to calculate θ (θ ), as illustrated in Section 3.1. Empirical results show that the proposed reward schema achieves better results compared to baseline approaches.</p><formula xml:id="formula_3">select b T = E π [r T ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Details</head><p>In the machine reading comprehension tasks, a training dataset is a collection of triplets of query q, passage p, and answer a. Say q n , p n , a n is the n-th training instance.</p><p>The rst step is to extract memory M from p n by mapping each symbolic in the passage to a contextual representation given by the concatenation of forward and backward RNN hidden states, i.e.,</p><formula xml:id="formula_4">m k = [ − → p n k , ← − p n |p n |−k +1 ]</formula><p>, and extract initial state s 1 from q n by assigning</p><formula xml:id="formula_5">s 1 = [ − → q n |q n | , ← − q n 1 ]</formula><p>. Given M and s 1 for the n-th training instance, a ReasoNet executes |A † | episodes, where all possible episodes A † can be enumerated by setting a maximum step. Each episode generates actions and a reward from the last step: (t 1:T , a T ), r T (t 1:T ,a T )∈A † . Therefore, the gradient of can be rewritten as:</p><formula xml:id="formula_6">θ (θ ) = (t 1:T ,a T )∈A † π (t 1:T , a T ; θ ) [ θ logπ (t 1:T , a T ; θ )(r T − b)]</formula><p>where the baseline b = (t 1:T ,a T )∈A † π (t 1:T , a T ; θ )r T is the average reward on the |A † | episodes for the n-th training instance. It allows diierent baselines for diierent training instances. This can be benecial since the complexity of training instances varies signiicantly. In experiments, we empirically nd using ( r T b − 1) in replace of (r T − b) can lead to a faster convergence. Therefore, we adopt this approach to train ReasoNets in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the performance of ReasoNets in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CNN and Daily Mail Datasets</head><p>We examine the performance of ReasoNets on CNN and Daily Mail datasets. <ref type="bibr" target="#b2">3</ref> The detailed settings of the ReasoNet model are as follows.</p><p>Vocab Size: For training our ReasoNet, we keep the most frequent |V | = 101k words (not including 584 entities and 1 placeholder marker) in the CNN dataset, and |V | = 151k words (not including 530 entities and 1 placeholder marker) in the Daily Mail dataset.</p><p>Embedding Layer: We choose 300-dimensional word embeddings, and use the 300-dimensional pretrained Glove word embeddings <ref type="bibr" target="#b16">[17]</ref> for initialization. We also apply dropout with probability 0.2 to the embedding layer.</p><p>Bi-GRU Encoder: We apply bidirectional GRU for encoding query and passage into vector representations. We set the number of hidden units to be 256 and 384 for the CNN and Daily Mail datasets, respectively. The recurrent weights of GRUs are initialized with random orthogonal matrices. The other weights in GRU cell are initialized from a uniform distribution between −0.01 and 0.01. We use a shared GRU model for both query and passage.</p><p>Memory and Attention: The memory of the ReasoNet on CNN and Daily Mail dataset is composed of query memory and passage memory. M = (M quer , M doc ), where M quer and M doc are extracted from query bidirectional-GRU encoder and passage bidirectional-GRU encoder respectively. We choose projected cosine similarity function as the attention module. The attention score a doc t,i on memory m doc i given the state s t is computed as follows: </p><formula xml:id="formula_7">a doc t,i = softmax i=1, ..., |M doc | γ cos(W doc 1 m doc i ,W doc 2 s t ),</formula><note type="other">,W quer 2 ,W doc 1 ,W doc 2 ); Internal State Controller: We choose GRU model as the internal state controller. The number of hidden units in the GRU state controller is 256 for CNN and 384 for Daily Mail. The initial state of the GRU controller is set to be the last-word of the query representation by a bidirectional-GRU encoder.</note><p>Termination Module: We adopt a logistical regression to model the termination variable at each time step:</p><formula xml:id="formula_8">f t (s t ; θ t ) = sigmoid(W t s t + b t ); θ t = (W t , b t )</formula><p>where W t and b t are the weight matrix and bias vector, respectively.</p><p>Answer Module: We apply a linear projection from GRU outputs and make predictions on the entity candidates. Following the  settings in AS Reader <ref type="bibr" target="#b8">[9]</ref>, we sum up scores from the same candidate and make a prediction. Thus, AS Reader can be viewed as a special case of ReasoNets with T max = 1. <ref type="bibr" target="#b3">4</ref> Other Details: The maximum reasoning step, T max is set to 5 in experiments on both CNN and Daily Mail datasets. We use ADAM optimizer <ref type="bibr" target="#b10">[11]</ref> for parameter optimization with an initial learning rate of 0.0005, β 1 = 0.9 and β 2 = 0.999; The absolute value of gradient on each parameter is clipped within 0.001. The batch size is 64 for both CNN and Daily Mail datasets. For each batch of the CNN and Daily Mail datasets, we randomly reshuue the assignment of named entities <ref type="bibr" target="#b6">[7]</ref>. This forces the model to treat the named entities as semantically meaningless labels. In the prediction of test cases, we randomly reshuue named entities up to 4 times, and report the averaged answer. Models are trained on GTX TitanX 12GB. It takes 7 hours per epoch to train on the Daily Mail dataset and 3 hours per epoch to train on the CNN dataset. The models are usually converged within 6 epochs on both CNN and Daily Mail datasets. <ref type="bibr" target="#b3">4</ref> When ReasoNet is set with T max = 1 in CNN and Daily Mail, it directly applies s 0 to make predictions on the entity candidates, without performing attention on the memory module. The prediction module in ReasoNets is the same as in AS Reader. It sums up the scores from the same entity candidates, where the scores are calculated by the inner product between s t and m d oc e , where m d oc e is an embedding vector of one entity candidate in the passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query: passenger @placeholder , 36 , died at the scene</head><p>Passage: ( @entity0 ) what was supposed to be a fantasy sports car ride at @entity3 turned deadly when a @entity4 crashed into a guardrail . the crash took place sunday at the @entity8 , which bills itself as a chance to drive your dream car on a racetrack . the @entity4 's passenger , 36 -year -old @entity14 of @entity15 , @entity16 , died at the scene , @entity13 said . the driver of the @entity4 , 24 -year -old @entity18 of @entity19 , @entity16 , lost control of the vehicle , the @entity13 said . he was hospitalized with minor injuries . @entity24 , which operates the @entity8 at @entity3 , released a statement sunday night about the crash . " on behalf of everyone in the organization , it is with a very heavy heart that we extend our deepest sympathies to those involved in today 's tragic accident in @entity36 , " the company said . @entity24 also operates the @entity3 --a chance to drive or ride in @entity39 race cars named for the winningest driver in the sport 's history . @entity0 's @entity43 and @entity44 contributed to this report .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: @entity14</head><p>Step Termination Probability</p><p>Attention Sum 1 0.0011 0.4916 2 0.5747 0.5486 3 0.9178 0.5577</p><p>Step <ref type="table" target="#tab_1">3   1  1 1   1 1  2 3 3   3  1   2 2</ref> Step 1</p><p>Step 2 <ref type="figure">Figure 3</ref>: Results of a test example 69e1f777e41bf67d5a22b7c69ae76f0ae873cf43.story from the CNN dataset. The numbers next to the underline bars indicate the rank of the attention scores. The corresponding termination probability and the sum of attention scores for the answer entity are shown in the table on the right.</p><p>Results: <ref type="table" target="#tab_1">Table 1</ref> shows the performance of all the existing single model baselines and our proposed ReasoNet. Among all the baselines, AS Reader could be viewed as a special case of ReasoNet with T max = 1. Comparing with the AS Reader, ReasoNet shows the signiicant improvement by capturing multi-turn reasoning in the paragraph. Iterative Attention Reader, EpiReader and GA Reader are the three multi-turn reasoning models with xed reasoning steps. ReasoNet also outperforms all of them by integrating termination gate in the model which allows diierent reasoning steps for diierent test cases. AoA Reader is another single-turn reasoning model, it captures the word alignment signals between query and passage, and shows a big improvement over AS Reader. ReasoNet obtains comparable results with AoA Reader on CNN test set. We expect that ReasoNet could be improved further by incorporating the word alignment information in the memory module as suggested in AoA Reader.</p><p>We show the distribution of termination step distribution of ReasoNets in the CNN dataset in <ref type="figure" target="#fig_2">Figure 2</ref>. The distributions spread out across diierent steps. Around 70% of the instances terminate in the last step. <ref type="figure">Figure 3</ref> gives a test example on CNN dataset, which illustrates the inference process of the ReasoNet. The model initially focuses on wrong entities with low termination probability. In the second and third steps, the model focuses on the right clue with higher termination probability. Interestingly, we also nd its query attention focuses on the placeholder token throughout all the steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SQuAD Dataset</head><p>In this section, we evaluate ReasoNet model on the task of question answering using the SQuAD dataset <ref type="bibr" target="#b17">[18]</ref>. <ref type="bibr" target="#b4">5</ref> SQuAD is a machine comprehension dataset on 536 Wikipedia articles, with more than 100,000 questions. Two metrics are used to evaluate models: Exact Match (EM) and a softer metric, F1 score, which measures the weighted average of the precision and recall rate at the character level. The dataset consists of 90k/10k training/dev question-contextanswer tuples with a large hidden test set. The model architecture used for this task is as follows:   <ref type="bibr" target="#b35">[36]</ref> 68.7 77.4 --Document Reader <ref type="bibr" target="#b3">[4]</ref> 69.9 78.9 --R-Net <ref type="bibr" target="#b26">[27]</ref> 71.3 79.7 75.9 82.9</p><p>Vocab Size: We use the python NLTK tokenizer 6 to preprocess passages and questions, and obtain about 100K words in the vocabulary.</p><p>Embedding Layer: We use the 100-dimensional pretrained Glove vectors <ref type="bibr" target="#b16">[17]</ref> as word embeddings. These Glove vectors are xed during the model training. To alleviate the out-of-vocabulary issue, we adopt one layer 100-dimensional convolutional neural network on character-level with a width size of 5 and each character encoded as an 8-dimensional vector following the work <ref type="bibr" target="#b19">[20]</ref>. The 100-dimensional Glove word vector and the 100-dimensional character-level vector are concatenated to obtain a 200-dimensional vector for each word.</p><p>Bi-GRU Encoder: We apply bidirectional GRU for encoding query and passage into vector representations. The number of hidden units is set to 128.</p><p>Memory: We use bidirectional-GRU encoders to extract the query representation M quer and the passage representation M doc , given a query and a passage. We compute the similarity matrix  <ref type="table">Table 4</ref>: Small and large random graph in the Graph Reachability dataset. Note that "A → B" represents an edge connected from A to B and the # symbol is used as a delimiter between diierent edges.</p><p>Small Graph Large Graph between each word in the query and each word in the passage. The similarity matrix is denoted as S ∈ R T× , where T and are the number of words in the passage and query, respectively, and</p><formula xml:id="formula_9">Graph Description 0 → 0 # 0 → 2 # 1 → 2 # 2 → 1 # 0 → 17 # 1 → 3 # 1 → 14 # 1 → 6 # 3 → 2 # 3 → 3 # 3 → 6 # 3 → 7 # 2 → 11 # 2 → 13 # 2 → 15 # 3 → 7# 4 → 0 # 4 → 1 # 4 → 4 # 5 → 7 # 5 → 0 # 5 → 7 # 6 → 10 # 6 → 5# 6 → 0 # 6 → 1 # 7 → 0 # 7 → 15 # 7 → 7 # 8 → 11 # 8 → 7 # 10</formula><formula xml:id="formula_10">S t j = w S [M doc :t ; M quer :j ; M doc :t • M quer :j</formula><p>] ∈ R, where w S is a trainable weight vector, • denotes the elementwise multiplication, and <ref type="bibr">[; ]</ref> is the vector concatenation across row. We then compute the context-to-query attention and query-to-context attention from the similarity matrix S by following recent co-attention work <ref type="bibr" target="#b19">[20]</ref> to obtain the query-aware passage representation G. We feed G to a 128-dimensional bidirectional GRU to obtain the memory M = bidirectional-GRU(G), where M ∈ R 256×T .</p><p>Internal State Controller: We use a GRU model with 256-dimensional hidden units as the internal state controller. The initial state of the GRU controller is the last-word representation of the query bidirectional-GRU encoder.</p><p>Termination Module: We use the same termination module as in the CNN and Daily Mail experiments.</p><p>Answer Module: SQuAD task requires the model to nd a span in the passage to answer the query. Thus the answer module requires to predict the start and end indices of the answer span in the passage. The probability distribution of selecting the start index over the passage at state s t is computed by :</p><formula xml:id="formula_11">p 1 t = softmax(w p 1 [M; M • S t ])</formula><p>where S t is given via tiling s t by T times across the column and w p 1 is a trainable weight vector. The probability distribution of selecting the end index over passage is computed in a similar manner:</p><formula xml:id="formula_12">p 2 t = softmax(w p 2 [M; M • S t ])</formula><p>Other Details: The maximum reasoning step T max is set to 10 in SQuAD experiments. We use AdaDelta optimizer <ref type="bibr" target="#b34">[35]</ref> for parameter optimization with an initial learning rate of 0.5 and a batch size of 32. Models are trained on GTX TitanX 12GB. It takes about 40 minutes per epoch for training, with 18 epochs in total.</p><p>Results : In the <ref type="table" target="#tab_2">Table 2</ref>, we report the performance of all models in the SQuAD leaderboard. <ref type="bibr" target="#b6">7</ref> In the upper part of the <ref type="table" target="#tab_2">Table 2</ref>, we compare ReasoNet with all published baselines at the time of submission. Speciically, BiDAF model could be viewed as a special case of ReasoNet with T max = 1. It is worth noting that this SQuAD leaderboard is highly active and competitive. The test set is hidden to all models and all the results on the leaderboard are produced and reported by the organizer; thus all the results here are reproducible. In <ref type="table" target="#tab_2">Table 2</ref>, we demonstrate that ReasoNet outperforms all existing published approaches. While we compare ReasoNet with BiDAF, ReasoNet exceeds BiDAF both in single model and ensemble model cases. This demonstrates the importance of the dynamic multi-turn reasoning over a passage. In the bottom part of <ref type="table" target="#tab_2">Table 2</ref>, we compare ReasoNet with all unpublished methods at the time of this submission, ReasoNet holds the second position in all the competing approaches in the SQuAD leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Reachability Task</head><p>Recent analysis and results <ref type="bibr" target="#b2">[3]</ref> on the cloze-style machine comprehension tasks have suggested some simple models without multiturn reasoning can achieve reasonable performance. Based on these results, we construct a synthetic structured Graph Reachability dataset 8 to evaluate longer range machine inference and reasoning capability, since we anticipate ReasoNets to have the capability to handle long range relationships.</p><p>We generate two synthetic datasets: a small graph dataset and a large graph dataset. In the small graph dataset, it contains 500K small graphs, where each graph contains 9 nodes and 16 direct Step 1</p><p>Step 2</p><p>Step 0</p><p>Step 3</p><p>Step 4, 5, 7</p><p>Step 6, 8</p><p>Step 9</p><p>Step Termination Probability Prediction 1 Step 2</p><p>Step 3</p><p>Step 1</p><p>Step 4</p><p>Steps 5, 6, 8</p><p>Steps 7, 9</p><p>Step 10</p><p>Figure 4: An example of graph reachability result, given a query "10 → 17" (Answer: Yes). The red circles highlight the nodes/edges which have the highest attention in each step. The corresponding termination probability and prediction results are shown in the table. The model terminates at step 10.</p><p>edges to randomly connect pairs of nodes. The large graph dataset contains 500K graphs, where each graph contains 18 nodes and 32 random direct edges. Duplicated edges are removed. <ref type="table" target="#tab_3">Table 3</ref> shows the graph reachability statistics on the two datasets.</p><p>In <ref type="table">Table 4</ref>, we show examples of a small graph and a large graph in the synthetic dataset. Both graph and query are represented by a sequence of symbols. The details settings of the ReasoNet are listed as follows in the reachability tasks.</p><p>Embedding Layer We use a 100-dimensional embedding vector for each symbol in the query and graph description.</p><p>Bi-LSTM Encoder: We apply a bidirectional-LSTM layer with 128 and 256 cells on query embeddings in the small and large graph datasets, respectively. The last states of bidirectional-LSTM on query are concatenated to be the initial internal state Answer Module: The nal answer is either "Yes" or "No" and hence logistical regression is used as the answer module:</p><formula xml:id="formula_13">s 1 = [ − → q |q | , ← − q 1 ] in the ReasoNet.</formula><formula xml:id="formula_14">a t = σ (W a s t + b a ); θ a = (W a , b a ).</formula><p>Termination Module: We use the same termination module as in the CNN and Daily Mail experiments.</p><p>Other Details: The maximum reasoning step T max is set to 15 and 25 for the small graph and large graph dataset, respectively. We use AdaDelta optimizer <ref type="bibr" target="#b34">[35]</ref> for parameter optimization with an initial learning rate of 0.5 and a batch size of 32.</p><p>We denote "ReasoNet" as the standard ReasoNet with termination gate, as described in Section 3.1. To study the eeectiveness of the termination gate in ReasoNets, we remove the termination gate and use the prediction from the last state, ˆ a = a T max (T max is the maximum reasoning step), denoted as "ReasoNet-Last". To study the eeectiveness of multi-turn reasoning, we choose "ReasoNet-T max = 2", which only has single-turn reasoning. We compare ReasoNets with a two layer deep LSTM model <ref type="bibr" target="#b6">[7]</ref> with 128 hidden units, denoted as "Deep LSTM Reader", as a baseline. <ref type="table" target="#tab_5">Table 5</ref> shows the performance of these models on the graph reachability dataset. Deep LSTM Reader achieves 90.92% and 71.55% accuracy in the small and large graph dataset, respectively, which indicates the graph reachibility task is not trivial. The results of ReasoNet-T max = 2 are comparable with the results of Deep LSTM Reader, since both Deep LSTM Reader and ReasoNet-T max = 2 perform single-turn reasoning. The ReasoNet-Last model achieves 100% accuracy on the small graph dataset, while the ReasoNet-Last model achieves</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">-&gt; 16 # -&gt; 12 # -&gt; 14 # -&gt; 7 # 2 -&gt; 17 # 3 -&gt; # 4 -&gt; 0 # 4 -&gt; # 4 -&gt; 12 # 4 -&gt; 6 # 6 -&gt; 0 # 6 -&gt; 3 # 6 -&gt; 7 # 8 -&gt; 2 # 8 -&gt; 4 # 8 -&gt; 13 # 8 -&gt; 14 # 9 -&gt; 16</head><p># 10 -&gt; 0 # 10 -&gt; 6 # 11 -&gt; 10 # 11 -&gt; 2 # 12 -&gt; 2 # 13 -&gt; 2 # 13 -&gt; 6 # 14 -&gt; 2 # 14 -&gt; 7 # 16 -&gt; 13 # 16 -&gt; 14 # 17 -&gt; 0 # 17 -&gt; 13 #</p><p>Step Termination Probability Prediction 1</p><p>1.40E-05 4.49E-04 2 0.999 1.40E-05</p><p>Step 1</p><p>Step 2</p><p>Step 2</p><p>Step 1</p><p>Step 1</p><p>Step <ref type="table" target="#tab_1">1   3  1   2   2  1   3</ref> Step 2</p><p>Figure 5: An example of graph reachability result, given a query "4 → 9" (Answer: No). The numbers next to the underline bars indicate the rank of the attention scores. The corresponding termination probability and prediction results are shown in the table.</p><p>only 78.95% accuracy on the large graph dataset, as the task becomes more challenging. Meanwhile, the ReasoNet model converges faster than the ReasoNet-Last model. The ReasoNet model converges in 20 epochs in the small graph dataset, and 40 epochs in the large graph dataset, while the ReasoNet-Last model converges around 40 epochs in the small graph dataset, and 70 epochs in the large graph dataset. The results suggest that the termination gate variable in the ReasoNet is helpful when training with sophisticated examples, and makes models converge faster. Both the ReasoNet and ReasoNet-Last models perform better than the ReasoNet-T max = 2 model, which demonstrates the importance of the multi-turn reasoning.</p><p>To further understand the inference process in ReasoNets, <ref type="figure">Fig- ures 4</ref> and 5 show test examples of the large graph dataset. In <ref type="figure">Figure 4</ref>, we can observe that the model does not make a rm prediction till step 9. The highest attention word at each step shows the reasoning process of the model. Interestingly, the model starts from the end node (17), traverses backward till nding the starting node (10) in step 9, and makes a rm termination prediction. On the other hand, in <ref type="figure">Figure 5</ref>, the model learns to stop in step 2. In step 1, the model looks for neighbor nodes <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16</ref>) to 4 and 9. Then, the model gives up in step 2 and predict "No". All of these demonstrate the dynamic termination characteristic and potential reasoning capability of ReasoNets.</p><p>To better grasp when ReasoNets stop reasoning, we show the distribution of termination steps in ReasoNets on the test set. The termination step is chosen with the maximum termination probability p(k) = t k k −1 i=1 (1 − t i ), where t i is the termination probability at step i. <ref type="figure" target="#fig_5">Figure 6</ref> shows the termination step distribution of ReasoNets in the graph reachability dataset. The distributions spread out across diierent steps. Around 16% and 35% of the instances terminate in the last step for the small and large graph, respectively. We study the correlation between the termination steps and the complexity of test instances in <ref type="figure" target="#fig_6">Figure 7</ref>. Given the query, we use the Breadth-First Search (BFS) algorithm over the target graph to analyze the complexity of test instances. For example, BFS-Step = 2 indicates that there are two intermediate nodes in the shortest reachability path. Test instances with larger BFS-Steps are more challenging. We denote BFS-Step = −1 as there is no reachable path for the given query. <ref type="figure" target="#fig_6">Figure 7</ref> shows that test instances with larger BFS-Steps require more reasoning steps. The correlation between BFS steps and ReasoNet termination steps in the graph reachability dataset, where T max is set to 15 and 25 in the small graph and large graph dataset, respectively, and BFS-Step= −1 denotes unreachable cases. The value indicates the number of instances in each case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose ReasoNets that dynamically decide whether to continue or to terminate the inference process in machine comprehension tasks. With the use of the instance-dependent baseline method, our proposed model achieves superior results in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a proposed structured Graph Reachability dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: A ReasoNet architecture with an episode of {t 1 = 0, . . ., t t +1 = 0, t t +2 = 1}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where γ is set to 10. W doc 1 and W doc 2 are weight vectors asso- ciated with m doc i and s t , respectively, and are joint trained in the ReasoNet. Thus, the attention vector on passage is given by x doc t = |M d oc | i a doc t,i m doc i . Similarly, the attention vector on query is x quer t = |M que r | i a quer t,i m quer i . The nal attention vector is the concatenation of the query attention vector and the pas- sage attention vector x t = (x quer t , x doc t ). The attention module is parameterized by θ x = (W quer 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The termination step distribution of a ReasoNet (T max = 5) in the CNN dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5</head><label></label><figDesc>SQuAD Competition Website is https://rajpurkar.github.io/SQuAD-explorer/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Memory: We apply another bidirectional-LSTM layer with 128 and 256 cells on graph description embeddings in the small and large graph datasets, respectively. It maps each symbol i to a contextual representation given by the concatenation of forward and backward LSTM hidden states m i = [ − → i , ← − | |−i+1 ]. Internal State Controller: We use a GRU model with 128- dimensional and 256-dimensional hidden units as the internal state controller for the small and large graph datasets, respectively. The initial state of the GRU controller is s 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Termination step distribution of ReasoNets in the graph reachability dataset, where T max is set to 15 and 25 in the small graph and large graph dataset, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The correlation between BFS steps and ReasoNet termination steps in the graph reachability dataset, where T max is set to 15 and 25 in the small graph and large graph dataset, respectively, and BFS-Step= −1 denotes unreachable cases. The value indicates the number of instances in each case.</figDesc><graphic url="image-3.png" coords="9,107.96,210.96,114.13,88.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : The performance of Reasoning Network on CNN and Daily Mail dataset.</head><label>1</label><figDesc></figDesc><table>CNN 
Daily Mail 

valid test valid test 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Results on the SQuAD test leaderboard.</head><label>2</label><figDesc></figDesc><table>Single Model Ensemble Model 

EM 
F1 
EM 
F1 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Reachability statistics of the Graph Reachability dataset.</head><label>3</label><figDesc></figDesc><table>Small Graph 
Large Graph 

Reachable Step No Reach 
1-3 
4-6 
7-9 No Reach 
1-3 
4-6 
7-13 
Train (%) 
44.16 
42.06 13.51 0.27 
49.02 
25.57 21.92 3.49 
Test (%) 
45.00 
41.35 13.44 0.21 
49.27 
25.46 21.74 3.53 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : The performance of Reasoning Network on the Graph Reachability dataset.</head><label>5</label><figDesc></figDesc><table>Small Graph 
Large Graph 

ROC-AUC PR-AUC Accuracy ROC-AUC PR-AUC Accuracy 
Deep LSTM Reader 0.9619 
0.9565 
0.9092 
0.7988 
0.7887 
0.7155 
ReasoNet-T max = 2 0.9638 
0.9677 
0.8961 
0.8477 
0.8388 
0.7607 
ReasoNet-Last 
1 
1 
1 
0.8836 
0.8742 
0.7895 
ReasoNet 
1 
1 
1 
0.9988 
0.9989 
0.9821 

</table></figure>

			<note place="foot" n="1"> https://arxiv.org/abs/1609.05284 2 http://www.stanford-qa.com</note>

			<note place="foot" n="3"> The CNN and Daily Mail datasets are available at https://github.com/deepmind/rcdata</note>

			<note place="foot" n="6"> NLTK package could be downloaded from http://www.nltk.org/</note>

			<note place="foot" n="7"> Results shown here reeect the SQuAD leaderboard (stanford-qa.com) as of 17 Feb 2017, 9pm PST. We include the reference in the camera-ready version. α : Fudan University. 8 The dataset is available at https://github.com/MSRDL/graph_reachability_dataset</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>We thank Ming-Wei Chang, Li Deng, Lihong Li, and Xiaodong Liu for their thoughtful feedback and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From machine learning to machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>CoRR abs/1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention-over-Attention Neural Networks for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR abs/1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gated-Attention Readers for Text Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>CoRR abs/1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Karm Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Goldilocks Principle: Reading Children&apos;S Books With Explicit Memory Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547v1[cs.CL</idno>
		<idno>arXiv:1603.01547</idno>
		<title level="m">Text Understanding with the Attention Sum Reader Network</title>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artiicial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic Entity Representation with Max-pooling Improves Machine Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL-HLT)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning Recurrent Span Representations for Extractive Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno>CoRR abs/1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WebNav: A New Large-Scale Task for Natural Language based Sequential Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeerey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SQuAD: 100, 000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bidirectional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>CoRR abs/1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Iterative Alternating Neural Attention for Machine Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1606.02245</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Temporal Credit Assignment in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Policy Gradient Methods for Reinforcement Learning with Function Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural Language Comprehension with the EpiReader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Machine Comprehension Using Match-LSTM and Answer Pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR abs/1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated Self-Matching Networks for Reading Comprehension and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-Perspective Context Matching for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno>CoRR abs/1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Separating Answers from Queries for Neural Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<idno>CoRR abs/1607.03316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">FastQA: A Simple and EEcient Neural Architecture for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiie</surname></persName>
		</author>
		<idno>CoRR abs/1703.04816</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dynamic Coattention Networks For Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR abs/1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Words or Characters? Fine-grained Gating for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>CoRR abs/1611.01724</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR abs/1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Exploring Question Understanding and Adaptation in Neural-NetworkBased Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Rong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR abs/1703.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
