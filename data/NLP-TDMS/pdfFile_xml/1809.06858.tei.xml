<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FRAGE: Frequency-Agnostic Word Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
							<email>cygong@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
							<email>di_he@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">MOE</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<email>xu.tan@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>wanglw@cis.pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">MOE</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Institute of Big Data Research</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tie-yan.liu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FRAGE: Frequency-Agnostic Word Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn FRequency-AGnostic word Embedding (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings, which are distributed and continuous vector representations for word tokens, have been one of the basic building blocks for many neural network-based models used in natural language processing (NLP) tasks, such as language modeling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref>, text classification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8]</ref> and machine translation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12]</ref>. Different from classic one-hot representation, the learned word embeddings contain semantic information which can measure the semantic similarity between words <ref type="bibr" target="#b29">[30]</ref>, and can also be transferred into other learning tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In deep learning approaches for NLP tasks, word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters. As the inputs of the neural network, word embeddings carry all the information of words that will be further processed by the network, and the quality of embeddings is critical and highly impacts the final performance of the learning task <ref type="bibr" target="#b15">[16]</ref>. Unfortunately, we find the word embeddings learned by many deep learning approaches are far from perfect. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and 1(b), in the embedding space learned by word2vec model, the nearest neighbors of word "Peking" includes "quickest", "multicellular", and "epigenetic", which are not semantically similar, while semantically related words such as "Beijing" and "China" are far from it. Similar phenomena are observed from the word embeddings learned from translation tasks.</p><p>With a careful study, we find a more general problem which is rooted in low-frequency words in the text corpus. Without any confusion, we also call high-frequency words as popular words and call low-frequency words as rare words. As is well known <ref type="bibr" target="#b24">[25]</ref>, the frequency distribution of words roughly follows a simple mathematical form known as Zipf's law. When the size of a text corpus grows, the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words. Interestingly, the learned embeddings of rare words and popular words behave differently. (1) In the embedding space, a popular word usually has semantically related neighbors, while a rare word usually does not. Moreover, the nearest neighbors of more than 85% rare words are rare words. (2) Word embeddings encode frequency information. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and 1(b), the embeddings of rare words and popular words actually lie in different subregions of the space. Such a phenomenon is also observed in <ref type="bibr" target="#b30">[31]</ref>.</p><p>We argue that the different behaviors of the embeddings of popular words and rare words are problematic. First, such embeddings will affect the semantic understanding of words. We observe more than half of the rare words are nouns or variants of popular words. Those rare words should have similar meanings or share the same topics with popular words. Second, the neighbors of a large number of rare words are semantically unrelated rare words. To some extent, those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding. It will consequently limit the performance of down-stream tasks using the embeddings. For example, in text classification, it cannot be well guaranteed that the label of a sentence does not change when you replace one popular/rare word in the sentence by its rare/popular alternatives.</p><p>To address this problem, in this paper, we propose an adversarial training method to learn FRequencyAGnostic word Embedding (FRAGE). For a given NLP task, in addition to minimize the task-specific loss by optimizing the task-specific parameters together with word embeddings, we introduce another discriminator, which takes a word embedding as input and classifies whether it is a popular/rare word. The discriminator optimizes its parameters to maximize its classification accuracy, while word embeddings are optimized towards a low task-dependent loss as well as fooling the discriminator to mis-classify the popular and rare words. When the whole training process converges and the system achieves an equilibrium, the discriminator cannot well differentiate popular words from rare words. Consequently, rare words lie in the same region as and are mixed with popular words in the embedding space. Then FRAGE will catch better semantic information and help the task-specific model to perform better.</p><p>We conduct experiments on four types of NLP tasks, including three word similarity tasks, two language modeling tasks, three sentiment classification tasks and two machine translation tasks to test our method. In all tasks, FRAGE outperforms the baselines. Specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on PTB, WT2 and WMT14 English-German datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Representation</head><p>Words are the basic units of natural languages, and distributed word representations (i.e., word embeddings) are the basic units of many models in NLP tasks including language modeling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> and machine translation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12]</ref>. It has been demonstrated that word representations learned from one task can be transferred to other tasks and achieve competitive performance <ref type="bibr" target="#b2">[3]</ref>.</p><p>While word embeddings play an important role in neural network-based models in NLP and achieve great success, one technical challenge is that the embeddings of rare words are difficult to train due to their low frequency of occurrences. <ref type="bibr" target="#b38">[39]</ref> develops a novel way to split word into sub-word units which is widely used in neural machine translation. However, the low-frequency sub-word units are still difficult to train: <ref type="bibr" target="#b32">[33]</ref> provides a comprehensive study which shows that the rare (sub)words are usually under-estimated in neural machine translation: during inference step, the model tends to choose popular words over their rare alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Training</head><p>The basic idea of our work to address the above problem is adversarial training, in which two or more models learn together by pursuing competing goals. A representative example of adversarial training is Generative Adversarial Networks (GANs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref> for image generation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b1">2]</ref>, in which a discriminator and a generator compete with each other: the generator aims to generate images similar to the natural ones, and the discriminator aims to detect the generated ones from the natural ones. Recently, adversarial training has been successfully applied to NLP tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref>. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref> introduce an additional discriminator to differentiate the semantics learned from different languages in non-parallel bilingual data. <ref type="bibr" target="#b22">[23]</ref> develops a discriminator to classify whether a sentence is created by human or generated by a model. Our proposed method is under the adversarial training framework but not exactly the conventional generator-discriminator approach since there is no generator in our scenario. For an NLP task and its neural network model (including word embeddings), we introduce a discriminator to differentiate embeddings of popular words and rare words; while the NN model aims to fool the discriminator and minimize the task-specific loss simultaneously.</p><p>Our work is also weakly related to adversarial domain adaptation which attempts to mitigate the negative effects of domain shift between training and testing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>. The difference between this work and adversarial domain adaptation is that we do not target at the mismatch between training and testing; instead, we aim to improve the effectiveness of word embeddings and consequently improve the performance of end-to-end NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Study</head><p>In this section, we study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English-German translation task using Transformer <ref type="bibr" target="#b41">[42]</ref>. The implementation details can be found in the supplementary material (part A).</p><p>Experimental Design In both tasks, we simply set the top 20% frequent words in vocabulary as popular words and denote the rest as rare words (roughly speaking, we set a word as a rare word if its relative frequency is lower than 10 −6 in WMT14 dataset and 10 −7 in Google News dataset). We have tried other thresholds such as 10% or 25% and found the observations are similar.</p><p>We study whether the semantic relationship between two words is reasonable. To achieve this, we randomly sampled some rare/popular words and checked the embeddings trained from different tasks. For each sampled word, we determined its nearest neighbors based on the cosine similarity between its embeddings and others'. <ref type="bibr" target="#b1">2</ref> We also manually chose words which are semantically similar to it. For simplicity, for each word, we call the nearest words predicted from the embeddings as model-predicted neighbors, and call our chosen words as semantic neighbors.</p><p>Observation To visualize word embeddings, we reduce their dimensionalities by SVD and plot two cases in <ref type="figure" target="#fig_0">Figure 1</ref>. More cases and other studies without dimensionality reduction can be found in the supplementary material (part C).</p><p>We find that the embeddings trained from different tasks share some common patterns. For both tasks, more than 90% of model-predicted neighbors of rare words are rare words. For each rare word, the model-predicted neighbor is usually not semantically related to this word, and semantic neighbors we chose are far away from it in the embedding space. In contrast, the model-predicted neighbors of popular words are very reasonable.</p><p>As the patterns in rare words are different from that of popular words, we further check the whole embedding matrix to make a general understanding. We also visualize the word embeddings using SVD by keeping the two directions with top-2 largest eigenvalues as in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> and plot them in Figure 1: Case study of the embeddings trained from WMT14 translation task using Transformer and trained from Google News dataset using word2vec is shown in (a) and (b). (c) and (d) show the visualization of embeddings trained from WMT14 translation task using Transformer and trained from Google News dataset using word2vec. Red points represent rare words and blue points represent popular words. Red points represent rare words and blue points represent popular words. In (a) and (b), we highlight the semantic neighbors in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Tokens Word Embeddings</head><p>Task-specific Outputs <ref type="table">Task</ref> a certain degree: the rare words and popular words lie in different regions after this linear projection, and thus they occupy different regions in the original embedding space. This strange phenomenon is also observed in other learned embeddings (e.g.CBOW and GLOVE) and mentioned in <ref type="bibr" target="#b31">[32]</ref>.</p><p>Explanation From the empirical study above, we can see that the occupied spaces of popular words and rare words are different and here we intuitively explain a possible reason. We simply take word2vec as an example which is trained by stochastic gradient descent. During training, the sample rate of a popular word is high and the embedding of a popular word updates frequently. For a rare word, the sample rate is low and its embedding rarely updates. According to our study, on average, the moving distance of the embedding for a popular word is twice longer than that of a rare word during training. As all word embeddings are usually initialized around the origin with a small variance, we observe in the final model, the embeddings of rare words are still around the origin and the popular words have moved far away.</p><p>Discussion We have strong evidence that the current phenomena are problematic. First, according to our study, 3 in both tasks, more than half of the rare words are nouns, e.g., company names, city names. They may share some similar topics to popular entities, e.g., big companies and cities; around 10% percent of rare words include a hyphen (which is usually used to join popular words), and over 30% rare words are different PoSs of popular words. These words should have mixed or similar semantics to some popular words. These facts show that rare words and popular words should lie in the same region of the embedding space, which is different from what we observed. Second, as we can see from the cases, for rare words, model-predicted neighbors are usually not semantically related words but frequency-related words (rare words). This shows, for rare words, the embeddings encode more frequency information than semantic information. It is not good to use such word embeddings into semantic understanding tasks, e.g., text classification, language modeling, language understanding and translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Method</head><p>In this section, we present our method to improve word representations. As we have a strong prior that many rare words should share the same region in the embedding space as popular words, the basic idea of our algorithm is to train the word embeddings in an adversarial framework: We introduce a discriminator to categorize word embeddings into two classes: popular ones or rare ones. We hope the learned word embeddings not only minimize the task-specific training loss but also fool the discriminator. By doing so, the frequency information is removed from the embedding and we call our method frequency-agnostic word embedding (FRAGE).</p><p>We first define some notations and then introduce our algorithm. We develop three types of notations: embeddings, task-specific parameters/loss, and discriminator parameters/loss.</p><p>Denote θ emb ∈ R d×|V | as the word embedding matrix to be learned, where d is the dimension of the embedding vectors and |V | is the vocabulary size. Let V pop denote the set of popular words and V rare = V \ V pop denote the set of rare words. Then the embedding matrix θ emb can be divided into two parts: θ emb pop for popular words and θ emb rare for rare words. Let θ emb w denote the embedding of word w. Let θ model denote all the other task-specific parameters except word embeddings. For instance, for language modeling, θ model is the parameters of the RNN or LSTM; for neural machine translation, θ model is the parameters of the encoder, attention module and decoder.</p><p>Let L T (S; θ model , θ emb ) denote the task-specific loss over a dataset S. Taking language modeling as an example, the loss L T (S; θ model , θ emb ) is defined as the negative log likelihood of the data:</p><formula xml:id="formula_0">L T (S; θ model , θ emb ) = − 1 |S| y∈S log P (y; θ model , θ emb ),<label>(1)</label></formula><p>where y is a sentence.</p><p>Let f θ D denote a discriminator with parameters θ D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word. Let L D (V ; θ D , θ emb ) denote the loss of the discriminator:</p><formula xml:id="formula_1">L D (V ; θ D , θ emb ) = 1 |V pop | w∈Vpop log f θ D (θ emb w ) + 1 |V rare | w∈Vrare log(1 − f θ D (θ emb w )).<label>(2)</label></formula><p>Following the principle of adversarial training, we develop a minimax objective to train the taskspecific model (θ model and θ emb ) and the discriminator (θ D ) as below:</p><formula xml:id="formula_2">min θ model ,θ emb max θ D L T (S; θ model , θ emb ) − λL D (V ; θ D , θ emb ),<label>(3)</label></formula><p>where λ is a coefficient to trade off the two loss terms. We can see that when the model parameter θ model and the embedding θ emb are fixed, the optimization of the discriminator θ D becomes</p><formula xml:id="formula_3">max θ D −λL D (V ; θ D , θ emb ),<label>(4)</label></formula><p>which is to minimize the classification error of popular and rare words. When the discriminator θ D is fixed, the optimization of θ model and θ emb becomes</p><formula xml:id="formula_4">min θ model ,θ emb L T (S; θ model , θ emb ) − λL D (V ; θ D , θ emb ),<label>(5)</label></formula><p>i.e., to optimize the task performance as well as fooling the discriminator. We train θ model , θ emb and θ D iteratively by stochastic gradient descent or its variants. The general training process is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We test our method on a wide range of tasks, including word similarity, language modeling, machine translation and text classification. For each task, we choose the state-of-the-art architecture together with the state-of-the-art training method as our baseline 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Proposed Algorithm</head><formula xml:id="formula_5">1: Input: Dataset S, vocabulary V = V pop ∪ V rare , θ model , θ emb , θ D . 2: repeat 3:</formula><p>Sample a minibatchˆSminibatchˆ minibatchˆS from S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Sample a minibatchˆVminibatchˆ minibatchˆV = ˆ V pop ∪ ˆ V rare from V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update θ model , θ emb by gradient descent according to Eqn. <ref type="formula" target="#formula_4">(5)</ref> with datâ S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update θ D by gradient ascent according to Eqn. (4) with vocabularyˆVvocabularyˆ vocabularyˆV . 7: until Converge 8: Output: θ model , θ emb , θ D .</p><p>For fair comparisons, for each task, our method shares the same model architecture as the baseline. The only difference is that we use the original task-specific loss function with an additional adversarial loss as in Eqn. (3). Due to space limitations, we put dataset description, model description, hyperparameter configuration into supplementary material (part A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>We conduct experiments on the following tasks.</p><p>Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity: it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground-truth, in terms of Spearman's rank correlation. We use the skip-gram model as our baseline model <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b4">5</ref> , and train the embeddings using Enwik9 <ref type="bibr" target="#b5">6</ref> . We test the baseline and our method on three datasets: RG65, WS and RW. The RW dataset is a dataset for the evaluation of rare words. Following common practice <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31]</ref>, we use cosine distance while computing the similarity between two word embeddings.</p><p>Language Modeling is a basic task in natural language processing. The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity. We do experiments on two widely used datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45]</ref>, Penn Treebank (PTB) <ref type="bibr" target="#b28">[29]</ref> and WikiText-2 (WT2) <ref type="bibr" target="#b27">[28]</ref>. We choose two recent works as our baselines: the AWD-LSTM model <ref type="bibr" target="#b6">7</ref>  <ref type="bibr" target="#b26">[27]</ref> and the AWD-LSTM-MoS model, 8 <ref type="bibr" target="#b44">[45]</ref> which achieves state-of-the-art performance.</p><p>Machine Translation is a popular task in both deep learning and natural language processing. We choose two datasets: WMT14 English-German and IWSLT14 German-English datasets, which are evaluated in terms of BLEU score <ref type="bibr" target="#b8">9</ref> . We use Transformer <ref type="bibr" target="#b41">[42]</ref> as the baseline model, which achieves state-of-the-art accuracy on multiple translation datasets. We use transformer_base and transformer_big configurations following tensor2tensor <ref type="bibr" target="#b40">[41]</ref>  <ref type="bibr" target="#b9">10</ref> .</p><p>Text Classification is a conventional machine learning task and is evaluated by accuracy. Following the setting in <ref type="bibr" target="#b21">[22]</ref>, we implement a Recurrent CNN-based model <ref type="bibr" target="#b10">11</ref> and test it on AG's news corpus (AGs), IMDB movie review dataset (IMDB) and 20 Newsgroups (20NG).</p><p>In all tasks, we simply set the top 20% frequent words in vocabulary as popular words and denote the rest as rare words, which is the same as our empirical study. For all the tasks except word embedding, we use full-batch gradient descent to update the discriminator. For word embedding, mini-batch stochastic gradient descent is used to update the discriminator with a batch size 3000, since the vocabulary size is large. For language modeling and machine translation tasks, we use logistic regression as the discriminator. For other tasks, we find using a shallow neural network with <ref type="bibr" target="#b4">5</ref>   <ref type="bibr" target="#b9">10</ref> To improve the training for imbalanced labeled data, a common method is to adjust loss function by reweighting the training samples; To regularize the parameter space, a common method is to use l2 regularization. We tested these methods in machine translation and found the performance is not good. Detailed analysis is provided in the supplementary material (part B).</p><p>11 https://github.com/brightmart/text_classification one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size. In all tasks, we set the hyper-parameter λ to 0.1. We list other hyper-parameters related to different task-specific models in the supplementary material (part A). In this subsection, we provide the experimental results of all tasks. For simplicity, we use "with FRAGE" as our proposed method in the tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RG65</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Similarity</head><p>The results on three word similarity tasks are listed in <ref type="table">Table 1</ref>. From the table, we can see that our method consistently outperforms the baseline on all datasets. In particular, we outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our method improves the representation of words, especially the rare words.   <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>. "Paras" denotes the number of model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paras</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Modeling</head><p>The results of language modeling on PTB and WT2 datasets are presented in <ref type="table" target="#tab_3">Ta- ble 2</ref>. We test our model and the baselines at several checkpoints used in the baseline papers: without finetune, with finetune, with post-process (continuous cache pointer <ref type="bibr" target="#b13">[14]</ref> or dynamic evaluation <ref type="bibr" target="#b20">[21]</ref>). In all these settings, our method outperforms the two baselines. On PTB dataset, our method improves the AWD-LSTM and AWD-LSTM-MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints. On WT2 dataset, which contains more rare words, our method achieves larger improvements. We improve the results of AWD-LSTM and AWD-LSTM-MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Translation</head><p>The results of neural machine translation on WMT14 English-German and IWSLT14 German-English tasks are shown in <ref type="table">Table 3</ref>. We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English-German</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IWSLT De→En Method</head><p>BLEU Method BLEU ByteNet <ref type="bibr" target="#b18">[19]</ref> 23.75 DeepConv <ref type="bibr" target="#b10">[11]</ref> 30.04 ConvS2S <ref type="bibr" target="#b11">[12]</ref> 25.16 Dual transfer learning <ref type="bibr" target="#b42">[43]</ref> 32.35 Transformer Base <ref type="bibr" target="#b41">[42]</ref> 27.30 ConvS2S+SeqNLL <ref type="bibr" target="#b8">[9]</ref> 32.68 Transformer Base <ref type="bibr">with FRAGE 28.36 ConvS2S+Risk [9]</ref> 32.93 Transformer Big <ref type="bibr" target="#b41">[42]</ref> 28 <ref type="table">Table 3</ref>: BLEU scores on test set on WMT2014 English-German and IWSLT German-English tasks.</p><note type="other">.40 Transformer 33.12 Transformer Big with FRAGE 29.11 Transformer with FRAGE 33.97</note><p>task, respectively. The model learned from adversarial training also outperforms original one in IWSLT14 German-English task by 0.85. These results show improving word embeddings can achieve better results in more complicated tasks and larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Classification</head><p>The results are listed in  <ref type="table" target="#tab_4">Table 4</ref>: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB) and 20 Newsgroups (20NG) for text classification.</p><p>As a summary, our experiments on four different tasks with 10 datasets verify the effectiveness of our method. We provide some case studies and visualizations of our method in the supplementary material (part C), which show that the semantic similarities are reasonable and the popular/rare words are better mixed together in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. We propose a neat, simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks.</p><p>We will explore several directions in the future. First, we will investigate the theoretical aspects of word embedding learning and our adversarial training method. Second, we will study more applications which have the similar problem even beyond NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Description</head><p>For word similarity, we use three test datasets. WordSim-353 (WS) dataset consists of 353 pairs of commonly used verbs and nouns; The rare-words (RW) dataset contains rarely used words; The RG65 dataset contains 65 word pairs, and the similarity values in the dataset are the means of judgments made by 51 subjects.</p><p>For language modeling tasks, we use Penn Treebank dataset and WikiText-2 dataset. The details of the datasets are provided in <ref type="table" target="#tab_6">Table 5</ref>.  For machine translation, we use WMT14 English-German and IWSLT14 German-English datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Penn Treebank</head><p>The training set of WMT14 English-German task consists of 4.5M sentence pairs. Source and target tokens are processed into 37K shared sub-word units based on byte-pair encoding (BPE) <ref type="bibr" target="#b38">[39]</ref>. We use the concatenation of newstest2012 and newstest2013 as the validation set and use newstest2014 as the test set following all previous works. IWSLT14 German-English dataset contains 160K training sentence pairs and 7K validation sentence pairs. Tokens are processed using BPE and eventually we obtain a shared vocabulary of about 32K tokens. We use the concatenation of dev2010, tst2010, tst2011 and tst2011 as the test set, which is widely adopted in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>For text classification tasks, we use three datasets: AG's News, IMDB and 20NG. AG's news corpus is a news article corpus with categorized articles from more than 2,000 news. IMDB movie review dataset is a sentiment classification dataset. It consists of movie review comments with binary sentiment labels. 20 Newsgroups is an email collection dataset, in which the emails are categorized into 20 different groups. We use the bydate version and select 4 major categories (comp, politics, rec, and religion) following <ref type="bibr" target="#b14">[15]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-parameter configurations</head><p>The hyper-parameters used for AWD-LSTM with/without MoS in language modeling experiment is shown in <ref type="table" target="#tab_10">Table 7</ref>.</p><p>For machine translation tasks, we choose Adam optimizer with β 1 = 0.9, β 2 = 0.98, ε = 10 −9 , and follow the learning rate schedule in <ref type="bibr" target="#b41">[42]</ref>. For evaluation, we use the case-sensitive tokenized BLEU score <ref type="bibr" target="#b33">[34]</ref> for WMT14 English-German and case-insensitive tokenized BLEU score <ref type="bibr" target="#b33">[34]</ref> for IWSLT14 German-English. The hyper-parameters used in machine translation task are summarized in <ref type="table" target="#tab_11">Table 8</ref>. The hyper-parameters used in word embedding task are summarized in <ref type="table">Table 9</ref>.</p><p>For all text classification tasks, we use convolutional kernel with size 2, 3, 5. We implement batch normalization and shortcut connection, and use Adam optimizer with β 1 = 0.9, β 2 = 0.99, ε = 10 −8 .    <ref type="table">Table 9</ref>: Hyper-parameter used for word embedding training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AWD-LSTM + MoS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Models Description</head><p>We use task-specific baseline models. In language modeling, AWD-LSTM <ref type="bibr" target="#b26">[27]</ref> is a weight-dropped LSTM which uses Drop Connect on hidden-to-hidden weights as a means of recurrent regularization. The model is trained by NT-ASGD, which is a variant of the averaged stochastic gradient method. The training process has two steps, in the second step, the model is finetuned using another configuration of NT-ASGD. AWD-LSTM-MoS <ref type="bibr" target="#b44">[45]</ref> uses the Mixture of Softmaxes structure to the vanilla AWD-LSTM and achieves the state-of-the-art result on PTB and WT2.</p><p>For machine translation, Transformer <ref type="bibr" target="#b41">[42]</ref> is a recently developed architecture in which the selfattention network is used during encoding and decoding step. It achieves the best performances on several machine translation tasks, e.g. WMT14 English-German, WMT14 English-French datasets.</p><p>Word2vec <ref type="bibr" target="#b29">[30]</ref> is one of the pioneer works on using deep learning to NLP tasks. Based on the co-occurrence of words, it produces distributed representations of words (word embeddings).</p><p>RCNN <ref type="bibr" target="#b21">[22]</ref> contains both recurrent and convolutional layers to catch the key components in texts, and is widely used in text classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Comparisons</head><p>We compare some other simple methods with ours on machine translation tasks, which include reweighting method and l 2 regularization (weight decay). Results are listed in <ref type="table">Table 10</ref>. We notice that those simple methods do not work for the tasks, even have negative effects.  <ref type="table">Table 10</ref>: BLEU scores on test set of the WMT14 English-German task and IWSLT14 German-English task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WMT En→De IWSLT De→En</head><p>Our method is denoted as "FRAGE", "Reweighting" denotes reweighting the loss of each word by reciprocal of its frequency, and "Weight Decay" denotes putting weight decay rate (0.2) on embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Case Study on Original Models and Qualitative Analysis of Our Method</head><p>We provide more word similarity cases in <ref type="table">Table 11</ref> to justify our statement in Section 3. We also present the effectiveness of our method by showcase and embedding visualizations. From the cases and visualizations in <ref type="table" target="#tab_3">Table 12</ref> and <ref type="figure">Figure 3</ref>, we find the word similarities are improved and popular/rare words are better mixed together.</p><p>(a) (b) <ref type="figure">Figure 3</ref>: These figures show that, in different tasks, the embeddings of rare and popular words are better mixed together after applying our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 (</head><label>1</label><figDesc>c) and 1(d). From the figure, we can see that the embeddings actually encode frequencies to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: The proposed learning framework includes a task-specific predictor and a discriminator, whose function is to classify rare and popular words. Both modules use word embeddings as the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the 
perplexity, better the result. Baseline results are obtained from </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Our method outperforms the baseline method 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Statistics of the Penn Treebank, and WikiText-2 dataset used in language modeling. The out 
of vocabulary (OOV) words will be replaced by &lt;unk&gt; during training and testing. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 shows detailed information.</head><label>6</label><figDesc></figDesc><table>Dataset Ave. Len Max Len #Classes #Train : #Text 
AG's News 
34 
211 
4 
120000 : 7600 
IMDB 
281 
2956 
2 
25000 : 25000 
20NG 
429 
11924 
4 
7250 : 5563 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 : Detailed statistics about text classification datasets.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Hyper-parameter used for AWD-LSTM and AWD-LSTM-MoS on PTB and WT2. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Hyper-parameter used for neural machine translation on IWSLT14 German-English and 
WMT14 English-German. 

hyper-parameter Skip-gram 
Learning rate 
0.20 
Embedding size 
300 
Negative Samples 
100 
Window size 
5 
Min count 
5 

</table></figure>

			<note place="foot" n="32">nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.</note>

			<note place="foot" n="1"> https://code.google.com/archive/p/word2vec/ 2 Cosine distance is the most popularly used metric in literature to measure semantic similarity [30, 35, 31]. We also have tried other metrics, e.g., Euclid distance, and the phenomena still exist.</note>

			<note place="foot" n="3"> We use the POS tagger from Natural Language Toolkit, https://github.com/nltk.</note>

			<note place="foot" n="4"> Code for our implement is available at https://github.com/ChengyueGongR/FrequencyAgnostic</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04956</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02344</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno>abs/1612.04426</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document classification by topic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hingmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chougule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Palshikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 36th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR &apos;13</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-01" />
			<biblScope unit="page" from="877" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fix your classifier: the marginal value of training the last weight layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05565</idno>
		<title level="m">Neural phrase-based machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<idno>abs/1709.07432</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G A P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4601" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="852" to="853" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1708.02182</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01417</idno>
		<title level="m">All-but-the-top: simple and effective postprocessing for word representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">All-but-the-top: Simple and effective postprocessing for word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viswanath</surname></persName>
		</author>
		<idno>abs/1702.01417</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00047</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dual transfer learning for neural machine translation with marginal distribution regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tie-Yan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno>abs/1711.03953</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
