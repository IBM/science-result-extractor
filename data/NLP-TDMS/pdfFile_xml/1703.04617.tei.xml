<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T08:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
							<email>xiaodan.zhu@nrc-cnrc.gc.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">National Research Council Canada 3 iFLYTEK Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
							<email>lrdai@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
							<email>siwei@iflytek.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in <ref type="bibr" target="#b15">[Richardson et al., 2013</ref><ref type="bibr" target="#b5">, Hermann et al., 2015</ref><ref type="bibr" target="#b6">, Hill et al., 2015</ref><ref type="bibr" target="#b14">, Rajpurkar et al., 2016</ref><ref type="bibr" target="#b12">, Nguyen et al., 2016</ref><ref type="bibr" target="#b0">, Berant et al., 2014</ref>. Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs.</p><p>The recent availability of relatively large training datasets (see Section 2 for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective.</p><p>In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. <ref type="bibr" target="#b15">Richardson et al. [2013]</ref> released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset <ref type="bibr" target="#b5">[Hermann et al., 2015]</ref> contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) <ref type="bibr" target="#b6">[Hill et al., 2015]</ref> leverages named entities, common nouns, verbs, and prepositions to test reading comprehension. The Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b14">[Rajpurkar et al., 2016]</ref> is more recently released dataset, which consists of more than 100,000 questions for documents taken from Wikipedia across a wide range of topics. The question-answer pairs are annotated through crowdsourcing. Answers are spans of text marked in the original documents. In this paper, we use SQuAD to evaluate our models.</p><p>Many neural network models have been studied on the SQuAD task. <ref type="bibr" target="#b20">Wang and Jiang [2016]</ref> proposed match LSTM to associate documents and questions and adapted the so-called pointer Network <ref type="bibr" target="#b19">[Vinyals et al., 2015]</ref> to determine the positions of the answer text spans. <ref type="bibr" target="#b24">Yu et al. [2016]</ref> proposed a dynamic chunk reader to extract and rank a set of answer candidates. <ref type="bibr" target="#b24">Yang et al. [2016]</ref> focused on word representation and presented a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on the properties of words.  proposed a multi-perspective context matching (MPCM) model, which matched an encoded document and question from multiple perspectives. Xiong et al. <ref type="bibr">[2016]</ref> proposed a dynamic decoder and so-called highway maxout network to improve the effectiveness of the decoder. The bi-directional attention flow (BIDAF) <ref type="bibr" target="#b16">[Seo et al., 2016]</ref> used the bi-directional attention to obtain a question-aware context representation.</p><p>In this paper, we introduce syntactic information to encode questions with a specific form of recursive neural networks <ref type="bibr" target="#b25">[Zhu et al., 2015</ref><ref type="bibr" target="#b18">, Tai et al., 2015</ref><ref type="bibr" target="#b1">, Chen et al., 2016</ref><ref type="bibr" target="#b17">, Socher et al., 2011</ref>. More specifically, we explore a tree-structured LSTM <ref type="bibr" target="#b25">[Zhu et al., 2015</ref><ref type="bibr" target="#b18">, Tai et al., 2015</ref> which extends the linear-chain long short-term memory (LSTM) <ref type="bibr" target="#b7">[Hochreiter and Schmidhuber, 1997</ref>] to a recursive structure, which has the potential to capture long-distance interactions over the structures.</p><p>Different types of questions are often used to seek for different types of information. For example, a "what" question could have very different property from that of a "why" question, while they may share information and need to be trained together instead of separately. We view this as a "adaptation" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas "i-vector" <ref type="bibr" target="#b3">[Dehak et al., 2011]</ref> in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Baseline Model</head><p>Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word embedding</head><p>We concatenate embedding at two levels to represent a word: the character composition and word-level embedding. The character composition feeds all characters of a word into a convolutional neural network (CNN) <ref type="bibr" target="#b8">[Kim, 2014]</ref> to obtain a representation for the word.</p><p>And we use the pre-trained 300-D GloVe vectors <ref type="bibr" target="#b13">[Pennington et al., 2014]</ref> (see the experiment section for details) to initialize our word-level embedding. Each word is therefore represented as the concatenation of the character-composition vector and word-level embedding. This is performed on both questions and documents, resulting in two matrices: the Q e ∈ R N ×dw for a question and the D e ∈ R M ×dw for a document, where N is the question length (number of word tokens), M is the document length, and d w is the embedding dimensionality.</p><p>Input encoding The above word representation focuses on representing individual words, and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context. We use bi-directional GRU (BiGRU) <ref type="bibr" target="#b2">[Cho et al., 2014]</ref> for both documents and questions.</p><formula xml:id="formula_0">Q c i = BiGRU(Q e i , i), ∀i ∈ [1, . . . , N ] (1) D c j = BiGRU(D e j , j), ∀j ∈ [1, . . . , M ]<label>(2)</label></formula><p>A BiGRU runs a forward and backward GRU on a sequence starting from the left and the right end, respectively. By concatenating the hidden states of these two GRUs for each word, we obtain the a representation for a question or document: Q c ∈ R N ×dc for a question and D c ∈ R M ×dc for a document.</p><p>Alignment Questions and documents interact closely. As in most previous work, our framework use both soft attention over questions and that over documents to capture the interaction between them. More specifically, in this soft-alignment layer, we first feed the contextual representation matrix Q c and D c to obtain alignment matrix U ∈ R N ×M :</p><formula xml:id="formula_1">U ij = Q c i · D cT j , ∀i ∈ [1, . . . , N ], ∀j ∈ [1, . . . , M ]<label>(3)</label></formula><p>Each U ij represents the similarity between a question word Q c i and a document word D c j . Word-level Q-code Similar as in <ref type="bibr" target="#b16">[Seo et al., 2016]</ref>, we obtain a word-level Q-code. Specifically, for each document word w j , we find which words in the question are relevant to it. To this end, a j ∈ R N is computed with the following equation and used as a soft attention weight:</p><formula xml:id="formula_2">a j = sof tmax(U :j ), ∀j ∈ [1, . . . , M ]<label>(4)</label></formula><p>With the attention weights computed, we obtain the encoding of the question for each document word w j as follows, which we call word-level Q-code in this paper:</p><formula xml:id="formula_3">Q w = a T · Q c ∈ R M ×dc (5)</formula><p>Question-based filtering To better explore question understanding, we design this question-based filtering layer. As detailed later, different question representation can be easily incorporated to this layer in addition to being used as a filter to find key information in the document based on the question. This layer is expandable with more complicated question modeling.</p><p>In the basic form of question-based filtering, for each question word w i , we find which words in the document are associated. Similar to a j discussed above, we can obtain the attention weights on document words for each question word w i :</p><formula xml:id="formula_4">b i = sof tmax(U i: ) ∈ R M , ∀i ∈ [1, . . . , N ]<label>(6)</label></formula><p>By pooling b ∈ R N ×M , we can obtain a question-based filtering weight b f :</p><formula xml:id="formula_5">b f = norm(pooling(b)) ∈ R M (7) norm(x) = x i x i (8)</formula><p>where the specific pooling function we used include max-pooling and mean-pooling. Then the document softly filtered based on the corresponding question D f can be calculated by:</p><formula xml:id="formula_6">D fmax j = b fmax j D c j , ∀j ∈ [1, . . . , M ]<label>(9)</label></formula><formula xml:id="formula_7">D fmean j = b fmean j D c j , ∀j ∈ [1, . . . , M ]<label>(10)</label></formula><formula xml:id="formula_8">D f = [D fmax , D fmean ]<label>(11)</label></formula><p>Through concatenating the document representation D c , word-level Q-code Q w and question-filtered document D f , we can finally obtain the alignment layer representation:</p><formula xml:id="formula_9">I = [D c , Q w , D c • Q w , D c − Q w , D f , b fmax , b fmean ] ∈ R M ×(6dc+2)<label>(12)</label></formula><p>where "•" stands for element-wise multiplication and "−" is simply the vector subtraction.</p><p>Aggregation After acquiring the local alignment representation, key information in document and question has been collected, and the aggregation layer is then performed to find answers. We use three BiGRU layers to model the process that aggregates local information to make the global decision to find the answer spans. We found a residual architecture <ref type="bibr" target="#b4">[He et al., 2016]</ref> as described in <ref type="figure">Figure 2</ref> is very effective in this aggregation process:  <ref type="figure">Figure 2</ref>: The inference layer implemented with a residual network.</p><formula xml:id="formula_10">I 1 i = BiGRU(I i )<label>(13)</label></formula><formula xml:id="formula_11">I 2 i = I 1 i + BiGRU(I 1 i )<label>(14)</label></formula><formula xml:id="formula_12">I 3 i = I 2 i + BiGRU(I 2 i )<label>(15</label></formula><p>Prediction The SQuAD QA task requires a span of text to answer a question. We use a pointer network <ref type="bibr" target="#b19">[Vinyals et al., 2015]</ref> to predict the starting and end position of answers as in <ref type="bibr" target="#b20">[Wang and Jiang, 2016]</ref>. Different from their methods, we use a two-directional prediction to obtain the positions. For one direction, we first predict the starting position of the answer span followed by predicting the end position, which is implemented with the following equations:</p><formula xml:id="formula_13">P (s+) = sof tmax(W s+ · I 3 )<label>(16)</label></formula><formula xml:id="formula_14">P (e+) = sof tmax(W e+ · I 3 + W h+ · h s+ )<label>(17)</label></formula><p>where I 3 is inference layer output, h s+ is the hidden state of the first step, and all W are trainable matrices. We also perform this by predicting the end position first and then the starting position:</p><formula xml:id="formula_15">P (e−) = sof tmax(W e− · I 3 )<label>(18)</label></formula><formula xml:id="formula_16">P (s−) = sof tmax(W s− · I 3 + W h− · h e− )<label>(19)</label></formula><p>We finally identify the span of an answer with the following equation:</p><formula xml:id="formula_17">P (s) = pooling([P (s+), P (s−)])<label>(20)</label></formula><formula xml:id="formula_18">P (e) = pooling([P (e+), P (e−)])<label>(21)</label></formula><p>We use the mean-pooling here as it is more effective on the development set than the alternatives such as the max-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Understanding and Adaptation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Introducing syntactic information for neural question encoding</head><p>The interplay of syntax and semantics of natural language questions is of interest for question representation. We attempt to incorporate syntactic information in questions representation with TreeLSTM [ <ref type="bibr" target="#b25">Zhu et al., 2015</ref><ref type="bibr" target="#b18">, Tai et al., 2015</ref>. In general a TreeLSTM could perform semantic composition over given syntactic structures.</p><p>Unlike the chain-structured LSTM <ref type="bibr" target="#b7">[Hochreiter and Schmidhuber, 1997]</ref>, the TreeLSTM captures long-distance interaction on a tree. The update of a TreeLSTM node is described at a high level with Equation <ref type="formula" target="#formula_0">(22)</ref>, and the detailed computation is described in <ref type="figure" target="#fig_1">(23-29)</ref>. Specifically, the input of a TreeLSTM node is used to configure four gates: the input gate i t , output gate o t , and the two forget gates f L t for the left child input and f R t for the right. The memory cell c t considers each child's cell vector, c L t−1 and c R t−1 , which are gated by the left forget gate f L t and right forget gate f R t , respectively.</p><formula xml:id="formula_19">h t = TreeLSTM(x t , h L t−1 , h R t−1 ),<label>(22)</label></formula><formula xml:id="formula_20">h t = o t • tanh(c t ),<label>(23)</label></formula><formula xml:id="formula_21">o t = σ(W o x t + U L o h L t−1 + U R o h R t−1 ),<label>(24)</label></formula><formula xml:id="formula_22">c t = f L t • c L t−1 + f R t • c R t−1 + i t • u t ,<label>(25)</label></formula><formula xml:id="formula_23">f L t = σ(W f x t + U LL f h L t−1 + U LR f h R t−1 ),<label>(26)</label></formula><formula xml:id="formula_24">f R t = σ(W f x t + U RL f h L t−1 + U RR f h R t−1 ),<label>(27)</label></formula><formula xml:id="formula_25">i t = σ(W i x t + U L i h L t−1 + U R i h R t−1 ),<label>(28)</label></formula><formula xml:id="formula_26">u t = tanh(W c x t + U L c h L t−1 + U R c h R t−1 ),<label>(29)</label></formula><p>where σ is the sigmoid function, • is the element-wise multiplication of two vectors, and all W, U are trainable matrices.</p><p>To obtain the parse tree information, we use Stanford CoreNLP (PCFG Parser) <ref type="bibr">[Manning et al., 2014, Klein and</ref><ref type="bibr" target="#b10">Manning, 2003</ref>] to produce a binarized constituency parse for each question and build the TreeLSTM based on the parse tree. The root node of TreeLSTM is used as the representation for the whole question. More specifically, we use it as TreeLSTM Q-code Q T L ∈ R dc , by not only simply concatenating it to the alignment layer output but also using it as a question filter, just as we discussed in the question-based filtering section:</p><formula xml:id="formula_27">Q T L = TreeLSTM(Q e ) ∈ R dc (30) b T L = norm(Q T L · D cT ) ∈ R M (31) D T L j = b T L j D c j , ∀j ∈ [1, . . . , M ]<label>(32)</label></formula><formula xml:id="formula_28">I new = [I, repmat(Q T L ), D T L , b T L ]<label>(33)</label></formula><p>where I new is the new output of alignment layer, and function repmat copies Q T L for M times to fit with I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Question Adaptation</head><p>Questions by nature are often composed to fulfill different types of information needs. For example, a "when" question seeks for different types of information (i.e., temporal information) than those for a "why" question. Different types of questions and the corresponding answers could potentially have different distributional regularity.</p><p>Explicit question-type embedding The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are. We explicitly encode question-type information to be an 11-dimensional one-hot vector (the top-10 question types and "other" question type). Each question type is with a trainable embedding vector. We call this explicit question type code, ET ∈ R d ET . Then the vector for each question type is tuned during training, and is added to the system with the following equation:</p><formula xml:id="formula_29">I new = [I, repmat(ET)]<label>(34)</label></formula><p>Question adaptation As discussed, different types of questions and their answers may share common regularity and have separate property at the same time. We also view this as an adaptation problem in order to let different types of questions share a basic model but still discriminate them when needed. Specifically, we borrow ideas from speaker adaptation <ref type="bibr" target="#b3">[Dehak et al., 2011]</ref> in speech recognition, where neural-network-based adaptation is performed among different groups of speakers.</p><p>Conceptually we regard a type of questions as a group of acoustically similar speakers. Specifically we propose a question discriminative block or simply called a discriminative block <ref type="figure" target="#fig_1">(Figure 3</ref>) below to perform question adaptation. The main idea is described below:</p><formula xml:id="formula_30">x = f ([x, ¯ x c , δ x ])<label>(35)</label></formula><p>For each input question x, we can decompose it to two parts: the cluster it belong(i.e., question type) and the diverse in the cluster. The information of the cluster is encoded in a vector ¯ x c . In order to keep calculation differentiable, we compute the weight of all the clusters based on the distances of x and each cluster center vector, in stead of just choosing the closest cluster. Then the discriminative vector δ x with regard to these most relevant clusters are computed. All this information is combined to obtain the discriminative information. In order to keep the full information of input, we also copy the input question x, together with the acquired discriminative information, to a feed-forward layer to obtain a new representation x for the question. More specifically, the adaptation algorithm contains two steps: adapting and updating, which is detailed as follows:</p><p>• Adapting In the adapting step, we first compute the similarity score between an input question vector x ∈ R h and each centroid vector of K clusters ¯ x ∈ R K×h . Each cluster here models a question type. Unlike the explicit question type modeling discussed above, here we do not specify what question types we are modeling but let the system to learn. Specifically, we only need to pre-specific how many clusters, K, we are modeling. The similarity between an input question and cluster centroid can be used to compute similarity weight w a :</p><formula xml:id="formula_31">w a k = sof tmax(cos_sim(x, ¯ x k ), α), ∀k ∈ [1, . . . , K]<label>(36)</label></formula><formula xml:id="formula_32">cos_sim(u, v) = &lt; u, v &gt; ||u|| · ||v|| (37) sof tmax(x i , α) = e αxi j e αxj<label>(38)</label></formula><p>We set α equals 50 to make sure only closest class will have a high weight while maintain differentiable. Then we acquire a soft class-center vector ¯ x c :</p><formula xml:id="formula_33">¯ x c = k w a k ¯ x k ∈ R h<label>(39)</label></formula><p>We then compute a discriminative vector δ x between the input question with regard to the soft class-center vector:</p><formula xml:id="formula_34">δ x = x − ¯ x c<label>(40)</label></formula><p>Note that ¯ x c here models the cluster information and δ x represents the discriminative information in the cluster. By feeding x, ¯ x c and δ x into feedforward layer with Relu, we obtain x ∈ R K :</p><formula xml:id="formula_35">x = Relu(W · [x, ¯ x c , δ x ])<label>(41)</label></formula><p>With x ready, we can apply Discriminative Block to any question code and obtain its adaptation Q-code. In this paper, we use TreeLSTM Q-code as the input vector x, and obtain TreeLSTM adaptation Q-code Q T La ∈ R dc . Similar to TreeLSTM Q-code Q T L , we concatenate Q T La to alignment output I and also use it as a question filter:</p><formula xml:id="formula_36">Q T La = Relu(W · [Q T L , Q T L c , δ Q TL ])<label>(42)</label></formula><formula xml:id="formula_37">b T La = norm(Q T La · D cT ) ∈ R M<label>(43)</label></formula><formula xml:id="formula_38">D T La j = b T La j D c j , ∀j ∈ [1, . . . , M ]<label>(44)</label></formula><formula xml:id="formula_39">I new = [I, repmat(Q T La ), D T La , b T La ]<label>(45)</label></formula><p>• Updating The updating stage attempts to modify the center vectors of the K clusters in order to fit each cluster to model different types of questions. The updating is performed according to the following formula:</p><formula xml:id="formula_40">¯ x k = (1 − βw a k )¯ x k + βw a k x, ∀k ∈ [1, . . . , K]<label>(46)</label></formula><p>In the equation, β is an updating rate used to control the amount of each updating, and we set it to 0.01. When x is far away from K-th cluster center ¯ x k , w a k is close to be value 0 and the k-th cluster center ¯ x k tends not to be updated. If x is instead close to the j-th cluster center ¯ x j , w a k is close to the value 1 and the centroid of the j-th cluster ¯ x j will be updated more aggressively using x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Set-Up</head><p>We test our models on Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b14">[Rajpurkar et al., 2016]</ref>. The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles, and the answer to each question is a span of text in the Wikipedia articles. Training data includes 87,599 instances and validation set has 10,570 instances. The test data is hidden and kept by the organizer. The evaluation of SQuAD is Exact Match (EM) and F1 score.</p><p>We use pre-trained 300-D Glove 840B vectors <ref type="bibr" target="#b13">[Pennington et al., 2014]</ref> to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. CharCNN filter length is 1,3,5, each is 50 dimensions. All vectors including word embedding are updated during training. The cluster number K in discriminative block is 100. The Adam method <ref type="bibr" target="#b9">[Kingma and Ba, 2014]</ref> is used for optimization. And the first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. We will half learning rate when meet a bad iteration, and the patience is 7. Our early stop evaluation is the EM and F1 score of validation set. All hidden states of GRUs, and TreeLSTMs are 500 dimensions, while word-level embedding d w is 300 dimensions. We set max length of document to 500, and drop the question-document pairs beyond this on training set. Explicit question-type dimension d ET is 50. We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EM F1</head><p>Logistic Regression Baseline <ref type="bibr" target="#b14">[Rajpurkar et al., 2016]</ref> 40.4 51.0</p><p>Match-LSTM with Ans-Ptr (Sentence) <ref type="bibr" target="#b20">[Wang and Jiang, 2016]</ref> 54.505 67.748 Match-LSTM with Ans-Ptr (Boundary) <ref type="bibr" target="#b20">[Wang and Jiang, 2016]</ref> 60.474 70.695 Dynamic Chunk Reader <ref type="bibr" target="#b24">[Yu et al., 2016]</ref> 62.499 70.956 Fine-Grained Gating  62.446 73.327 Match-LSTM with Bi-Ans-Ptr (Boundary) <ref type="bibr" target="#b20">[Wang and Jiang, 2016]</ref> 64  <ref type="table">Table 1</ref>: The official leaderboard of single models on SQuAD test set as we submitted our systems (January 20, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Overall results <ref type="table">Table 1</ref> shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling).   <ref type="table" target="#tab_3">Table 2</ref> shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types (K) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in <ref type="table">Table 1</ref>. Furthermore, after submitted our result, we have experimented with a large value of K and found that when K = 100, we can achieve a better performance of 69.10%/78.38% on the development set.   <ref type="figure">Figure 5</ref> shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>78.38%</head><p>69.10%*100%</p><p>30.90%*30.03%</p><p>14.89%*0%</p><p>16.01%*57.96%</p><p>Figure 5: F1 Score Analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Closely modelling questions could be of importance for question answering and machine reading. In this paper, we introduce syntactic information to help encode questions in neural networks. We view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A high level view of our basic model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The discriminative block for question discrimination and adaptation.</figDesc><graphic url="image-106.png" coords="6,226.68,396.74,158.97,120.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 (</head><label>4</label><figDesc>Figure 4(a) shows the EM/F1 scores of different question types while Figure 4(b) is the question type amount distribution on the development set. In Figure 4(a) we can see that the average EM/F1 of the "when" question is highest and those of the "why" question is the lowest. From Figure 4(b) we can see the "what" question is the major class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Question Type Analysis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Performance of various Q-code on the development set.</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Vander Linden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brittany</forename><surname>Harding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1499" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Enhancing and combining sequential and tree lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06038</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réda</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><forename type="middle">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chiung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Bellevue, Washington, Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-28" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01724</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end answer chunk extraction and ranking for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
