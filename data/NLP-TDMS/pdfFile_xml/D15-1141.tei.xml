<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long Short-Term Memory Neural Networks for Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
							<email>xinchichen13@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long Short-Term Memory Neural Networks for Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features are mostly extracted from a local context. These methods cannot utilize the long distance information which is also crucial for word segmentation. In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neu-ral network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem <ref type="bibr" target="#b27">(Xue, 2003;</ref><ref type="bibr" target="#b14">Peng et al., 2004</ref>). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) <ref type="bibr" target="#b1">(Berger et al., 1996)</ref> and Conditional Random Fields (CRF) ( <ref type="bibr" target="#b8">Lafferty et al., 2001</ref>). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus.</p><p>Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering <ref type="bibr">(Collobert * Corresponding author.</ref> et al., <ref type="bibr" target="#b18">Socher et al., 2013;</ref><ref type="bibr" target="#b24">Turian et al., 2010;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b0">Bengio et al., 2003)</ref>. <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. <ref type="bibr" target="#b31">Zheng et al. (2013)</ref> applied the architecture of <ref type="bibr" target="#b4">Collobert et al. (2011)</ref> to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. <ref type="bibr" target="#b13">Pei et al. (2014)</ref> models tag-tag interactions, tagcharacter interactions and character-character interactions based on <ref type="bibr" target="#b31">Zheng et al. (2013)</ref>.  proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods.</p><p>Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window. Intuitively, many words are difficult to segment based on the local information only. For example, the segmentation of the following sentence needs the information of the long distance collocation. 冬 天 (winter)， 能 (can) 穿 (wear) 多 少 (amount) 穿 (wear) 多 少 (amount)； 夏 天 (summer)，能 (can) 穿 (wear) 多 (more) 少 (little) 穿 (wear) 多 (more) 少 (little)。 Without the word "夏天 (summer)" or "冬天 (winter)", it is difficult to segment the phrase "能 穿多少穿多少". Therefore, we usually need utilize the non-local information for more accurate word segmentation. However, it does not work by simply increasing the context window size. As reported in ( <ref type="bibr" target="#b31">Zheng et al., 2013)</ref>, the performance drops smoothly when the window size is larger than 3. The reason is that the number of its parameters is so large that the trained network has overfitted on training data. Therefore, it is necessary to capture the potential long-distance dependencies without increasing the size of the context window.</p><p>In order to address this problem, we propose a neural model based on Long Short-Term Memory Neural Network (LSTM) <ref type="bibr">(Hochreiter and Schmid- huber, 1997</ref>) that explicitly model the previous information by exploiting input, output and forget gates to decide how to utilize and update the memory of pervious information. Intuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it easily carries this information (the existence of the feature) over a long distance, hence, capturing the potential useful long-distance information. We evaluate our model on three popular benchmark datasets (PKU, MSRA and CTB6), and the experimental results show that our model achieves the state-of-the-art performance with the smaller context window size (0,2).</p><p>The contributions of this paper can be summarized as follows.</p><p>• We first introduce the LSTM neural network for Chinese word segmentation. The LSTM can capture potential long-distance dependencies and keep the previous useful information in memory, which avoids the limit of the size of context window.</p><p>• Although there are relatively few researches of applying dropout method to the LSTM, we investigate several dropout strategies and find that dropout is also effective to avoid the overfitting of the LSTM.</p><p>• Despite Chinese word segmentation being a specific case, our model can be easily generalized and applied to the other sequence labeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Model for Chinese Word Segmentation</head><p>Chinese word segmentation is usually regarded as character-based sequence labeling. Each character is labeled as one of {B, M, E, S} to indicate the segmentation. {B, M, E} represent Begin, Middle, End of a multi-character segmentation respectively, and S represents a Single character segmentation.</p><p>The neural model is usually characterized by three specialized layers: (1) a character embedding layer; (2) a series of classical neural network layers and (3) tag inference layer. An illustration is shown in <ref type="figure">Figure 1</ref>.</p><p>The most common tagging approach is based on a local window. The window approach assumes that the tag of a character largely depends on its neighboring characters. Given an input sentence c (1:n) , a window of size k slides over the sentence from character c (1) to c (n) , where n is the length of the sentence. As shown in <ref type="figure">Figure  1</ref>, for each character c (t) (1 ≤ t ≤ n), the context characters (c (t−2) ,c (t−1) ,c (t) ,c (t+1) ,c (t+2) ) are fed into the lookup table layer when the window size k is 5. The characters exceeding the sentence boundaries are mapped to one of two special symbols, namely "start" and "end" symbols. The character embeddings extracted by the lookup table layer are then concatenated into a single vector x (t) ∈ R H 1 , where H 1 = k × d is the size of layer 1. Then x (t) is fed into the next layer which performs linear transformation followed by an element-wise activation function g such as sigmoid function σ(x) = (1+e −x ) −1 and hyperbolic tangent function ϕ(x) = e x −e −x e x +e −x here.</p><formula xml:id="formula_0">h (t) = g(W 1 x (t) + b 1 ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">W 1 ∈ R H 2 ×H 1 , b 1 ∈ R H 2 , h (t) ∈ R H 2 . H 2</formula><p>is a hyper-parameter which indicates the number of hidden units in layer 2. Given a set of tags T of size |T |, a similar linear transformation is performed except that no non-linear function is followed:</p><formula xml:id="formula_2">y (t) = W 2 h (t) + b 2 ,<label>(2)</label></formula><p>where W 2 ∈ R |T |×H 2 , b 2 ∈ R |T | . y (t) ∈ R |T | is the score vector for each possible tag. In Chinese word segmentation, the most prevalent tag set T T is {B, M, E, S} as mentioned above.</p><p>To model the tag dependency, a transition score A ij is introduced to measure the probability of jumping from tag i ∈ T to tag j ∈ T ( <ref type="bibr" target="#b4">Collobert et al., 2011</ref>). Although this model works well for Chinese word segmentation and other sequence labeling tasks, it just utilizes the information of context of a limited-length window. Some useful long distance information is neglected.  </p><formula xml:id="formula_3">z (t) = W 1 × x (t) + b 1 h (t) Linear y (t) = W 2 × h (t) + b 2 h (t) =σ(z (t) ) Concatenate Tag Inference B M E S y (t) y (2) y (1) y (n-1) y (n) A y (t) c (t-2) c (t-1) c (t) c (t+1) c (t+2) … … … … …</formula><p>Figure 1: General architecture of neural model for Chinese word segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Character Embeddings</head><p>The first step of using neural network to process symbolic data is to represent them into distributed vectors, also called embeddings ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b3">Collobert and Weston, 2008)</ref>. Formally, in Chinese word segmentation task, we have a character dictionary C of size |C|. Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c ∈ C is represented as a real-valued vector (character embedding) v c ∈ R d where d is the dimensionality of the vector space. The character embeddings are then stacked into an embedding matrix M ∈ R d×|C| . For a character c ∈ C, the corresponding character embedding v c ∈ R d is retrieved by the lookup table layer. And the lookup table layer can be regarded as a simple projection layer where the character embedding for each context character is achieved by table lookup operation according to its index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LSTM</head><p>The long short term memory neural network (LSTM) <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) is an extension of the recurrent neural network (RNN).</p><p>The RNN has recurrent hidden states whose output at each time is dependent on that of the previous time. More formally, given a sequence x (1:n) = (x (1) , x (2) , . . . , x (t) , . . . , x (n) ), the RNN updates its recurrent hidden state h (t) by</p><formula xml:id="formula_4">h (t) = g(Uh (t−1) + Wx (t) + b),<label>(3)</label></formula><p>where g is a nonlinear function as mentioned above.</p><p>Though RNN has been proven successful on many tasks such as speech recognition <ref type="bibr" target="#b25">(Vinyals et al., 2012</ref>), language modeling ( <ref type="bibr" target="#b10">Mikolov et al., 2010</ref>) and text generation (Sutskever et al., 2011), it can be difficult to train them to learn longterm dynamics, likely due in part to the vanishing and exploding gradient problem <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification ( <ref type="bibr" target="#b9">Liu et al., 2015</ref>) and machine translation ( ).</p><p>The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see <ref type="figure">Figure  2</ref>) . The behavior of the cell is controlled by three "gates", namely input gate i, forget gate f and output gate o. The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector. The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of upper layer of neural network at current time step t. The definitions of the gates, cell update and output are as follows:</p><formula xml:id="formula_5">i (t) = σ(Wixx (t) + W ih h (t−1) + Wicc (t−1) ),<label>(4)</label></formula><formula xml:id="formula_6">f (t) = σ(W f x x (t) + W f h h (t−1) + W f c c (t−1) ),<label>(5)</label></formula><formula xml:id="formula_7">c (t) = f (t) ⊙ c (t−1) + i (t) ⊙ ϕ(Wcxx (t) + W ch h (t−1) ),<label>(6)</label></formula><formula xml:id="formula_8">o (t) = σ(Woxx (t) + W oh h (t−1) + Wocc (t) ),<label>(7)</label></formula><formula xml:id="formula_9">h (t) = o (t) ⊙ ϕ(c (t) ),<label>(8)</label></formula><formula xml:id="formula_10">x (t) c σ σ σ φ φ Output Gate o Input Gate i Forget Gate f LSTM Memory Unit c (t-1) c (t-1) c (t) c (t) c (t-1) o (t) i (t) f (t) h (t) h (t-1) h (t-1) h (t-1) h (t-1)</formula><p>Figure 2: LSTM Memory Unit. The memory unit contains a cell c which is controlled by three gates. The green links show the signals at time t − 1, while the black links show the current signals. The dashed links represent the weight matrices from beginning to end are diagonal. Moreover, the solid pointers mean there are weight matrices on the connections, and hollow pointers mean none. The current output signal, h (t) , will fed back to the next time t + 1 via three gates, and is the input of the higher layer of the neural network as well.</p><p>where σ and ϕ are the logistic sigmoid function and hyperbolic tangent function respectively; i (t) , f (t) , o (t) and c (t) are respectively the input gate, forget gate, output gate, and memory cell activation vector at time step t, all of which have the same size as the hidden vector h (t) ∈ R H 2 ; the parameter matrices W s with different subscripts are all square matrices; ⊙ denotes the element-wise product of the vectors. Note that W ic , W f c and W oc are diagonal matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LSTM Architectures for Chinese Word Segmentation</head><p>To fully utilize the LSTM, we propose four different structures of neural network to select the effective features via memory units. <ref type="figure">Figure 3</ref> illustrates our proposed architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM-1</head><p>The LSTM-1 simply replace the hidden neurons in Eq. (1) with LSTM units (See <ref type="figure">Fig- ure 3a)</ref>.</p><p>The input of the LSTM unit is from a window of context characters. For each character, c (t) , (1 ≤ t ≤ n), the input of the LSTM unit x (t) ,</p><formula xml:id="formula_11">x (t) = v (t−k 1 ) c ⊕ · · · ⊕ v (t+k 2 ) c ,<label>(9)</label></formula><p>is concatenated from character embeddings of c (t−k 1 ):(t+k 2 ) , where k 1 and k 2 represent the numbers of characters from left and right contexts respectively. The output of the LSTM unit is used in final inference function (Eq. <ref type="formula" target="#formula_0">(11)</ref> ) after a linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM-2</head><p>The LSTM-2 can be created by stacking multiple LSTM hidden layers on top of each other, with the output sequence of one layer forming the input sequence for the next (See <ref type="figure">Figure 3b)</ref>. Here we use two LSTM layers. Specifically, input of the upper LSTM layer takes h (t) from the lower LSTM layer without any transformation. The input of the first layer is same to LSTM-1, and the output of the second layer is as same operation as LSTM-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM-3</head><p>The LSTM-3 is a extension of LSTM-1, which adopts a local context of LSTM layer as input of the last layer (See <ref type="figure">Figure 3c</ref>). For each time step t, we concatenate the outputs of a window of the LSTM layer into a vectorˆhvectorˆ vectorˆh (t) ,</p><formula xml:id="formula_12">ˆ h (t) = h (t−m 1 ) ⊕ · · · ⊕ h (t+m 2 ) ,<label>(10)</label></formula><p>where m 1 and m 2 represent the lengths of time lags before and after current time step.Finally, ˆ h (t) is used in final inference function (Eq. <ref type="formula" target="#formula_0">(11)</ref> ) after a linear transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM-4</head><p>The LSTM-4 (see <ref type="figure">Figure 3d</ref>) is a mixture of the LSTM-2 and LSTM-3, which consists of two LSTM layers. The output sequence of the lower LSTM layer forms the input sequence of the upper LSTM layer. The final layer adopts a local context of upper LSTM layer as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference at Sentence Level</head><p>To model the tag dependency, previous neural network models <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref><ref type="bibr" target="#b31">Zheng et al., 2013;</ref><ref type="bibr" target="#b13">Pei et al., 2014</ref>) introduced the transition score A ij for measuring the probability of jumping from tag i ∈ T to tag j ∈ T . For a input sentence c (1:n) with a tag sequence y (1:n) , a sentencelevel score is then given by the sum of tag transition scores and network tagging scores:</p><formula xml:id="formula_13">LSTM LSTM LSTM y (t-1) y (t) y (t+1) x (t-1) x (t) x (t+1) (a) LSTM-1 LSTM LSTM LSTM y (t-1) y (t) y (t+1) x (t-1) x (t) x (t+1) LSTM LSTM LSTM (b) LSTM-2 LSTM LSTM LSTM y (t-1) y (t) y (t+1) x (t-1) x (t) x (t+1) (c) LSTM-3 LSTM LSTM LSTM y (t-1) y (t) y (t+1) x (t-1) x (t) x (t+1) LSTM LSTM LSTM (d) LSTM-4</formula><p>Figure 3: Our proposed LSTM architectures for Chinese word segmentation.</p><formula xml:id="formula_14">s(c (1:n) , y (1:n) , θ) = n ∑ t=1 ( A y (t−1) y (t) + y (t) y (t) ) ,<label>(11)</label></formula><p>where y (t) y (t) indicates the score of tag y (t) , and y (t) is computed by the network as in Eq. (2). The parameter set of our model θ = {M, A,</p><formula xml:id="formula_15">W ic , W f c , W oc , W ix , W f x , W ox , W ih , W f h , W oh , W cx , W ch }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Max-Margin criterion</head><p>We use the Max-Margin criterion to train our model. Intuitively, the Max-Margin criterion provides an alternative to probabilistic, likelihood based estimation methods by concentrating directly on the robustness of the decision boundary of a model ( <ref type="bibr">Taskar et al., 2005</ref>). We use Y (x i ) to denote the set of all possible tag sequences for a given sentence x i and the correct tag sequence for x i is y i . The parameter set of our model is θ. We first define a structured margin loss ∆(y i , ˆ y) for predicted tag sequencê y:</p><formula xml:id="formula_16">∆(y i , ˆ y) = n ∑ t η1{y (t) i ̸ = ˆ y (t) },<label>(12)</label></formula><p>where n is the length of sentence x i and η is a discount parameter. The loss is proportional to the number of characters with incorrect tags in the proposed tag sequence. For a given training instance (x i , y i ),the predicted tag sequencê y i ∈ Y (x i ) is the one with the highest score:</p><formula xml:id="formula_17">ˆ y i = arg max y∈Y (x i ) s(x i , y, θ),<label>(13)</label></formula><p>where the function s(·) is sentence-level score and defined in equation <ref type="formula" target="#formula_0">(11)</ref>. Given a set of training set D, the regularized objective function is the loss function J(θ) including a l 2 -norm term:</p><formula xml:id="formula_18">J(θ) = 1 |D| ∑ (x i ,y i )∈D l i (θ) + λ 2 ∥θ∥ 2 2 ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_19">l i (θ) = max(0, s(x i , ˆ y i , θ) + ∆(y i , ˆ y i ) − s(x i , y i , θ)).</formula><p>To minimize J(θ), we use a generalization of gradient descent called subgradient method ( <ref type="bibr" target="#b16">Ratliff et al., 2007</ref>) which computes a gradientlike direction.</p><p>Following <ref type="bibr" target="#b18">(Socher et al., 2013</ref>), we also use the diagonal variant of AdaGrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>) with minibatchs to minimize the objective. The parameter update for the i-th parameter θ t,i at time step t is as follows:</p><formula xml:id="formula_20">θ t,i = θ t−1,i − α √ ∑ t τ =1 g 2 τ,i g t,i ,<label>(15)</label></formula><p>where α is the initial learning rate and g τ ∈ R |θ i | is the subgradient at time step τ for parameter θ i . In addition, the process of back propagation is followd Hochreiter and Schmidhuber (1997).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dropout</head><p>Dropout is one of prevalent methods to avoid overfitting in neural networks ( <ref type="bibr" target="#b19">Srivastava et al., 2014</ref>). When dropping a unit out, we temporarily remove it from the network, along with all its incoming and outgoing connections. In the simplest case, each unit is omitted with a fixed probability p independent of other units, namely dropout rate, where p is also chosen on development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use three popular datasets, PKU, MSRA and CTB6, to evaluate our model. The PKU   Figure 5: Performances of LSTM-1 (0,2) with 20% dropout on PKU development set.</p><p>and MSRA data are provided by the second International Chinese Word Segmentation Bakeoff ( <ref type="bibr" target="#b6">Emerson, 2005)</ref>, and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) ( <ref type="bibr" target="#b26">Xue et al., 2005</ref>), which is a segmented, part-of-speech tagged and fully bracketed corpus in the constituency formalism. These datasets are commonly used by previous state-of-the-art models and neural network models. In addition, we use the first 90% sentences of the training data as training set and the rest 10% sentences as development set for PKU and MSRA datasets. For CTB6 dataset, we divide the training, development and test sets according to <ref type="bibr" target="#b28">(Yang and Xue, 2012</ref>) All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag.</p><p>For evaluation, we use the standard bake-off scoring program to calculate precision, recall, F1-score and out-of-vocabulary (OOV) word recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyper-parameters</head><p>Hyper-parameters of neural model impact the performance of the algorithm significantly. According to experiment results, we choose the hyperparameters of our model as showing in <ref type="figure">Figure  1</ref>. The minibatch size is set to 20. Generally, the number of hidden units has a limited impact on the performance as long as it is large enough. We found that 150 is a good trade-off between speed and model performance. The dimensionality of character embedding is set to 100 which achieved the best performance. All these hyperparameters are chosen according to their average performances on three development sets.</p><p>For the context lengths (k 1 , k 2 ) and dropout strategy, we give detailed analysis in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dropout and Context Length</head><p>We first investigate the different dropout strategies, including dropout at different layers and with different dropout rate p. As a result, we found that it is a good trade-off between speed and model performance to drop the input layer only with dropout rate p input = 0.2. However, it does not show any significant improvement to dropout on hidden LSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Length</head><p>Dropout rate=20% Dropout rate=50% without <ref type="table" target="#tab_2">Dropout  P  R  F  P  R  F  P  R  F  LSTM-1 (2,2)</ref> 95.8 95.3 95.6 94.8 94.4 94.6 95.2 94.9 95.1 LSTM-1 (1,2) 95.7 95.3 95.5 94.8 94.4 94.6 95.4 94.9 95.2 LSTM-1 (0,2) 95.8 95.5 95.7 94.6 94.2 94.4 95.4 95.0 95.2  <ref type="table">Table 3</ref>: Performance on our four proposed models on PKU test set.</p><p>Due to space constraints, we just give the performances of LSTM-1 model on PKU dataset with different context lengths (k 1 , k 2 ) and dropout rates in <ref type="figure" target="#fig_1">Figure 4</ref> and <ref type="table" target="#tab_3">Table 2</ref>. From <ref type="figure" target="#fig_1">Figure 4</ref>, we can see that 20% dropout converges slightly slower than the one without dropout, but avoids overfitting. 50% or higher dropout rate seems to be underfitting since its training error is also high. <ref type="table" target="#tab_3">Table 2</ref> shows that the LSTM-1 model performs consistently well with the different context length, but the LSTM-1 model with short context length saves computational resource, and gets more efficiency. At the meanwhile, the LSTM-1 model with context length (0,2) can receive the same or better performance than that with context length (2,2), which shows that the LSTM model can well model the pervious information, and it is more robust for its insensitivity of window size variation.</p><p>We employ context length (0,2) with the 20% dropout rate in the following experiments to balance the tradeoff between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Model Selection</head><p>We also evaluate the our four proposed models with the hyper-parameter settings in <ref type="table" target="#tab_2">Table 1</ref>. For LSTM-3 and LSTM-4 models, the context window length of top LSTM layer is set to (2,0). For LSTM-2 and LSTM-4,the number of upper hidden LSTM layer is set to 100. We use PKU dataset to select the best model. <ref type="figure">Figure 5</ref> shows the results of the four models on PKU development set from first epoch to 60-th epoch. We see that the LSTM-1 is the fastest one to converge and achieves the best performance. The LSTM-2 (two LSTM layers) get worse, which shows the performance seems not to benefit from deep model. The LSTM-3 and LSTM-4 models do not converge, which could be caused by the complexity of models.</p><p>The results on PKU test set are also shown in Table 3, which again show that the LSTM-1 achieves the best performance. Therefore, in the rest of the paper we will give more analysis based on the LSTM-1 with hyper-parameter settings as showing in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experiment Results</head><p>In this section, we give comparisons of the LSTM-1 with pervious neural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets.</p><p>We first compare our model with two neural models ( <ref type="bibr" target="#b31">Zheng et al., 2013;</ref><ref type="bibr" target="#b13">Pei et al., 2014</ref>) on Chinese word segmentation task with random initialized character embeddings. As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit. And more notably, our window size of the context characters is set to (0,2), while the size of the other models is <ref type="bibr">(2,</ref><ref type="bibr">2)</ref>.</p><p>Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data. We use word2vec <ref type="bibr">1 (Mikolov et al., 2013a</ref>) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus. The obtained embeddings are used to initialize the character lookup table instead of random initialization. Inspired by <ref type="bibr" target="#b13">(Pei et al., 2014</ref>), we also utilize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters.    with previous neural models with pre-trained embedding and bigram embeddings. <ref type="table" target="#tab_7">Table 6</ref> lists the performances of our model as well as previous state-of-the-art systems. ( <ref type="bibr" target="#b29">Zhang and Clark, 2007</ref>) is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a window of characters. Moreover, some systems (such as Sun and Xu (2011) and <ref type="bibr" target="#b30">Zhang et al. (2013)</ref>) also exploit kinds of extra information such as unlabeled data or other knowledge. Despite our model only uses simple bigram features, it outperforms previous state-of-the-art models which use more complex features.</p><p>Since that we do not focus on the speed of the algorithm in this paper, we do not optimize the speed a lot. On PKU dataset, it takes about 3 days to train the model (last row of <ref type="table" target="#tab_4">Table 5</ref>) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling <ref type="bibr" target="#b27">(Xue, 2003)</ref>. Recently, researchers have tended to explore neural network based approaches <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) to reduce efforts of feature engineering ( <ref type="bibr" target="#b31">Zheng et al., 2013;</ref><ref type="bibr" target="#b13">Pei et al., 2014;</ref><ref type="bibr" target="#b15">Qi et al., 2014;</ref>. The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model the potential long-distance features. Though our model use smaller context window size (0,2), it still outperforms the previous neural models with context window size <ref type="bibr">(2,</ref><ref type="bibr">2)</ref>. Besides, our model can also be easily generalized and applied to other sequence labeling tasks.</p><p>Although our model achieves state-of-the-art performance, it only makes use of previous context. The future context is also useful for Chinese word segmentation. In future work, we would like to adopt the bidirectional recurrent neural network <ref type="bibr" target="#b17">(Schuster and Paliwal, 1997)</ref> to process the sequence in both directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performances of LSTM-1 with the different context lengths and dropout rates on PKU development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table Linear x (t)</head><label>Linear</label><figDesc></figDesc><table>Sigmoid 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Settings of the hyper-parameters. 

0 
20 
40 
60 
80 

85 

90 

95 

epoches 

F-value(%) 

LSTM-1 
LSTM-2 

LSTM-3 
LSTM-4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set. 

models 
Contextr Length = (0,2) 
P 
R 
F 
LSTM-1 95.8 95.5 
95.7 
LSTM-2 95.1 94.5 
94.8 
LSTM-3 89.1 90.4 
89.8 
LSTM-4 92.1 91.7 
91.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 shows</head><label>5</label><figDesc></figDesc><table>the performances with addi-
tional pre-trained and bigram character embed-
dings. Again, the performances boost significantly 
as a result. Moreover, when we use bigram embed-
dings only, which means we do close test without 
pre-training the embeddings on other extra corpus, 
our model still perform competitively compared </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performances on three test sets with random initialized character embeddings. The results with 
* symbol are from our implementations of their methods. 

models 
PKU 
MSRA 
CTB6 
P 
R 
F 
P 
R 
F 
P 
R 
F 
+Pre-train 
(Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* 
(Pei et al., 2014) 
94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* 
LSTM 
96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 

+bigram 
LSTM 
96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 

+Pre-train+bigram 
(Pei et al., 2014) 
-
-95.2 -
-97.2 
-
-
-
LSTM 
96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performances on three test sets with pre-trained and bigram character embeddings. The results 
with * symbol are from our implementations of their methods. 

Models 
PKU MSRA CTB6 
(Tseng et al., 2005) 
95.0 96.4 
-
(Zhang and Clark, 2007) 95.1 97.2 
-
(Sun and Xu, 2011) 
-
-
95.7 
(Zhang et al., 2013) 
96.1 97.4 
-
This work 
96.5 97.4 96.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison of our model with state-of-
the-art methods on three test sets. 

</table></figure>

			<note place="foot" n="3"> Long Short-Term Memory Neural Network for Chinese Word Segmentation In this section, we introduce the LSTM neural network for Chinese word segmentation.</note>

			<note place="foot" n="1"> http://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), National High Technology Research and Development Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gated recursive neural network for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The second international Chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-timescale long short-term memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<editor>IN-TERSPEECH</editor>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Baobao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for character-based information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="668" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">(online) subgradient methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nathan D Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Conference on Artificial Intelligence and Statistics (AIStats)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing Chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting recurrent neural networks for robust asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Suman V Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4085" to="4088" />
		</imprint>
	</monogr>
	<note>2012 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as character tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Chinese Language Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chinese comma disambiguation for discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="786" to="794" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
