As shown in, all DNN-based models achieve significant improvements compared to Random guess and TF-IDF, which implies the effectiveness of DNN models in the task of response selection.

IMDB and Elec are for sentiment classification (positive vs. negative) of movie reviews and Amazon electronics product reviews, respectively.

Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test.

In this paper , we introduce a hierarchical recurrent neural network that is capable of extracting information from raw sentences for relation classification.

In this paper, we present a simple method for common sense reasoning with neural networks, using unsupervised learning.

We consider the problem of part-of-speech tagging for informal, online conversational text.

We evaluate our tagger on the NPS Chat Corpus, a PTB-part-of-speech annotated dataset of Internet Relay Chat (IRC) room messages from 2006.

In order to learn models for disambiguating a large set of content words, a high-coverage sense-annotated corpus is required.

Tracking the user's intention throughout the course of a dialog , called dialog state tracking, is an important component of any dialog system.

We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input.

We evaluate our approach on three datasets: TriviaQA unfiltered (, a dataset of questions from trivia databases paired with documents found by completing a web search of the questions; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question document pair where the document contains the question answer as an individual training point; and SQuAD (, a collection of Wikipedia articles and crowdsourced questions.

We evaluate our system on two data sets for two sequence labeling tasks-Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER).

Table 4: POS tagging accuracy of our model on test data from WSJ proportion of PTB, together with top-performance systems.

accuracy for POS tagging and 91.21% F1 for NER

In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases.

Table 4. Filtered Mean Reciprocal Rank (MRR) for the models tested on each relation of the Wordnet dataset (WN18).

Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on the WSJ benchmark.

We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details , and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document.

Table 1: CNN/DailyMail summarization results.

For the Penn Treebank word-level language modeling task, we report results both with and without weighttying (WT) of input and output mappings for fair comparisons.

Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.

Given two sets of annotations (e.g., one each from two time normalization systems), we define the overall precision, P , as the average of interval precisions where each annotation from the first set is paired with all annotations that textually overlap it in the second set.

This paper presents the first model for time normalization trained on the SCATE corpus.

We perform experiments on two tasks: transition-based dependency parsing and neural machine translation.

Table 2: P@N for relation extraction using variable number of sentences in bags (with more than one sentence) in Riedel dataset.

Our parser obtained 59% Smatch on the SemEval test set.

Oxford at SemEval-2017 Task 9: Neural AMR Parsing with Pointer-Augmented Attention

For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources.

We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.

The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review.

A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering "Who did what to whom".

We apply our method to an image classification task with CIFAR-10 and a language modeling task with Penn Treebank, two of the most benchmarked datasets in deep learning.

Table 2: Single model perplexity on the test set of the Penn Treebank language modeling task.

We use the English coreference resolution data from the CoNLL-2012 shared task in our experiments.

Table 2: Dependency parsing results for English and Czech.

Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text.

Table 3: Comparison of F1 for relation classification on SemEval-2010 Task 8.

In this paper we study the problem of answering cloze-style questions over documents.

We present CATENA, a sieve-based system to perform temporal and causal relation extraction and classification from English texts, exploiting the interaction between the temporal and the causal model.

In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora.

In this paper we benchmark MARN's understanding of human communication on three tasks: 1) multimodal sentiment analysis, 2) multimodal speaker traits recognition and 3) multimodal emotion recognition.

Our model achieves significant and consistent improvements on relation extraction as compared with baselines.

We propose a novel neural network model for joint part-of-speech (POS) tagging and dependency parsing.

We also use DUC-2002, which is also a long-paragraph summarization dataset of news articles.

In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging.

Table 2: Comparison of tagging performance on POS, chunking and NER tasks for various models.

We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD).

We train our model separately on two sentiment analysis datasets: the Stanford Sentiment Treebank (SST) and the IMDb dataset.

In this section, we evaluate our model on the task of question answering using the recently released SQuAD.

The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.

Table 3: AMR parsing Smatch scores for the experiments in this work.

Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.

Previously, neural methods in grammatical error correction (GEC) did not reach state-of-the-art results compared to phrase-based statistical machine translation (SMT) baselines.

We used IMDB, Elec, and RCV1 for our semi-supervised experiments.

We used four datasets: IMDB, Elec, RCV1 (second-level topics), and 20-newsgroup (20NG) 3 , to facilitate direct comparison with JZ15 and DL15.

Experimental results on the SemEval-2010 Task 8 dataset demonstrate that our model is comparable to the state-of-the-art without using any hand-crafted features.

In our experiments, we evaluate our model on the SemEval-2010 Task 8 dataset, which is one of the most widely used benchmarks for relation classification.

We conduct experiments to evaluate our methods on two tasks: Pronoun Disambiguation Problems and Winograd Schema Challenge.

Table 5: Accuracy on Winograd Schema Challenge, making use of STORIES corpus.

IRC is another medium of online conversational text, with similar emoticons, misspellings, abbreviations and acronyms as Twitter data.

Table 5: F1 performance of different models on the Senseval-2 English Lexical Sample task.

Tracking the user's intention throughout the course of a dialog , called dialog state tracking, is an important component of any dialog system.

We evaluate our system on two data sets for two sequence labeling tasks-Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER).

Table 4. Filtered Mean Reciprocal Rank (MRR) for the models tested on each relation of the Wordnet dataset (WN18).

Table 2. Filtered and Raw Mean Reciprocal Rank (MRR) for the models tested on the FB15K and WN18 datasets.

Table 2: f-scores from evaluating the rerank- ing parser on three held-out sections after adding reranked sentences from NANC to WSJ training.

We evaluate the proposed method on Senseval-2, Senseval-3, SemEval-2007, SemEval-2013 and SemEval-2015 English All-Word WSD datasets and show that it outperforms the state-of-the-art unsupervised knowledge-based WSD system by a significant margin.

Table 3: Entropy in Bits Per Character (BPC) on the text8 test set (results under 1.5 BPC & without dynamic evaluation).

We train and evaluate our models on the SCATE corpus described in Section 4.

Experimental results show that our method outperforms the state-of-the-art approaches on the SemEval-2010 Task 8 dataset.

We train our models with the two AMR datasets provided for the shared task: LDC2016E25, a large corpus of newswire, weblog and discussion forum text with a training set of 35,498 sentences, and a smaller dataset in the biomedical domain (Bio AMR Corpus) with 5,542 training sentences.

Evaluation was performed on the well-established CoNLL-2003 NER shared task dataset) and the much larger but less-studied OntoNotes 5.0 dataset.

In this section, we conduct several experiments on two public DA datasets SwDA and MRDA, and show the effectiveness of our approach CRF-ASN for dialogue act recognition.

The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).

To evaluate the performance of our proposed method, we use the SemEval-2010 Task 8 dataset.

We show that this improves tracking of rare states and achieves state-of-the-art performance on the WoZ and DSTC2 state tracking tasks.

In our experiments, we evaluate the VNER model on two benchmark datasets for Vietnamese NER which are VLSP-2016 NER task and VLSP-2018 NER task.

In addition to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets.

To create a dataset without this property, Toutanova and Chen (2015) introduced FB15k-237 -a subset of FB15k where inverse relations are removed.

For fair comparison, we use the benchmark datasets proposed by which includes five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions.
They are Senseval-2 (SE2), Senseval-3 task 1 (SE3), SemEval-07 task 17 (SE7), SemEval-13 task 12 (SE13), and SemEval-15 task 13 (SE15).

For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).

Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.

Accuracy on 3-class prediction about SemEval 2014 Task 4 which includes restaurants and laptops.

The SemEval 2014 dataset is composed of reviews in two categories: Restaurant and Laptop.

This paper provides evidence that the use of more unlabeled data in semi-supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part-of-speech tagging, syntactic chunking, and named entity recognition.

Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL'00 and '03 shared task data for the above three NLP tasks, respectively.

This paper describes 300-sparsans' participation in SemEval-2018 Task 9: Hypernym Discovery , with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings.

This paper presents the participation of Apollo's team in the SemEval-2018 Task 9 "Hypernym Discovery", Subtask 1: "Gen-eral-Purpose Hypernym Discovery", which tries to produce a ranked list of hypernyms for a specific term.

For MRPC and STSB we consider only the F1 score and Spearman correlations respectively and we also multiply the SICK-R scores by 100 to have all differences in the same scale.

We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.

It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.

We conduct all experiments on the AMR corpus used in SemEval-2016 Task 8 (LDC2015E86), which contains 16,833/1,368/1,371 train/dev/test examples.

Additionally, we improve the state-of-the-art (SoTA) results of bpc/perplexity from 1.06 to 0.99 on enwiki8, from 1.13 to 1.08 on text8, from 20.5 to 18.3 on WikiText-103, from 23.7 to 21.8 on One Billion Word, and from 55.3 to 54.5 on Penn Treebank (without finetuning).

Finally, DCU-LSTM significantly outperforms all models in terms of ROUGE-L, including BiDAF on this dataset.

There are two subsets of RACE, namely RACE-M (Middle school) and RACE-H (High school).

We also propose the AUC (area under the ROC curve) metric for both aspect and sentiment detection tasks.

The numbers in the y-axes are accuracies for POS tagging, and chunk-level F1 scores for chunking and NER.

We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) F1 scores.

Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset.

Table 3: AMR parsing performance on the news wire test set of LDC2013E117.
