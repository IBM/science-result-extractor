
As shown in, all DNN - based models achieve significant improvements compared to Random guess and TF - IDF, which implies the effectiveness of DNN models in the task of response selection.
IMDB and Elec are for sentiment classification (positive vs. negative) of movie reviews and Amazon electronics product reviews, respectively.
Several large cloze - style context - question - answer datasets have been introduced recently : the CNN and Daily Mail news data and the Children ' s Book Test.
In this paper, we introduce a hierarchical recurrent neural network that is capable of extracting information from raw sentences for relation classification.
In this paper, we present a simple method for common sense reasoning with neural networks, using unsupervised learning.
We consider the problem of part - of - speech tagging for informal, online conversational text.
We evaluate our tagger on the NPS Chat Corpus, a PTB - part - of - speech annotated dataset of Internet Relay Chat (IRC) room messages from 2006.
In order to learn models for disambiguating a large set of content words, a high - coverage sense - annotated corpus is required.
Tracking the user ' s intention throughout the course of a dialog, called dialog state tracking, is an important component of any dialog system.
We consider the problem of adapting neural paragraph - level question answering models to the case where entire documents are given as input.
We evaluate our approach on three datasets : TriviaQA unfiltered (, a dataset of questions from trivia databases paired with documents found by completing a web search of the questions ; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question document pair where the document contains the question answer as an individual training point ; and SQuAD (, a collection of Wikipedia articles and crowdsourced questions.
We evaluate our system on two data sets for two sequence labeling tasks - Penn Treebank WSJ corpus for part - of - speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER).
Table 4 : POS tagging accuracy of our model on test data from WSJ proportion of PTB, together with top - performance systems.
accuracy for POS tagging and 91. 21 % F1 for NER
In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases.
Table 4. Filtered Mean Reciprocal Rank (MRR) for the models tested on each relation of the Wordnet dataset (WN18).
Based on stack - LSTM, our psycholinguistically motivated constituent parsing system achieves 91. 8 F1 on the WSJ benchmark.
We improve these important aspects of abstractive summarization via multi - task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning - worthy details, and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document.
Table 1 : CNN / DailyMail summarization results.
For the Penn Treebank word - level language modeling task, we report results both with and without weighttying (WT) of input and output mappings for fair comparisons.
Table 1 : Validation and test set perplexity of recent state of the art word - level language models on the Penn Treebank dataset.
Given two sets of annotations (e. g., one each from two time normalization systems), we define the overall precision, P, as the average of interval precisions where each annotation from the first set is paired with all annotations that textually overlap it in the second set.
This paper presents the first model for time normalization trained on the SCATE corpus.
We perform experiments on two tasks : transition - based dependency parsing and neural machine translation.
Table 2 : P @ N for relation extraction using variable number of sentences in bags (with more than one sentence) in Riedel dataset.
Our parser obtained 59 % Smatch on the SemEval test set.
Oxford at SemEval - 2017 Task 9 : Neural AMR Parsing with Pointer - Augmented Attention
For AMR parsing, our model achieves competitive results of 62. 1 SMATCH, the current best score reported without significant use of external semantic resources.
We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.
The fine - grained sentiment classification task in the Stanford Sentiment Treebank (SST - 5 ; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review.
A semantic role labeling (SRL) system models the predicate - argument structure of a sentence, and is often described as answering "Who did what to whom".
We apply our method to an image classification task with CIFAR - 10 and a language modeling task with Penn Treebank, two of the most benchmarked datasets in deep learning.
Table 2 : Single model perplexity on the test set of the Penn Treebank language modeling task.
We use the English coreference resolution data from the CoNLL - 2012 shared task in our experiments.
Table 2 : Dependency parsing results for English and Czech.
Distantly supervised open - domain question answering (DS - QA) aims to find answers in collections of unlabeled text.
Table 3 : Comparison of F1 for relation classification on SemEval - 2010 Task 8.
In this paper we study the problem of answering cloze - style questions over documents.
We present CATENA, a sieve - based system to perform temporal and causal relation extraction and classification from English texts, exploiting the interaction between the temporal and the causal model.
In this work, we model abstractive text summarization using Attentional Encoder - Decoder Recurrent Neural Networks, and show that they achieve state - of - the - art performance on two different corpora.
In this paper we benchmark MARN ' s understanding of human communication on three tasks : 1) multimodal sentiment analysis, 2) multimodal speaker traits recognition and 3) multimodal emotion recognition.
Our model achieves significant and consistent improvements on relation extraction as compared with baselines.
We propose a novel neural network model for joint part - of - speech (POS) tagging and dependency parsing.
We also use DUC - 2002, which is also a long - paragraph summarization dataset of news articles.
In this paper, we propose a variety of Long Short - Term Memory (LSTM) based models for sequence tagging.
Table 2 : Comparison of tagging performance on POS, chunking and NER tasks for various models.
We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks : sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD).
We train our model separately on two sentiment analysis datasets : the Stanford Sentiment Treebank (SST) and the IMDb dataset.
In this section, we evaluate our model on the task of question answering using the recently released SQuAD.
The experimental results on the SemEval - 2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.
Table 3 : AMR parsing Smatch scores for the experiments in this work.
Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine - grained classification tasks.
Previously, neural methods in grammatical error correction (GEC) did not reach state - of - the - art results compared to phrase - based statistical machine translation (SMT) baselines.
We used IMDB, Elec, and RCV1 for our semi - supervised experiments.
We used four datasets : IMDB, Elec, RCV1 (second - level topics), and 20 - newsgroup (20NG) 3, to facilitate direct comparison with JZ15 and DL15.
Experimental results on the SemEval - 2010 Task 8 dataset demonstrate that our model is comparable to the state - of - the - art without using any hand - crafted features.
In our experiments, we evaluate our model on the SemEval - 2010 Task 8 dataset, which is one of the most widely used benchmarks for relation classification.
We conduct experiments to evaluate our methods on two tasks : Pronoun Disambiguation Problems and Winograd Schema Challenge.
Table 5 : Accuracy on Winograd Schema Challenge, making use of STORIES corpus.
IRC is another medium of online conversational text, with similar emoticons, misspellings, abbreviations and acronyms as Twitter data.
Table 5 : F1 performance of different models on the Senseval - 2 English Lexical Sample task.
Tracking the user ' s intention throughout the course of a dialog, called dialog state tracking, is an important component of any dialog system.
We evaluate our system on two data sets for two sequence labeling tasks - Penn Treebank WSJ corpus for part - of - speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER).
Table 4. Filtered Mean Reciprocal Rank (MRR) for the models tested on each relation of the Wordnet dataset (WN18).
Table 2. Filtered and Raw Mean Reciprocal Rank (MRR) for the models tested on the FB15K and WN18 datasets.
Table 2 : f - scores from evaluating the rerank - ing parser on three held - out sections after adding reranked sentences from NANC to WSJ training.
We evaluate the proposed method on Senseval - 2, Senseval - 3, SemEval - 2007, SemEval - 2013 and SemEval - 2015 English All - Word WSD datasets and show that it outperforms the state - of - the - art unsupervised knowledge - based WSD system by a significant margin.
Table 3 : Entropy in Bits Per Character (BPC) on the text8 test set (results under 1. 5 BPC & without dynamic evaluation).
We train and evaluate our models on the SCATE corpus described in Section 4.
Experimental results show that our method outperforms the state - of - the - art approaches on the SemEval - 2010 Task 8 dataset.
We train our models with the two AMR datasets provided for the shared task : LDC2016E25, a large corpus of newswire, weblog and discussion forum text with a training set of 35, 498 sentences, and a smaller dataset in the biomedical domain (Bio AMR Corpus) with 5, 542 training sentences.
Evaluation was performed on the well - established CoNLL - 2003 NER shared task dataset) and the much larger but less - studied OntoNotes 5. 0 dataset.
In this section, we conduct several experiments on two public DA datasets SwDA and MRDA, and show the effectiveness of our approach CRF - ASN for dialogue act recognition.
The parser achieves the best reported results on the standard benchmark (74. 4 % on LDC2016E25).
To evaluate the performance of our proposed method, we use the SemEval - 2010 Task 8 dataset.
We show that this improves tracking of rare states and achieves state - of - the - art performance on the WoZ and DSTC2 state tracking tasks.
In our experiments, we evaluate the VNER model on two benchmark datasets for Vietnamese NER which are VLSP - 2016 NER task and VLSP - 2018 NER task.
In addition to the model, we release WikiSQL, a dataset of 80654 hand - annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia that is an order of magnitude larger than comparable datasets.
To create a dataset without this property, Toutanova and Chen (2015) introduced FB15k - 237 - a subset of FB15k where inverse relations are removed.
For fair comparison, we use the benchmark datasets proposed by which includes five standard all - words fine - grained WSD datasets from the Senseval and SemEval competitions.
They are Senseval - 2 (SE2), Senseval - 3 task 1 (SE3), SemEval - 07 task 17 (SE7), SemEval - 13 task 12 (SE13), and SemEval - 15 task 13 (SE15).
For instance, we achieve absolute improvements of 8. 9 % on commonsense reasoning (Stories Cloze Test), 5. 7 % on question answering (RACE), and 1. 5 % on textual entailment (MultiNLI).
Our model achieves 28. 4 BLEU on the WMT 2014 English - to - German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.
Accuracy on 3 - class prediction about SemEval 2014 Task 4 which includes restaurants and laptops.
The SemEval 2014 dataset is composed of reviews in two categories : Restaurant and Laptop.
This paper provides evidence that the use of more unlabeled data in semi - supervised learning can improve the performance of Natural Language Processing (NLP) tasks, such as part - of - speech tagging, syntactic chunking, and named entity recognition.
Then, we describe experiments performed on widely used test collections, namely, PTB III data, CoNLL ' 00 and ' 03 shared task data for the above three NLP tasks, respectively.
This paper describes 300 - sparsans ' participation in SemEval - 2018 Task 9 : Hypernym Discovery, with a system based on sparse coding and a formal concept hierarchy obtained from word embeddings.
This paper presents the participation of Apollo ' s team in the SemEval - 2018 Task 9 "Hypernym Discovery", Subtask 1 : "Gen - eral - Purpose Hypernym Discovery", which tries to produce a ranked list of hypernyms for a specific term.
For MRPC and STSB we consider only the F1 score and Spearman correlations respectively and we also multiply the SICK - R scores by 100 to have all differences in the same scale.
We propose a simple and effective method to address this issue, and improve the state - of - the - art perplexities on Penn Treebank and WikiText - 2 to 47. 69 and 40. 68 respectively.
It obtains new state - of - the - art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80. 4 % (7. 6 % absolute improvement), MultiNLI accuracy to 86. 7 % (5. 6 % absolute improvement) and the SQuAD v1. 1 question answering Test F1 to 93. 2 (1. 5 absolute improvement), outperforming human performance by 2. 0.
We conduct all experiments on the AMR corpus used in SemEval - 2016 Task 8 (LDC2015E86), which contains 16, 833 / 1, 368 / 1, 371 train / dev / test examples.
Additionally, we improve the state - of - the - art (SoTA) results of bpc / perplexity from 1. 06 to 0. 99 on enwiki8, from 1. 13 to 1. 08 on text8, from 20. 5 to 18. 3 on WikiText - 103, from 23. 7 to 21. 8 on One Billion Word, and from 55. 3 to 54. 5 on Penn Treebank (without finetuning).
Finally, DCU - LSTM significantly outperforms all models in terms of ROUGE - L, including BiDAF on this dataset.
There are two subsets of RACE, namely RACE - M (Middle school) and RACE - H (High school).
We also propose the AUC (area under the ROC curve) metric for both aspect and sentiment detection tasks.
The numbers in the y - axes are accuracies for POS tagging, and chunk - level F1 scores for chunking and NER.
We report ROUGE - 1 (R1), ROUGE - 2 (R2), and ROUGE - L (RL) F1 scores.
Our parser gets state of the art or near state of the art performance on standard treebanks for six different languages, achieving 95. 7 % UAS and 94. 1 % LAS on the most popular English PTB dataset.
Table 3 : AMR parsing performance on the news wire test set of LDC2013E117.
shows an example of this word overlap problem from the NTCIR - 8 ACLIA2 Japanese question answering test collection.
Methods based on word strings (e. g., BLEU (), NIST), METEOR (), ROUGE - L (), and IMPACT) calculate matching scores using only common words between MT outputs and references from bilingual humans.
The system based on our method obtained the evaluation scores for 1, 200 English output sentences related to the patent sentences.
Moreover, three human judges evaluated 1, 200 English output sentences from the perspective of adequacy and fluency on a scale of 1 - 5.
Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings..
There are two fundamental problems in research on approximate string search : (1) how to build a model that can archive both high accuracy and efficiency, and (2) how to develop a data structure and algorithm that can facilitate efficient retrieval of the top k candidates.
However, the drop of accuracy by our method is much smaller than that by generative, which means our method is more powerful when the vocabulary is large, e. g., for web search.
Syntax - to - Morphology Mapping in Factored Phrase - Based Statistical Machine Translation from English to Turkish
Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non - local syntactic structures as complex structural tags which appear as additional factors in the training data.
On the target side (Turkish), we only perform morphological analysis and disam - biguation but treat the complete complex morphological tag as a factor, instead of separating morphemes.
Earlier work on translation from English to Turkish has used an approach which relied on identifying the contextually correct parts - of - speech, roots and any morphemes on the English side, and the complete sequence of roots and overt derivational and inflectional morphemes for each word on the Turkish side.
Ave : BLEU scores of after reordering transformations Factored phrase - based SMT allows the use of multiple language models for the target side, for different factors during decoding.
We used the English portion of the TDT5 corpora (LDC2006T18) as our unlabeled data for inducing word clusters.
For relation extraction, we used the benchmark ACE 2004 training data.
Preprocessing of the ACE documents : We used the Stanford parser 6 for syntactic and dependency parsing.
We present a novel approach to the automatic acquisition of a Verbnet like classification of French verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources.
While there has been much work on automatically acquiring verb classes for English (and to a lesser extent for German (; Schulte im), Japanese) and Italian (), few studies have been conducted on the automatic classification of French verbs.
We show that the approach yields promising results (F - measure of 70 %) and that the clustering produced systematically associates verbs with syntactic frames and thematic grids thereby providing an interesting basis for the creation and evaluation of a Verbnet - like classification.
With the development of Web 2. 0, community question answering (CQA) services like Yahoo!
Although exploited bilingual translation for question retrieval and obtained the better performance than traditional monolingual translation models.
Evaluation Metrics : We evaluate the performance of question retrieval using the following metrics : Mean Average Precision (MAP) and.
sentences and part - of - speech tags), and is much cheaper than the syntactically annotated data required for supervised training.
However, because the model is formulated within the Adaptor Grammar framework, the time complexity of its inference algorithm is cubic in the length of each text fragment, and so it is not feasible to apply the AG - colloc model to large collections of text documents.
In the classification task, we used three datasets : the movie review dataset (Pang and Lee, 2012) (MReviews), the 20 Newsgroups dataset, and the Reuters - 21578 dataset.
The 20 Newsgroups dataset is organised : Comparison of all models in the classification task (accuracy in %) and the information retrieval task (MAP scores in %) on small corpora.
The AG - colloc model yields the highest classification accuracy, and our TCM with / without sparsity performs as well as the AG - colloc model according to the Wilcoxon signed rank test.
shows the classification accuracy of those three models on the larger datasets, i. e., the 20 Newsgroups dataset, and the Reuters - 21578 dataset.
Table 1 : Comparison of all models in the classi - fication task (accuracy in %) and the information retrieval task (MAP scores in %) on small corpora.
Table 3 : Mean average Precision (MAP in %) scores in the information retrieval task.
As training data, we use both labeled and unlabeled data, utilizing an expectation maximization algorithm for parameter estimation.
It uses diverse types of semantic knowledge, a mixture of labeled and unlabeled data for training data, a logistic regression classi - fier, and expectation maximization (EM) for parameter estimation 2) Collins is the established baseline among PP attachment algorithms.
Our parser is also able to generalize well across languages with little tuning : it achieves state - of - the - art results on multilingual parsing, scoring higher than the best single - parser system from the SPMRL 2013 Shared Task on a range of languages, as well as on the competition ' s average F1 metric.
Table 1 : Results for the Penn Treebank development set, reported in F1 on sentences of length â¤ 40 on Section 22, for a number of incrementally growing feature sets.
Table 5 : Fine - grained sentiment analysis results on the Stanford Sentiment Treebank of Socher et al.
Knowledge of temporal counterparts can help to alleviate the problem of terminology gap for users searching within temporal document collections such as archives.
We demonstrate that derived constraints aid grammar induction by training Klein and Manning ' s Dependency Model with Valence (DMV) on this data set : parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50. 4 %, beating previous state - of - the - art by more than 5 %.
Unsupervised learning of hierarchical syntactic structure from free - form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation.
This conditioning on partial parses addressed all three problems, leading to : (i) linguistically reasonable constituent boundaries and induced grammars more likely to agree with qualitative judgments of sentence structure, which is underdetermined by unannotated text ; (ii) fewer iterations needed to reach a good grammar, countering convergence properties that sharply deteriorate with the number of non - terminal symbols, due to a proliferation of local maxima ; and (iii) better (in the best case, linear) time complexity per iteration, versus running time that is ordinarily cubic in both sentence length and the total number of non - terminals, rendering sufficiently large grammars computationally impractical.
We judged each one by its accuracy on WSJ45, using standard directed scoring - the fraction of correct dependencies over randomized "best" parse trees.
Since we cannot graph the full (six - dimensional) set of results, we begin with a simple linear regression, using accuracy on WSJ45 as the dependent variable.
To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric ' s responses, before finally contrasting the behaviour of our chance - corrected metric with that of un - corrected parser evaluation metrics on real corpora.
For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fallback to simple accuracy measures.
Next, we present a number of synthetic experiments performed in order to find the best distance function for this kind of annotation ; finally we contrast our new metric and simple accuracy scores as applied to real - world corpora before concluding and presenting some potential avenues for future work.
We report a 0. 9 point improvement in terms of BLEU score on English - Chinese technical documents..
In this paper, we propose a method for discovering matches between problem reports and aid messages.
The 2011 Great East Japan Earthquake in March 11, 2011 killed 15, 883 people and destroyed over 260, 000 households (National Police Agency of Japan, 2013).
Just after the disaster, many people used Twitter for posting problem reports and aid messages as it functioned while most communication channels suffered disruptions).
We developed methods for recognizing problem reports and aid messages in tweets and finding proper matches between them.
We used 600 million Web pages for this similarity estimation.
Currently, workflows for annotating monolingual and bilingual resources of various formats are provided (e. g.
Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora
This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora.
Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works.
The so - called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words.
The polysemy rate indicates how much words in the comparable corpora are associated to more than one translation in the seed bilingual dictionary.
In bilingual terminology extraction from com - parable corpora, a reference list is required to evaluate the performance of the alignment.
Specifically, given a corpus consisting of both machine - translated English text (English being the target language) and native English text (not necessarily the reference translation of the machine - translated text), we measure the accuracy of the system in classifying the sentences in the corpus as machine - translated or not.
To do this, we use the French - English data from the 8th Workshop on Statistical Machine Translation - WMT13 ' MT Engine Example Google Translate "These days, all but one were subject to a vote, and all had a direct link to the post September 11th."
We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights.
The experimental results on a public corpus that contains both news articles and relevant tweets show that our proposed tweets guided sentence compression method can improve the summarization performance significantly compared to the baseline generic sentence compression method..
Instead of using a manually generated corpus, we investigate using existing external sources to guide sentence compression for the purpose of compressive news highlights generation.
In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights.
This is a pioneer study for using such parallel data to guide sentence compression for document summarization.
The result shows that generic compression hurts the performance of highlights generation, while sentence compression guided by relevant tweets of the news article can improve the performance.
In the first mode of operation, we provide afield linguist with tools for running custom elicitation sessions based on a collection of 3D scenes.
We will demonstrate the features of WELT for use in fieldwork, including designing elicitation sessions, building scenes, recording audio, and adding descriptions and glosses to a scene.
In this study, although there is still room for improvement on performance with more feature engineering, we obtain better results on Turkish IMST treebank between static and dynamic oracles with our newly proposed method for parsing DAGs.
Con - textualized word vectors constructed from the GigaWord Corpus provide a method for implicit Word Sense Disambiguation (WSD), whose reliability helps this system outperform baselines and achieve comparable results to those of systems with full WSD modules.
In this paper, we investigate the importance of proper word sense decisions for PP - attachment disambiguation, and describe a highly - accurate system that encodes sense information in contextualized distributional data.
An easy - to - use web interface empowers users to participate in data collection and MT customisation to increase the quality, domain coverage, and usage of MT.
For evaluation of English - Latvian translation, TILDE created a MT system using a significantly larger corpus of 5. 37M parallel sentence pairs, including 1. 29M pairs in the IT domain.
First, we show that for frame identification on the FrameNet corpus (, we outperform the prior state of the art ().
10 10 Note that describe the state of the art in FrameNet - based analysis, but their argument identification strategy considered all possible dependency subtrees in Frame Lexicon In our experimental setup, we scanned the XML files in the "frames" directory of the FrameNet 1. 5 release, which lists all the frames, the corresponding roles and the associated lexical units, and created a frame lexicon to be used in our frame and argument identification models.
a parse, resulting in a much larger search space. : Full frame - structure prediction results for Propbank.
presents results on the full frame - semantic parsing task (measured by a reimplementation of the SemEval 2007 shared task evaluation script) when our argument identification model (Â§ 4) is used after frame identification.
Table 2 : Frame identification results for FrameNet.
Table 3 : Full structure prediction results for FrameNet ; this reports frame and argument identification performance jointly.
Table 4 : Frame identification accuracy results for PropBank.
Table 6 : Argument only evaluation (semantic role labeling metrics) using the CoNLL 2005 shared task evaluation script (Carreras and M ` arquez, 2005).
We address the issue of automatically identifying and resolving implicit arguments in Chinese discourse.
Data : Experimental data set comes from Semantic Computing and Chinese FrameNet Research Centor of Shanxi University.
Our 164 discourses had been annotated by one person (to make it consistent), and they consist of 57 discourses from People ' s Daily and 107 discourses from Chinese reading comprehension, which cover technology, healthcare, social, geography and other fields.
Null Instantiation Detection gives the performance of NI detection, which achieves 72. 71 %, 86. 12 % and 78. 84 % in precision, recall and F - score, respectively.
It shows that DNI identification based on maximum entropy model achieves the performance of 67. 86 %, 69. 93 % and 68. 88 % in terms of precision, recall and F - score respectively, which are better than the results using SVM classifier, as well as the results employing Lei et al. ' s method on our data.
Hence, document representations that reveal information about writing style are required to achieve good accuracy in AA.
Also, we experimented with reducedimbalanced data sets using the same imbalance rates reported in (Plakias and Stamatatos, 2008b ; Plakias and Stamatatos, 2008a) : we tried settings 2 â 10, 5 â 10, and 10 â 20, where, for example, setting 2 - 10 means that we use at least 2 and at most 10 documents per author (we call this setting IRBC).
However, the diffusion kernel outperformed most of the results obtained with other kernels ; confirming the results obtained by other researchers (: Authorship attribution accuracy when using bags of local histograms and different kernels for word - based and character - based representations.
The benefits of such expansion become more notorious as the number of available documents per author decreases. : AA accuracy in RBC (columns 2 - 6) and IRBC (columns 7 - 9) data sets when using words as terms.
For reference (last row), we also include the best result reported in), when available, for each configuration. : AA accuracy in the RBC and IRBC data sets when using character n - grams as terms.
Table 3 : Authorship attribution accuracy when using bags of local histograms and different kernels for word - based and character - based representations.
Table 5 : AA accuracy in RBC (columns 2 - 6) and IRBC (columns 7 - 9) data sets when using words as terms.
Table 6 : AA accuracy in the RBC and IRBC data sets when using character n - grams as terms.
Distinguishing between the two argument types has been discussed extensively in various formulations in the NLP literature, notably in PP attachment, semantic role labeling (SRL) and subcategorization acquisition.
This scenario decouples the accuracy of the algorithm from the quality of the unsupervised POS tagging.
We see accuracy as important on its own right since increasing coverage is often straightforward given easily obtainable larger training corpora.
Machine translation suffers acutely from the domain - mismatch problem caused by microblog text.
This paper introduces a method for finding naturally occurring parallel microblog text, which helps address the domain - mismatch problem.
We report on machine translation experiments using our harvested data in two domains : edited news and microblogs.
We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to + 1. 92 BLEU for Chinese as target..
The data for the English to Chinese task is composed of parliament proceedings and news articles.
BLEU scores for 200K and 400K training sentence pairs.
Automatic processing of metaphor can be clearly divided into two subtasks : metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (iden - tifying the intended literal meaning of a metaphorical expression).
There are, however, inherent problems in the entity repositories : (a) coverage : although coverage of head entity types is often reliable, the tail can be sparse ; (b) noise : created by spammers, extraction errors or errors in crowdsourced content ; (c) ambiguity : multiple types or entity identifiers are often associated with the same surface string ; and (d) over - expression : many entities have types that are never used in the context of Web search.
In the related field of web search ranking, automatically learned non - linear features have brought dramatic improvements in quality (; Wu * This research was conducted during the author ' s internship at Microsoft Research et al., 2010).
For Chinese - English, the training corpus consists of approximately one million sentence pairs from the FBIS and HongKong portions of the LDC data for the NIST MT evaluation and the Dev - Train and Test sets are from NIST competitions.
In conclusion, we proposed anew method to induce feature combinations for machine translation, which do not increase the decoding complexity.
An empirical evaluation on a widely used sentiment corpus shows an improvement of 1. 45 point inaccuracy over the baseline resulting from a combination of bag - of - words and high - impact parse features (Section 4).
We tackle these issues by making the following contributions : â¢ we introduce Topic - Driven Relevance Models, a model - based feedback approach) for integrating topic models into relevance models by learning topics on pseudo - relevant feedback documents (as opposed to the entire document collection), â¢ we explore the coherence of those generated topics using the queries of two major and well - established TREC test collections, â¢ we evaluate the effects coherent topics have on ad hoc IR using the same test collections.
For summarization of spontaneous speech, sentence compression is especially important, since unlike fluent and wellstructured written text, spontaneous speech contains a lot of disfluencies and much redundancy.
BLEU is a widely used metric in evaluating machine translation systems that often use multiple references.
Since there is a great variation inhuman compression results, and we have 8 reference compressions, we explore using BLEU for our sentence compression task.
Regarding the two training settings in reranking, we find that there is no gain from reranking when using only one best compression, however, training with multiple references improves BLEU scores.
This indicates the discriminative training used in maximum entropy reranking is consistent with the performance metrics.
This provides some insight into the evalb scores, and the problem of domain adaptation with the web data.
In this paper we present a novel algorithm for rapidly prototyping virtual instructors from human - human corpora without manual annotation.
In this paper, we present novel a algorithm for generating virtual instructors from automatically annotated human - human corpora.
Currently, WebLicht offers LRT services that were developed independently at the Institut f Ã¼ r Informatik, Abteilung Automatische Sprachverarbeitung at the University of Leipzig (tokenizer, lemmatizer, co - occurrence extraction, and frequency analyzer), at the Institut f Ã¼ r Maschinelle Sprachverarbeitung at the University of Stuttgart (tokenizer, tagger / lemmatizer, German morphological analyser SMOR, constituent and dependency parsers), at the Berlin Brandenburgische Akademie der Wissenschaften (conversion of plain text to D - Spin format, tokenizer, taggers, NE recog -.
This paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity.
Annotated data have become an indispensable resource for many natural language processing (NLP) applications.
On one hand, the amount of existing labeled data is not sufficient ; on the other hand, however there exists multiple annotated data with incompatible annotation guidelines for the same NLP task.
For example, the People ' s Daily corpus () and Chinese Penn Treebank (CTB) () are publicly available for Chinese segmentation.
In addition to the most popular CTB, Tsinghua Chinese Treebank (TC - T)) is another real large - scale treebank for Chinese constituent parsing.
The transformed treebank is used to provide better annotation guideline for the parallel training data of next iteration.
In contrast to such data - independent hashing schemes, recent research has been geared to studying data - dependent hashing through learning compact hash codes from a training dataset.
Despite achieving data - dependent hash codes, most of these "learning to hash" methods cannot guarantee a high success rate of looking a query code up in a hash table (referred to as hash table lookup in literature), which is critical to the high efficacy of exploiting hashing in practical uses.
As we know, document summarization is a very useful means for people to quickly read and browse news articles in the big data era.
The results demonstrate that BrailleSUM can produce much shorter braille summaries while not sacrificing the summaries ' content quality.
Unfortunately, the reliance on role - annotated data which is expensive and time - consuming to produce for every language and domain, presents a major bottleneck to the widespread application of semantic role labeling.
We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment problems..
Table 1 : Time consumption and accuracy on a sample of 10 6000 - character texts.
Unsupervised Discovery of Domain - Specific Knowledge from Text
American football was the first official evaluation domain in the DARPA - sponsored Machine Reading program, and provides the background fora number of LbR systems.
Experiment on the SANCL 2012 shared task show that our approach achieves 93. 15 % average tagging accuracy, which is the best accuracy reported so far on this data set, higher than those given by ensembled syntactic parsers..
However, state - of - the - art POS taggers in the literature are mainly optimized on the the Penn Treebank (PTB), and when shifted to web data, tagging accuracies drop significantly ().
The problem we face here can be considered as a special case of domain adaptation, where we have access to labelled data on the source domain (PTB) and unlabelled data on the target domain (web data).
Exploiting useful information from the web data can be the key to improving web domain tagging.
In the pre - training phase, we learn an encoder that converts the web text into an intermediate representation, which acts as useful features for prediction tasks.
We show why such a paradigm represents a promising direction and present some recent progress on the development of effective methods for construction and mining of structured heterogeneous information networks from text data.
In the context of the simplification task however, the automatically generated sentences are not necessarily well formed so that the FKG index reduces to a measure of the sentence length (in terms of words and syllables) approximating the simplicity level of an output sentence irrespective of the length of the corresponding input.
To assess simplification, we instead use metrics that are directly related to the simplification task namely, the number of splits in the overall (test and training) data and in average per sentences ; the number of generated sentences with no edits i. e., which are identical to the original, complex one ; and the average Levenshtein distance between the system ' s output and both the complex and the simple reference sentences.
Table 3 : Proportion of Split Sentences (% split) in the training / test data and in average per sen - tence (average split / sentence).
Although there is a growing interest in automatically constructing knowledge graphs, e. g., from unstructured web data (, the problem of providing evidence on why two entities are related in a knowledge graph remains largely unaddressed.
Our main contributions area robust and effective method for explaining entity relationships, detailed insights into the performance of our method and features, and a manually annotated dataset.
In this section we describe the dataset, manual annotations, learning to rank algorithm, and evaluation metrics that we use to answer our research questions.
Recent studies in the difficult task of Word Sense Disambiguation, WSD) have shown the impact of the amount and quality of lexical knowledge (Cuadros and) : richer knowledge sources can be of great benefit to both knowledge - lean systems and supervised classifiers ().
But while a great deal of work has been recently devoted to the automatic extraction of structured information from Wikipedia (, inter alia), the knowledge extracted is organized in a looser way than in a computational lexicon such as WordNet.
The average polysemy is 1. 3 and 2. 5 for WordNet senses and Wikipages, respectively (2. 8 and 4. 7 when excluding monosemous words).
The results show that the different regions of BabelNet contain translations of different quality : while on average translations for WordNet - only synsets have a precision around 72 %, when Wikipedia comes into play the performance increases considerably (around 80 % in the intersection and 95 % with Wikipedia - only translations).
The challenges of Named Entities Recognition (NER) for tweets lie in the insufficient information in a tweet and the unavailabil - ity of training data.
For example, the average F1 of the Stanford NER (), which is trained on the CoNLL03 shared task data set and achieves state - of - the - art performance on that task, drops from 90. 8 %) to 45. 8 % on tweets.
Firstly, a K - Nearest Neighbors (KNN) based classifier is adopted to conduct word level classification, leveraging the similar and recently labeled tweets.
For every type of named entity, Precision (Pre.), recall (Rec.) and F1 are used as the evaluation metrics.
Precision is a measure of what percentage the output labels are correct, and recall tells us to what percentage the labels in the gold - standard data set are correctly labeled, while F1 is the harmonic mean of precision and recall.
The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems..
The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set.
Moreover, ILP scale doubles recall comparing to the rules from the Sherlock resource, while maintaining comparable precision.
Though recent knowledge graph embeddings () integrate the relational structure among entities, they primarily target at link prediction and lack an explicit relatedness measure.
In the entity linking task, our approach improves the F1 score by 10 % over state - of - the - art results.
We show that including N - gram count features can advance the state - of - the - art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part - of - speech dis - ambiguation.
We address these questions on two generation and two analysis tasks, using both existing N - gram data and a novel web - scale N - gram corpus that includes part - of - speech information (Section 2).
Systems trained on labeled data can learn the domain usage and leverage other regularities, such as suffixes and transitivity for adjective ordering.
With these benefits, systems trained on labeled data have become the dominant technology in academic NLP.
Our results show that using web - scale N - gram data in supervised systems advances the state - ofthe - art performance on standard analysis and generation tasks.
We evaluate the benefit of N - gram data on multiclass classification problems.
We plot learning curves to measure the accuracy of the classifier when the number of labeled training examples varies.
In an experimental evaluation on the tasks of synonym extraction and bilingual lexicon extraction, CoSimRank is faster or more accurate than previous approaches..
This is a crucial task for financial and security analysts who are interested in pulling together relevant information from unstructured and noisy data streams.
We employ the post - ordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method.
The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English.
In English - Japanese translation, proposed a simple pre - ordering method that achieved the best quality inhuman evaluations, which were conducted for the NTCIR - 9 patent machine translation task ().
Pre - ordering using the head finalization rule naturally cannot be applied to Japanese - English translation, because English is not a head - final language.
2 Post - ordering for Japanese to English proposed a post - ordering method for Japanese - English translation.
Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews
Our experiments on a movie review corpus demonstrate, that an unsupervised anaphora resolution algorithm significantly improves the opinion target extraction.
This paper is structured as follows : Section 2 discusses the related work on opinion target identification and OM on movie reviews.
Over four different datasets spanning from the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico - syntactic features.
Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91. 2 % accuracy with 14 % error reduction..
Over four different datasets spanning from the product review domain to the essay domain, we find that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico - syntactic features.
Our results improve the best published result on the hotel review data of reaching 91. 2 % accuracy with 14 % error reduction.
We also achieve substantial improvement over the essay data of, obtaining upto 85. 0 % accuracy.
For Chinese, we use CTB 5. 1 and the split suggested by) for both tagging and dependency parsing.
Table 5 : Parsing results on CTB test set.
Part - of - Speech Tagging for Twitter : Annotation, Features, and Experiments
We address the problem of part - of - speech tagging for English data from the popular micro - blogging service Twitter.
We develop a tagset, annotate data, develop features, and report tagging results nearing 90 % accuracy.
The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets..
Tagging performance degrades on out - of - domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140 - character limit of each message ("tweet").
Our contributions are as follows : â¢ we developed a POS tagset for Twitter, â¢ we manually tagged 1, 827 tweets, â¢ we developed features for Twitter POS tagging and conducted experiments to evaluate them, and â¢ we provide our annotated corpus and trained POS tagger to the research community.
The success of this approach demonstrates that with careful design, supervised machine learning can be applied to rapidly produce effective language technology in new domains.
Our evaluation was designed to test the efficacy of this feature set for part - of - speech tagging given limited training data.
Table 2 : Memory usage and average per - sentence running time, in seconds, for decoding a Chinese - English test set.
In this paper we quantify and disentangle the impact of genre and topic differences on translation quality by introducing anew data set that has controlled topic and genre distributions.
Training corpora for statistical machine translation (SMT) are typically collected from a wide variety of sources and therefore have varying textual characteristics such as writing style and vocabulary.
Since most work on domain adaptation in SMT uses in - domain and out - of - domain data that differ on both the topic and the genre level, it is unclear whether the proposed solutions address topic or genre differences.
We demonstrate that statistics over discourse relations, collected via explicit discourse markers as proxies, can be utilized as salient indicators for paradig - matic relations in multiple languages, out - performing patterns in terms of recall and F 1 - score.
In almost all classification tasks, our markerbased model achieves a higher recall and F 1 - score than the pattern - based approach.
in recall and F 1 - score, leading to the best 3 - way classification results.
A System for Summarizing Scientific Topics Starting from Keywords
We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents (surveys and tutorials) on seven topics in Natural Language Processing.
We built a factoid inventory for seven topics in NLP based on manual written surveys in the following way
As compared to the previous works, our method takes advantage of both the in - domain monolingual corpora and the out - of - domain bilingual corpus to incorporate the topic information into our translation model, thus breaking down the corpus barrier for translation quality improvement.
On general domain and speech translation tasks where test conditions substantially differ from standard government and news training text, web - mined training data improves performance substantially, resulting in improvements of up to 1. 5 BLEU on standard test sets, and 5 BLEU on test sets outside of the news domain.
2 Edit rate for paraphrase alignment TER - PLUS (Translation Edit Rate Plus)) is a score designed for evaluation of Machine Translation (MT) output.
In selecting high - quality training data, it is important to take two aspects of data into consideration : quantity and quality.
We present Conditional Random Fields based approaches for detecting agreement / disagreement between speakers in English broadcast conversation shows.
When reduced to a binary classification evaluation using the VU Amsterdam Metaphor Corpus, the system achieves an F - Measure of 0. 608, slightly lower than the comparable binary classification system ' s 0. 638 and competitive with existing approaches..
(i) we present anew framework of Scalable Online Learning Algorithms for Ranking, which tackles the pairwise learning to ranking problem via a scalable online learning approach ;
(ii) we present two SOLAR algorithms : a first - order learning algorithm (SOLAR - I) and a second - order learning algorithm (SOLAR - II) ;
(iii) we analyze the theoretical bounds of the proposed algorithms in terms of standard IR performance measures ;
and (iv) finally we examine the efficacy of the proposed algorithms by an extensive set of empirical studies on benchmark datasets.
Recently, these cleanup templates have been used for automatically identifying articles with particular quality flaws in order to support Wikipedia ' s quality assurance process in Wikipedia.
Hierarchies can combat data sparsity : if data is too sparse to place the term "Pau Gasol" within the Chicago Bulls topic, perhaps it can be appropriately modeled at somewhat less precision within the Basketball topic.
Motivated by the idea of using topic model and external knowledge mentioned above, we present an LDA - based enriching method using the news corpus, and apply it to the task of microblog classification.
Prior research on language identification focused primarily on text and speech.
Treebanks, sets of parsed sentences annotated with a sytactic structure, are an important resource in NLP.
Bilingual sentence alignment is a fundamental task to undertake for the purpose of facilitating many important natural language processing applications such as statistical machine translation (, bilingual lexicography (, and cross - language information retrieval).
Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human - written abstracts for each new meeting domain (e. g.
This means that users cannot switch between information and navigation tasks in a natural and fluid manner. : An example interaction with the evaluated system In contrast to many existing mobile apps, Spacebook has a speech - only interface and addresses both problems in an integrated way.
Past work, e. g., generally focuses on semi - automatic acquisition of the remaining members of the set by mining large amounts of unlabeled data.
We present a set of dependency - based pre - ordering rules which improved the BLEU score by 1. 61 on the NIST 2006 evaluation data.
Experiments on Chinese - to - English show that our RST - based approach achieves improvements of 2. 3 / 0. 77 / 1. 43 BLEU points on NIST04 / NIST05 / CWMT2008 respectively.
As such, we have begun to see some research efforts in mining experience - related attributes such as time, location, topic, and experiencer, and their relations from weblogs.
On the Wikipedia development data, we annotated 500 logical forms, underspecified logical forms and constant mappings for ontology matching.
We perform all of our experiments on en - ja and ja - en translation over data from the NTCIR PatentMT task), the most standard benchmark task for these language pairs.
Summarisation of time - series data refers to the task of automatically generating text from variables whose values changeover time.
In this paper, we introduce a methodological approach to temporal anchoring of relations automatically extracted from unrestricted text.
This demonstrates that, bilingual active learning with jointly selecting the unlabeled instances cannot only enhance relation classification for its own language, but also help relation classification for the other language due to the complementary nature of relation instances between Chinese and English.
We evaluate our parsers on the Penn Tree - bank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93. 04 % and 87. 38 %, respectively..
When updating models overtime based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author ' s tweeting frequency..
Table 6 : Average accuracy and oracle scores on development data in various system settings.
For discontinuous parsing, we surpass the current state of the art by a wide margin on two German datasets (TIGER and NEGRA), while achieving fast parsing speeds.
Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F - measure of 89. 84..
Taking sentiment analysis for dialogues as an example, the topic of the document and the author ' s identity are both valuable for mining user ' s opinions in the conversation.
For Spanish, we removed workers who disagreed with the majority more than 50 % of the time (83 deletions), leaving 6. 5 annotations for each alignment (85. 47 % inter - annotator agreement.)
Most of them have been proposed in order to make translation systems perform better for resource - scarce domains when most training data comes from resourcerich domains, and ignore performance on a more generic domain without domain bias (.
Mining Informal Language from Chinese Microtext : Joint Word Recognition and Segmentation
We integrate the two proposed models into phrase - based statistical machine translation and conduct experiments on large - scale training data to investigate their effectiveness.
The automatically - converted corpus would be of use fora wide variety of NLP tasks.
Third, we show that dialectal data can be handled in the framework of domain adaptation.
4. 1) in its pre - processing module but MARS is replaced by a previously developed system) for pronominal anaphora resolution in Bengali.
However, as observe, because only 5. 9 % of words in the Switchboard corpus are "edited", the trivial baseline classifier which assigns all words the "not edited" label achieves a labelling accuracy of 94. 1 %.
We propose an automatic standardization for the construction of cross - lingual similarity datasets, and provide an evaluation, demonstrating its reliability and robustness.
Our research contributions are as follows : (1) we propose concurrent learning using multi - agent RL as away to deal with some of the issues of current approaches to dialogue policy learning (i. e., the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users ;
(2) we show that concurrent learning can address changes in user behavior overtime, and requires multi - agent RL techniques and variable exploration rates ;
(3) to our knowledge this is the first time that PHC and PHCWoLF are used for learning dialogue policies ;
In recent years, a number of works () attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data.
Based on the following criteria we selected the maximum number of proposals allowed : 1) Quality : the content and scope of the proposal, and the competence and experience of the presenters ; 2) Diversity : We sought a range of different topics and approaches ; 3) Appeal : Whether the tutorial topic would be likely to attract a reasonable number of participants ; and 4) Novelty : Tutorial topics featured at very recent ACL events were dispreferred (unless the content was clearly novel and different).
We evaluate its performance on a creative sentence generation task, showing its capability of generating well - formed, catchy and effective sentences that have all the good qualities of slogans produced by human copywriters..
Unsupervised induction and bilingual projection run according to totally different principles, the former mines the underlying structure of the monolingual language, while the latter leverages the syntactic knowledge of the parsed counter - part language.
Beyond reproducing the categorical data in WALS and extending it to hundreds of other languages, we also provide quantitative data for the relative frequencies of different word orders, and show the usefulness of this for language comparison.
This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences.
For example with Spanish - English, by using context - vector similarity only, we obtained very high recall / precision for the classification of "Non - Translation", but null precision / recall for the classification of "Translation".
The comprehensiveness of Wikipedia has made the on - line encyclopedia an increasingly popular target for disambiguation.
Second, the original and opposite training samples are used together for training a sentiment classifier (called dual training), and the original and opposite test samples are used together for prediction (called dual prediction).
We present in this paper the embedding models that achieve an F - score of 92 % on the widely - used, publicly available dataset, the GRE "most contrasting word" questions (Mohammad et al., 2008).
Preposition attachment mistakes are particularly bad when translating into Japanese () which uses a different postposition for different attachments ; conjunction mistakes can cause word ordering mistakes when translating into Chinese.
A typical knowledge - based question answering (KB - QA) system faces two challenges : one is to transform natural language questions into their meaning representations (MRs) ; the other is to retrieve answers from knowledge bases (KBs) using generated MRs.
In particular, we discover three new challenges on using temporality for relation understanding in comparable corpora, which we discuss in detail in Section 3. 2.
The entity set expansion problem is defined as follows : Given a set S of seed entities of a particular class, and a set D of candidate entities (e. g., extracted from a text corpus), we wish to determine which of the entities in D belong to S.
Tri - Training for Authorship Attribution with Limited Training Data
We conduct experiments on the data from the task B of Sentiment Analysis in Twitter in SemEval - 2013.
Unsupervised Part - of - Speech Tagging with Bilingual Graph - Based Projections
Some of the future research directions of semantic parsing with potentially large impacts include mapping entire natural language documents into machine processable form to enable automated reasoning about them and to convert natural language web pages into machine processable representations for the Semantic Web to support automated high - end web applications.
Comparing Multi - label Classification with Reinforcement Learning for Summarisation of Time - series Data
Applying our label candidate generation methodology to these 228 topics produced approximately 6000 labels - an average of 27 labels per topic. : A screenshot of the topic label evaluation task on Amazon Mechanical Turk.
To date, the conventional wisdom in the error detection community has been to avoid the use of statistical parsers under the belief that a WSJ trained parser ' s performance would degrade too much on noisy learner texts and that the traditionally hard problem of prepositional phrase attachment would be even harder when parsing ESL writing.
Topic model based algorithms applied to social media data have become a mainstream technique in performing various tasks including sentiment analysis and event detection ().
Table 1 : Statistics for the bilingual training, development and evaluation datasets.
Zero - shot cross - space mapping methods hold great promise as away to scale up annotation tasks well beyond the labels in the training data (e. g., recognizing objects that were never seen in training).
We also compare different approaches for topic modeling, such as cross - domain topic identification by utilizing data from newswire domain.
Table 1 : Example German - English translations showing the effect of relaxed matching in the 1 - mNCD score (for rows S) compared with METEOR using the exact module only, since the modules stem and synonym are already used in the similarized reference.
Wikipedia, WordNet) for supporting the automatic labelling of topics by deriving candidate labels by means of lexical (or graphbased () algorithms applied on these sources.
ConceptResolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data.
In machine learning, there is a class of semisupervised learning algorithms that learns from positive and unlabeled examples (PU learning for short).
To our knowledge, related research has not studied the task of identifying double entendres in text or speech.
A comprehensive CTH is believed to be more useful for information browsing, document organization and topic extraction in new text corpus ().
Precision is the fraction of correct alignment judgments returned by the system and recall is the fraction of alignment judgments in the gold standard dataset that are correctly returned by the system.
Also, CluewebMapping gives reasonably good precision on its prediction, despite the noisy nature of web data.
Most early works in this area were designed for supervised Information Extraction competitions such as MUC) and ACE), which rely on the availability of annotated data.
This dataset is used, instead of a more recent one, because no human judgments on adequacy and fluency have been conducted in WMT after year 2007, and human evaluation data is not freely available from MetricsMATR.
Currently most of state - of - the - art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus.
By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach..
Our aim in this paper is this task of lexical normalisation of noisy English text, with a particular focus on Twitter and SMS messages.
These two aspects are not only important from the perspective of developing computer applications for natural languages but also form the key components of language evolution and change.
We compared the four systems on datasets used in previous language identification research) (EUROGOV, TCL, WIKIPEDIA), as well as an extract from a biomedical parallel corpus (Tiedemann, 2009) (EMEA) and a corpus of samples from the Europarl Parallel Corpus () (EUROPARL).
We evaluate against current unsupervised segmentation models to show that including person - specific information improves segmentation performance on meeting corpora and on political debates.
Given that turkers annotated answers based on the topic page via a browser, this supports the assumption that the same answer would be located in the topic graph, which is then passed to the QA engine for feature extraction and classification.
According to the statistics of the bilingual discourse annotation in progress, about 1 / 4 of the Chinese implicit DCs are translated to explicit DCs in English.
Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non - adaptive human - machine interaction, by using a RL framework and a statistical user simulation.
Several studies have already shown the validity of using parallel corpora for sense discrimination (e. g.
Wiktionary paradigm table formats, however, are often complex, nested, 2 - 3 dimensional structures intended for human readability rather than machine parsing, and are broadly inconsistent across languages and Wiktionary editions.
We present an intrinsic evaluation of the extraction components, and of the informativeness of the summaries ; and a user study of the impact of the song review summaries on users ' decision making processes.
In automatic processing of such multilingual texts, they must first be segmented by language, and the language of each segment must be identified, since many state - of - the - art NLP applications are built by learning a gold standard for one specific language.
Specifically, we propose a novel non - linear distribution spreading algorithm, which first uses Delta IDF technique () to weight features, and then leverages the distribution of Delta IDF scores of a feature across different classes to efficiently recognize discriminative features for the classification task in the presence of mislabeled data.
The proposed models are trained on a public Wikipedia corpus, and the learned representations are evaluated on word analogy and word similarity tasks.
Our main contributions include : â¢ A news event knowledge base, Storybase, with a search interface for news storylines ;
The introduction of Wikipedia current events as resources for building event knowledge bases and as datasets for event detection and storyline construction ;
New approaches for event clustering and chaining with experimental comparisons to other baselines.
Overall process of building Storybase 2 Overview and Definitions shows the overall process for building Storybase.
The filtered corpus is used for training phrase - based translation models, which can be used directly in translation tasks or combined with base - line models.
For this language pair, we evaluate alignment error rate using the manual alignment corpus described by.
The lower precision for the Blog corpus appears to be due to the text not being as clean as NYT, introducing parser errors.
Our approach gives an 18 % relative BLEU gain for Levantine dialectal Arabic.
For example one of the results for the query "bell helmet" had a snippet containing "Bell cycling helmets" and we failed to match helmet to helmets. : Precision and recall of the NNP tag on the longtail data for the best baseline method and the three transfer methods using that baseline.
The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (.
We used four standard text datasets : two for multiclass document categorization (WebKB and R8), one for spam detection (LingSpam) and one for opinion mining (Amazon) so as to coverall the main subtasks of text categorization : â¢ WebKB : 4 most frequent categories among labeled webpages from various CS departments - split into 2, 803 for training and 1, 396 for test).
The decipherment approach for MT has recently gained popularity for training and adapting translation models using only monolingual data.
We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank.
Table 4 : Results (BLEU %) of Chinese - to - English large data (CE _ LD) and small data (CE _ SD) NIST task by applying one feature.
Experiments on Chinese - English translation on four NIST MT test sets show that the HD - HPB model significantly outperforms Chiang ' s model with average gains of 1. 91 points absolute in BLEU..
In large - scale experiments for patent prior art search and cross - lingual retrieval in Wikipedia, our approach yields considerable improvements over learning - to - rank with either only dense or only sparse features, and over very competitive baselines that combine state - of - the - art machine translation and retrieval..
This shows the adequacy of the PCFG - LA methodology for parsing the Hebrew treebank, but also goes to show the highly ambiguous nature of the tagging.
Although HILDA ' s bottom - up approach is aimed at building a discourse tree for the full text, it does not explicitly employ different strategies for withinsentence text spans and cross - sentence text spans.
A few annotated corpora exist, but they are relatively small and most were developed for broad coverage NLP.
Note that we do not use GEOQUERY test data in SMT training.
As we are not really interested in predicting something on the language created by the translator, but rather on the real one, it maybe better to further diminish the role of the translated texts in the learning process.
Template filling is an IE task where the goal is to populate the fields of a target relation, for example to extract the attributes of a job posting or to recover the details of a corporate acquisition event from a news story).
use relationships learned between people, organizations, and locations from Wikipedia to aid in toponym resolution when such named entities are present, but do not exploit any other textual context.
On the other hand, the impact of normalization or NSW detection on NER has not been well studied in social media domain.
To answer this question, we focus on two broad - coverage lexical resources of a different nature : Word - Net, as a de - facto standard used in Word Sense Disambiguation experiments ; and Wikipedia, as a large coverage, updated encyclopaedic resource which may have a better coverage of relevant senses in Web pages.
Chinese Word segmentation is a necessary step in Chinese - English statistical machine translation (SMT) because Chinese sentences do not delimit words by spaces.
Maximum matching word segmentation is used with a large word vocabulary V extracted from web data provided by ().
To evaluate cross - lingual setting a, the ES training set is translated into English (see Section 4), and an English classifier is trained on the translated data and evaluated on the EN test set (1 CL a).
By incorporating a mixture of labeled and unlabeled data, we are able to improve relation classification accuracy, reduce the need for annotated data, while still retaining the capacity to use labeled data to ensure that specific desired relations are learned.
Learning to temporally order events in clinical text is fundamental to understanding patient narratives and key to applications such as longitudinal studies, question answering, document summarization and information retrieval with temporal constraints.
One of the challenges for automatic argumentation analysis is that suitable annotated corpora are still very rare, in spite of work by many researchers.
The script is based on and produces identical scores to eval06. pl, the official evaluation for the CoNLL - X Shared Task on Multilingual Dependency Parsing ().
Internet forums, blogs, wikis, podcasts, instant messaging, and social networking.
Unsupervised Segmentation We compare the phonetic boundaries proposed by our model to the manual labels provided in the TIMIT dataset.
Then, we test our RE system on the ACE 2005 data, which exploits kernels, structures and similarities for domain adaptation.
The word segmentation was done by BaseSeg () for Chinese and Mecab 2 for Japanese.
Table 5 : Translation performance on Arabic to English translation, showing BLEU %.
We propose an approach to cross - lingual named entity recognition model transfer without the use of parallel corpora.
In this paper we present an analysis of interruption and resumption behaviour in human - human in - vehicle dialogues and also propose some implications for resumption strategies in an in - vehicle dialogue system..
All experiments were done on UrduEnglish and we evaluate reordering in two ways : Firstly, we evaluate reordering performance directly by comparing the reordered source sentence in Urdu with a reference reordering obtained from the manual word alignments using BLEU) (we call this measure monolingual BLEU or mBLEU).
Both Spanish / English sides of TRAIN are used for parallel MT training, whereas decipherment uses only monolingual English data for training LMs.
The main contributions of this paper are summarized as follows : â¢ We propose a novel framework for new word detection from large - scale user - generated data.
Our first group of experiments is to investigate whether our proposed label propagation model with both bilingual and sentimental information can improve emotion detection in code - switching texts.
Discovery of Topically Coherent Sentences for Extractive Summarization
In statistical machine translation (SMT), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy.
We will focus first on producing CCG labeled predicate - argument dependencies for English and Chinese and will then apply our best settings to produce a comparison with the tree structures of the languages of the PASCAL Shared Task.
This paper describes a method for automatically extracting information about typical durations for events from tweets posted to the Twitter microblogging site.
This is the problem we are facing when we perform word segmentation on Chinese patent data.
It is well known that parsing accuracy suffers when a model is applied to out - of - domain data.
We describe the rules specific for Spanish language constructions and their implementation in EXTRHECH, an Open IE system for Spanish.
Open - Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition
The Excitement Open Platform for Textual Inferences
WoSIT : A Word Sense Induction Toolkit for Search Result Clustering and Diversification
A Rule - Augmented Statistical Phrase - based Translation System
FAdR : A System for Recognizing False Online Advertisements
Simplified Dependency Annotations with GFL - Web Cross - Lingual Information to the Rescue in Keyword Extraction
Experiments confirmed the effectiveness of our method for Japanese - English and Chinese - English translation, using NTCIR - 9 Patent Machine Translation Task data sets).
To obtain manually corrected sentences for evaluation, the test and evaluation set sentences and were put on Amazon Mechanical Turk as a correction task.
We compare this approach to self - training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection.
MODEST will provide enterprises with the services of retrieving news from websites, extracting commercial information, exploring customers ' opinions, and analyzing collaborative / competitive social networks.
Table 3 : Minutes used for alignment and phase pair extraction in the FBIS data set.
These drafts include lab reports, data analysis, argumentative essays, and article summaries.
compares our accuracies with those reported in () for Bulgarian and Spanish.
Abstract Meaning Representation (AMR) () is a semantic formalism that expresses the logical meanings of English sentences in the form of a directed, acyclic graph.
Existing studies have utilized Wikipedia for various knowledge acquisition tasks.
We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus.
The NIST "03 test set is used as our development corpus and the NIST" 05 and NIST "08 test sets are our test sets.
Finally, we report accuracy and F1 for temporal types, as defined in Section 2, for the TempEval dataset (WikiWars does not include type labels).
Moreover, the tree / forest - string structure is more widely used than the tree - tree structure, presumably because using two parsers on the source and target languages is subject to more problems than making use of a parser on one language, such as the shortage of high precision / recall parsers for languages other than English, compound parse error rates, and inconsistency of errors.
Large - scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP).
Developing a supervised or semi - supervised model of discourse segmentation would require ground truth annotated based on a well - established representation scheme, but as of right now no such annotation exists for Chinese to the best of our knowledge.
In order to overcome this acquisition bottleneck (sense - tagged corpora are scarce for languages other than English), we decided to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (.
We report the results that the SSWE feature performs comparably with hand - crafted features in the top - performed system in SemEval 2013 ; â¢ We release the sentiment - specific word embedding learned from 10 million tweets, which can be adopted off - the - shell in other sentiment analysis tasks.
and Rapp (1999) adopted VSM for the application of extracting translation pairs from comparable or even unrelated corpora.
The reported measures are Adjusted Rand Index (ARI), Jaccard Index (JI) and F1.
We present Code - Switched LDA (csLDA), which infers language specific topic distributions based on code - switched documents to facilitate multilingual corpus analysis.
Evaluated in French by 10 - fold - cross validation, the system achieves a 9. 3 % Word Error Rate and a 0. 83 BLEU score..
We selected to build our dataset in the Consumer Health domain, a popular domain in the web providing medical information at various levels of complexity, ranging from layman and up to expert information, because consumer health illustrates the need for exploratory search.
In the past decades, significant efforts have been put on constructing biomedical knowledge bases) and developing natural language processing (NLP) tools, such as MetaMap, to utilize the information from the knowledge bases).
Table 1 : Results of SP 2 estimation on the Samala corpus.
We use Dirichlet multinomial regression (Mimno and McCallum, 2012) to learn a mapping between cipher text and plaintext word embeddings from non - parallel data.
Table 4 : Multi - Span - HMM has a much smaller drop - off in F1 than comparable systems on out - of - domain test data vs in - domain test data.
Our experiments in English - Chinese and English - Japanese translations demonstrate the effectiveness of the proposed approach : we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric..
Translation quality is measured with the BLEU metric () on the news test2014 test set (which has 3003 sentences).
We modified Joshua to optionally use our LM implementations during decoding, and measured the time required to decode all 2051 sentences of the 2008 News test set.
There has been considerable work on coreference resolution in written text, but comparatively little work on spoken text, with a few exceptions of systems for pronoun resolution in transcripts of spoken text e. g.,.
Take reachability indexing for example, hop - based papers try to get the "optimum labeling" by finding the "densest intermediate hops" to encode reachability information captured by an intermediate data structure called "transitive closure contour".
In English Penn Treebank experiments, our parser achieves an F1 score of 91. 1 on test set at a speed of 13. 6 sentences per second.
Table 2 : Summarization results on TAC ' 09 data (initial summaries).
Table 2 : Mean system level correlations over all translation tasks into English for variants of mNCD and NCD.
Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7. 8 % to 6. 0 % error rate.
